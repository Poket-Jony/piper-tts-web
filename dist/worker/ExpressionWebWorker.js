var Cf = Object.defineProperty;
var Jm = (ke) => {
  throw TypeError(ke);
};
var kf = (ke, A, s) => A in ke ? Cf(ke, A, { enumerable: !0, configurable: !0, writable: !0, value: s }) : ke[A] = s;
var ge = (ke, A, s) => kf(ke, typeof A != "symbol" ? A + "" : A, s), Zm = (ke, A, s) => A.has(ke) || Jm("Cannot " + s);
var Ec = (ke, A, s) => (Zm(ke, A, "read from private field"), s ? s.call(ke) : A.get(ke)), Ip = (ke, A, s) => A.has(ke) ? Jm("Cannot add the same private member more than once") : A instanceof WeakSet ? A.add(ke) : A.set(ke, s), Pc = (ke, A, s, f) => (Zm(ke, A, "write to private field"), f ? f.call(ke, s) : A.set(ke, s), s);
var t_ = {
  /***/
  "./node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm": (
    /*!****************************************************************************!*\
      !*** ./node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm ***!
      \****************************************************************************/
    /***/
    (ke, A, s) => {
      ke.exports = s.p + "ort-wasm-simd-threaded.jsep.wasm";
    }
  ),
  /***/
  "?2ce3": (
    /*!**********************************!*\
      !*** onnxruntime-node (ignored) ***!
      \**********************************/
    /***/
    () => {
    }
  ),
  /***/
  "?7a2c": (
    /*!********************!*\
      !*** fs (ignored) ***!
      \********************/
    /***/
    () => {
    }
  ),
  /***/
  "?a42a": (
    /*!**********************!*\
      !*** path (ignored) ***!
      \**********************/
    /***/
    () => {
    }
  ),
  /***/
  "?2b25": (
    /*!***********************!*\
      !*** sharp (ignored) ***!
      \***********************/
    /***/
    () => {
    }
  ),
  /***/
  "?569f": (
    /*!********************!*\
      !*** fs (ignored) ***!
      \********************/
    /***/
    () => {
    }
  ),
  /***/
  "?3f59": (
    /*!**********************!*\
      !*** path (ignored) ***!
      \**********************/
    /***/
    () => {
    }
  ),
  /***/
  "?154a": (
    /*!*********************!*\
      !*** url (ignored) ***!
      \*********************/
    /***/
    () => {
    }
  ),
  /***/
  "./node_modules/@huggingface/jinja/dist/index.js": (
    /*!*******************************************************!*\
      !*** ./node_modules/@huggingface/jinja/dist/index.js ***!
      \*******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Environment: () => (
          /* binding */
          Ze
        ),
        /* harmony export */
        Interpreter: () => (
          /* binding */
          at
        ),
        /* harmony export */
        Template: () => (
          /* binding */
          gt
        ),
        /* harmony export */
        parse: () => (
          /* binding */
          re
        ),
        /* harmony export */
        tokenize: () => (
          /* binding */
          M
        )
        /* harmony export */
      });
      var f = Object.freeze({
        Text: "Text",
        // The text between Jinja statements or expressions
        NumericLiteral: "NumericLiteral",
        // e.g., 123
        BooleanLiteral: "BooleanLiteral",
        // true or false
        NullLiteral: "NullLiteral",
        // none
        StringLiteral: "StringLiteral",
        // 'string'
        Identifier: "Identifier",
        // Variables, functions, etc.
        Equals: "Equals",
        // =
        OpenParen: "OpenParen",
        // (
        CloseParen: "CloseParen",
        // )
        OpenStatement: "OpenStatement",
        // {%
        CloseStatement: "CloseStatement",
        // %}
        OpenExpression: "OpenExpression",
        // {{
        CloseExpression: "CloseExpression",
        // }}
        OpenSquareBracket: "OpenSquareBracket",
        // [
        CloseSquareBracket: "CloseSquareBracket",
        // ]
        OpenCurlyBracket: "OpenCurlyBracket",
        // {
        CloseCurlyBracket: "CloseCurlyBracket",
        // }
        Comma: "Comma",
        // ,
        Dot: "Dot",
        // .
        Colon: "Colon",
        // :
        Pipe: "Pipe",
        // |
        CallOperator: "CallOperator",
        // ()
        AdditiveBinaryOperator: "AdditiveBinaryOperator",
        // + -
        MultiplicativeBinaryOperator: "MultiplicativeBinaryOperator",
        // * / %
        ComparisonBinaryOperator: "ComparisonBinaryOperator",
        // < > <= >= == !=
        UnaryOperator: "UnaryOperator",
        // ! - +
        // Keywords
        Set: "Set",
        If: "If",
        For: "For",
        In: "In",
        Is: "Is",
        NotIn: "NotIn",
        Else: "Else",
        EndIf: "EndIf",
        ElseIf: "ElseIf",
        EndFor: "EndFor",
        And: "And",
        Or: "Or",
        Not: "UnaryOperator",
        Macro: "Macro",
        EndMacro: "EndMacro"
      }), L = Object.freeze({
        set: f.Set,
        for: f.For,
        in: f.In,
        is: f.Is,
        if: f.If,
        else: f.Else,
        endif: f.EndIf,
        elif: f.ElseIf,
        endfor: f.EndFor,
        and: f.And,
        or: f.Or,
        not: f.Not,
        "not in": f.NotIn,
        macro: f.Macro,
        endmacro: f.EndMacro,
        // Literals
        true: f.BooleanLiteral,
        false: f.BooleanLiteral,
        none: f.NullLiteral,
        // NOTE: According to the Jinja docs: The special constants true, false, and none are indeed lowercase.
        // Because that caused confusion in the past, (True used to expand to an undefined variable that was considered false),
        // all three can now also be written in title case (True, False, and None). However, for consistency, (all Jinja identifiers are lowercase)
        // you should use the lowercase versions.
        True: f.BooleanLiteral,
        False: f.BooleanLiteral,
        None: f.NullLiteral
      }), j = class {
        /**
         * Constructs a new Token.
         * @param {string} value The raw value as seen inside the source code.
         * @param {TokenType} type The type of token.
         */
        constructor(F, ne) {
          this.value = F, this.type = ne;
        }
      };
      function J(F) {
        return /\w/.test(F);
      }
      function W(F) {
        return /[0-9]/.test(F);
      }
      var w = [
        // Control sequences
        ["{%", f.OpenStatement],
        ["%}", f.CloseStatement],
        ["{{", f.OpenExpression],
        ["}}", f.CloseExpression],
        // Single character tokens
        ["(", f.OpenParen],
        [")", f.CloseParen],
        ["{", f.OpenCurlyBracket],
        ["}", f.CloseCurlyBracket],
        ["[", f.OpenSquareBracket],
        ["]", f.CloseSquareBracket],
        [",", f.Comma],
        [".", f.Dot],
        [":", f.Colon],
        ["|", f.Pipe],
        // Comparison operators
        ["<=", f.ComparisonBinaryOperator],
        [">=", f.ComparisonBinaryOperator],
        ["==", f.ComparisonBinaryOperator],
        ["!=", f.ComparisonBinaryOperator],
        ["<", f.ComparisonBinaryOperator],
        [">", f.ComparisonBinaryOperator],
        // Arithmetic operators
        ["+", f.AdditiveBinaryOperator],
        ["-", f.AdditiveBinaryOperator],
        ["*", f.MultiplicativeBinaryOperator],
        ["/", f.MultiplicativeBinaryOperator],
        ["%", f.MultiplicativeBinaryOperator],
        // Assignment operator
        ["=", f.Equals]
      ], x = /* @__PURE__ */ new Map([
        ["n", `
`],
        // New line
        ["t", "	"],
        // Horizontal tab
        ["r", "\r"],
        // Carriage return
        ["b", "\b"],
        // Backspace
        ["f", "\f"],
        // Form feed
        ["v", "\v"],
        // Vertical tab
        ["'", "'"],
        // Single quote
        ['"', '"'],
        // Double quote
        ["\\", "\\"]
        // Backslash
      ]);
      function y(F, ne = {}) {
        return F.endsWith(`
`) && (F = F.slice(0, -1)), F = F.replace(/{#.*?#}/gs, "{##}"), ne.lstrip_blocks && (F = F.replace(/^[ \t]*({[#%])/gm, "$1")), ne.trim_blocks && (F = F.replace(/([#%]})\n/g, "$1")), F.replace(/{##}/g, "").replace(/-%}\s*/g, "%}").replace(/\s*{%-/g, "{%").replace(/-}}\s*/g, "}}").replace(/\s*{{-/g, "{{");
      }
      function M(F, ne = {}) {
        var st, pt, It;
        const K = [], pe = y(F, ne);
        let Oe = 0;
        const Qe = (St) => {
          let Ot = "";
          for (; St(pe[Oe]); ) {
            if (pe[Oe] === "\\") {
              if (++Oe, Oe >= pe.length)
                throw new SyntaxError("Unexpected end of input");
              const At = pe[Oe++], nr = x.get(At);
              if (nr === void 0)
                throw new SyntaxError(`Unexpected escaped character: ${At}`);
              Ot += nr;
              continue;
            }
            if (Ot += pe[Oe++], Oe >= pe.length)
              throw new SyntaxError("Unexpected end of input");
          }
          return Ot;
        };
        e:
          for (; Oe < pe.length; ) {
            const St = (st = K.at(-1)) == null ? void 0 : st.type;
            if (St === void 0 || St === f.CloseStatement || St === f.CloseExpression) {
              let At = "";
              for (; Oe < pe.length && // Keep going until we hit the next Jinja statement or expression
              !(pe[Oe] === "{" && (pe[Oe + 1] === "%" || pe[Oe + 1] === "{")); )
                At += pe[Oe++];
              if (At.length > 0) {
                K.push(new j(At, f.Text));
                continue;
              }
            }
            Qe((At) => /\s/.test(At));
            const Ot = pe[Oe];
            if (Ot === "-" || Ot === "+") {
              const At = (pt = K.at(-1)) == null ? void 0 : pt.type;
              if (At === f.Text || At === void 0)
                throw new SyntaxError(`Unexpected character: ${Ot}`);
              switch (At) {
                case f.Identifier:
                case f.NumericLiteral:
                case f.BooleanLiteral:
                case f.NullLiteral:
                case f.StringLiteral:
                case f.CloseParen:
                case f.CloseSquareBracket:
                  break;
                default: {
                  ++Oe;
                  const nr = Qe(W);
                  K.push(
                    new j(`${Ot}${nr}`, nr.length > 0 ? f.NumericLiteral : f.UnaryOperator)
                  );
                  continue;
                }
              }
            }
            for (const [At, nr] of w)
              if (pe.slice(Oe, Oe + At.length) === At) {
                K.push(new j(At, nr)), Oe += At.length;
                continue e;
              }
            if (Ot === "'" || Ot === '"') {
              ++Oe;
              const At = Qe((nr) => nr !== Ot);
              K.push(new j(At, f.StringLiteral)), ++Oe;
              continue;
            }
            if (W(Ot)) {
              const At = Qe(W);
              K.push(new j(At, f.NumericLiteral));
              continue;
            }
            if (J(Ot)) {
              const At = Qe(J), nr = Object.hasOwn(L, At) ? L[At] : f.Identifier;
              nr === f.In && ((It = K.at(-1)) == null ? void 0 : It.type) === f.Not ? (K.pop(), K.push(new j("not in", f.NotIn))) : K.push(new j(At, nr));
              continue;
            }
            throw new SyntaxError(`Unexpected character: ${Ot}`);
          }
        return K;
      }
      var b = class {
        constructor() {
          ge(this, "type", "Statement");
        }
      }, D = class extends b {
        constructor(ne) {
          super();
          ge(this, "type", "Program");
          this.body = ne;
        }
      }, q = class extends b {
        constructor(ne, K, pe) {
          super();
          ge(this, "type", "If");
          this.test = ne, this.body = K, this.alternate = pe;
        }
      }, se = class extends b {
        constructor(ne, K, pe, Oe) {
          super();
          ge(this, "type", "For");
          this.loopvar = ne, this.iterable = K, this.body = pe, this.defaultBlock = Oe;
        }
      }, oe = class extends b {
        constructor(ne, K) {
          super();
          ge(this, "type", "Set");
          this.assignee = ne, this.value = K;
        }
      }, z = class extends b {
        constructor(ne, K, pe) {
          super();
          ge(this, "type", "Macro");
          this.name = ne, this.args = K, this.body = pe;
        }
      }, V = class extends b {
        constructor() {
          super(...arguments);
          ge(this, "type", "Expression");
        }
      }, Y = class extends V {
        constructor(ne, K, pe) {
          super();
          ge(this, "type", "MemberExpression");
          this.object = ne, this.property = K, this.computed = pe;
        }
      }, O = class extends V {
        constructor(ne, K) {
          super();
          ge(this, "type", "CallExpression");
          this.callee = ne, this.args = K;
        }
      }, $ = class extends V {
        /**
         * @param {string} value The name of the identifier
         */
        constructor(ne) {
          super();
          ge(this, "type", "Identifier");
          this.value = ne;
        }
      }, g = class extends V {
        constructor(ne) {
          super();
          ge(this, "type", "Literal");
          this.value = ne;
        }
      }, C = class extends g {
        constructor() {
          super(...arguments);
          ge(this, "type", "NumericLiteral");
        }
      }, v = class extends g {
        constructor() {
          super(...arguments);
          ge(this, "type", "StringLiteral");
        }
      }, ee = class extends g {
        constructor() {
          super(...arguments);
          ge(this, "type", "BooleanLiteral");
        }
      }, X = class extends g {
        constructor() {
          super(...arguments);
          ge(this, "type", "NullLiteral");
        }
      }, le = class extends g {
        constructor() {
          super(...arguments);
          ge(this, "type", "ArrayLiteral");
        }
      }, ue = class extends g {
        constructor() {
          super(...arguments);
          ge(this, "type", "TupleLiteral");
        }
      }, fe = class extends g {
        constructor() {
          super(...arguments);
          ge(this, "type", "ObjectLiteral");
        }
      }, Ce = class extends V {
        constructor(ne, K, pe) {
          super();
          ge(this, "type", "BinaryExpression");
          this.operator = ne, this.left = K, this.right = pe;
        }
      }, xe = class extends V {
        constructor(ne, K) {
          super();
          ge(this, "type", "FilterExpression");
          this.operand = ne, this.filter = K;
        }
      }, Le = class extends V {
        constructor(ne, K) {
          super();
          ge(this, "type", "SelectExpression");
          this.iterable = ne, this.test = K;
        }
      }, qe = class extends V {
        constructor(ne, K, pe) {
          super();
          ge(this, "type", "TestExpression");
          this.operand = ne, this.negate = K, this.test = pe;
        }
      }, Ue = class extends V {
        constructor(ne, K) {
          super();
          ge(this, "type", "UnaryExpression");
          this.operator = ne, this.argument = K;
        }
      }, ut = class extends V {
        constructor(ne = void 0, K = void 0, pe = void 0) {
          super();
          ge(this, "type", "SliceExpression");
          this.start = ne, this.stop = K, this.step = pe;
        }
      }, de = class extends V {
        constructor(ne, K) {
          super();
          ge(this, "type", "KeywordArgumentExpression");
          this.key = ne, this.value = K;
        }
      };
      function re(F) {
        const ne = new D([]);
        let K = 0;
        function pe(nt, _t) {
          const Ft = F[K++];
          if (!Ft || Ft.type !== nt)
            throw new Error(`Parser Error: ${_t}. ${Ft.type} !== ${nt}.`);
          return Ft;
        }
        function Oe() {
          switch (F[K].type) {
            case f.Text:
              return pt();
            case f.OpenStatement:
              return It();
            case f.OpenExpression:
              return St();
            default:
              throw new SyntaxError(`Unexpected token type: ${F[K].type}`);
          }
        }
        function Qe(...nt) {
          return K + nt.length <= F.length && nt.some((_t, Ft) => _t !== F[K + Ft].type);
        }
        function st(...nt) {
          return K + nt.length <= F.length && nt.every((_t, Ft) => _t === F[K + Ft].type);
        }
        function pt() {
          return new v(pe(f.Text, "Expected text token").value);
        }
        function It() {
          pe(f.OpenStatement, "Expected opening statement token");
          let nt;
          switch (F[K].type) {
            case f.Set:
              ++K, nt = Ot(), pe(f.CloseStatement, "Expected closing statement token");
              break;
            case f.If:
              ++K, nt = At(), pe(f.OpenStatement, "Expected {% token"), pe(f.EndIf, "Expected endif token"), pe(f.CloseStatement, "Expected %} token");
              break;
            case f.Macro:
              ++K, nt = nr(), pe(f.OpenStatement, "Expected {% token"), pe(f.EndMacro, "Expected endmacro token"), pe(f.CloseStatement, "Expected %} token");
              break;
            case f.For:
              ++K, nt = kr(), pe(f.OpenStatement, "Expected {% token"), pe(f.EndFor, "Expected endfor token"), pe(f.CloseStatement, "Expected %} token");
              break;
            default:
              throw new SyntaxError(`Unknown statement type: ${F[K].type}`);
          }
          return nt;
        }
        function St() {
          pe(f.OpenExpression, "Expected opening expression token");
          const nt = Ar();
          return pe(f.CloseExpression, "Expected closing expression token"), nt;
        }
        function Ot() {
          const nt = Ar();
          if (st(f.Equals)) {
            ++K;
            const _t = Ot();
            return new oe(nt, _t);
          }
          return nt;
        }
        function At() {
          var lr, bs, tr, ts, Rs, Js, Ns, vs;
          const nt = Ar();
          pe(f.CloseStatement, "Expected closing statement token");
          const _t = [], Ft = [];
          for (; !(((lr = F[K]) == null ? void 0 : lr.type) === f.OpenStatement && (((bs = F[K + 1]) == null ? void 0 : bs.type) === f.ElseIf || ((tr = F[K + 1]) == null ? void 0 : tr.type) === f.Else || ((ts = F[K + 1]) == null ? void 0 : ts.type) === f.EndIf)); )
            _t.push(Oe());
          if (((Rs = F[K]) == null ? void 0 : Rs.type) === f.OpenStatement && ((Js = F[K + 1]) == null ? void 0 : Js.type) !== f.EndIf)
            if (++K, st(f.ElseIf))
              pe(f.ElseIf, "Expected elseif token"), Ft.push(At());
            else
              for (pe(f.Else, "Expected else token"), pe(f.CloseStatement, "Expected closing statement token"); !(((Ns = F[K]) == null ? void 0 : Ns.type) === f.OpenStatement && ((vs = F[K + 1]) == null ? void 0 : vs.type) === f.EndIf); )
                Ft.push(Oe());
          return new q(nt, _t, Ft);
        }
        function nr() {
          const nt = as();
          if (nt.type !== "Identifier")
            throw new SyntaxError("Expected identifier following macro statement");
          const _t = Bs();
          pe(f.CloseStatement, "Expected closing statement token");
          const Ft = [];
          for (; Qe(f.OpenStatement, f.EndMacro); )
            Ft.push(Oe());
          return new z(nt, _t, Ft);
        }
        function gr(nt = !1) {
          const _t = nt ? as : Ar, Ft = [_t()], lr = st(f.Comma);
          for (; lr && (++K, Ft.push(_t()), !!st(f.Comma)); )
            ;
          return lr ? new ue(Ft) : Ft[0];
        }
        function kr() {
          const nt = gr(!0);
          if (!(nt instanceof $ || nt instanceof ue))
            throw new SyntaxError(`Expected identifier/tuple for the loop variable, got ${nt.type} instead`);
          pe(f.In, "Expected `in` keyword following loop variable");
          const _t = Ar();
          pe(f.CloseStatement, "Expected closing statement token");
          const Ft = [];
          for (; Qe(f.OpenStatement, f.EndFor) && Qe(f.OpenStatement, f.Else); )
            Ft.push(Oe());
          const lr = [];
          if (st(f.OpenStatement, f.Else))
            for (++K, ++K, pe(f.CloseStatement, "Expected closing statement token"); Qe(f.OpenStatement, f.EndFor); )
              lr.push(Oe());
          return new se(nt, _t, Ft, lr);
        }
        function Ar() {
          return Qr();
        }
        function Qr() {
          const nt = is();
          if (st(f.If)) {
            ++K;
            const _t = is();
            if (st(f.Else)) {
              ++K;
              const Ft = is();
              return new q(_t, [nt], [Ft]);
            } else
              return new Le(nt, _t);
          }
          return nt;
        }
        function is() {
          let nt = Xs();
          for (; st(f.Or); ) {
            const _t = F[K];
            ++K;
            const Ft = Xs();
            nt = new Ce(_t, nt, Ft);
          }
          return nt;
        }
        function Xs() {
          let nt = zs();
          for (; st(f.And); ) {
            const _t = F[K];
            ++K;
            const Ft = zs();
            nt = new Ce(_t, nt, Ft);
          }
          return nt;
        }
        function zs() {
          let nt;
          for (; st(f.Not); ) {
            const _t = F[K];
            ++K;
            const Ft = zs();
            nt = new Ue(_t, Ft);
          }
          return nt ?? Ms();
        }
        function Ms() {
          let nt = Nt();
          for (; st(f.ComparisonBinaryOperator) || st(f.In) || st(f.NotIn); ) {
            const _t = F[K];
            ++K;
            const Ft = Nt();
            nt = new Ce(_t, nt, Ft);
          }
          return nt;
        }
        function Nt() {
          let nt = cs();
          for (; st(f.AdditiveBinaryOperator); ) {
            const _t = F[K];
            ++K;
            const Ft = cs();
            nt = new Ce(_t, nt, Ft);
          }
          return nt;
        }
        function Qs() {
          const nt = $s();
          return st(f.OpenParen) ? ks(nt) : nt;
        }
        function ks(nt) {
          let _t = new O(nt, Bs());
          return st(f.OpenParen) && (_t = ks(_t)), _t;
        }
        function Bs() {
          pe(f.OpenParen, "Expected opening parenthesis for arguments list");
          const nt = Ss();
          return pe(f.CloseParen, "Expected closing parenthesis for arguments list"), nt;
        }
        function Ss() {
          const nt = [];
          for (; !st(f.CloseParen); ) {
            let _t = Ar();
            if (st(f.Equals)) {
              if (++K, !(_t instanceof $))
                throw new SyntaxError("Expected identifier for keyword argument");
              const Ft = Ar();
              _t = new de(_t, Ft);
            }
            nt.push(_t), st(f.Comma) && ++K;
          }
          return nt;
        }
        function os() {
          const nt = [];
          let _t = !1;
          for (; !st(f.CloseSquareBracket); )
            st(f.Colon) ? (nt.push(void 0), ++K, _t = !0) : (nt.push(Ar()), st(f.Colon) && (++K, _t = !0));
          if (nt.length === 0)
            throw new SyntaxError("Expected at least one argument for member/slice expression");
          if (_t) {
            if (nt.length > 3)
              throw new SyntaxError("Expected 0-3 arguments for slice expression");
            return new ut(...nt);
          }
          return nt[0];
        }
        function $s() {
          let nt = as();
          for (; st(f.Dot) || st(f.OpenSquareBracket); ) {
            const _t = F[K];
            ++K;
            let Ft;
            const lr = _t.type !== f.Dot;
            if (lr)
              Ft = os(), pe(f.CloseSquareBracket, "Expected closing square bracket");
            else if (Ft = as(), Ft.type !== "Identifier")
              throw new SyntaxError("Expected identifier following dot operator");
            nt = new Y(nt, Ft, lr);
          }
          return nt;
        }
        function cs() {
          let nt = As();
          for (; st(f.MultiplicativeBinaryOperator); ) {
            const _t = F[K];
            ++K;
            const Ft = As();
            nt = new Ce(_t, nt, Ft);
          }
          return nt;
        }
        function As() {
          let nt = Ys();
          for (; st(f.Is); ) {
            ++K;
            const _t = st(f.Not);
            _t && ++K;
            let Ft = as();
            if (Ft instanceof ee ? Ft = new $(Ft.value.toString()) : Ft instanceof X && (Ft = new $("none")), !(Ft instanceof $))
              throw new SyntaxError("Expected identifier for the test");
            nt = new qe(nt, _t, Ft);
          }
          return nt;
        }
        function Ys() {
          let nt = Qs();
          for (; st(f.Pipe); ) {
            ++K;
            let _t = as();
            if (!(_t instanceof $))
              throw new SyntaxError("Expected identifier for the filter");
            st(f.OpenParen) && (_t = ks(_t)), nt = new xe(nt, _t);
          }
          return nt;
        }
        function as() {
          const nt = F[K];
          switch (nt.type) {
            case f.NumericLiteral:
              return ++K, new C(Number(nt.value));
            case f.StringLiteral:
              return ++K, new v(nt.value);
            case f.BooleanLiteral:
              return ++K, new ee(nt.value.toLowerCase() === "true");
            case f.NullLiteral:
              return ++K, new X(null);
            case f.Identifier:
              return ++K, new $(nt.value);
            case f.OpenParen: {
              ++K;
              const _t = gr();
              if (F[K].type !== f.CloseParen)
                throw new SyntaxError(`Expected closing parenthesis, got ${F[K].type} instead`);
              return ++K, _t;
            }
            case f.OpenSquareBracket: {
              ++K;
              const _t = [];
              for (; !st(f.CloseSquareBracket); )
                _t.push(Ar()), st(f.Comma) && ++K;
              return ++K, new le(_t);
            }
            case f.OpenCurlyBracket: {
              ++K;
              const _t = /* @__PURE__ */ new Map();
              for (; !st(f.CloseCurlyBracket); ) {
                const Ft = Ar();
                pe(f.Colon, "Expected colon between key and value in object literal");
                const lr = Ar();
                _t.set(Ft, lr), st(f.Comma) && ++K;
              }
              return ++K, new fe(_t);
            }
            default:
              throw new SyntaxError(`Unexpected token: ${nt.type}`);
          }
        }
        for (; K < F.length; )
          ne.body.push(Oe());
        return ne;
      }
      function he(F, ne, K = 1) {
        ne === void 0 && (ne = F, F = 0);
        const pe = [];
        for (let Oe = F; Oe < ne; Oe += K)
          pe.push(Oe);
        return pe;
      }
      function Ee(F, ne, K, pe = 1) {
        const Oe = Math.sign(pe);
        Oe >= 0 ? (ne = (ne ?? (ne = 0)) < 0 ? Math.max(F.length + ne, 0) : Math.min(ne, F.length), K = (K ?? (K = F.length)) < 0 ? Math.max(F.length + K, 0) : Math.min(K, F.length)) : (ne = (ne ?? (ne = F.length - 1)) < 0 ? Math.max(F.length + ne, -1) : Math.min(ne, F.length - 1), K = (K ?? (K = -1)) < -1 ? Math.max(F.length + K, -1) : Math.min(K, F.length - 1));
        const Qe = [];
        for (let st = ne; Oe * st < Oe * K; st += pe)
          Qe.push(F[st]);
        return Qe;
      }
      function Be(F) {
        return F.replace(/\b\w/g, (ne) => ne.toUpperCase());
      }
      var et = class {
        /**
         * Creates a new RuntimeValue.
         */
        constructor(F = void 0) {
          ge(this, "type", "RuntimeValue");
          ge(this, "value");
          /**
           * A collection of built-in functions for this type.
           */
          ge(this, "builtins", /* @__PURE__ */ new Map());
          this.value = F;
        }
        /**
         * Determines truthiness or falsiness of the runtime value.
         * This function should be overridden by subclasses if it has custom truthiness criteria.
         * @returns {BooleanValue} BooleanValue(true) if the value is truthy, BooleanValue(false) otherwise.
         */
        __bool__() {
          return new Je(!!this.value);
        }
      }, Xe = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "NumericValue");
        }
      }, ie = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "StringValue");
          ge(this, "builtins", /* @__PURE__ */ new Map([
            [
              "upper",
              new je(() => new ie(this.value.toUpperCase()))
            ],
            [
              "lower",
              new je(() => new ie(this.value.toLowerCase()))
            ],
            [
              "strip",
              new je(() => new ie(this.value.trim()))
            ],
            [
              "title",
              new je(() => new ie(Be(this.value)))
            ],
            ["length", new Xe(this.value.length)],
            [
              "rstrip",
              new je(() => new ie(this.value.trimEnd()))
            ],
            [
              "lstrip",
              new je(() => new ie(this.value.trimStart()))
            ]
          ]));
        }
      }, Je = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "BooleanValue");
        }
      }, De = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "ObjectValue");
          ge(this, "builtins", /* @__PURE__ */ new Map([
            [
              "get",
              new je(([ne, K]) => {
                if (!(ne instanceof ie))
                  throw new Error(`Object key must be a string: got ${ne.type}`);
                return this.value.get(ne.value) ?? K ?? new Ve();
              })
            ],
            [
              "items",
              new je(() => new ve(
                Array.from(this.value.entries()).map(([ne, K]) => new ve([new ie(ne), K]))
              ))
            ]
          ]));
        }
        /**
         * NOTE: necessary to override since all JavaScript arrays are considered truthy,
         * while only non-empty Python arrays are consider truthy.
         *
         * e.g.,
         *  - JavaScript:  {} && 5 -> 5
         *  - Python:      {} and 5 -> {}
         */
        __bool__() {
          return new Je(this.value.size > 0);
        }
      }, ce = class extends De {
        constructor() {
          super(...arguments);
          ge(this, "type", "KeywordArgumentsValue");
        }
      }, ve = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "ArrayValue");
          ge(this, "builtins", /* @__PURE__ */ new Map([["length", new Xe(this.value.length)]]));
        }
        /**
         * NOTE: necessary to override since all JavaScript arrays are considered truthy,
         * while only non-empty Python arrays are consider truthy.
         *
         * e.g.,
         *  - JavaScript:  [] && 5 -> 5
         *  - Python:      [] and 5 -> []
         */
        __bool__() {
          return new Je(this.value.length > 0);
        }
      }, Re = class extends ve {
        constructor() {
          super(...arguments);
          ge(this, "type", "TupleValue");
        }
      }, je = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "FunctionValue");
        }
      }, Ve = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "NullValue");
        }
      }, Ne = class extends et {
        constructor() {
          super(...arguments);
          ge(this, "type", "UndefinedValue");
        }
      }, Ze = class {
        constructor(F) {
          /**
           * The variables declared in this environment.
           */
          ge(this, "variables", /* @__PURE__ */ new Map([
            [
              "namespace",
              new je((F) => {
                if (F.length === 0)
                  return new De(/* @__PURE__ */ new Map());
                if (F.length !== 1 || !(F[0] instanceof De))
                  throw new Error("`namespace` expects either zero arguments or a single object argument");
                return F[0];
              })
            ]
          ]));
          /**
           * The tests available in this environment.
           */
          ge(this, "tests", /* @__PURE__ */ new Map([
            ["boolean", (F) => F.type === "BooleanValue"],
            ["callable", (F) => F instanceof je],
            [
              "odd",
              (F) => {
                if (F.type !== "NumericValue")
                  throw new Error(`Cannot apply test "odd" to type: ${F.type}`);
                return F.value % 2 !== 0;
              }
            ],
            [
              "even",
              (F) => {
                if (F.type !== "NumericValue")
                  throw new Error(`Cannot apply test "even" to type: ${F.type}`);
                return F.value % 2 === 0;
              }
            ],
            ["false", (F) => F.type === "BooleanValue" && !F.value],
            ["true", (F) => F.type === "BooleanValue" && F.value],
            ["none", (F) => F.type === "NullValue"],
            ["string", (F) => F.type === "StringValue"],
            ["number", (F) => F.type === "NumericValue"],
            ["integer", (F) => F.type === "NumericValue" && Number.isInteger(F.value)],
            ["iterable", (F) => F.type === "ArrayValue" || F.type === "StringValue"],
            ["mapping", (F) => F.type === "ObjectValue"],
            [
              "lower",
              (F) => {
                const ne = F.value;
                return F.type === "StringValue" && ne === ne.toLowerCase();
              }
            ],
            [
              "upper",
              (F) => {
                const ne = F.value;
                return F.type === "StringValue" && ne === ne.toUpperCase();
              }
            ],
            ["none", (F) => F.type === "NullValue"],
            ["defined", (F) => F.type !== "UndefinedValue"],
            ["undefined", (F) => F.type === "UndefinedValue"],
            ["equalto", (F, ne) => F.value === ne.value],
            ["eq", (F, ne) => F.value === ne.value]
          ]));
          this.parent = F;
        }
        /**
         * Set the value of a variable in the current environment.
         */
        set(F, ne) {
          return this.declareVariable(F, ft(ne));
        }
        declareVariable(F, ne) {
          if (this.variables.has(F))
            throw new SyntaxError(`Variable already declared: ${F}`);
          return this.variables.set(F, ne), ne;
        }
        // private assignVariable(name: string, value: AnyRuntimeValue): AnyRuntimeValue {
        // 	const env = this.resolve(name);
        // 	env.variables.set(name, value);
        // 	return value;
        // }
        /**
         * Set variable in the current scope.
         * See https://jinja.palletsprojects.com/en/3.0.x/templates/#assignments for more information.
         */
        setVariable(F, ne) {
          return this.variables.set(F, ne), ne;
        }
        /**
         * Resolve the environment in which the variable is declared.
         * @param {string} name The name of the variable.
         * @returns {Environment} The environment in which the variable is declared.
         */
        resolve(F) {
          if (this.variables.has(F))
            return this;
          if (this.parent)
            return this.parent.resolve(F);
          throw new Error(`Unknown variable: ${F}`);
        }
        lookupVariable(F) {
          try {
            return this.resolve(F).variables.get(F) ?? new Ne();
          } catch {
            return new Ne();
          }
        }
      }, at = class {
        constructor(F) {
          ge(this, "global");
          this.global = F ?? new Ze();
        }
        /**
         * Run the program.
         */
        run(F) {
          return this.evaluate(F, this.global);
        }
        /**
         * Evaluates expressions following the binary operation type.
         */
        evaluateBinaryExpression(F, ne) {
          const K = this.evaluate(F.left, ne);
          switch (F.operator.value) {
            case "and":
              return K.__bool__().value ? this.evaluate(F.right, ne) : K;
            case "or":
              return K.__bool__().value ? K : this.evaluate(F.right, ne);
          }
          const pe = this.evaluate(F.right, ne);
          switch (F.operator.value) {
            case "==":
              return new Je(K.value == pe.value);
            case "!=":
              return new Je(K.value != pe.value);
          }
          if (K instanceof Ne || pe instanceof Ne)
            throw new Error("Cannot perform operation on undefined values");
          if (K instanceof Ve || pe instanceof Ve)
            throw new Error("Cannot perform operation on null values");
          if (K instanceof Xe && pe instanceof Xe)
            switch (F.operator.value) {
              case "+":
                return new Xe(K.value + pe.value);
              case "-":
                return new Xe(K.value - pe.value);
              case "*":
                return new Xe(K.value * pe.value);
              case "/":
                return new Xe(K.value / pe.value);
              case "%":
                return new Xe(K.value % pe.value);
              case "<":
                return new Je(K.value < pe.value);
              case ">":
                return new Je(K.value > pe.value);
              case ">=":
                return new Je(K.value >= pe.value);
              case "<=":
                return new Je(K.value <= pe.value);
            }
          else if (K instanceof ve && pe instanceof ve)
            switch (F.operator.value) {
              case "+":
                return new ve(K.value.concat(pe.value));
            }
          else if (pe instanceof ve) {
            const Oe = pe.value.find((Qe) => Qe.value === K.value) !== void 0;
            switch (F.operator.value) {
              case "in":
                return new Je(Oe);
              case "not in":
                return new Je(!Oe);
            }
          }
          if (K instanceof ie || pe instanceof ie)
            switch (F.operator.value) {
              case "+":
                return new ie(K.value.toString() + pe.value.toString());
            }
          if (K instanceof ie && pe instanceof ie)
            switch (F.operator.value) {
              case "in":
                return new Je(pe.value.includes(K.value));
              case "not in":
                return new Je(!pe.value.includes(K.value));
            }
          if (K instanceof ie && pe instanceof De)
            switch (F.operator.value) {
              case "in":
                return new Je(pe.value.has(K.value));
              case "not in":
                return new Je(!pe.value.has(K.value));
            }
          throw new SyntaxError(`Unknown operator "${F.operator.value}" between ${K.type} and ${pe.type}`);
        }
        evaluateArguments(F, ne) {
          const K = [], pe = /* @__PURE__ */ new Map();
          for (const Oe of F)
            if (Oe.type === "KeywordArgumentExpression") {
              const Qe = Oe;
              pe.set(Qe.key.value, this.evaluate(Qe.value, ne));
            } else {
              if (pe.size > 0)
                throw new Error("Positional arguments must come before keyword arguments");
              K.push(this.evaluate(Oe, ne));
            }
          return [K, pe];
        }
        /**
         * Evaluates expressions following the filter operation type.
         */
        evaluateFilterExpression(F, ne) {
          const K = this.evaluate(F.operand, ne);
          if (F.filter.type === "Identifier") {
            const pe = F.filter;
            if (pe.value === "tojson")
              return new ie(dt(K));
            if (K instanceof ve)
              switch (pe.value) {
                case "list":
                  return K;
                case "first":
                  return K.value[0];
                case "last":
                  return K.value[K.value.length - 1];
                case "length":
                  return new Xe(K.value.length);
                case "reverse":
                  return new ve(K.value.reverse());
                case "sort":
                  return new ve(
                    K.value.sort((Oe, Qe) => {
                      if (Oe.type !== Qe.type)
                        throw new Error(`Cannot compare different types: ${Oe.type} and ${Qe.type}`);
                      switch (Oe.type) {
                        case "NumericValue":
                          return Oe.value - Qe.value;
                        case "StringValue":
                          return Oe.value.localeCompare(Qe.value);
                        default:
                          throw new Error(`Cannot compare type: ${Oe.type}`);
                      }
                    })
                  );
                default:
                  throw new Error(`Unknown ArrayValue filter: ${pe.value}`);
              }
            else if (K instanceof ie)
              switch (pe.value) {
                case "length":
                  return new Xe(K.value.length);
                case "upper":
                  return new ie(K.value.toUpperCase());
                case "lower":
                  return new ie(K.value.toLowerCase());
                case "title":
                  return new ie(Be(K.value));
                case "capitalize":
                  return new ie(K.value.charAt(0).toUpperCase() + K.value.slice(1));
                case "trim":
                  return new ie(K.value.trim());
                case "indent":
                  return new ie(
                    K.value.split(`
`).map(
                      (Oe, Qe) => (
                        // By default, don't indent the first line or empty lines
                        Qe === 0 || Oe.length === 0 ? Oe : "    " + Oe
                      )
                    ).join(`
`)
                  );
                case "string":
                  return K;
                default:
                  throw new Error(`Unknown StringValue filter: ${pe.value}`);
              }
            else if (K instanceof Xe)
              switch (pe.value) {
                case "abs":
                  return new Xe(Math.abs(K.value));
                default:
                  throw new Error(`Unknown NumericValue filter: ${pe.value}`);
              }
            else if (K instanceof De)
              switch (pe.value) {
                case "items":
                  return new ve(
                    Array.from(K.value.entries()).map(([Oe, Qe]) => new ve([new ie(Oe), Qe]))
                  );
                case "length":
                  return new Xe(K.value.size);
                default:
                  throw new Error(`Unknown ObjectValue filter: ${pe.value}`);
              }
            throw new Error(`Cannot apply filter "${pe.value}" to type: ${K.type}`);
          } else if (F.filter.type === "CallExpression") {
            const pe = F.filter;
            if (pe.callee.type !== "Identifier")
              throw new Error(`Unknown filter: ${pe.callee.type}`);
            const Oe = pe.callee.value;
            if (Oe === "tojson") {
              const [, Qe] = this.evaluateArguments(pe.args, ne), st = Qe.get("indent") ?? new Ve();
              if (!(st instanceof Xe || st instanceof Ve))
                throw new Error("If set, indent must be a number");
              return new ie(dt(K, st.value));
            }
            if (K instanceof ve) {
              switch (Oe) {
                case "selectattr":
                case "rejectattr": {
                  const Qe = Oe === "selectattr";
                  if (K.value.some((At) => !(At instanceof De)))
                    throw new Error(`\`${Oe}\` can only be applied to array of objects`);
                  if (pe.args.some((At) => At.type !== "StringLiteral"))
                    throw new Error(`arguments of \`${Oe}\` must be strings`);
                  const [st, pt, It] = pe.args.map((At) => this.evaluate(At, ne));
                  let St;
                  if (pt) {
                    const At = ne.tests.get(pt.value);
                    if (!At)
                      throw new Error(`Unknown test: ${pt.value}`);
                    St = At;
                  } else
                    St = (...At) => At[0].__bool__().value;
                  const Ot = K.value.filter((At) => {
                    const nr = At.value.get(st.value), gr = nr ? St(nr, It) : !1;
                    return Qe ? gr : !gr;
                  });
                  return new ve(Ot);
                }
                case "map": {
                  const [, Qe] = this.evaluateArguments(pe.args, ne);
                  if (Qe.has("attribute")) {
                    const st = Qe.get("attribute");
                    if (!(st instanceof ie))
                      throw new Error("attribute must be a string");
                    const pt = Qe.get("default"), It = K.value.map((St) => {
                      if (!(St instanceof De))
                        throw new Error("items in map must be an object");
                      return St.value.get(st.value) ?? pt ?? new Ne();
                    });
                    return new ve(It);
                  } else
                    throw new Error("`map` expressions without `attribute` set are not currently supported.");
                }
              }
              throw new Error(`Unknown ArrayValue filter: ${Oe}`);
            } else if (K instanceof ie) {
              switch (Oe) {
                case "indent": {
                  const [Qe, st] = this.evaluateArguments(pe.args, ne), pt = Qe.at(0) ?? st.get("width") ?? new Xe(4);
                  if (!(pt instanceof Xe))
                    throw new Error("width must be a number");
                  const It = Qe.at(1) ?? st.get("first") ?? new Je(!1), St = Qe.at(2) ?? st.get("blank") ?? new Je(!1), Ot = K.value.split(`
`), At = " ".repeat(pt.value), nr = Ot.map(
                    (gr, kr) => !It.value && kr === 0 || !St.value && gr.length === 0 ? gr : At + gr
                  );
                  return new ie(nr.join(`
`));
                }
              }
              throw new Error(`Unknown StringValue filter: ${Oe}`);
            } else
              throw new Error(`Cannot apply filter "${Oe}" to type: ${K.type}`);
          }
          throw new Error(`Unknown filter: ${F.filter.type}`);
        }
        /**
         * Evaluates expressions following the test operation type.
         */
        evaluateTestExpression(F, ne) {
          const K = this.evaluate(F.operand, ne), pe = ne.tests.get(F.test.value);
          if (!pe)
            throw new Error(`Unknown test: ${F.test.value}`);
          const Oe = pe(K);
          return new Je(F.negate ? !Oe : Oe);
        }
        /**
         * Evaluates expressions following the unary operation type.
         */
        evaluateUnaryExpression(F, ne) {
          const K = this.evaluate(F.argument, ne);
          switch (F.operator.value) {
            case "not":
              return new Je(!K.value);
            default:
              throw new SyntaxError(`Unknown operator: ${F.operator.value}`);
          }
        }
        evalProgram(F, ne) {
          return this.evaluateBlock(F.body, ne);
        }
        evaluateBlock(F, ne) {
          let K = "";
          for (const pe of F) {
            const Oe = this.evaluate(pe, ne);
            Oe.type !== "NullValue" && Oe.type !== "UndefinedValue" && (K += Oe.value);
          }
          return new ie(K);
        }
        evaluateIdentifier(F, ne) {
          return ne.lookupVariable(F.value);
        }
        evaluateCallExpression(F, ne) {
          const [K, pe] = this.evaluateArguments(F.args, ne);
          pe.size > 0 && K.push(new ce(pe));
          const Oe = this.evaluate(F.callee, ne);
          if (Oe.type !== "FunctionValue")
            throw new Error(`Cannot call something that is not a function: got ${Oe.type}`);
          return Oe.value(K, ne);
        }
        evaluateSliceExpression(F, ne, K) {
          if (!(F instanceof ve || F instanceof ie))
            throw new Error("Slice object must be an array or string");
          const pe = this.evaluate(ne.start, K), Oe = this.evaluate(ne.stop, K), Qe = this.evaluate(ne.step, K);
          if (!(pe instanceof Xe || pe instanceof Ne))
            throw new Error("Slice start must be numeric or undefined");
          if (!(Oe instanceof Xe || Oe instanceof Ne))
            throw new Error("Slice stop must be numeric or undefined");
          if (!(Qe instanceof Xe || Qe instanceof Ne))
            throw new Error("Slice step must be numeric or undefined");
          return F instanceof ve ? new ve(Ee(F.value, pe.value, Oe.value, Qe.value)) : new ie(Ee(Array.from(F.value), pe.value, Oe.value, Qe.value).join(""));
        }
        evaluateMemberExpression(F, ne) {
          const K = this.evaluate(F.object, ne);
          let pe;
          if (F.computed) {
            if (F.property.type === "SliceExpression")
              return this.evaluateSliceExpression(K, F.property, ne);
            pe = this.evaluate(F.property, ne);
          } else
            pe = new ie(F.property.value);
          let Oe;
          if (K instanceof De) {
            if (!(pe instanceof ie))
              throw new Error(`Cannot access property with non-string: got ${pe.type}`);
            Oe = K.value.get(pe.value) ?? K.builtins.get(pe.value);
          } else if (K instanceof ve || K instanceof ie)
            if (pe instanceof Xe)
              Oe = K.value.at(pe.value), K instanceof ie && (Oe = new ie(K.value.at(pe.value)));
            else if (pe instanceof ie)
              Oe = K.builtins.get(pe.value);
            else
              throw new Error(`Cannot access property with non-string/non-number: got ${pe.type}`);
          else {
            if (!(pe instanceof ie))
              throw new Error(`Cannot access property with non-string: got ${pe.type}`);
            Oe = K.builtins.get(pe.value);
          }
          return Oe instanceof et ? Oe : new Ne();
        }
        evaluateSet(F, ne) {
          const K = this.evaluate(F.value, ne);
          if (F.assignee.type === "Identifier") {
            const pe = F.assignee.value;
            ne.setVariable(pe, K);
          } else if (F.assignee.type === "MemberExpression") {
            const pe = F.assignee, Oe = this.evaluate(pe.object, ne);
            if (!(Oe instanceof De))
              throw new Error("Cannot assign to member of non-object");
            if (pe.property.type !== "Identifier")
              throw new Error("Cannot assign to member with non-identifier property");
            Oe.value.set(pe.property.value, K);
          } else
            throw new Error(`Invalid LHS inside assignment expression: ${JSON.stringify(F.assignee)}`);
          return new Ve();
        }
        evaluateIf(F, ne) {
          const K = this.evaluate(F.test, ne);
          return this.evaluateBlock(K.__bool__().value ? F.body : F.alternate, ne);
        }
        evaluateFor(F, ne) {
          const K = new Ze(ne);
          let pe, Oe;
          if (F.iterable.type === "SelectExpression") {
            const St = F.iterable;
            Oe = this.evaluate(St.iterable, K), pe = St.test;
          } else
            Oe = this.evaluate(F.iterable, K);
          if (!(Oe instanceof ve))
            throw new Error(`Expected iterable type in for loop: got ${Oe.type}`);
          const Qe = [], st = [];
          for (let St = 0; St < Oe.value.length; ++St) {
            const Ot = new Ze(K), At = Oe.value[St];
            let nr;
            if (F.loopvar.type === "Identifier")
              nr = (gr) => gr.setVariable(F.loopvar.value, At);
            else if (F.loopvar.type === "TupleLiteral") {
              const gr = F.loopvar;
              if (At.type !== "ArrayValue")
                throw new Error(`Cannot unpack non-iterable type: ${At.type}`);
              const kr = At;
              if (gr.value.length !== kr.value.length)
                throw new Error(`Too ${gr.value.length > kr.value.length ? "few" : "many"} items to unpack`);
              nr = (Ar) => {
                for (let Qr = 0; Qr < gr.value.length; ++Qr) {
                  if (gr.value[Qr].type !== "Identifier")
                    throw new Error(`Cannot unpack non-identifier type: ${gr.value[Qr].type}`);
                  Ar.setVariable(gr.value[Qr].value, kr.value[Qr]);
                }
              };
            } else
              throw new Error(`Invalid loop variable(s): ${F.loopvar.type}`);
            pe && (nr(Ot), !this.evaluate(pe, Ot).__bool__().value) || (Qe.push(At), st.push(nr));
          }
          let pt = "", It = !0;
          for (let St = 0; St < Qe.length; ++St) {
            const Ot = /* @__PURE__ */ new Map([
              ["index", new Xe(St + 1)],
              ["index0", new Xe(St)],
              ["revindex", new Xe(Qe.length - St)],
              ["revindex0", new Xe(Qe.length - St - 1)],
              ["first", new Je(St === 0)],
              ["last", new Je(St === Qe.length - 1)],
              ["length", new Xe(Qe.length)],
              ["previtem", St > 0 ? Qe[St - 1] : new Ne()],
              ["nextitem", St < Qe.length - 1 ? Qe[St + 1] : new Ne()]
            ]);
            K.setVariable("loop", new De(Ot)), st[St](K);
            const At = this.evaluateBlock(F.body, K);
            pt += At.value, It = !1;
          }
          if (It) {
            const St = this.evaluateBlock(F.defaultBlock, K);
            pt += St.value;
          }
          return new ie(pt);
        }
        /**
         * See https://jinja.palletsprojects.com/en/3.1.x/templates/#macros for more information.
         */
        evaluateMacro(F, ne) {
          return ne.setVariable(
            F.name.value,
            new je((K, pe) => {
              var st;
              const Oe = new Ze(pe);
              K = K.slice();
              let Qe;
              ((st = K.at(-1)) == null ? void 0 : st.type) === "KeywordArgumentsValue" && (Qe = K.pop());
              for (let pt = 0; pt < F.args.length; ++pt) {
                const It = F.args[pt], St = K[pt];
                if (It.type === "Identifier") {
                  const Ot = It;
                  if (!St)
                    throw new Error(`Missing positional argument: ${Ot.value}`);
                  Oe.setVariable(Ot.value, St);
                } else if (It.type === "KeywordArgumentExpression") {
                  const Ot = It, At = St ?? // Try positional arguments first
                  (Qe == null ? void 0 : Qe.value.get(Ot.key.value)) ?? // Look in user-passed kwargs
                  this.evaluate(Ot.value, Oe);
                  Oe.setVariable(Ot.key.value, At);
                } else
                  throw new Error(`Unknown argument type: ${It.type}`);
              }
              return this.evaluateBlock(F.body, Oe);
            })
          ), new Ve();
        }
        evaluate(F, ne) {
          if (F === void 0)
            return new Ne();
          switch (F.type) {
            case "Program":
              return this.evalProgram(F, ne);
            case "Set":
              return this.evaluateSet(F, ne);
            case "If":
              return this.evaluateIf(F, ne);
            case "For":
              return this.evaluateFor(F, ne);
            case "Macro":
              return this.evaluateMacro(F, ne);
            case "NumericLiteral":
              return new Xe(Number(F.value));
            case "StringLiteral":
              return new ie(F.value);
            case "BooleanLiteral":
              return new Je(F.value);
            case "NullLiteral":
              return new Ve(F.value);
            case "ArrayLiteral":
              return new ve(F.value.map((K) => this.evaluate(K, ne)));
            case "TupleLiteral":
              return new Re(F.value.map((K) => this.evaluate(K, ne)));
            case "ObjectLiteral": {
              const K = /* @__PURE__ */ new Map();
              for (const [pe, Oe] of F.value) {
                const Qe = this.evaluate(pe, ne);
                if (!(Qe instanceof ie))
                  throw new Error(`Object keys must be strings: got ${Qe.type}`);
                K.set(Qe.value, this.evaluate(Oe, ne));
              }
              return new De(K);
            }
            case "Identifier":
              return this.evaluateIdentifier(F, ne);
            case "CallExpression":
              return this.evaluateCallExpression(F, ne);
            case "MemberExpression":
              return this.evaluateMemberExpression(F, ne);
            case "UnaryExpression":
              return this.evaluateUnaryExpression(F, ne);
            case "BinaryExpression":
              return this.evaluateBinaryExpression(F, ne);
            case "FilterExpression":
              return this.evaluateFilterExpression(F, ne);
            case "TestExpression":
              return this.evaluateTestExpression(F, ne);
            default:
              throw new SyntaxError(`Unknown node type: ${F.type}`);
          }
        }
      };
      function ft(F) {
        switch (typeof F) {
          case "number":
            return new Xe(F);
          case "string":
            return new ie(F);
          case "boolean":
            return new Je(F);
          case "undefined":
            return new Ne();
          case "object":
            return F === null ? new Ve() : Array.isArray(F) ? new ve(F.map(ft)) : new De(
              new Map(Object.entries(F).map(([ne, K]) => [ne, ft(K)]))
            );
          case "function":
            return new je((ne, K) => {
              const pe = F(...ne.map((Oe) => Oe.value)) ?? null;
              return ft(pe);
            });
          default:
            throw new Error(`Cannot convert to runtime value: ${F}`);
        }
      }
      function dt(F, ne, K) {
        const pe = K ?? 0;
        switch (F.type) {
          case "NullValue":
          case "UndefinedValue":
            return "null";
          case "NumericValue":
          case "StringValue":
          case "BooleanValue":
            return JSON.stringify(F.value);
          case "ArrayValue":
          case "ObjectValue": {
            const Oe = ne ? " ".repeat(ne) : "", Qe = `
` + Oe.repeat(pe), st = Qe + Oe;
            if (F.type === "ArrayValue") {
              const pt = F.value.map((It) => dt(It, ne, pe + 1));
              return ne ? `[${st}${pt.join(`,${st}`)}${Qe}]` : `[${pt.join(", ")}]`;
            } else {
              const pt = Array.from(F.value.entries()).map(([It, St]) => {
                const Ot = `"${It}": ${dt(St, ne, pe + 1)}`;
                return ne ? `${st}${Ot}` : Ot;
              });
              return ne ? `{${pt.join(",")}${Qe}}` : `{${pt.join(", ")}}`;
            }
          }
          default:
            throw new Error(`Cannot convert to JSON: ${F.type}`);
        }
      }
      var gt = class {
        /**
         * @param {string} template The template string
         */
        constructor(F) {
          ge(this, "parsed");
          const ne = M(F, {
            lstrip_blocks: !0,
            trim_blocks: !0
          });
          this.parsed = re(ne);
        }
        render(F) {
          const ne = new Ze();
          ne.set("false", !1), ne.set("true", !0), ne.set("raise_exception", (Oe) => {
            throw new Error(Oe);
          }), ne.set("range", he);
          for (const [Oe, Qe] of Object.entries(F))
            ne.set(Oe, Qe);
          return new at(ne).run(this.parsed).value;
        }
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/backend-impl.js": (
    /*!******************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/backend-impl.js ***!
      \******************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        registerBackend: () => (
          /* binding */
          j
        ),
        /* harmony export */
        resolveBackendAndExecutionProviders: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      const f = /* @__PURE__ */ new Map(), L = [], j = (w, x, y) => {
        if (x && typeof x.init == "function" && typeof x.createInferenceSessionHandler == "function") {
          const M = f.get(w);
          if (M === void 0)
            f.set(w, { backend: x, priority: y });
          else {
            if (M.priority > y)
              return;
            if (M.priority === y && M.backend !== x)
              throw new Error(`cannot register backend "${w}" using priority ${y}`);
          }
          if (y >= 0) {
            const b = L.indexOf(w);
            b !== -1 && L.splice(b, 1);
            for (let D = 0; D < L.length; D++)
              if (f.get(L[D]).priority <= y) {
                L.splice(D, 0, w);
                return;
              }
            L.push(w);
          }
          return;
        }
        throw new TypeError("not a valid backend");
      }, J = async (w) => {
        const x = f.get(w);
        if (!x)
          return "backend not found.";
        if (x.initialized)
          return x.backend;
        if (x.aborted)
          return x.error;
        {
          const y = !!x.initPromise;
          try {
            return y || (x.initPromise = x.backend.init(w)), await x.initPromise, x.initialized = !0, x.backend;
          } catch (M) {
            return y || (x.error = `${M}`, x.aborted = !0), x.error;
          } finally {
            delete x.initPromise;
          }
        }
      }, W = async (w) => {
        const x = w.executionProviders || [], y = x.map((oe) => typeof oe == "string" ? oe : oe.name), M = y.length === 0 ? L : y;
        let b;
        const D = [], q = /* @__PURE__ */ new Set();
        for (const oe of M) {
          const z = await J(oe);
          typeof z == "string" ? D.push({ name: oe, err: z }) : (b || (b = z), b === z && q.add(oe));
        }
        if (!b)
          throw new Error(`no available backend found. ERR: ${D.map((oe) => `[${oe.name}] ${oe.err}`).join(", ")}`);
        for (const { name: oe, err: z } of D)
          y.includes(oe) && console.warn(`removing requested execution provider "${oe}" from session options because it is not available: ${z}`);
        const se = x.filter((oe) => q.has(typeof oe == "string" ? oe : oe.name));
        return [
          b,
          new Proxy(w, {
            get: (oe, z) => z === "executionProviders" ? se : Reflect.get(oe, z)
          })
        ];
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/backend.js": (
    /*!*************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/backend.js ***!
      \*************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        registerBackend: () => (
          /* reexport safe */
          f.registerBackend
        )
        /* harmony export */
      });
      var f = s(
        /*! ./backend-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/backend-impl.js"
      );
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/env-impl.js": (
    /*!**************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/env-impl.js ***!
      \**************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        env: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ./version.js */
        "./node_modules/onnxruntime-common/dist/esm/version.js"
      );
      let L = "warning";
      const j = {
        wasm: {},
        webgl: {},
        webgpu: {},
        versions: { common: f.version },
        set logLevel(J) {
          if (J !== void 0) {
            if (typeof J != "string" || ["verbose", "info", "warning", "error", "fatal"].indexOf(J) === -1)
              throw new Error(`Unsupported logging level: ${J}`);
            L = J;
          }
        },
        get logLevel() {
          return L;
        }
      };
      Object.defineProperty(j, "logLevel", { enumerable: !0 });
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/env.js": (
    /*!*********************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/env.js ***!
      \*********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        env: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ./env-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/env-impl.js"
      );
      const L = f.env;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/index.js": (
    /*!***********************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/index.js ***!
      \***********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        InferenceSession: () => (
          /* reexport safe */
          j.InferenceSession
        ),
        /* harmony export */
        TRACE: () => (
          /* reexport safe */
          W.TRACE
        ),
        /* harmony export */
        TRACE_FUNC_BEGIN: () => (
          /* reexport safe */
          W.TRACE_FUNC_BEGIN
        ),
        /* harmony export */
        TRACE_FUNC_END: () => (
          /* reexport safe */
          W.TRACE_FUNC_END
        ),
        /* harmony export */
        Tensor: () => (
          /* reexport safe */
          J.Tensor
        ),
        /* harmony export */
        TrainingSession: () => (
          /* reexport safe */
          w.TrainingSession
        ),
        /* harmony export */
        env: () => (
          /* reexport safe */
          L.env
        ),
        /* harmony export */
        registerBackend: () => (
          /* reexport safe */
          f.registerBackend
        )
        /* harmony export */
      });
      var f = s(
        /*! ./backend.js */
        "./node_modules/onnxruntime-common/dist/esm/backend.js"
      ), L = s(
        /*! ./env.js */
        "./node_modules/onnxruntime-common/dist/esm/env.js"
      ), j = s(
        /*! ./inference-session.js */
        "./node_modules/onnxruntime-common/dist/esm/inference-session.js"
      ), J = s(
        /*! ./tensor.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor.js"
      );
      s(
        /*! ./tensor-conversion.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-conversion.js"
      ), s(
        /*! ./tensor-factory.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-factory.js"
      );
      var W = s(
        /*! ./trace.js */
        "./node_modules/onnxruntime-common/dist/esm/trace.js"
      );
      s(
        /*! ./onnx-model.js */
        "./node_modules/onnxruntime-common/dist/esm/onnx-model.js"
      ), s(
        /*! ./onnx-value.js */
        "./node_modules/onnxruntime-common/dist/esm/onnx-value.js"
      );
      var w = s(
        /*! ./training-session.js */
        "./node_modules/onnxruntime-common/dist/esm/training-session.js"
      );
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/inference-session-impl.js": (
    /*!****************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/inference-session-impl.js ***!
      \****************************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        InferenceSession: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ./backend-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/backend-impl.js"
      ), L = s(
        /*! ./tensor.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor.js"
      ), j = s(
        /*! ./trace.js */
        "./node_modules/onnxruntime-common/dist/esm/trace.js"
      );
      class J {
        constructor(w) {
          this.handler = w;
        }
        async run(w, x, y) {
          (0, j.TRACE_FUNC_BEGIN)();
          const M = {};
          let b = {};
          if (typeof w != "object" || w === null || w instanceof L.Tensor || Array.isArray(w))
            throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");
          let D = !0;
          if (typeof x == "object") {
            if (x === null)
              throw new TypeError("Unexpected argument[1]: cannot be null.");
            if (x instanceof L.Tensor)
              throw new TypeError("'fetches' cannot be a Tensor");
            if (Array.isArray(x)) {
              if (x.length === 0)
                throw new TypeError("'fetches' cannot be an empty array.");
              D = !1;
              for (const oe of x) {
                if (typeof oe != "string")
                  throw new TypeError("'fetches' must be a string array or an object.");
                if (this.outputNames.indexOf(oe) === -1)
                  throw new RangeError(`'fetches' contains invalid output name: ${oe}.`);
                M[oe] = null;
              }
              if (typeof y == "object" && y !== null)
                b = y;
              else if (typeof y < "u")
                throw new TypeError("'options' must be an object.");
            } else {
              let oe = !1;
              const z = Object.getOwnPropertyNames(x);
              for (const V of this.outputNames)
                if (z.indexOf(V) !== -1) {
                  const Y = x[V];
                  (Y === null || Y instanceof L.Tensor) && (oe = !0, D = !1, M[V] = Y);
                }
              if (oe) {
                if (typeof y == "object" && y !== null)
                  b = y;
                else if (typeof y < "u")
                  throw new TypeError("'options' must be an object.");
              } else
                b = x;
            }
          } else if (typeof x < "u")
            throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");
          for (const oe of this.inputNames)
            if (typeof w[oe] > "u")
              throw new Error(`input '${oe}' is missing in 'feeds'.`);
          if (D)
            for (const oe of this.outputNames)
              M[oe] = null;
          const q = await this.handler.run(w, M, b), se = {};
          for (const oe in q)
            if (Object.hasOwnProperty.call(q, oe)) {
              const z = q[oe];
              z instanceof L.Tensor ? se[oe] = z : se[oe] = new L.Tensor(z.type, z.data, z.dims);
            }
          return (0, j.TRACE_FUNC_END)(), se;
        }
        async release() {
          return this.handler.dispose();
        }
        static async create(w, x, y, M) {
          (0, j.TRACE_FUNC_BEGIN)();
          let b, D = {};
          if (typeof w == "string") {
            if (b = w, typeof x == "object" && x !== null)
              D = x;
            else if (typeof x < "u")
              throw new TypeError("'options' must be an object.");
          } else if (w instanceof Uint8Array) {
            if (b = w, typeof x == "object" && x !== null)
              D = x;
            else if (typeof x < "u")
              throw new TypeError("'options' must be an object.");
          } else if (w instanceof ArrayBuffer || typeof SharedArrayBuffer < "u" && w instanceof SharedArrayBuffer) {
            const z = w;
            let V = 0, Y = w.byteLength;
            if (typeof x == "object" && x !== null)
              D = x;
            else if (typeof x == "number") {
              if (V = x, !Number.isSafeInteger(V))
                throw new RangeError("'byteOffset' must be an integer.");
              if (V < 0 || V >= z.byteLength)
                throw new RangeError(`'byteOffset' is out of range [0, ${z.byteLength}).`);
              if (Y = w.byteLength - V, typeof y == "number") {
                if (Y = y, !Number.isSafeInteger(Y))
                  throw new RangeError("'byteLength' must be an integer.");
                if (Y <= 0 || V + Y > z.byteLength)
                  throw new RangeError(`'byteLength' is out of range (0, ${z.byteLength - V}].`);
                if (typeof M == "object" && M !== null)
                  D = M;
                else if (typeof M < "u")
                  throw new TypeError("'options' must be an object.");
              } else if (typeof y < "u")
                throw new TypeError("'byteLength' must be a number.");
            } else if (typeof x < "u")
              throw new TypeError("'options' must be an object.");
            b = new Uint8Array(z, V, Y);
          } else
            throw new TypeError("Unexpected argument[0]: must be 'path' or 'buffer'.");
          const [q, se] = await (0, f.resolveBackendAndExecutionProviders)(D), oe = await q.createInferenceSessionHandler(b, se);
          return (0, j.TRACE_FUNC_END)(), new J(oe);
        }
        startProfiling() {
          this.handler.startProfiling();
        }
        endProfiling() {
          this.handler.endProfiling();
        }
        get inputNames() {
          return this.handler.inputNames;
        }
        get outputNames() {
          return this.handler.outputNames;
        }
      }
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/inference-session.js": (
    /*!***********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/inference-session.js ***!
      \***********************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        InferenceSession: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ./inference-session-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/inference-session-impl.js"
      );
      const L = f.InferenceSession;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/onnx-model.js": (
    /*!****************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/onnx-model.js ***!
      \****************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/onnx-value.js": (
    /*!****************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/onnx-value.js ***!
      \****************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-conversion-impl.js": (
    /*!****************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-conversion-impl.js ***!
      \****************************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        tensorToDataURL: () => (
          /* binding */
          f
        ),
        /* harmony export */
        tensorToImageData: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      const f = (j, J) => {
        const W = typeof document < "u" ? document.createElement("canvas") : new OffscreenCanvas(1, 1);
        W.width = j.dims[3], W.height = j.dims[2];
        const w = W.getContext("2d");
        if (w != null) {
          let x, y;
          (J == null ? void 0 : J.tensorLayout) !== void 0 && J.tensorLayout === "NHWC" ? (x = j.dims[2], y = j.dims[3]) : (x = j.dims[3], y = j.dims[2]);
          const M = (J == null ? void 0 : J.format) !== void 0 ? J.format : "RGB", b = J == null ? void 0 : J.norm;
          let D, q;
          b === void 0 || b.mean === void 0 ? D = [255, 255, 255, 255] : typeof b.mean == "number" ? D = [b.mean, b.mean, b.mean, b.mean] : (D = [b.mean[0], b.mean[1], b.mean[2], 0], b.mean[3] !== void 0 && (D[3] = b.mean[3])), b === void 0 || b.bias === void 0 ? q = [0, 0, 0, 0] : typeof b.bias == "number" ? q = [b.bias, b.bias, b.bias, b.bias] : (q = [b.bias[0], b.bias[1], b.bias[2], 0], b.bias[3] !== void 0 && (q[3] = b.bias[3]));
          const se = y * x;
          let oe = 0, z = se, V = se * 2, Y = -1;
          M === "RGBA" ? (oe = 0, z = se, V = se * 2, Y = se * 3) : M === "RGB" ? (oe = 0, z = se, V = se * 2) : M === "RBG" && (oe = 0, V = se, z = se * 2);
          for (let O = 0; O < y; O++)
            for (let $ = 0; $ < x; $++) {
              const g = (j.data[oe++] - q[0]) * D[0], C = (j.data[z++] - q[1]) * D[1], v = (j.data[V++] - q[2]) * D[2], ee = Y === -1 ? 255 : (j.data[Y++] - q[3]) * D[3];
              w.fillStyle = "rgba(" + g + "," + C + "," + v + "," + ee + ")", w.fillRect($, O, 1, 1);
            }
          if ("toDataURL" in W)
            return W.toDataURL();
          throw new Error("toDataURL is not supported");
        } else
          throw new Error("Can not access image data");
      }, L = (j, J) => {
        const W = typeof document < "u" ? document.createElement("canvas").getContext("2d") : new OffscreenCanvas(1, 1).getContext("2d");
        let w;
        if (W != null) {
          let x, y, M;
          (J == null ? void 0 : J.tensorLayout) !== void 0 && J.tensorLayout === "NHWC" ? (x = j.dims[2], y = j.dims[1], M = j.dims[3]) : (x = j.dims[3], y = j.dims[2], M = j.dims[1]);
          const b = J !== void 0 && J.format !== void 0 ? J.format : "RGB", D = J == null ? void 0 : J.norm;
          let q, se;
          D === void 0 || D.mean === void 0 ? q = [255, 255, 255, 255] : typeof D.mean == "number" ? q = [D.mean, D.mean, D.mean, D.mean] : (q = [D.mean[0], D.mean[1], D.mean[2], 255], D.mean[3] !== void 0 && (q[3] = D.mean[3])), D === void 0 || D.bias === void 0 ? se = [0, 0, 0, 0] : typeof D.bias == "number" ? se = [D.bias, D.bias, D.bias, D.bias] : (se = [D.bias[0], D.bias[1], D.bias[2], 0], D.bias[3] !== void 0 && (se[3] = D.bias[3]));
          const oe = y * x;
          if (J !== void 0 && (J.format !== void 0 && M === 4 && J.format !== "RGBA" || M === 3 && J.format !== "RGB" && J.format !== "BGR"))
            throw new Error("Tensor format doesn't match input tensor dims");
          const z = 4;
          let V = 0, Y = 1, O = 2, $ = 3, g = 0, C = oe, v = oe * 2, ee = -1;
          b === "RGBA" ? (g = 0, C = oe, v = oe * 2, ee = oe * 3) : b === "RGB" ? (g = 0, C = oe, v = oe * 2) : b === "RBG" && (g = 0, v = oe, C = oe * 2), w = W.createImageData(x, y);
          for (let X = 0; X < y * x; V += z, Y += z, O += z, $ += z, X++)
            w.data[V] = (j.data[g++] - se[0]) * q[0], w.data[Y] = (j.data[C++] - se[1]) * q[1], w.data[O] = (j.data[v++] - se[2]) * q[2], w.data[$] = ee === -1 ? 255 : (j.data[ee++] - se[3]) * q[3];
        } else
          throw new Error("Can not access image data");
        return w;
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-conversion.js": (
    /*!***********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-conversion.js ***!
      \***********************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-factory-impl.js": (
    /*!*************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-factory-impl.js ***!
      \*************************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        bufferToTensor: () => (
          /* binding */
          L
        ),
        /* harmony export */
        tensorFromGpuBuffer: () => (
          /* binding */
          W
        ),
        /* harmony export */
        tensorFromImage: () => (
          /* binding */
          j
        ),
        /* harmony export */
        tensorFromMLTensor: () => (
          /* binding */
          w
        ),
        /* harmony export */
        tensorFromPinnedBuffer: () => (
          /* binding */
          x
        ),
        /* harmony export */
        tensorFromTexture: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ./tensor-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js"
      );
      const L = (y, M) => {
        if (y === void 0)
          throw new Error("Image buffer must be defined");
        if (M.height === void 0 || M.width === void 0)
          throw new Error("Image height and width must be defined");
        if (M.tensorLayout === "NHWC")
          throw new Error("NHWC Tensor layout is not supported yet");
        const { height: b, width: D } = M, q = M.norm ?? { mean: 255, bias: 0 };
        let se, oe;
        typeof q.mean == "number" ? se = [q.mean, q.mean, q.mean, q.mean] : se = [q.mean[0], q.mean[1], q.mean[2], q.mean[3] ?? 255], typeof q.bias == "number" ? oe = [q.bias, q.bias, q.bias, q.bias] : oe = [q.bias[0], q.bias[1], q.bias[2], q.bias[3] ?? 0];
        const z = M.format !== void 0 ? M.format : "RGBA", V = M.tensorFormat !== void 0 && M.tensorFormat !== void 0 ? M.tensorFormat : "RGB", Y = b * D, O = V === "RGBA" ? new Float32Array(Y * 4) : new Float32Array(Y * 3);
        let $ = 4, g = 0, C = 1, v = 2, ee = 3, X = 0, le = Y, ue = Y * 2, fe = -1;
        z === "RGB" && ($ = 3, g = 0, C = 1, v = 2, ee = -1), V === "RGBA" ? fe = Y * 3 : V === "RBG" ? (X = 0, ue = Y, le = Y * 2) : V === "BGR" && (ue = 0, le = Y, X = Y * 2);
        for (let xe = 0; xe < Y; xe++, g += $, v += $, C += $, ee += $)
          O[X++] = (y[g] + oe[0]) / se[0], O[le++] = (y[C] + oe[1]) / se[1], O[ue++] = (y[v] + oe[2]) / se[2], fe !== -1 && ee !== -1 && (O[fe++] = (y[ee] + oe[3]) / se[3]);
        return V === "RGBA" ? new f.Tensor("float32", O, [1, 4, b, D]) : new f.Tensor("float32", O, [1, 3, b, D]);
      }, j = async (y, M) => {
        const b = typeof HTMLImageElement < "u" && y instanceof HTMLImageElement, D = typeof ImageData < "u" && y instanceof ImageData, q = typeof ImageBitmap < "u" && y instanceof ImageBitmap, se = typeof y == "string";
        let oe, z = M ?? {};
        const V = () => {
          if (typeof document < "u")
            return document.createElement("canvas");
          if (typeof OffscreenCanvas < "u")
            return new OffscreenCanvas(1, 1);
          throw new Error("Canvas is not supported");
        }, Y = (O) => typeof HTMLCanvasElement < "u" && O instanceof HTMLCanvasElement || O instanceof OffscreenCanvas ? O.getContext("2d") : null;
        if (b) {
          const O = V();
          O.width = y.width, O.height = y.height;
          const $ = Y(O);
          if ($ != null) {
            let g = y.height, C = y.width;
            if (M !== void 0 && M.resizedHeight !== void 0 && M.resizedWidth !== void 0 && (g = M.resizedHeight, C = M.resizedWidth), M !== void 0) {
              if (z = M, M.tensorFormat !== void 0)
                throw new Error("Image input config format must be RGBA for HTMLImageElement");
              z.tensorFormat = "RGBA", z.height = g, z.width = C;
            } else
              z.tensorFormat = "RGBA", z.height = g, z.width = C;
            $.drawImage(y, 0, 0), oe = $.getImageData(0, 0, C, g).data;
          } else
            throw new Error("Can not access image data");
        } else if (D) {
          let O, $;
          if (M !== void 0 && M.resizedWidth !== void 0 && M.resizedHeight !== void 0 ? (O = M.resizedHeight, $ = M.resizedWidth) : (O = y.height, $ = y.width), M !== void 0 && (z = M), z.format = "RGBA", z.height = O, z.width = $, M !== void 0) {
            const g = V();
            g.width = $, g.height = O;
            const C = Y(g);
            if (C != null)
              C.putImageData(y, 0, 0), oe = C.getImageData(0, 0, $, O).data;
            else
              throw new Error("Can not access image data");
          } else
            oe = y.data;
        } else if (q) {
          if (M === void 0)
            throw new Error("Please provide image config with format for Imagebitmap");
          const O = V();
          O.width = y.width, O.height = y.height;
          const $ = Y(O);
          if ($ != null) {
            const g = y.height, C = y.width;
            return $.drawImage(y, 0, 0, C, g), oe = $.getImageData(0, 0, C, g).data, z.height = g, z.width = C, L(oe, z);
          } else
            throw new Error("Can not access image data");
        } else {
          if (se)
            return new Promise((O, $) => {
              const g = V(), C = Y(g);
              if (!y || !C)
                return $();
              const v = new Image();
              v.crossOrigin = "Anonymous", v.src = y, v.onload = () => {
                g.width = v.width, g.height = v.height, C.drawImage(v, 0, 0, g.width, g.height);
                const ee = C.getImageData(0, 0, g.width, g.height);
                z.height = g.height, z.width = g.width, O(L(ee.data, z));
              };
            });
          throw new Error("Input data provided is not supported - aborted tensor creation");
        }
        if (oe !== void 0)
          return L(oe, z);
        throw new Error("Input data provided is not supported - aborted tensor creation");
      }, J = (y, M) => {
        const { width: b, height: D, download: q, dispose: se } = M, oe = [1, D, b, 4];
        return new f.Tensor({ location: "texture", type: "float32", texture: y, dims: oe, download: q, dispose: se });
      }, W = (y, M) => {
        const { dataType: b, dims: D, download: q, dispose: se } = M;
        return new f.Tensor({ location: "gpu-buffer", type: b ?? "float32", gpuBuffer: y, dims: D, download: q, dispose: se });
      }, w = (y, M) => {
        const { dataType: b, dims: D, download: q, dispose: se } = M;
        return new f.Tensor({ location: "ml-tensor", type: b ?? "float32", mlTensor: y, dims: D, download: q, dispose: se });
      }, x = (y, M, b) => new f.Tensor({ location: "cpu-pinned", type: y, data: M, dims: b ?? [M.length] });
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-factory.js": (
    /*!********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-factory.js ***!
      \********************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A);
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-impl-type-mapping.js": (
    /*!******************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-impl-type-mapping.js ***!
      \******************************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        NUMERIC_TENSOR_TYPEDARRAY_TO_TYPE_MAP: () => (
          /* binding */
          L
        ),
        /* harmony export */
        NUMERIC_TENSOR_TYPE_TO_TYPEDARRAY_MAP: () => (
          /* binding */
          f
        ),
        /* harmony export */
        checkTypedArray: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      const f = /* @__PURE__ */ new Map([
        ["float32", Float32Array],
        ["uint8", Uint8Array],
        ["int8", Int8Array],
        ["uint16", Uint16Array],
        ["int16", Int16Array],
        ["int32", Int32Array],
        ["bool", Uint8Array],
        ["float64", Float64Array],
        ["uint32", Uint32Array],
        ["int4", Uint8Array],
        ["uint4", Uint8Array]
      ]), L = /* @__PURE__ */ new Map([
        [Float32Array, "float32"],
        [Uint8Array, "uint8"],
        [Int8Array, "int8"],
        [Uint16Array, "uint16"],
        [Int16Array, "int16"],
        [Int32Array, "int32"],
        [Float64Array, "float64"],
        [Uint32Array, "uint32"]
      ]);
      let j = !1;
      const J = () => {
        if (!j) {
          j = !0;
          const W = typeof BigInt64Array < "u" && BigInt64Array.from, w = typeof BigUint64Array < "u" && BigUint64Array.from, x = typeof Float16Array < "u" && Float16Array.from;
          W && (f.set("int64", BigInt64Array), L.set(BigInt64Array, "int64")), w && (f.set("uint64", BigUint64Array), L.set(BigUint64Array, "uint64")), x ? (f.set("float16", Float16Array), L.set(Float16Array, "float16")) : f.set("float16", Uint16Array);
        }
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js": (
    /*!*****************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-impl.js ***!
      \*****************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Tensor: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      var f = s(
        /*! ./tensor-conversion-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-conversion-impl.js"
      ), L = s(
        /*! ./tensor-factory-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-factory-impl.js"
      ), j = s(
        /*! ./tensor-impl-type-mapping.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl-type-mapping.js"
      ), J = s(
        /*! ./tensor-utils-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-utils-impl.js"
      );
      class W {
        /**
         * implementation.
         */
        constructor(x, y, M) {
          (0, j.checkTypedArray)();
          let b, D;
          if (typeof x == "object" && "location" in x)
            switch (this.dataLocation = x.location, b = x.type, D = x.dims, x.location) {
              case "cpu-pinned": {
                const se = j.NUMERIC_TENSOR_TYPE_TO_TYPEDARRAY_MAP.get(b);
                if (!se)
                  throw new TypeError(`unsupported type "${b}" to create tensor from pinned buffer`);
                if (!(x.data instanceof se))
                  throw new TypeError(`buffer should be of type ${se.name}`);
                this.cpuData = x.data;
                break;
              }
              case "texture": {
                if (b !== "float32")
                  throw new TypeError(`unsupported type "${b}" to create tensor from texture`);
                this.gpuTextureData = x.texture, this.downloader = x.download, this.disposer = x.dispose;
                break;
              }
              case "gpu-buffer": {
                if (b !== "float32" && b !== "float16" && b !== "int32" && b !== "int64" && b !== "uint32" && b !== "uint8" && b !== "bool" && b !== "uint4" && b !== "int4")
                  throw new TypeError(`unsupported type "${b}" to create tensor from gpu buffer`);
                this.gpuBufferData = x.gpuBuffer, this.downloader = x.download, this.disposer = x.dispose;
                break;
              }
              case "ml-tensor": {
                if (b !== "float32" && b !== "float16" && b !== "int32" && b !== "int64" && b !== "uint32" && b !== "uint64" && b !== "int8" && b !== "uint8" && b !== "bool")
                  throw new TypeError(`unsupported type "${b}" to create tensor from MLTensor`);
                this.mlTensorData = x.mlTensor, this.downloader = x.download, this.disposer = x.dispose;
                break;
              }
              default:
                throw new Error(`Tensor constructor: unsupported location '${this.dataLocation}'`);
            }
          else {
            let se, oe;
            if (typeof x == "string")
              if (b = x, oe = M, x === "string") {
                if (!Array.isArray(y))
                  throw new TypeError("A string tensor's data must be a string array.");
                se = y;
              } else {
                const z = j.NUMERIC_TENSOR_TYPE_TO_TYPEDARRAY_MAP.get(x);
                if (z === void 0)
                  throw new TypeError(`Unsupported tensor type: ${x}.`);
                if (Array.isArray(y)) {
                  if (x === "float16" && z === Uint16Array || x === "uint4" || x === "int4")
                    throw new TypeError(`Creating a ${x} tensor from number array is not supported. Please use ${z.name} as data.`);
                  x === "uint64" || x === "int64" ? se = z.from(y, BigInt) : se = z.from(y);
                } else if (y instanceof z)
                  se = y;
                else if (y instanceof Uint8ClampedArray)
                  if (x === "uint8")
                    se = Uint8Array.from(y);
                  else
                    throw new TypeError("A Uint8ClampedArray tensor's data must be type of uint8");
                else
                  throw new TypeError(`A ${b} tensor's data must be type of ${z}`);
              }
            else if (oe = y, Array.isArray(x)) {
              if (x.length === 0)
                throw new TypeError("Tensor type cannot be inferred from an empty array.");
              const z = typeof x[0];
              if (z === "string")
                b = "string", se = x;
              else if (z === "boolean")
                b = "bool", se = Uint8Array.from(x);
              else
                throw new TypeError(`Invalid element type of data array: ${z}.`);
            } else if (x instanceof Uint8ClampedArray)
              b = "uint8", se = Uint8Array.from(x);
            else {
              const z = j.NUMERIC_TENSOR_TYPEDARRAY_TO_TYPE_MAP.get(x.constructor);
              if (z === void 0)
                throw new TypeError(`Unsupported type for tensor data: ${x.constructor}.`);
              b = z, se = x;
            }
            if (oe === void 0)
              oe = [se.length];
            else if (!Array.isArray(oe))
              throw new TypeError("A tensor's dims must be a number array");
            D = oe, this.cpuData = se, this.dataLocation = "cpu";
          }
          const q = (0, J.calculateSize)(D);
          if (this.cpuData && q !== this.cpuData.length && !((b === "uint4" || b === "int4") && Math.ceil(q / 2) === this.cpuData.length))
            throw new Error(`Tensor's size(${q}) does not match data length(${this.cpuData.length}).`);
          this.type = b, this.dims = D, this.size = q;
        }
        // #endregion
        // #region factory
        static async fromImage(x, y) {
          return (0, L.tensorFromImage)(x, y);
        }
        static fromTexture(x, y) {
          return (0, L.tensorFromTexture)(x, y);
        }
        static fromGpuBuffer(x, y) {
          return (0, L.tensorFromGpuBuffer)(x, y);
        }
        static fromMLTensor(x, y) {
          return (0, L.tensorFromMLTensor)(x, y);
        }
        static fromPinnedBuffer(x, y, M) {
          return (0, L.tensorFromPinnedBuffer)(x, y, M);
        }
        // #endregion
        // #region conversions
        toDataURL(x) {
          return (0, f.tensorToDataURL)(this, x);
        }
        toImageData(x) {
          return (0, f.tensorToImageData)(this, x);
        }
        // #endregion
        // #region properties
        get data() {
          if (this.ensureValid(), !this.cpuData)
            throw new Error("The data is not on CPU. Use `getData()` to download GPU data to CPU, or use `texture` or `gpuBuffer` property to access the GPU data directly.");
          return this.cpuData;
        }
        get location() {
          return this.dataLocation;
        }
        get texture() {
          if (this.ensureValid(), !this.gpuTextureData)
            throw new Error("The data is not stored as a WebGL texture.");
          return this.gpuTextureData;
        }
        get gpuBuffer() {
          if (this.ensureValid(), !this.gpuBufferData)
            throw new Error("The data is not stored as a WebGPU buffer.");
          return this.gpuBufferData;
        }
        get mlTensor() {
          if (this.ensureValid(), !this.mlTensorData)
            throw new Error("The data is not stored as a WebNN MLTensor.");
          return this.mlTensorData;
        }
        // #endregion
        // #region methods
        async getData(x) {
          switch (this.ensureValid(), this.dataLocation) {
            case "cpu":
            case "cpu-pinned":
              return this.data;
            case "texture":
            case "gpu-buffer":
            case "ml-tensor": {
              if (!this.downloader)
                throw new Error("The current tensor is not created with a specified data downloader.");
              if (this.isDownloading)
                throw new Error("The current tensor is being downloaded.");
              try {
                this.isDownloading = !0;
                const y = await this.downloader();
                return this.downloader = void 0, this.dataLocation = "cpu", this.cpuData = y, x && this.disposer && (this.disposer(), this.disposer = void 0), y;
              } finally {
                this.isDownloading = !1;
              }
            }
            default:
              throw new Error(`cannot get data from location: ${this.dataLocation}`);
          }
        }
        dispose() {
          if (this.isDownloading)
            throw new Error("The current tensor is being downloaded.");
          this.disposer && (this.disposer(), this.disposer = void 0), this.cpuData = void 0, this.gpuTextureData = void 0, this.gpuBufferData = void 0, this.mlTensorData = void 0, this.downloader = void 0, this.isDownloading = void 0, this.dataLocation = "none";
        }
        // #endregion
        // #region tensor utilities
        ensureValid() {
          if (this.dataLocation === "none")
            throw new Error("The tensor is disposed.");
        }
        reshape(x) {
          if (this.ensureValid(), this.downloader || this.disposer)
            throw new Error("Cannot reshape a tensor that owns GPU resource.");
          return (0, J.tensorReshape)(this, x);
        }
      }
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor-utils-impl.js": (
    /*!***********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor-utils-impl.js ***!
      \***********************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        calculateSize: () => (
          /* binding */
          L
        ),
        /* harmony export */
        tensorReshape: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ./tensor-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js"
      );
      const L = (J) => {
        let W = 1;
        for (let w = 0; w < J.length; w++) {
          const x = J[w];
          if (typeof x != "number" || !Number.isSafeInteger(x))
            throw new TypeError(`dims[${w}] must be an integer, got: ${x}`);
          if (x < 0)
            throw new RangeError(`dims[${w}] must be a non-negative integer, got: ${x}`);
          W *= x;
        }
        return W;
      }, j = (J, W) => {
        switch (J.location) {
          case "cpu":
            return new f.Tensor(J.type, J.data, W);
          case "cpu-pinned":
            return new f.Tensor({
              location: "cpu-pinned",
              data: J.data,
              type: J.type,
              dims: W
            });
          case "texture":
            return new f.Tensor({
              location: "texture",
              texture: J.texture,
              type: J.type,
              dims: W
            });
          case "gpu-buffer":
            return new f.Tensor({
              location: "gpu-buffer",
              gpuBuffer: J.gpuBuffer,
              type: J.type,
              dims: W
            });
          case "ml-tensor":
            return new f.Tensor({
              location: "ml-tensor",
              mlTensor: J.mlTensor,
              type: J.type,
              dims: W
            });
          default:
            throw new Error(`tensorReshape: tensor location ${J.location} is not supported`);
        }
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/tensor.js": (
    /*!************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/tensor.js ***!
      \************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Tensor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ./tensor-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor-impl.js"
      );
      const L = f.Tensor;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/trace.js": (
    /*!***********************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/trace.js ***!
      \***********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        TRACE: () => (
          /* binding */
          L
        ),
        /* harmony export */
        TRACE_FUNC_BEGIN: () => (
          /* binding */
          J
        ),
        /* harmony export */
        TRACE_FUNC_END: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      var f = s(
        /*! ./env-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/env-impl.js"
      );
      const L = (w, x) => {
        (typeof f.env.trace > "u" ? !f.env.wasm.trace : !f.env.trace) || console.timeStamp(`${w}::ORT::${x}`);
      }, j = (w, x) => {
        var b;
        const y = ((b = new Error().stack) == null ? void 0 : b.split(/\r\n|\r|\n/g)) || [];
        let M = !1;
        for (let D = 0; D < y.length; D++) {
          if (M && !y[D].includes("TRACE_FUNC")) {
            let q = `FUNC_${w}::${y[D].trim().split(" ")[1]}`;
            x && (q += `::${x}`), L("CPU", q);
            return;
          }
          y[D].includes("TRACE_FUNC") && (M = !0);
        }
      }, J = (w) => {
        (typeof f.env.trace > "u" ? !f.env.wasm.trace : !f.env.trace) || j("BEGIN", w);
      }, W = (w) => {
        (typeof f.env.trace > "u" ? !f.env.wasm.trace : !f.env.trace) || j("END", w);
      };
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/training-session-impl.js": (
    /*!***************************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/training-session-impl.js ***!
      \***************************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        TrainingSession: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ./backend-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/backend-impl.js"
      ), L = s(
        /*! ./tensor.js */
        "./node_modules/onnxruntime-common/dist/esm/tensor.js"
      );
      const j = "Training backend could not be resolved. Make sure you're using the correct configuration & WebAssembly files.";
      class J {
        constructor(w, x, y) {
          this.handler = w, this.hasOptimizerModel = x, this.hasEvalModel = y;
        }
        get trainingInputNames() {
          return this.handler.inputNames;
        }
        get trainingOutputNames() {
          return this.handler.outputNames;
        }
        get evalInputNames() {
          if (this.hasEvalModel)
            return this.handler.evalInputNames;
          throw new Error("This training session has no evalModel loaded.");
        }
        get evalOutputNames() {
          if (this.hasEvalModel)
            return this.handler.evalOutputNames;
          throw new Error("This training session has no evalModel loaded.");
        }
        static async create(w, x) {
          const y = w.evalModel || "", M = w.optimizerModel || "", b = x || {}, [D, q] = await (0, f.resolveBackendAndExecutionProviders)(b);
          if (D.createTrainingSessionHandler) {
            const se = await D.createTrainingSessionHandler(w.checkpointState, w.trainModel, y, M, q);
            return new J(se, !!w.optimizerModel, !!w.evalModel);
          } else
            throw new Error(j);
        }
        /**
         * Helper function for runTrainStep and future runStep methods that handles the type-narrowing conversion from
         * the given parameters to SessionHandler.FetchesType and RunOptions.
         *
         * @param inputNames the feeds object is checked that they contain all input names in the provided list of input
         * names.
         * @param outputNames the fetches object is checked that their keys match up with valid names in the list of output
         * names.
         * @param feeds the required input
         * @param arg1 narrowed & converted into the SessionHandler.FetchesType or RunOptions object
         * @param arg2 optional RunOptions object.
         * @returns
         */
        typeNarrowingForRunStep(w, x, y, M, b) {
          const D = {};
          let q = {};
          if (typeof y != "object" || y === null || y instanceof L.Tensor || Array.isArray(y))
            throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");
          let se = !0;
          if (typeof M == "object") {
            if (M === null)
              throw new TypeError("Unexpected argument[1]: cannot be null.");
            if (M instanceof L.Tensor)
              throw new TypeError("'fetches' cannot be a Tensor");
            if (Array.isArray(M)) {
              if (M.length === 0)
                throw new TypeError("'fetches' cannot be an empty array.");
              se = !1;
              for (const oe of M) {
                if (typeof oe != "string")
                  throw new TypeError("'fetches' must be a string array or an object.");
                if (x.indexOf(oe) === -1)
                  throw new RangeError(`'fetches' contains invalid output name: ${oe}.`);
                D[oe] = null;
              }
              if (typeof b == "object" && b !== null)
                q = b;
              else if (typeof b < "u")
                throw new TypeError("'options' must be an object.");
            } else {
              let oe = !1;
              const z = Object.getOwnPropertyNames(M);
              for (const V of x)
                if (z.indexOf(V) !== -1) {
                  const Y = M[V];
                  (Y === null || Y instanceof L.Tensor) && (oe = !0, se = !1, D[V] = Y);
                }
              if (oe) {
                if (typeof b == "object" && b !== null)
                  q = b;
                else if (typeof b < "u")
                  throw new TypeError("'options' must be an object.");
              } else
                q = M;
            }
          } else if (typeof M < "u")
            throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");
          for (const oe of w)
            if (typeof y[oe] > "u")
              throw new Error(`input '${oe}' is missing in 'feeds'.`);
          if (se)
            for (const oe of x)
              D[oe] = null;
          return [D, q];
        }
        /**
         * Helper method for runTrainStep and any other runStep methods. Takes the ReturnType result from the SessionHandler
         * and changes it into a map of Tensors.
         *
         * @param results
         * @returns
         */
        convertHandlerReturnTypeToMapOfTensors(w) {
          const x = {};
          for (const y in w)
            if (Object.hasOwnProperty.call(w, y)) {
              const M = w[y];
              M instanceof L.Tensor ? x[y] = M : x[y] = new L.Tensor(M.type, M.data, M.dims);
            }
          return x;
        }
        async lazyResetGrad() {
          await this.handler.lazyResetGrad();
        }
        async runTrainStep(w, x, y) {
          const [M, b] = this.typeNarrowingForRunStep(this.trainingInputNames, this.trainingOutputNames, w, x, y), D = await this.handler.runTrainStep(w, M, b);
          return this.convertHandlerReturnTypeToMapOfTensors(D);
        }
        async runOptimizerStep(w) {
          if (this.hasOptimizerModel)
            await this.handler.runOptimizerStep(w || {});
          else
            throw new Error("This TrainingSession has no OptimizerModel loaded.");
        }
        async runEvalStep(w, x, y) {
          if (this.hasEvalModel) {
            const [M, b] = this.typeNarrowingForRunStep(this.evalInputNames, this.evalOutputNames, w, x, y), D = await this.handler.runEvalStep(w, M, b);
            return this.convertHandlerReturnTypeToMapOfTensors(D);
          } else
            throw new Error("This TrainingSession has no EvalModel loaded.");
        }
        async getParametersSize(w = !0) {
          return this.handler.getParametersSize(w);
        }
        async loadParametersBuffer(w, x = !0) {
          const y = await this.getParametersSize(x);
          if (w.length !== 4 * y)
            throw new Error("Size of the buffer passed into loadParametersBuffer must match the number of parameters in the model. Please use getParametersSize method to check.");
          return this.handler.loadParametersBuffer(w, x);
        }
        async getContiguousParameters(w = !0) {
          return this.handler.getContiguousParameters(w);
        }
        async release() {
          return this.handler.dispose();
        }
      }
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/training-session.js": (
    /*!**********************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/training-session.js ***!
      \**********************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        TrainingSession: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ./training-session-impl.js */
        "./node_modules/onnxruntime-common/dist/esm/training-session-impl.js"
      );
      const L = f.TrainingSession;
    }
  ),
  /***/
  "./node_modules/onnxruntime-common/dist/esm/version.js": (
    /*!*************************************************************!*\
      !*** ./node_modules/onnxruntime-common/dist/esm/version.js ***!
      \*************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        version: () => (
          /* binding */
          f
        )
        /* harmony export */
      });
      const f = "1.20.1";
    }
  ),
  /***/
  "./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs": (
    /*!**************************************************************!*\
      !*** ./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs ***!
      \**************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        InferenceSession: () => (
          /* binding */
          dt
        ),
        /* harmony export */
        TRACE: () => (
          /* binding */
          Re
        ),
        /* harmony export */
        TRACE_FUNC_BEGIN: () => (
          /* binding */
          Ve
        ),
        /* harmony export */
        TRACE_FUNC_END: () => (
          /* binding */
          Ne
        ),
        /* harmony export */
        Tensor: () => (
          /* binding */
          ce
        ),
        /* harmony export */
        default: () => (
          /* binding */
          a_
        ),
        /* harmony export */
        env: () => (
          /* binding */
          v
        ),
        /* harmony export */
        registerBackend: () => (
          /* binding */
          q
        )
        /* harmony export */
      });
      /*!
       * ONNX Runtime Web v1.21.0-dev.20241205-d27fecd3d3
       * Copyright (c) Microsoft Corporation. All rights reserved.
       * Licensed under the MIT License.
       */
      var f = Object.defineProperty, L = Object.getOwnPropertyDescriptor, j = Object.getOwnPropertyNames, J = Object.prototype.hasOwnProperty, W = ((e) => typeof require < "u" ? require : typeof Proxy < "u" ? new Proxy(e, { get: (t, r) => (typeof require < "u" ? require : t)[r] }) : e)(function(e) {
        if (typeof require < "u") return require.apply(this, arguments);
        throw Error('Dynamic require of "' + e + '" is not supported');
      }), w = (e, t) => () => (e && (t = e(e = 0)), t), x = (e, t) => {
        for (var r in t) f(e, r, { get: t[r], enumerable: !0 });
      }, y = (e, t, r, n) => {
        if (t && typeof t == "object" || typeof t == "function") for (let i of j(t)) !J.call(e, i) && i !== r && f(e, i, { get: () => t[i], enumerable: !(n = L(t, i)) || n.enumerable });
        return e;
      }, M = (e) => y(f({}, "__esModule", { value: !0 }), e), b, D, q, se, oe, z = w(() => {
        b = /* @__PURE__ */ new Map(), D = [], q = (e, t, r) => {
          if (t && typeof t.init == "function" && typeof t.createInferenceSessionHandler == "function") {
            let n = b.get(e);
            if (n === void 0) b.set(e, { backend: t, priority: r });
            else {
              if (n.priority > r) return;
              if (n.priority === r && n.backend !== t) throw new Error(`cannot register backend "${e}" using priority ${r}`);
            }
            if (r >= 0) {
              let i = D.indexOf(e);
              i !== -1 && D.splice(i, 1);
              for (let a = 0; a < D.length; a++) if (b.get(D[a]).priority <= r) {
                D.splice(a, 0, e);
                return;
              }
              D.push(e);
            }
            return;
          }
          throw new TypeError("not a valid backend");
        }, se = async (e) => {
          let t = b.get(e);
          if (!t) return "backend not found.";
          if (t.initialized) return t.backend;
          if (t.aborted) return t.error;
          {
            let r = !!t.initPromise;
            try {
              return r || (t.initPromise = t.backend.init(e)), await t.initPromise, t.initialized = !0, t.backend;
            } catch (n) {
              return r || (t.error = `${n}`, t.aborted = !0), t.error;
            } finally {
              delete t.initPromise;
            }
          }
        }, oe = async (e) => {
          let t = e.executionProviders || [], r = t.map((p) => typeof p == "string" ? p : p.name), n = r.length === 0 ? D : r, i, a = [], o = /* @__PURE__ */ new Set();
          for (let p of n) {
            let h = await se(p);
            typeof h == "string" ? a.push({ name: p, err: h }) : (i || (i = h), i === h && o.add(p));
          }
          if (!i) throw new Error(`no available backend found. ERR: ${a.map((p) => `[${p.name}] ${p.err}`).join(", ")}`);
          for (let { name: p, err: h } of a) r.includes(p) && console.warn(`removing requested execution provider "${p}" from session options because it is not available: ${h}`);
          let d = t.filter((p) => o.has(typeof p == "string" ? p : p.name));
          return [i, new Proxy(e, { get: (p, h) => h === "executionProviders" ? d : Reflect.get(p, h) })];
        };
      }), V = w(() => {
        z();
      }), Y, O = w(() => {
        Y = "1.21.0-dev.20241205-6ed77cc374";
      }), $, g, C = w(() => {
        O(), $ = "warning", g = { wasm: {}, webgl: {}, webgpu: {}, versions: { common: Y }, set logLevel(e) {
          if (e !== void 0) {
            if (typeof e != "string" || ["verbose", "info", "warning", "error", "fatal"].indexOf(e) === -1) throw new Error(`Unsupported logging level: ${e}`);
            $ = e;
          }
        }, get logLevel() {
          return $;
        } }, Object.defineProperty(g, "logLevel", { enumerable: !0 });
      }), v, ee = w(() => {
        C(), v = g;
      }), X, le, ue = w(() => {
        X = (e, t) => {
          let r = typeof document < "u" ? document.createElement("canvas") : new OffscreenCanvas(1, 1);
          r.width = e.dims[3], r.height = e.dims[2];
          let n = r.getContext("2d");
          if (n != null) {
            let i, a;
            (t == null ? void 0 : t.tensorLayout) !== void 0 && t.tensorLayout === "NHWC" ? (i = e.dims[2], a = e.dims[3]) : (i = e.dims[3], a = e.dims[2]);
            let o = (t == null ? void 0 : t.format) !== void 0 ? t.format : "RGB", d = t == null ? void 0 : t.norm, p, h;
            d === void 0 || d.mean === void 0 ? p = [255, 255, 255, 255] : typeof d.mean == "number" ? p = [d.mean, d.mean, d.mean, d.mean] : (p = [d.mean[0], d.mean[1], d.mean[2], 0], d.mean[3] !== void 0 && (p[3] = d.mean[3])), d === void 0 || d.bias === void 0 ? h = [0, 0, 0, 0] : typeof d.bias == "number" ? h = [d.bias, d.bias, d.bias, d.bias] : (h = [d.bias[0], d.bias[1], d.bias[2], 0], d.bias[3] !== void 0 && (h[3] = d.bias[3]));
            let k = a * i, S = 0, u = k, B = k * 2, R = -1;
            o === "RGBA" ? (S = 0, u = k, B = k * 2, R = k * 3) : o === "RGB" ? (S = 0, u = k, B = k * 2) : o === "RBG" && (S = 0, B = k, u = k * 2);
            for (let N = 0; N < a; N++) for (let Z = 0; Z < i; Z++) {
              let te = (e.data[S++] - h[0]) * p[0], Q = (e.data[u++] - h[1]) * p[1], _e = (e.data[B++] - h[2]) * p[2], me = R === -1 ? 255 : (e.data[R++] - h[3]) * p[3];
              n.fillStyle = "rgba(" + te + "," + Q + "," + _e + "," + me + ")", n.fillRect(Z, N, 1, 1);
            }
            if ("toDataURL" in r) return r.toDataURL();
            throw new Error("toDataURL is not supported");
          } else throw new Error("Can not access image data");
        }, le = (e, t) => {
          let r = typeof document < "u" ? document.createElement("canvas").getContext("2d") : new OffscreenCanvas(1, 1).getContext("2d"), n;
          if (r != null) {
            let i, a, o;
            (t == null ? void 0 : t.tensorLayout) !== void 0 && t.tensorLayout === "NHWC" ? (i = e.dims[2], a = e.dims[1], o = e.dims[3]) : (i = e.dims[3], a = e.dims[2], o = e.dims[1]);
            let d = t !== void 0 && t.format !== void 0 ? t.format : "RGB", p = t == null ? void 0 : t.norm, h, k;
            p === void 0 || p.mean === void 0 ? h = [255, 255, 255, 255] : typeof p.mean == "number" ? h = [p.mean, p.mean, p.mean, p.mean] : (h = [p.mean[0], p.mean[1], p.mean[2], 255], p.mean[3] !== void 0 && (h[3] = p.mean[3])), p === void 0 || p.bias === void 0 ? k = [0, 0, 0, 0] : typeof p.bias == "number" ? k = [p.bias, p.bias, p.bias, p.bias] : (k = [p.bias[0], p.bias[1], p.bias[2], 0], p.bias[3] !== void 0 && (k[3] = p.bias[3]));
            let S = a * i;
            if (t !== void 0 && (t.format !== void 0 && o === 4 && t.format !== "RGBA" || o === 3 && t.format !== "RGB" && t.format !== "BGR")) throw new Error("Tensor format doesn't match input tensor dims");
            let u = 4, B = 0, R = 1, N = 2, Z = 3, te = 0, Q = S, _e = S * 2, me = -1;
            d === "RGBA" ? (te = 0, Q = S, _e = S * 2, me = S * 3) : d === "RGB" ? (te = 0, Q = S, _e = S * 2) : d === "RBG" && (te = 0, _e = S, Q = S * 2), n = r.createImageData(i, a);
            for (let ye = 0; ye < a * i; B += u, R += u, N += u, Z += u, ye++) n.data[B] = (e.data[te++] - k[0]) * h[0], n.data[R] = (e.data[Q++] - k[1]) * h[1], n.data[N] = (e.data[_e++] - k[2]) * h[2], n.data[Z] = me === -1 ? 255 : (e.data[me++] - k[3]) * h[3];
          } else throw new Error("Can not access image data");
          return n;
        };
      }), fe, Ce, xe, Le, qe, Ue, ut = w(() => {
        De(), fe = (e, t) => {
          if (e === void 0) throw new Error("Image buffer must be defined");
          if (t.height === void 0 || t.width === void 0) throw new Error("Image height and width must be defined");
          if (t.tensorLayout === "NHWC") throw new Error("NHWC Tensor layout is not supported yet");
          let { height: r, width: n } = t, i = t.norm ?? { mean: 255, bias: 0 }, a, o;
          typeof i.mean == "number" ? a = [i.mean, i.mean, i.mean, i.mean] : a = [i.mean[0], i.mean[1], i.mean[2], i.mean[3] ?? 255], typeof i.bias == "number" ? o = [i.bias, i.bias, i.bias, i.bias] : o = [i.bias[0], i.bias[1], i.bias[2], i.bias[3] ?? 0];
          let d = t.format !== void 0 ? t.format : "RGBA", p = t.tensorFormat !== void 0 && t.tensorFormat !== void 0 ? t.tensorFormat : "RGB", h = r * n, k = p === "RGBA" ? new Float32Array(h * 4) : new Float32Array(h * 3), S = 4, u = 0, B = 1, R = 2, N = 3, Z = 0, te = h, Q = h * 2, _e = -1;
          d === "RGB" && (S = 3, u = 0, B = 1, R = 2, N = -1), p === "RGBA" ? _e = h * 3 : p === "RBG" ? (Z = 0, Q = h, te = h * 2) : p === "BGR" && (Q = 0, te = h, Z = h * 2);
          for (let me = 0; me < h; me++, u += S, R += S, B += S, N += S) k[Z++] = (e[u] + o[0]) / a[0], k[te++] = (e[B] + o[1]) / a[1], k[Q++] = (e[R] + o[2]) / a[2], _e !== -1 && N !== -1 && (k[_e++] = (e[N] + o[3]) / a[3]);
          return p === "RGBA" ? new Je("float32", k, [1, 4, r, n]) : new Je("float32", k, [1, 3, r, n]);
        }, Ce = async (e, t) => {
          let r = typeof HTMLImageElement < "u" && e instanceof HTMLImageElement, n = typeof ImageData < "u" && e instanceof ImageData, i = typeof ImageBitmap < "u" && e instanceof ImageBitmap, a = typeof e == "string", o, d = t ?? {}, p = () => {
            if (typeof document < "u") return document.createElement("canvas");
            if (typeof OffscreenCanvas < "u") return new OffscreenCanvas(1, 1);
            throw new Error("Canvas is not supported");
          }, h = (k) => typeof HTMLCanvasElement < "u" && k instanceof HTMLCanvasElement || k instanceof OffscreenCanvas ? k.getContext("2d") : null;
          if (r) {
            let k = p();
            k.width = e.width, k.height = e.height;
            let S = h(k);
            if (S != null) {
              let u = e.height, B = e.width;
              if (t !== void 0 && t.resizedHeight !== void 0 && t.resizedWidth !== void 0 && (u = t.resizedHeight, B = t.resizedWidth), t !== void 0) {
                if (d = t, t.tensorFormat !== void 0) throw new Error("Image input config format must be RGBA for HTMLImageElement");
                d.tensorFormat = "RGBA", d.height = u, d.width = B;
              } else d.tensorFormat = "RGBA", d.height = u, d.width = B;
              S.drawImage(e, 0, 0), o = S.getImageData(0, 0, B, u).data;
            } else throw new Error("Can not access image data");
          } else if (n) {
            let k, S;
            if (t !== void 0 && t.resizedWidth !== void 0 && t.resizedHeight !== void 0 ? (k = t.resizedHeight, S = t.resizedWidth) : (k = e.height, S = e.width), t !== void 0 && (d = t), d.format = "RGBA", d.height = k, d.width = S, t !== void 0) {
              let u = p();
              u.width = S, u.height = k;
              let B = h(u);
              if (B != null) B.putImageData(e, 0, 0), o = B.getImageData(0, 0, S, k).data;
              else throw new Error("Can not access image data");
            } else o = e.data;
          } else if (i) {
            if (t === void 0) throw new Error("Please provide image config with format for Imagebitmap");
            let k = p();
            k.width = e.width, k.height = e.height;
            let S = h(k);
            if (S != null) {
              let u = e.height, B = e.width;
              return S.drawImage(e, 0, 0, B, u), o = S.getImageData(0, 0, B, u).data, d.height = u, d.width = B, fe(o, d);
            } else throw new Error("Can not access image data");
          } else {
            if (a) return new Promise((k, S) => {
              let u = p(), B = h(u);
              if (!e || !B) return S();
              let R = new Image();
              R.crossOrigin = "Anonymous", R.src = e, R.onload = () => {
                u.width = R.width, u.height = R.height, B.drawImage(R, 0, 0, u.width, u.height);
                let N = B.getImageData(0, 0, u.width, u.height);
                d.height = u.height, d.width = u.width, k(fe(N.data, d));
              };
            });
            throw new Error("Input data provided is not supported - aborted tensor creation");
          }
          if (o !== void 0) return fe(o, d);
          throw new Error("Input data provided is not supported - aborted tensor creation");
        }, xe = (e, t) => {
          let { width: r, height: n, download: i, dispose: a } = t, o = [1, n, r, 4];
          return new Je({ location: "texture", type: "float32", texture: e, dims: o, download: i, dispose: a });
        }, Le = (e, t) => {
          let { dataType: r, dims: n, download: i, dispose: a } = t;
          return new Je({ location: "gpu-buffer", type: r ?? "float32", gpuBuffer: e, dims: n, download: i, dispose: a });
        }, qe = (e, t) => {
          let { dataType: r, dims: n, download: i, dispose: a } = t;
          return new Je({ location: "ml-tensor", type: r ?? "float32", mlTensor: e, dims: n, download: i, dispose: a });
        }, Ue = (e, t, r) => new Je({ location: "cpu-pinned", type: e, data: t, dims: r ?? [t.length] });
      }), de, re, he, Ee, Be = w(() => {
        de = /* @__PURE__ */ new Map([["float32", Float32Array], ["uint8", Uint8Array], ["int8", Int8Array], ["uint16", Uint16Array], ["int16", Int16Array], ["int32", Int32Array], ["bool", Uint8Array], ["float64", Float64Array], ["uint32", Uint32Array], ["int4", Uint8Array], ["uint4", Uint8Array]]), re = /* @__PURE__ */ new Map([[Float32Array, "float32"], [Uint8Array, "uint8"], [Int8Array, "int8"], [Uint16Array, "uint16"], [Int16Array, "int16"], [Int32Array, "int32"], [Float64Array, "float64"], [Uint32Array, "uint32"]]), he = !1, Ee = () => {
          if (!he) {
            he = !0;
            let e = typeof BigInt64Array < "u" && BigInt64Array.from, t = typeof BigUint64Array < "u" && BigUint64Array.from, r = typeof Float16Array < "u" && Float16Array.from;
            e && (de.set("int64", BigInt64Array), re.set(BigInt64Array, "int64")), t && (de.set("uint64", BigUint64Array), re.set(BigUint64Array, "uint64")), r ? (de.set("float16", Float16Array), re.set(Float16Array, "float16")) : de.set("float16", Uint16Array);
          }
        };
      }), et, Xe, ie = w(() => {
        De(), et = (e) => {
          let t = 1;
          for (let r = 0; r < e.length; r++) {
            let n = e[r];
            if (typeof n != "number" || !Number.isSafeInteger(n)) throw new TypeError(`dims[${r}] must be an integer, got: ${n}`);
            if (n < 0) throw new RangeError(`dims[${r}] must be a non-negative integer, got: ${n}`);
            t *= n;
          }
          return t;
        }, Xe = (e, t) => {
          switch (e.location) {
            case "cpu":
              return new Je(e.type, e.data, t);
            case "cpu-pinned":
              return new Je({ location: "cpu-pinned", data: e.data, type: e.type, dims: t });
            case "texture":
              return new Je({ location: "texture", texture: e.texture, type: e.type, dims: t });
            case "gpu-buffer":
              return new Je({ location: "gpu-buffer", gpuBuffer: e.gpuBuffer, type: e.type, dims: t });
            case "ml-tensor":
              return new Je({ location: "ml-tensor", mlTensor: e.mlTensor, type: e.type, dims: t });
            default:
              throw new Error(`tensorReshape: tensor location ${e.location} is not supported`);
          }
        };
      }), Je, De = w(() => {
        ue(), ut(), Be(), ie(), Je = class {
          constructor(e, t, r) {
            Ee();
            let n, i;
            if (typeof e == "object" && "location" in e) switch (this.dataLocation = e.location, n = e.type, i = e.dims, e.location) {
              case "cpu-pinned": {
                let o = de.get(n);
                if (!o) throw new TypeError(`unsupported type "${n}" to create tensor from pinned buffer`);
                if (!(e.data instanceof o)) throw new TypeError(`buffer should be of type ${o.name}`);
                this.cpuData = e.data;
                break;
              }
              case "texture": {
                if (n !== "float32") throw new TypeError(`unsupported type "${n}" to create tensor from texture`);
                this.gpuTextureData = e.texture, this.downloader = e.download, this.disposer = e.dispose;
                break;
              }
              case "gpu-buffer": {
                if (n !== "float32" && n !== "float16" && n !== "int32" && n !== "int64" && n !== "uint32" && n !== "uint8" && n !== "bool" && n !== "uint4" && n !== "int4") throw new TypeError(`unsupported type "${n}" to create tensor from gpu buffer`);
                this.gpuBufferData = e.gpuBuffer, this.downloader = e.download, this.disposer = e.dispose;
                break;
              }
              case "ml-tensor": {
                if (n !== "float32" && n !== "float16" && n !== "int32" && n !== "int64" && n !== "uint32" && n !== "uint64" && n !== "int8" && n !== "uint8" && n !== "bool" && n !== "uint4" && n !== "int4") throw new TypeError(`unsupported type "${n}" to create tensor from MLTensor`);
                this.mlTensorData = e.mlTensor, this.downloader = e.download, this.disposer = e.dispose;
                break;
              }
              default:
                throw new Error(`Tensor constructor: unsupported location '${this.dataLocation}'`);
            }
            else {
              let o, d;
              if (typeof e == "string") if (n = e, d = r, e === "string") {
                if (!Array.isArray(t)) throw new TypeError("A string tensor's data must be a string array.");
                o = t;
              } else {
                let p = de.get(e);
                if (p === void 0) throw new TypeError(`Unsupported tensor type: ${e}.`);
                if (Array.isArray(t)) {
                  if (e === "float16" && p === Uint16Array || e === "uint4" || e === "int4") throw new TypeError(`Creating a ${e} tensor from number array is not supported. Please use ${p.name} as data.`);
                  e === "uint64" || e === "int64" ? o = p.from(t, BigInt) : o = p.from(t);
                } else if (t instanceof p) o = t;
                else if (t instanceof Uint8ClampedArray) if (e === "uint8") o = Uint8Array.from(t);
                else throw new TypeError("A Uint8ClampedArray tensor's data must be type of uint8");
                else throw new TypeError(`A ${n} tensor's data must be type of ${p}`);
              }
              else if (d = t, Array.isArray(e)) {
                if (e.length === 0) throw new TypeError("Tensor type cannot be inferred from an empty array.");
                let p = typeof e[0];
                if (p === "string") n = "string", o = e;
                else if (p === "boolean") n = "bool", o = Uint8Array.from(e);
                else throw new TypeError(`Invalid element type of data array: ${p}.`);
              } else if (e instanceof Uint8ClampedArray) n = "uint8", o = Uint8Array.from(e);
              else {
                let p = re.get(e.constructor);
                if (p === void 0) throw new TypeError(`Unsupported type for tensor data: ${e.constructor}.`);
                n = p, o = e;
              }
              if (d === void 0) d = [o.length];
              else if (!Array.isArray(d)) throw new TypeError("A tensor's dims must be a number array");
              i = d, this.cpuData = o, this.dataLocation = "cpu";
            }
            let a = et(i);
            if (this.cpuData && a !== this.cpuData.length && !((n === "uint4" || n === "int4") && Math.ceil(a / 2) === this.cpuData.length)) throw new Error(`Tensor's size(${a}) does not match data length(${this.cpuData.length}).`);
            this.type = n, this.dims = i, this.size = a;
          }
          static async fromImage(e, t) {
            return Ce(e, t);
          }
          static fromTexture(e, t) {
            return xe(e, t);
          }
          static fromGpuBuffer(e, t) {
            return Le(e, t);
          }
          static fromMLTensor(e, t) {
            return qe(e, t);
          }
          static fromPinnedBuffer(e, t, r) {
            return Ue(e, t, r);
          }
          toDataURL(e) {
            return X(this, e);
          }
          toImageData(e) {
            return le(this, e);
          }
          get data() {
            if (this.ensureValid(), !this.cpuData) throw new Error("The data is not on CPU. Use `getData()` to download GPU data to CPU, or use `texture` or `gpuBuffer` property to access the GPU data directly.");
            return this.cpuData;
          }
          get location() {
            return this.dataLocation;
          }
          get texture() {
            if (this.ensureValid(), !this.gpuTextureData) throw new Error("The data is not stored as a WebGL texture.");
            return this.gpuTextureData;
          }
          get gpuBuffer() {
            if (this.ensureValid(), !this.gpuBufferData) throw new Error("The data is not stored as a WebGPU buffer.");
            return this.gpuBufferData;
          }
          get mlTensor() {
            if (this.ensureValid(), !this.mlTensorData) throw new Error("The data is not stored as a WebNN MLTensor.");
            return this.mlTensorData;
          }
          async getData(e) {
            switch (this.ensureValid(), this.dataLocation) {
              case "cpu":
              case "cpu-pinned":
                return this.data;
              case "texture":
              case "gpu-buffer":
              case "ml-tensor": {
                if (!this.downloader) throw new Error("The current tensor is not created with a specified data downloader.");
                if (this.isDownloading) throw new Error("The current tensor is being downloaded.");
                try {
                  this.isDownloading = !0;
                  let t = await this.downloader();
                  return this.downloader = void 0, this.dataLocation = "cpu", this.cpuData = t, e && this.disposer && (this.disposer(), this.disposer = void 0), t;
                } finally {
                  this.isDownloading = !1;
                }
              }
              default:
                throw new Error(`cannot get data from location: ${this.dataLocation}`);
            }
          }
          dispose() {
            if (this.isDownloading) throw new Error("The current tensor is being downloaded.");
            this.disposer && (this.disposer(), this.disposer = void 0), this.cpuData = void 0, this.gpuTextureData = void 0, this.gpuBufferData = void 0, this.mlTensorData = void 0, this.downloader = void 0, this.isDownloading = void 0, this.dataLocation = "none";
          }
          ensureValid() {
            if (this.dataLocation === "none") throw new Error("The tensor is disposed.");
          }
          reshape(e) {
            if (this.ensureValid(), this.downloader || this.disposer) throw new Error("Cannot reshape a tensor that owns GPU resource.");
            return Xe(this, e);
          }
        };
      }), ce, ve = w(() => {
        De(), ce = Je;
      }), Re, je, Ve, Ne, Ze = w(() => {
        C(), Re = (e, t) => {
          (typeof g.trace > "u" ? !g.wasm.trace : !g.trace) || console.timeStamp(`${e}::ORT::${t}`);
        }, je = (e, t) => {
          var i;
          let r = ((i = new Error().stack) == null ? void 0 : i.split(/\r\n|\r|\n/g)) || [], n = !1;
          for (let a = 0; a < r.length; a++) {
            if (n && !r[a].includes("TRACE_FUNC")) {
              let o = `FUNC_${e}::${r[a].trim().split(" ")[1]}`;
              t && (o += `::${t}`), Re("CPU", o);
              return;
            }
            r[a].includes("TRACE_FUNC") && (n = !0);
          }
        }, Ve = (e) => {
          (typeof g.trace > "u" ? !g.wasm.trace : !g.trace) || je("BEGIN", e);
        }, Ne = (e) => {
          (typeof g.trace > "u" ? !g.wasm.trace : !g.trace) || je("END", e);
        };
      }), at, ft = w(() => {
        z(), ve(), Ze(), at = class r_ {
          constructor(t) {
            this.handler = t;
          }
          async run(t, r, n) {
            Ve();
            let i = {}, a = {};
            if (typeof t != "object" || t === null || t instanceof ce || Array.isArray(t)) throw new TypeError("'feeds' must be an object that use input names as keys and OnnxValue as corresponding values.");
            let o = !0;
            if (typeof r == "object") {
              if (r === null) throw new TypeError("Unexpected argument[1]: cannot be null.");
              if (r instanceof ce) throw new TypeError("'fetches' cannot be a Tensor");
              if (Array.isArray(r)) {
                if (r.length === 0) throw new TypeError("'fetches' cannot be an empty array.");
                o = !1;
                for (let h of r) {
                  if (typeof h != "string") throw new TypeError("'fetches' must be a string array or an object.");
                  if (this.outputNames.indexOf(h) === -1) throw new RangeError(`'fetches' contains invalid output name: ${h}.`);
                  i[h] = null;
                }
                if (typeof n == "object" && n !== null) a = n;
                else if (typeof n < "u") throw new TypeError("'options' must be an object.");
              } else {
                let h = !1, k = Object.getOwnPropertyNames(r);
                for (let S of this.outputNames) if (k.indexOf(S) !== -1) {
                  let u = r[S];
                  (u === null || u instanceof ce) && (h = !0, o = !1, i[S] = u);
                }
                if (h) {
                  if (typeof n == "object" && n !== null) a = n;
                  else if (typeof n < "u") throw new TypeError("'options' must be an object.");
                } else a = r;
              }
            } else if (typeof r < "u") throw new TypeError("Unexpected argument[1]: must be 'fetches' or 'options'.");
            for (let h of this.inputNames) if (typeof t[h] > "u") throw new Error(`input '${h}' is missing in 'feeds'.`);
            if (o) for (let h of this.outputNames) i[h] = null;
            let d = await this.handler.run(t, i, a), p = {};
            for (let h in d) if (Object.hasOwnProperty.call(d, h)) {
              let k = d[h];
              k instanceof ce ? p[h] = k : p[h] = new ce(k.type, k.data, k.dims);
            }
            return Ne(), p;
          }
          async release() {
            return this.handler.dispose();
          }
          static async create(t, r, n, i) {
            Ve();
            let a, o = {};
            if (typeof t == "string") {
              if (a = t, typeof r == "object" && r !== null) o = r;
              else if (typeof r < "u") throw new TypeError("'options' must be an object.");
            } else if (t instanceof Uint8Array) {
              if (a = t, typeof r == "object" && r !== null) o = r;
              else if (typeof r < "u") throw new TypeError("'options' must be an object.");
            } else if (t instanceof ArrayBuffer || typeof SharedArrayBuffer < "u" && t instanceof SharedArrayBuffer) {
              let k = t, S = 0, u = t.byteLength;
              if (typeof r == "object" && r !== null) o = r;
              else if (typeof r == "number") {
                if (S = r, !Number.isSafeInteger(S)) throw new RangeError("'byteOffset' must be an integer.");
                if (S < 0 || S >= k.byteLength) throw new RangeError(`'byteOffset' is out of range [0, ${k.byteLength}).`);
                if (u = t.byteLength - S, typeof n == "number") {
                  if (u = n, !Number.isSafeInteger(u)) throw new RangeError("'byteLength' must be an integer.");
                  if (u <= 0 || S + u > k.byteLength) throw new RangeError(`'byteLength' is out of range (0, ${k.byteLength - S}].`);
                  if (typeof i == "object" && i !== null) o = i;
                  else if (typeof i < "u") throw new TypeError("'options' must be an object.");
                } else if (typeof n < "u") throw new TypeError("'byteLength' must be a number.");
              } else if (typeof r < "u") throw new TypeError("'options' must be an object.");
              a = new Uint8Array(k, S, u);
            } else throw new TypeError("Unexpected argument[0]: must be 'path' or 'buffer'.");
            let [d, p] = await oe(o), h = await d.createInferenceSessionHandler(a, p);
            return Ne(), new r_(h);
          }
          startProfiling() {
            this.handler.startProfiling();
          }
          endProfiling() {
            this.handler.endProfiling();
          }
          get inputNames() {
            return this.handler.inputNames;
          }
          get outputNames() {
            return this.handler.outputNames;
          }
        };
      }), dt, gt = w(() => {
        ft(), dt = at;
      }), F = w(() => {
      }), ne = w(() => {
      }), K = w(() => {
      }), pe = w(() => {
      }), Oe = {};
      x(Oe, { InferenceSession: () => dt, TRACE: () => Re, TRACE_FUNC_BEGIN: () => Ve, TRACE_FUNC_END: () => Ne, Tensor: () => ce, env: () => v, registerBackend: () => q });
      var Qe = w(() => {
        V(), ee(), gt(), ve(), F(), ne(), Ze(), K(), pe();
      }), st = w(() => {
      }), pt = {};
      x(pt, { default: () => Ot });
      var It, St, Ot, At = w(() => {
        var e;
        yh(), Ft(), Ss(), It = "ort-wasm-proxy-worker", St = ((e = globalThis.self) == null ? void 0 : e.name) === It, St && (self.onmessage = (t) => {
          let { type: r, in: n } = t.data;
          try {
            switch (r) {
              case "init-wasm":
                nt(n.wasm).then(() => {
                  wc(n).then(() => {
                    postMessage({ type: r });
                  }, (i) => {
                    postMessage({ type: r, err: i });
                  });
                }, (i) => {
                  postMessage({ type: r, err: i });
                });
                break;
              case "init-ep": {
                let { epName: i, env: a } = n;
                qs(a, i).then(() => {
                  postMessage({ type: r });
                }, (o) => {
                  postMessage({ type: r, err: o });
                });
                break;
              }
              case "copy-from": {
                let { buffer: i } = n, a = fp(i);
                postMessage({ type: r, out: a });
                break;
              }
              case "create": {
                let { model: i, options: a } = n;
                Rp(i, a).then((o) => {
                  postMessage({ type: r, out: o });
                }, (o) => {
                  postMessage({ type: r, err: o });
                });
                break;
              }
              case "release":
                Np(n), postMessage({ type: r });
                break;
              case "run": {
                let { sessionId: i, inputIndices: a, inputs: o, outputIndices: d, options: p } = n;
                Up(i, a, o, d, new Array(d.length).fill(null), p).then((h) => {
                  h.some((k) => k[3] !== "cpu") ? postMessage({ type: r, err: "Proxy does not support non-cpu tensor location." }) : postMessage({ type: r, out: h }, Vp([...o, ...h]));
                }, (h) => {
                  postMessage({ type: r, err: h });
                });
                break;
              }
              case "end-profiling":
                Wp(n), postMessage({ type: r });
                break;
              default:
            }
          } catch (i) {
            postMessage({ type: r, err: i });
          }
        }), Ot = St ? null : (t) => new Worker(t ?? is, { type: "module", name: It });
      }), nr = {};
      x(nr, { default: () => Ar });
      var gr, kr, Ar, Qr = w(() => {
        var e;
        kr = (gr = import.meta.url, async function(t = {}) {
          function r() {
            return jt.buffer != Gt.buffer && Er(), Gt;
          }
          function n() {
            return jt.buffer != Gt.buffer && Er(), Cr;
          }
          function i() {
            return jt.buffer != Gt.buffer && Er(), it;
          }
          function a() {
            return jt.buffer != Gt.buffer && Er(), Et;
          }
          function o() {
            return jt.buffer != Gt.buffer && Er(), cr;
          }
          function d() {
            return jt.buffer != Gt.buffer && Er(), Lr;
          }
          function p() {
            return jt.buffer != Gt.buffer && Er(), ys;
          }
          function h() {
            return jt.buffer != Gt.buffer && Er(), Ii;
          }
          var k, S, u = Object.assign({}, t), B = new Promise((l, m) => {
            k = l, S = m;
          }), R = typeof window == "object", N = typeof importScripts == "function", Z = N && self.name == "em-pthread";
          u.mountExternalData = (l, m) => {
            l.startsWith("./") && (l = l.substring(2)), (u.Fb || (u.Fb = /* @__PURE__ */ new Map())).set(l, m);
          }, u.unmountExternalData = () => {
            delete u.Fb;
          };
          var te = globalThis.SharedArrayBuffer ?? new WebAssembly.Memory({ initial: 0, maximum: 0, shared: !0 }).buffer.constructor;
          let Q = () => {
            let l = (E, I, G) => (...we) => {
              let Ke = pn, tt = I == null ? void 0 : I();
              we = E(...we);
              let bt = I == null ? void 0 : I();
              return tt !== bt && (E = bt, G(tt), I = G = null), pn != Ke ? new Promise((kt, Vt) => {
                dh = { resolve: kt, reject: Vt };
              }) : we;
            }, m = (E) => async (...I) => {
              var G;
              try {
                if (u.Gb) throw Error("Session already started");
                let we = u.Gb = { hc: I[0], errors: [] }, Ke = await E(...I);
                if (u.Gb !== we) throw Error("Session mismatch");
                (G = u.Hb) == null || G.flush();
                let tt = we.errors;
                if (0 < tt.length) {
                  let bt = await Promise.all(tt);
                  if (bt = bt.filter((kt) => kt), 0 < bt.length) throw Error(bt.join(`
`));
                }
                return Ke;
              } finally {
                u.Gb = null;
              }
            };
            u._OrtCreateSession = l(u._OrtCreateSession, () => u._OrtCreateSession, (E) => u._OrtCreateSession = E), u._OrtRun = m(l(u._OrtRun, () => u._OrtRun, (E) => u._OrtRun = E)), u._OrtRunWithBinding = m(l(u._OrtRunWithBinding, () => u._OrtRunWithBinding, (E) => u._OrtRunWithBinding = E)), u._OrtBindInput = l(u._OrtBindInput, () => u._OrtBindInput, (E) => u._OrtBindInput = E), Q = void 0;
          };
          u.jsepInit = (l, m) => {
            if (Q == null || Q(), l === "webgpu") {
              [u.Hb, u.Vb, u.Zb, u.Ob, u.Yb, u.kb, u.$b, u.cc, u.Wb, u.Xb, u.ac] = m;
              let E = u.Hb;
              u.jsepRegisterBuffer = (I, G, we, Ke) => E.registerBuffer(I, G, we, Ke), u.jsepGetBuffer = (I) => E.getBuffer(I), u.jsepCreateDownloader = (I, G, we) => E.createDownloader(I, G, we), u.jsepOnCreateSession = (I) => {
                E.onCreateSession(I);
              }, u.jsepOnReleaseSession = (I) => {
                E.onReleaseSession(I);
              }, u.jsepOnRunStart = (I) => E.onRunStart(I), u.dc = (I, G) => {
                E.upload(I, G);
              };
            } else if (l === "webnn") {
              [u.Hb, u.bc, u.Pb, u.jsepEnsureTensor, u.ec, u.jsepDownloadTensor] = m, u.jsepReleaseTensorId = u.Pb;
              let E = u.Hb;
              u.jsepOnRunStart = (I) => E.onRunStart(I), u.jsepRegisterMLContext = (I, G) => {
                E.registerMLContext(I, G);
              }, u.jsepOnReleaseSession = (I) => {
                E.onReleaseSession(I);
              }, u.jsepCreateMLTensorDownloader = (I, G) => E.createMLTensorDownloader(I, G), u.jsepRegisterMLTensor = (I, G, we) => E.registerMLTensor(I, G, we), u.jsepCreateMLContext = (I) => E.createMLContext(I), u.jsepRegisterMLConstant = (I, G, we, Ke, tt) => E.registerMLConstant(I, G, we, Ke, tt, u.Fb);
            }
          };
          var _e, me, ye = Object.assign({}, u), Ae = "./this.program", Ie = (l, m) => {
            throw m;
          }, Ge = "";
          (R || N) && (N ? Ge = self.location.href : typeof document < "u" && document.currentScript && (Ge = document.currentScript.src), gr && (Ge = gr), Ge = Ge.startsWith("blob:") ? "" : Ge.substr(0, Ge.replace(/[?#].*/, "").lastIndexOf("/") + 1), N && (me = (l) => {
            var m = new XMLHttpRequest();
            return m.open("GET", l, !1), m.responseType = "arraybuffer", m.send(null), new Uint8Array(m.response);
          }), _e = (l, m, E) => {
            var I = new XMLHttpRequest();
            I.open("GET", l, !0), I.responseType = "arraybuffer", I.onload = () => {
              I.status == 200 || I.status == 0 && I.response ? m(I.response) : E();
            }, I.onerror = E, I.send(null);
          });
          var lt, Tt = console.log.bind(console), Kt = console.error.bind(console), Yt = Tt, Ct = Kt;
          if (Object.assign(u, ye), ye = null, Z) {
            let l = function(m) {
              try {
                var E = m.data, I = E.cmd;
                if (I === "load") {
                  let G = [];
                  self.onmessage = (we) => G.push(we), self.startWorker = () => {
                    postMessage({ cmd: "loaded" });
                    for (let we of G) l(we);
                    self.onmessage = l;
                  };
                  for (let we of E.handlers) u[we] && !u[we].proxy || (u[we] = (...Ke) => {
                    postMessage({ Nb: "callHandler", pc: we, args: Ke });
                  }, we == "print" && (Yt = u[we]), we == "printErr" && (Ct = u[we]));
                  jt = E.wasmMemory, Er(), Jt(E.wasmModule);
                } else if (I === "run") {
                  mh(E.pthread_ptr, 0, 0, 1, 0, 0), lh(E.pthread_ptr), d_(), Wh(), $t || (Nm(), $t = !0);
                  try {
                    c_(E.start_routine, E.arg);
                  } catch (G) {
                    if (G != "unwind") throw G;
                  }
                } else I === "cancel" ? wa() && $p(-1) : E.target !== "setimmediate" && (I === "checkMailbox" ? $t && Mp() : I && (Ct(`worker: received unknown command ${I}`), Ct(E)));
              } catch (G) {
                throw jm(), G;
              }
            };
            var Jt, $t = !1;
            Ct = function(...m) {
              m = m.join(" "), console.error(m);
            }, self.alert = function(...m) {
              postMessage({ Nb: "alert", text: m.join(" "), rc: wa() });
            }, u.instantiateWasm = (m, E) => new Promise((I) => {
              Jt = (G) => {
                G = new WebAssembly.Instance(G, Bh()), E(G), I();
              };
            }), self.onunhandledrejection = (m) => {
              throw m.reason || m;
            }, self.onmessage = l;
          }
          u.wasmBinary && (lt = u.wasmBinary);
          var jt, vr, Ht, Gt, Cr, it, Et, cr, Lr, ys, Hr, un, Ii, Ir = !1;
          function Er() {
            var l = jt.buffer;
            u.HEAP8 = Gt = new Int8Array(l), u.HEAP16 = it = new Int16Array(l), u.HEAPU8 = Cr = new Uint8Array(l), u.HEAPU16 = Et = new Uint16Array(l), u.HEAP32 = cr = new Int32Array(l), u.HEAPU32 = Lr = new Uint32Array(l), u.HEAPF32 = ys = new Float32Array(l), u.HEAPF64 = Ii = new Float64Array(l), u.HEAP64 = Hr = new BigInt64Array(l), u.HEAPU64 = un = new BigUint64Array(l);
          }
          if (!Z) {
            if (!((jt = new WebAssembly.Memory({ initial: 256, maximum: 65536, shared: !0 })).buffer instanceof te)) throw Ct("requested a shared WebAssembly.Memory but the returned buffer is not a SharedArrayBuffer, indicating that while the browser has SharedArrayBuffer it does not have WebAssembly threads support - you may need to set a flag"), Error("bad memory");
            Er();
          }
          var Wn = [], bc = [], Xp = [], vc = 0, xc = null;
          function Fh() {
            if (--vc == 0 && xc) {
              var l = xc;
              xc = null, l();
            }
          }
          function Vn(l) {
            throw Ct(l = "Aborted(" + l + ")"), Ir = !0, Ht = 1, l = new WebAssembly.RuntimeError(l + ". Build with -sASSERTIONS for more info."), S(l), l;
          }
          var Qp, Oh = (l) => l.startsWith("data:application/octet-stream;base64,"), Dh = (l) => l.startsWith("file://");
          function Lh(l) {
            if (l == Qp && lt) return new Uint8Array(lt);
            if (me) return me(l);
            throw "both async and sync fetching of the wasm failed";
          }
          function zh(l, m, E) {
            return function(I) {
              if (!lt && (R || N)) {
                if (typeof fetch == "function" && !Dh(I)) return fetch(I, { credentials: "same-origin" }).then((G) => {
                  if (!G.ok) throw `failed to load wasm binary file at '${I}'`;
                  return G.arrayBuffer();
                }).catch(() => Lh(I));
                if (_e) return new Promise((G, we) => {
                  _e(I, (Ke) => G(new Uint8Array(Ke)), we);
                });
              }
              return Promise.resolve().then(() => Lh(I));
            }(l).then((I) => WebAssembly.instantiate(I, m)).then(E, (I) => {
              Ct(`failed to asynchronously prepare wasm: ${I}`), Vn(I);
            });
          }
          function Bh() {
            return { a: { O: u_, Aa: l_, b: h_, aa: Hh, B: Qh, qa: Yh, Y: Zh, _: em, ra: tm, oa: rm, ha: sm, na: nm, L: im, Z: om, W: am, pa: lm, X: um, va: m_, F: __, Q: f_, P: w_, E: M_, u: b_, q: v_, G: x_, A: $_, R: A_, ua: I_, ka: F_, U: O_, ba: D_, H: L_, ja: lh, ta: z_, t: B_, Ba: R_, x: U_, n: W_, l: G_, c: oh, o: K_, j: X_, w: Q_, p: Y_, f: J_, s: Z_, m: ef, e: tf, k: rf, i: sf, h: nf, d: of, ea: af, fa: lf, ga: uf, ca: xm, da: Tm, T: df, g: cf, D: pf, I: hf, M: mf, y: _f, sa: ff, V: gf, v: Pm, z: wf, N: yf, S: Mf, za: bf, ya: vf, la: Sm, ma: $m, $: th, C: Am, K: Im, ia: Fm, J: Om, a: jt, xa: eh, wa: zm, r: Ef } };
          }
          var Yp = { 874308: (l, m, E, I, G) => {
            if (u === void 0 || !u.Fb) return 1;
            if ((l = zr(Number(l >>> 0))).startsWith("./") && (l = l.substring(2)), !(l = u.Fb.get(l))) return 2;
            if (m = Number(m >>> 0), E = Number(E >>> 0), I = Number(I >>> 0), m + E > l.byteLength) return 3;
            try {
              let we = l.subarray(m, m + E);
              switch (G) {
                case 0:
                  n().set(we, I >>> 0);
                  break;
                case 1:
                  u.dc(I, we);
                  break;
                default:
                  return 4;
              }
              return 0;
            } catch {
              return 4;
            }
          }, 875023: (l, m, E) => {
            u.ec(l, n().subarray(m >>> 0, m + E >>> 0));
          }, 875086: () => u.bc(), 875127: (l) => {
            u.Pb(l);
          }, 875163: () => {
            u.Wb();
          }, 875194: () => {
            u.Xb();
          }, 875223: () => {
            u.ac();
          }, 875248: (l) => u.Vb(l), 875281: (l) => u.Zb(l), 875313: (l, m, E) => {
            u.Ob(Number(l), Number(m), Number(E), !0);
          }, 875376: (l, m, E) => {
            u.Ob(Number(l), Number(m), Number(E));
          }, 875433: () => typeof wasmOffsetConverter < "u", 875490: (l) => {
            u.kb("Abs", l, void 0);
          }, 875541: (l) => {
            u.kb("Neg", l, void 0);
          }, 875592: (l) => {
            u.kb("Floor", l, void 0);
          }, 875645: (l) => {
            u.kb("Ceil", l, void 0);
          }, 875697: (l) => {
            u.kb("Reciprocal", l, void 0);
          }, 875755: (l) => {
            u.kb("Sqrt", l, void 0);
          }, 875807: (l) => {
            u.kb("Exp", l, void 0);
          }, 875858: (l) => {
            u.kb("Erf", l, void 0);
          }, 875909: (l) => {
            u.kb("Sigmoid", l, void 0);
          }, 875964: (l, m, E) => {
            u.kb("HardSigmoid", l, { alpha: m, beta: E });
          }, 876043: (l) => {
            u.kb("Log", l, void 0);
          }, 876094: (l) => {
            u.kb("Sin", l, void 0);
          }, 876145: (l) => {
            u.kb("Cos", l, void 0);
          }, 876196: (l) => {
            u.kb("Tan", l, void 0);
          }, 876247: (l) => {
            u.kb("Asin", l, void 0);
          }, 876299: (l) => {
            u.kb("Acos", l, void 0);
          }, 876351: (l) => {
            u.kb("Atan", l, void 0);
          }, 876403: (l) => {
            u.kb("Sinh", l, void 0);
          }, 876455: (l) => {
            u.kb("Cosh", l, void 0);
          }, 876507: (l) => {
            u.kb("Asinh", l, void 0);
          }, 876560: (l) => {
            u.kb("Acosh", l, void 0);
          }, 876613: (l) => {
            u.kb("Atanh", l, void 0);
          }, 876666: (l) => {
            u.kb("Tanh", l, void 0);
          }, 876718: (l) => {
            u.kb("Not", l, void 0);
          }, 876769: (l, m, E) => {
            u.kb("Clip", l, { min: m, max: E });
          }, 876838: (l) => {
            u.kb("Clip", l, void 0);
          }, 876890: (l, m) => {
            u.kb("Elu", l, { alpha: m });
          }, 876948: (l) => {
            u.kb("Gelu", l, void 0);
          }, 877e3: (l) => {
            u.kb("Relu", l, void 0);
          }, 877052: (l, m) => {
            u.kb("LeakyRelu", l, { alpha: m });
          }, 877116: (l, m) => {
            u.kb("ThresholdedRelu", l, { alpha: m });
          }, 877186: (l, m) => {
            u.kb("Cast", l, { to: m });
          }, 877244: (l) => {
            u.kb("Add", l, void 0);
          }, 877295: (l) => {
            u.kb("Sub", l, void 0);
          }, 877346: (l) => {
            u.kb("Mul", l, void 0);
          }, 877397: (l) => {
            u.kb("Div", l, void 0);
          }, 877448: (l) => {
            u.kb("Pow", l, void 0);
          }, 877499: (l) => {
            u.kb("Equal", l, void 0);
          }, 877552: (l) => {
            u.kb("Greater", l, void 0);
          }, 877607: (l) => {
            u.kb("GreaterOrEqual", l, void 0);
          }, 877669: (l) => {
            u.kb("Less", l, void 0);
          }, 877721: (l) => {
            u.kb("LessOrEqual", l, void 0);
          }, 877780: (l, m, E, I, G) => {
            u.kb("ReduceMean", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 877955: (l, m, E, I, G) => {
            u.kb("ReduceMax", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 878129: (l, m, E, I, G) => {
            u.kb("ReduceMin", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 878303: (l, m, E, I, G) => {
            u.kb("ReduceProd", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 878478: (l, m, E, I, G) => {
            u.kb("ReduceSum", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 878652: (l, m, E, I, G) => {
            u.kb("ReduceL1", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 878825: (l, m, E, I, G) => {
            u.kb("ReduceL2", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 878998: (l, m, E, I, G) => {
            u.kb("ReduceLogSum", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 879175: (l, m, E, I, G) => {
            u.kb("ReduceSumSquare", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 879355: (l, m, E, I, G) => {
            u.kb("ReduceLogSumExp", l, { keepDims: !!m, noopWithEmptyAxes: !!E, axes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 879535: (l) => {
            u.kb("Where", l, void 0);
          }, 879588: (l, m, E) => {
            u.kb("Transpose", l, { perm: m ? Array.from(o().subarray(Number(m) >>> 0, Number(E) >>> 0)) : [] });
          }, 879712: (l, m, E, I) => {
            u.kb("DepthToSpace", l, { blocksize: m, mode: zr(E), format: I ? "NHWC" : "NCHW" });
          }, 879845: (l, m, E, I) => {
            u.kb("DepthToSpace", l, { blocksize: m, mode: zr(E), format: I ? "NHWC" : "NCHW" });
          }, 879978: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We, ar) => {
            u.kb("ConvTranspose", l, { format: bt ? "NHWC" : "NCHW", autoPad: m, dilations: [E], group: I, kernelShape: [G], pads: [we, Ke], strides: [tt], wIsConst: () => !!r()[kt >>> 0], outputPadding: Vt ? Array.from(o().subarray(Number(Vt) >>> 0, Number(pr) >>> 0)) : [], outputShape: Mr ? Array.from(o().subarray(Number(Mr) >>> 0, Number(We) >>> 0)) : [], activation: zr(ar) });
          }, 880411: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We) => {
            u.kb("ConvTranspose", l, { format: tt ? "NHWC" : "NCHW", autoPad: m, dilations: Array.from(o().subarray(Number(E) >>> 0, 2 + (Number(E) >>> 0) >>> 0)), group: I, kernelShape: Array.from(o().subarray(Number(G) >>> 0, 2 + (Number(G) >>> 0) >>> 0)), pads: Array.from(o().subarray(Number(we) >>> 0, 4 + (Number(we) >>> 0) >>> 0)), strides: Array.from(o().subarray(Number(Ke) >>> 0, 2 + (Number(Ke) >>> 0) >>> 0)), wIsConst: () => !!r()[bt >>> 0], outputPadding: kt ? Array.from(o().subarray(Number(kt) >>> 0, Number(Vt) >>> 0)) : [], outputShape: pr ? Array.from(o().subarray(Number(pr) >>> 0, Number(Mr) >>> 0)) : [], activation: zr(We) });
          }, 881072: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We, ar) => {
            u.kb("ConvTranspose", l, { format: bt ? "NHWC" : "NCHW", autoPad: m, dilations: [E], group: I, kernelShape: [G], pads: [we, Ke], strides: [tt], wIsConst: () => !!r()[kt >>> 0], outputPadding: Vt ? Array.from(o().subarray(Number(Vt) >>> 0, Number(pr) >>> 0)) : [], outputShape: Mr ? Array.from(o().subarray(Number(Mr) >>> 0, Number(We) >>> 0)) : [], activation: zr(ar) });
          }, 881505: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We) => {
            u.kb("ConvTranspose", l, { format: tt ? "NHWC" : "NCHW", autoPad: m, dilations: Array.from(o().subarray(Number(E) >>> 0, 2 + (Number(E) >>> 0) >>> 0)), group: I, kernelShape: Array.from(o().subarray(Number(G) >>> 0, 2 + (Number(G) >>> 0) >>> 0)), pads: Array.from(o().subarray(Number(we) >>> 0, 4 + (Number(we) >>> 0) >>> 0)), strides: Array.from(o().subarray(Number(Ke) >>> 0, 2 + (Number(Ke) >>> 0) >>> 0)), wIsConst: () => !!r()[bt >>> 0], outputPadding: kt ? Array.from(o().subarray(Number(kt) >>> 0, Number(Vt) >>> 0)) : [], outputShape: pr ? Array.from(o().subarray(Number(pr) >>> 0, Number(Mr) >>> 0)) : [], activation: zr(We) });
          }, 882166: (l, m) => {
            u.kb("GlobalAveragePool", l, { format: m ? "NHWC" : "NCHW" });
          }, 882257: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We) => {
            u.kb("AveragePool", l, { format: We ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: I, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Ke) >>> 0)) : [], kernel_shape: tt ? Array.from(o().subarray(Number(tt) >>> 0, Number(bt) >>> 0)) : [], pads: kt ? Array.from(o().subarray(Number(kt) >>> 0, Number(Vt) >>> 0)) : [], strides: pr ? Array.from(o().subarray(Number(pr) >>> 0, Number(Mr) >>> 0)) : [] });
          }, 882736: (l, m) => {
            u.kb("GlobalAveragePool", l, { format: m ? "NHWC" : "NCHW" });
          }, 882827: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We) => {
            u.kb("AveragePool", l, { format: We ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: I, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Ke) >>> 0)) : [], kernel_shape: tt ? Array.from(o().subarray(Number(tt) >>> 0, Number(bt) >>> 0)) : [], pads: kt ? Array.from(o().subarray(Number(kt) >>> 0, Number(Vt) >>> 0)) : [], strides: pr ? Array.from(o().subarray(Number(pr) >>> 0, Number(Mr) >>> 0)) : [] });
          }, 883306: (l, m) => {
            u.kb("GlobalMaxPool", l, { format: m ? "NHWC" : "NCHW" });
          }, 883393: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We) => {
            u.kb("MaxPool", l, { format: We ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: I, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Ke) >>> 0)) : [], kernel_shape: tt ? Array.from(o().subarray(Number(tt) >>> 0, Number(bt) >>> 0)) : [], pads: kt ? Array.from(o().subarray(Number(kt) >>> 0, Number(Vt) >>> 0)) : [], strides: pr ? Array.from(o().subarray(Number(pr) >>> 0, Number(Mr) >>> 0)) : [] });
          }, 883868: (l, m) => {
            u.kb("GlobalMaxPool", l, { format: m ? "NHWC" : "NCHW" });
          }, 883955: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We) => {
            u.kb("MaxPool", l, { format: We ? "NHWC" : "NCHW", auto_pad: m, ceil_mode: E, count_include_pad: I, storage_order: G, dilations: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Ke) >>> 0)) : [], kernel_shape: tt ? Array.from(o().subarray(Number(tt) >>> 0, Number(bt) >>> 0)) : [], pads: kt ? Array.from(o().subarray(Number(kt) >>> 0, Number(Vt) >>> 0)) : [], strides: pr ? Array.from(o().subarray(Number(pr) >>> 0, Number(Mr) >>> 0)) : [] });
          }, 884430: (l, m, E, I, G) => {
            u.kb("Gemm", l, { alpha: m, beta: E, transA: I, transB: G });
          }, 884534: (l) => {
            u.kb("MatMul", l, void 0);
          }, 884588: (l, m, E, I) => {
            u.kb("ArgMax", l, { keepDims: !!m, selectLastIndex: !!E, axis: I });
          }, 884696: (l, m, E, I) => {
            u.kb("ArgMin", l, { keepDims: !!m, selectLastIndex: !!E, axis: I });
          }, 884804: (l, m) => {
            u.kb("Softmax", l, { axis: m });
          }, 884867: (l, m) => {
            u.kb("Concat", l, { axis: m });
          }, 884927: (l, m, E, I, G) => {
            u.kb("Split", l, { axis: m, numOutputs: E, splitSizes: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 885083: (l) => {
            u.kb("Expand", l, void 0);
          }, 885137: (l, m) => {
            u.kb("Gather", l, { axis: Number(m) });
          }, 885208: (l, m) => {
            u.kb("GatherElements", l, { axis: Number(m) });
          }, 885287: (l, m) => {
            u.kb("GatherND", l, { batch_dims: Number(m) });
          }, 885366: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt) => {
            u.kb("Resize", l, { antialias: m, axes: E ? Array.from(o().subarray(Number(E) >>> 0, Number(I) >>> 0)) : [], coordinateTransformMode: zr(G), cubicCoeffA: we, excludeOutside: Ke, extrapolationValue: tt, keepAspectRatioPolicy: zr(bt), mode: zr(kt), nearestMode: zr(Vt) });
          }, 885728: (l, m, E, I, G, we, Ke) => {
            u.kb("Slice", l, { starts: m ? Array.from(o().subarray(Number(m) >>> 0, Number(E) >>> 0)) : [], ends: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [], axes: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Ke) >>> 0)) : [] });
          }, 885992: (l) => {
            u.kb("Tile", l, void 0);
          }, 886044: (l, m, E) => {
            u.kb("InstanceNormalization", l, { epsilon: m, format: E ? "NHWC" : "NCHW" });
          }, 886158: (l, m, E) => {
            u.kb("InstanceNormalization", l, { epsilon: m, format: E ? "NHWC" : "NCHW" });
          }, 886272: (l) => {
            u.kb("Range", l, void 0);
          }, 886325: (l, m) => {
            u.kb("Einsum", l, { equation: zr(m) });
          }, 886406: (l, m, E, I, G) => {
            u.kb("Pad", l, { mode: m, value: E, pads: I ? Array.from(o().subarray(Number(I) >>> 0, Number(G) >>> 0)) : [] });
          }, 886549: (l, m, E, I, G, we) => {
            u.kb("BatchNormalization", l, { epsilon: m, momentum: E, spatial: !!G, trainingMode: !!I, format: we ? "NHWC" : "NCHW" });
          }, 886718: (l, m, E, I, G, we) => {
            u.kb("BatchNormalization", l, { epsilon: m, momentum: E, spatial: !!G, trainingMode: !!I, format: we ? "NHWC" : "NCHW" });
          }, 886887: (l, m, E) => {
            u.kb("CumSum", l, { exclusive: Number(m), reverse: Number(E) });
          }, 886984: (l, m, E) => {
            u.kb("DequantizeLinear", l, { axis: m, blockSize: E });
          }, 887074: (l, m, E, I, G) => {
            u.kb("GridSample", l, { align_corners: m, mode: zr(E), padding_mode: zr(I), format: G ? "NHWC" : "NCHW" });
          }, 887244: (l, m, E, I, G) => {
            u.kb("GridSample", l, { align_corners: m, mode: zr(E), padding_mode: zr(I), format: G ? "NHWC" : "NCHW" });
          }, 887414: (l, m) => {
            u.kb("ScatterND", l, { reduction: zr(m) });
          }, 887499: (l, m, E, I, G, we, Ke, tt, bt) => {
            u.kb("Attention", l, { numHeads: m, isUnidirectional: E, maskFilterValue: I, scale: G, doRotary: we, qkvHiddenSizes: Ke ? Array.from(o().subarray(Number(tt) >>> 0, Number(tt) + Ke >>> 0)) : [], pastPresentShareBuffer: !!bt });
          }, 887771: (l) => {
            u.kb("BiasAdd", l, void 0);
          }, 887826: (l) => {
            u.kb("BiasSplitGelu", l, void 0);
          }, 887887: (l) => {
            u.kb("FastGelu", l, void 0);
          }, 887943: (l, m, E, I, G, we, Ke, tt, bt, kt, Vt, pr, Mr, We, ar, Br) => {
            u.kb("Conv", l, { format: pr ? "NHWC" : "NCHW", auto_pad: m, dilations: E ? Array.from(o().subarray(Number(E) >>> 0, Number(I) >>> 0)) : [], group: G, kernel_shape: we ? Array.from(o().subarray(Number(we) >>> 0, Number(Ke) >>> 0)) : [], pads: tt ? Array.from(o().subarray(Number(tt) >>> 0, Number(bt) >>> 0)) : [], strides: kt ? Array.from(o().subarray(Number(kt) >>> 0, Number(Vt) >>> 0)) : [], w_is_const: () => !!r()[Number(Mr) >>> 0], activation: zr(We), activation_params: ar ? Array.from(p().subarray(Number(ar) >>> 0, Number(Br) >>> 0)) : [] });
          }, 888527: (l) => {
            u.kb("Gelu", l, void 0);
          }, 888579: (l, m, E, I, G, we, Ke, tt, bt) => {
            u.kb("GroupQueryAttention", l, { numHeads: m, kvNumHeads: E, scale: I, softcap: G, doRotary: we, rotaryInterleaved: Ke, smoothSoftmax: tt, localWindowSize: bt });
          }, 888796: (l, m, E, I) => {
            u.kb("LayerNormalization", l, { axis: m, epsilon: E, simplified: !!I });
          }, 888907: (l, m, E, I) => {
            u.kb("LayerNormalization", l, { axis: m, epsilon: E, simplified: !!I });
          }, 889018: (l, m, E, I, G, we) => {
            u.kb("MatMulNBits", l, { k: m, n: E, accuracyLevel: I, bits: G, blockSize: we });
          }, 889145: (l, m, E, I, G, we) => {
            u.kb("MultiHeadAttention", l, { numHeads: m, isUnidirectional: E, maskFilterValue: I, scale: G, doRotary: we });
          }, 889304: (l, m) => {
            u.kb("QuickGelu", l, { alpha: m });
          }, 889368: (l, m, E, I, G) => {
            u.kb("RotaryEmbedding", l, { interleaved: !!m, numHeads: E, rotaryEmbeddingDim: I, scale: G });
          }, 889507: (l, m, E) => {
            u.kb("SkipLayerNormalization", l, { epsilon: m, simplified: !!E });
          }, 889609: (l, m, E) => {
            u.kb("SkipLayerNormalization", l, { epsilon: m, simplified: !!E });
          }, 889711: (l, m, E, I) => {
            u.kb("GatherBlockQuantized", l, { gatherAxis: m, quantizeAxis: E, blockSize: I });
          }, 889832: (l) => {
            u.$b(l);
          }, 889866: (l, m) => u.cc(Number(l), Number(m), u.Gb.hc, u.Gb.errors) };
          function l_(l, m, E) {
            return wm(async () => {
              await u.Yb(Number(l), Number(m), Number(E));
            });
          }
          function u_() {
            return typeof wasmOffsetConverter < "u";
          }
          function Jp(l) {
            this.name = "ExitStatus", this.message = `Program terminated with exit(${l})`, this.status = l;
          }
          var Zp = (l) => {
            l.terminate(), l.onmessage = () => {
            };
          }, Rh = (l) => {
            Gn.length == 0 && (Gh(), Vh(Gn[0]));
            var m = Gn.pop();
            if (!m) return 6;
            Oi.push(m), dn[l.Bb] = m, m.Bb = l.Bb;
            var E = { cmd: "run", start_routine: l.ic, arg: l.Rb, pthread_ptr: l.Bb };
            return m.postMessage(E, l.nc), 0;
          }, Fi = 0, Fr = (l, m, ...E) => {
            for (var I = 2 * E.length, G = gh(), we = fh(8 * I), Ke = we >>> 3, tt = 0; tt < E.length; tt++) {
              var bt = E[tt];
              typeof bt == "bigint" ? (Hr[Ke + 2 * tt] = 1n, Hr[Ke + 2 * tt + 1] = bt) : (Hr[Ke + 2 * tt] = 0n, h()[Ke + 2 * tt + 1 >>> 0] = bt);
            }
            return l = Um(l, 0, I, we, m), Ap(G), l;
          };
          function eh(l) {
            if (Z) return Fr(0, 1, l);
            if (Ht = l, !(0 < Fi)) {
              for (var m of Oi) Zp(m);
              for (m of Gn) Zp(m);
              Gn = [], Oi = [], dn = [], Ir = !0;
            }
            Ie(l, new Jp(l));
          }
          function Nh(l) {
            if (Z) return Fr(1, 0, l);
            th(l);
          }
          var th = (l) => {
            if (Ht = l, Z) throw Nh(l), "unwind";
            eh(l);
          }, Gn = [], Oi = [], jh = [], dn = {}, Uh = (l) => {
            var m = l.Bb;
            delete dn[m], Gn.push(l), Oi.splice(Oi.indexOf(l), 1), l.Bb = 0, _h(m);
          };
          function Wh() {
            jh.forEach((l) => l());
          }
          var Vh = (l) => new Promise((m) => {
            l.onmessage = (G) => {
              var we = (G = G.data).cmd;
              if (G.targetThread && G.targetThread != wa()) {
                var Ke = dn[G.targetThread];
                Ke ? Ke.postMessage(G, G.transferList) : Ct(`Internal error! Worker sent a message "${we}" to target pthread ${G.targetThread}, but that thread no longer exists!`);
              } else we === "checkMailbox" ? Mp() : we === "spawnThread" ? Rh(G) : we === "cleanupThread" ? Uh(dn[G.thread]) : we === "killThread" ? (G = G.thread, we = dn[G], delete dn[G], Zp(we), _h(G), Oi.splice(Oi.indexOf(we), 1), we.Bb = 0) : we === "cancelThread" ? dn[G.thread].postMessage({ cmd: "cancel" }) : we === "loaded" ? (l.loaded = !0, m(l)) : we === "alert" ? alert(`Thread ${G.threadId}: ${G.text}`) : G.target === "setimmediate" ? l.postMessage(G) : we === "callHandler" ? u[G.handler](...G.args) : we && Ct(`worker sent an unknown command ${we}`);
            }, l.onerror = (G) => {
              throw Ct(`worker sent an error! ${G.filename}:${G.lineno}: ${G.message}`), G;
            };
            var E, I = [];
            for (E of []) u.hasOwnProperty(E) && I.push(E);
            l.postMessage({ cmd: "load", handlers: I, wasmMemory: jt, wasmModule: vr });
          });
          function Gh() {
            var l = new Worker(new URL(import.meta.url), { type: "module", workerData: "em-pthread", name: "em-pthread" });
            Gn.push(l);
          }
          var yp = (l) => {
            for (; 0 < l.length; ) l.shift()(u);
          }, d_ = () => {
            var l = wa(), m = d()[l + 52 >>> 2 >>> 0];
            l = d()[l + 56 >>> 2 >>> 0], Vm(m, m - l), Ap(m);
          }, c_ = (l, m) => {
            Fi = 0, l = Gm(l, m), 0 < Fi ? Ht = l : $p(l);
          };
          class p_ {
            constructor(m) {
              this.Kb = m - 24;
            }
          }
          function h_(l, m, E) {
            var I = new p_(l >>>= 0);
            throw m >>>= 0, E >>>= 0, d()[I.Kb + 16 >>> 2 >>> 0] = 0, d()[I.Kb + 4 >>> 2 >>> 0] = m, d()[I.Kb + 8 >>> 2 >>> 0] = E, l;
          }
          function Kh(l, m, E, I) {
            return Z ? Fr(2, 1, l, m, E, I) : Hh(l, m, E, I);
          }
          function Hh(l, m, E, I) {
            if (l >>>= 0, m >>>= 0, E >>>= 0, I >>>= 0, te === void 0) return Ct("Current environment does not support SharedArrayBuffer, pthreads are not available!"), 6;
            var G = [];
            return Z && G.length === 0 ? Kh(l, m, E, I) : (l = { ic: E, Bb: l, Rb: I, nc: G }, Z ? (l.Nb = "spawnThread", postMessage(l, G), 0) : Rh(l));
          }
          var qh = typeof TextDecoder < "u" ? new TextDecoder("utf8") : void 0, Xh = (l, m, E) => {
            var I = (m >>>= 0) + E;
            for (E = m; l[E] && !(E >= I); ) ++E;
            if (16 < E - m && l.buffer && qh) return qh.decode(l.buffer instanceof te ? l.slice(m, E) : l.subarray(m, E));
            for (I = ""; m < E; ) {
              var G = l[m++];
              if (128 & G) {
                var we = 63 & l[m++];
                if ((224 & G) == 192) I += String.fromCharCode((31 & G) << 6 | we);
                else {
                  var Ke = 63 & l[m++];
                  65536 > (G = (240 & G) == 224 ? (15 & G) << 12 | we << 6 | Ke : (7 & G) << 18 | we << 12 | Ke << 6 | 63 & l[m++]) ? I += String.fromCharCode(G) : (G -= 65536, I += String.fromCharCode(55296 | G >> 10, 56320 | 1023 & G));
                }
              } else I += String.fromCharCode(G);
            }
            return I;
          }, zr = (l, m) => (l >>>= 0) ? Xh(n(), l, m) : "";
          function Qh(l, m, E) {
            return Z ? Fr(3, 1, l, m, E) : 0;
          }
          function Yh(l, m) {
            if (Z) return Fr(4, 1, l, m);
          }
          var rh = (l) => {
            for (var m = 0, E = 0; E < l.length; ++E) {
              var I = l.charCodeAt(E);
              127 >= I ? m++ : 2047 >= I ? m += 2 : 55296 <= I && 57343 >= I ? (m += 4, ++E) : m += 3;
            }
            return m;
          }, Jh = (l, m, E, I) => {
            if (!(0 < I)) return 0;
            var G = E >>>= 0;
            I = E + I - 1;
            for (var we = 0; we < l.length; ++we) {
              var Ke = l.charCodeAt(we);
              if (55296 <= Ke && 57343 >= Ke && (Ke = 65536 + ((1023 & Ke) << 10) | 1023 & l.charCodeAt(++we)), 127 >= Ke) {
                if (E >= I) break;
                m[E++ >>> 0] = Ke;
              } else {
                if (2047 >= Ke) {
                  if (E + 1 >= I) break;
                  m[E++ >>> 0] = 192 | Ke >> 6;
                } else {
                  if (65535 >= Ke) {
                    if (E + 2 >= I) break;
                    m[E++ >>> 0] = 224 | Ke >> 12;
                  } else {
                    if (E + 3 >= I) break;
                    m[E++ >>> 0] = 240 | Ke >> 18, m[E++ >>> 0] = 128 | Ke >> 12 & 63;
                  }
                  m[E++ >>> 0] = 128 | Ke >> 6 & 63;
                }
                m[E++ >>> 0] = 128 | 63 & Ke;
              }
            }
            return m[E >>> 0] = 0, E - G;
          }, fa = (l, m, E) => Jh(l, n(), m, E);
          function Zh(l, m) {
            if (Z) return Fr(5, 1, l, m);
          }
          function em(l, m, E) {
            if (Z) return Fr(6, 1, l, m, E);
          }
          function tm(l, m, E) {
            return Z ? Fr(7, 1, l, m, E) : 0;
          }
          function rm(l, m) {
            if (Z) return Fr(8, 1, l, m);
          }
          function sm(l, m, E) {
            if (Z) return Fr(9, 1, l, m, E);
          }
          function nm(l, m, E, I) {
            if (Z) return Fr(10, 1, l, m, E, I);
          }
          function im(l, m, E, I) {
            if (Z) return Fr(11, 1, l, m, E, I);
          }
          function om(l, m, E, I) {
            if (Z) return Fr(12, 1, l, m, E, I);
          }
          function am(l) {
            if (Z) return Fr(13, 1, l);
          }
          function lm(l, m) {
            if (Z) return Fr(14, 1, l, m);
          }
          function um(l, m, E) {
            if (Z) return Fr(15, 1, l, m, E);
          }
          var dm, Kn, m_ = () => {
            Vn("");
          }, cn = (l) => {
            for (var m = ""; n()[l >>> 0]; ) m += dm[n()[l++ >>> 0]];
            return m;
          }, sh = {}, nh = {};
          function Tn(l, m, E = {}) {
            if (!("argPackAdvance" in m)) throw new TypeError("registerType registeredInstance requires argPackAdvance");
            return function(I, G, we = {}) {
              var Ke = G.name;
              if (!I) throw new Kn(`type "${Ke}" must have a positive integer typeid pointer`);
              if (nh.hasOwnProperty(I)) {
                if (we.Tb) return;
                throw new Kn(`Cannot register type '${Ke}' twice`);
              }
              nh[I] = G, sh.hasOwnProperty(I) && (G = sh[I], delete sh[I], G.forEach((tt) => tt()));
            }(l, m, E);
          }
          var cm = (l, m, E) => {
            switch (m) {
              case 1:
                return E ? (I) => r()[I >>> 0] : (I) => n()[I >>> 0];
              case 2:
                return E ? (I) => i()[I >>> 1 >>> 0] : (I) => a()[I >>> 1 >>> 0];
              case 4:
                return E ? (I) => o()[I >>> 2 >>> 0] : (I) => d()[I >>> 2 >>> 0];
              case 8:
                return E ? (I) => Hr[I >>> 3] : (I) => un[I >>> 3];
              default:
                throw new TypeError(`invalid integer width (${m}): ${l}`);
            }
          };
          function __(l, m, E) {
            E >>>= 0, Tn(l >>>= 0, { name: m = cn(m >>> 0), fromWireType: (I) => I, toWireType: function(I, G) {
              if (typeof G != "bigint" && typeof G != "number") throw G = G === null ? "null" : (I = typeof G) == "object" || I === "array" || I === "function" ? G.toString() : "" + G, new TypeError(`Cannot convert "${G}" to ${this.name}`);
              return typeof G == "number" && (G = BigInt(G)), G;
            }, argPackAdvance: Hn, readValueFromPointer: cm(m, E, m.indexOf("u") == -1), Eb: null });
          }
          var Hn = 8;
          function f_(l, m, E, I) {
            Tn(l >>>= 0, { name: m = cn(m >>> 0), fromWireType: function(G) {
              return !!G;
            }, toWireType: function(G, we) {
              return we ? E : I;
            }, argPackAdvance: Hn, readValueFromPointer: function(G) {
              return this.fromWireType(n()[G >>> 0]);
            }, Eb: null });
          }
          var ih = [], En = [];
          function oh(l) {
            9 < (l >>>= 0) && --En[l + 1] == 0 && (En[l] = void 0, ih.push(l));
          }
          var Cs = (l) => {
            if (!l) throw new Kn("Cannot use deleted val. handle = " + l);
            return En[l];
          }, Ls = (l) => {
            switch (l) {
              case void 0:
                return 2;
              case null:
                return 4;
              case !0:
                return 6;
              case !1:
                return 8;
              default:
                let m = ih.pop() || En.length;
                return En[m] = l, En[m + 1] = 1, m;
            }
          };
          function ah(l) {
            return this.fromWireType(d()[l >>> 2 >>> 0]);
          }
          var g_ = { name: "emscripten::val", fromWireType: (l) => {
            var m = Cs(l);
            return oh(l), m;
          }, toWireType: (l, m) => Ls(m), argPackAdvance: Hn, readValueFromPointer: ah, Eb: null };
          function w_(l) {
            return Tn(l >>> 0, g_);
          }
          var y_ = (l, m) => {
            switch (m) {
              case 4:
                return function(E) {
                  return this.fromWireType(p()[E >>> 2 >>> 0]);
                };
              case 8:
                return function(E) {
                  return this.fromWireType(h()[E >>> 3 >>> 0]);
                };
              default:
                throw new TypeError(`invalid float width (${m}): ${l}`);
            }
          };
          function M_(l, m, E) {
            E >>>= 0, Tn(l >>>= 0, { name: m = cn(m >>> 0), fromWireType: (I) => I, toWireType: (I, G) => G, argPackAdvance: Hn, readValueFromPointer: y_(m, E), Eb: null });
          }
          function b_(l, m, E, I, G) {
            if (l >>>= 0, E >>>= 0, m = cn(m >>> 0), G === -1 && (G = 4294967295), G = (tt) => tt, I === 0) {
              var we = 32 - 8 * E;
              G = (tt) => tt << we >>> we;
            }
            var Ke = m.includes("unsigned") ? function(tt, bt) {
              return bt >>> 0;
            } : function(tt, bt) {
              return bt;
            };
            Tn(l, { name: m, fromWireType: G, toWireType: Ke, argPackAdvance: Hn, readValueFromPointer: cm(m, E, I !== 0), Eb: null });
          }
          function v_(l, m, E) {
            function I(we) {
              var Ke = d()[we >>> 2 >>> 0];
              return we = d()[we + 4 >>> 2 >>> 0], new G(r().buffer, we, Ke);
            }
            var G = [Int8Array, Uint8Array, Int16Array, Uint16Array, Int32Array, Uint32Array, Float32Array, Float64Array, BigInt64Array, BigUint64Array][m];
            Tn(l >>>= 0, { name: E = cn(E >>> 0), fromWireType: I, argPackAdvance: Hn, readValueFromPointer: I }, { Tb: !0 });
          }
          function x_(l, m) {
            l >>>= 0;
            var E = (m = cn(m >>> 0)) === "std::string";
            Tn(l, { name: m, fromWireType: function(I) {
              var G = d()[I >>> 2 >>> 0], we = I + 4;
              if (E) for (var Ke = we, tt = 0; tt <= G; ++tt) {
                var bt = we + tt;
                if (tt == G || n()[bt >>> 0] == 0) {
                  if (Ke = zr(Ke, bt - Ke), kt === void 0) var kt = Ke;
                  else kt += "\0", kt += Ke;
                  Ke = bt + 1;
                }
              }
              else {
                for (kt = Array(G), tt = 0; tt < G; ++tt) kt[tt] = String.fromCharCode(n()[we + tt >>> 0]);
                kt = kt.join("");
              }
              return hn(I), kt;
            }, toWireType: function(I, G) {
              G instanceof ArrayBuffer && (G = new Uint8Array(G));
              var we = typeof G == "string";
              if (!(we || G instanceof Uint8Array || G instanceof Uint8ClampedArray || G instanceof Int8Array)) throw new Kn("Cannot pass non-string to std::string");
              var Ke = E && we ? rh(G) : G.length, tt = Sp(4 + Ke + 1), bt = tt + 4;
              if (d()[tt >>> 2 >>> 0] = Ke, E && we) fa(G, bt, Ke + 1);
              else if (we) for (we = 0; we < Ke; ++we) {
                var kt = G.charCodeAt(we);
                if (255 < kt) throw hn(bt), new Kn("String has UTF-16 code units that do not fit in 8 bits");
                n()[bt + we >>> 0] = kt;
              }
              else for (we = 0; we < Ke; ++we) n()[bt + we >>> 0] = G[we];
              return I !== null && I.push(hn, tt), tt;
            }, argPackAdvance: Hn, readValueFromPointer: ah, Eb(I) {
              hn(I);
            } });
          }
          var pm = typeof TextDecoder < "u" ? new TextDecoder("utf-16le") : void 0, T_ = (l, m) => {
            for (var E = l >> 1, I = E + m / 2; !(E >= I) && a()[E >>> 0]; ) ++E;
            if (32 < (E <<= 1) - l && pm) return pm.decode(n().slice(l, E));
            for (E = "", I = 0; !(I >= m / 2); ++I) {
              var G = i()[l + 2 * I >>> 1 >>> 0];
              if (G == 0) break;
              E += String.fromCharCode(G);
            }
            return E;
          }, E_ = (l, m, E) => {
            if (E ?? (E = 2147483647), 2 > E) return 0;
            var I = m;
            E = (E -= 2) < 2 * l.length ? E / 2 : l.length;
            for (var G = 0; G < E; ++G) {
              var we = l.charCodeAt(G);
              i()[m >>> 1 >>> 0] = we, m += 2;
            }
            return i()[m >>> 1 >>> 0] = 0, m - I;
          }, P_ = (l) => 2 * l.length, C_ = (l, m) => {
            for (var E = 0, I = ""; !(E >= m / 4); ) {
              var G = o()[l + 4 * E >>> 2 >>> 0];
              if (G == 0) break;
              ++E, 65536 <= G ? (G -= 65536, I += String.fromCharCode(55296 | G >> 10, 56320 | 1023 & G)) : I += String.fromCharCode(G);
            }
            return I;
          }, k_ = (l, m, E) => {
            if (m >>>= 0, E ?? (E = 2147483647), 4 > E) return 0;
            var I = m;
            E = I + E - 4;
            for (var G = 0; G < l.length; ++G) {
              var we = l.charCodeAt(G);
              if (55296 <= we && 57343 >= we && (we = 65536 + ((1023 & we) << 10) | 1023 & l.charCodeAt(++G)), o()[m >>> 2 >>> 0] = we, (m += 4) + 4 > E) break;
            }
            return o()[m >>> 2 >>> 0] = 0, m - I;
          }, S_ = (l) => {
            for (var m = 0, E = 0; E < l.length; ++E) {
              var I = l.charCodeAt(E);
              55296 <= I && 57343 >= I && ++E, m += 4;
            }
            return m;
          };
          function $_(l, m, E) {
            if (l >>>= 0, m >>>= 0, E = cn(E >>>= 0), m === 2) var I = T_, G = E_, we = P_, Ke = (tt) => a()[tt >>> 1 >>> 0];
            else m === 4 && (I = C_, G = k_, we = S_, Ke = (tt) => d()[tt >>> 2 >>> 0]);
            Tn(l, { name: E, fromWireType: (tt) => {
              for (var bt, kt = d()[tt >>> 2 >>> 0], Vt = tt + 4, pr = 0; pr <= kt; ++pr) {
                var Mr = tt + 4 + pr * m;
                pr != kt && Ke(Mr) != 0 || (Vt = I(Vt, Mr - Vt), bt === void 0 ? bt = Vt : (bt += "\0", bt += Vt), Vt = Mr + m);
              }
              return hn(tt), bt;
            }, toWireType: (tt, bt) => {
              if (typeof bt != "string") throw new Kn(`Cannot pass non-string to C++ string type ${E}`);
              var kt = we(bt), Vt = Sp(4 + kt + m);
              return d()[Vt >>> 2 >>> 0] = kt / m, G(bt, Vt + 4, kt + m), tt !== null && tt.push(hn, Vt), Vt;
            }, argPackAdvance: Hn, readValueFromPointer: ah, Eb(tt) {
              hn(tt);
            } });
          }
          function A_(l, m) {
            Tn(l >>>= 0, { Ub: !0, name: m = cn(m >>> 0), argPackAdvance: 0, fromWireType: () => {
            }, toWireType: () => {
            } });
          }
          var I_ = () => 1;
          function F_(l) {
            mh(l >>> 0, !N, 1, !R, 131072, !1), Wh();
          }
          var hm = (l) => {
            if (!Ir) try {
              if (l(), !(0 < Fi)) try {
                Z ? $p(Ht) : th(Ht);
              } catch (m) {
                m instanceof Jp || m == "unwind" || Ie(1, m);
              }
            } catch (m) {
              m instanceof Jp || m == "unwind" || Ie(1, m);
            }
          };
          function lh(l) {
            l >>>= 0, typeof Atomics.oc == "function" && (Atomics.oc(o(), l >>> 2, l).value.then(Mp), l += 128, Atomics.store(o(), l >>> 2, 1));
          }
          var Mp = () => {
            var l = wa();
            l && (lh(l), hm(Wm));
          };
          function O_(l, m) {
            (l >>>= 0) == m >>> 0 ? setTimeout(Mp) : Z ? postMessage({ targetThread: l, cmd: "checkMailbox" }) : (l = dn[l]) && l.postMessage({ cmd: "checkMailbox" });
          }
          var uh = [];
          function D_(l, m, E, I, G) {
            for (m >>>= 0, I /= 2, uh.length = I, E = G >>> 0 >>> 3, G = 0; G < I; G++) uh[G] = Hr[E + 2 * G] ? Hr[E + 2 * G + 1] : h()[E + 2 * G + 1 >>> 0];
            return (m ? Yp[m] : Pf[l])(...uh);
          }
          function L_(l) {
            l >>>= 0, Z ? postMessage({ cmd: "cleanupThread", thread: l }) : Uh(dn[l]);
          }
          function z_(l) {
          }
          var bp = (l, m) => {
            var E = nh[l];
            if (E === void 0) throw l = Rm(l), E = cn(l), hn(l), new Kn(`${m} has unknown type ${E}`);
            return E;
          }, mm = (l, m, E) => {
            var I = [];
            return l = l.toWireType(I, E), I.length && (d()[m >>> 2 >>> 0] = Ls(I)), l;
          };
          function B_(l, m, E) {
            return m >>>= 0, E >>>= 0, l = Cs(l >>> 0), m = bp(m, "emval::as"), mm(m, E, l);
          }
          function R_(l, m) {
            return m >>>= 0, l = Cs(l >>> 0), (m = bp(m, "emval::as")).toWireType(null, l);
          }
          var vp = (l) => {
            try {
              l();
            } catch (m) {
              Vn(m);
            }
          }, qn = 0, pn = null, _m = 0, xp = [], fm = {}, gm = {}, N_ = 0, dh = null, j_ = [];
          function wm(l) {
            return function(m) {
              if (!Ir) {
                if (qn === 0) {
                  var E = !1, I = !1;
                  m((G = 0) => {
                    if (!Ir && (_m = G, E = !0, I)) {
                      qn = 2, vp(() => qm(pn)), typeof Browser < "u" && Browser.Lb.Sb && Browser.Lb.resume(), G = !1;
                      try {
                        var we = function() {
                          var bt = o()[pn + 8 >>> 2 >>> 0];
                          return bt = Rt[gm[bt]], --Fi, bt();
                        }();
                      } catch (bt) {
                        we = bt, G = !0;
                      }
                      var Ke = !1;
                      if (!pn) {
                        var tt = dh;
                        tt && (dh = null, (G ? tt.reject : tt.resolve)(we), Ke = !0);
                      }
                      if (G && !Ke) throw we;
                    }
                  }), I = !0, E || (qn = 1, pn = function() {
                    var G = Sp(65548), we = G + 12;
                    d()[G >>> 2 >>> 0] = we, d()[G + 4 >>> 2 >>> 0] = we + 65536, we = xp[0];
                    var Ke = fm[we];
                    return Ke === void 0 && (Ke = N_++, fm[we] = Ke, gm[Ke] = we), we = Ke, o()[G + 8 >>> 2 >>> 0] = we, G;
                  }(), typeof Browser < "u" && Browser.Lb.Sb && Browser.Lb.pause(), vp(() => Km(pn)));
                } else qn === 2 ? (qn = 0, vp(Xm), hn(pn), pn = null, j_.forEach(hm)) : Vn(`invalid state: ${qn}`);
                return _m;
              }
            }((m) => {
              l().then(m);
            });
          }
          function U_(l) {
            return l >>>= 0, wm(() => (l = Cs(l)).then(Ls));
          }
          var Tp = [];
          function W_(l, m, E, I) {
            return E >>>= 0, I >>>= 0, (l = Tp[l >>> 0])(null, m = Cs(m >>> 0), E, I);
          }
          var V_ = {}, Ep = (l) => {
            var m = V_[l];
            return m === void 0 ? cn(l) : m;
          };
          function G_(l, m, E, I, G) {
            return E >>>= 0, I >>>= 0, G >>>= 0, (l = Tp[l >>> 0])(m = Cs(m >>> 0), m[E = Ep(E)], I, G);
          }
          var ym = () => typeof globalThis == "object" ? globalThis : Function("return this")();
          function K_(l) {
            return (l >>>= 0) == 0 ? Ls(ym()) : (l = Ep(l), Ls(ym()[l]));
          }
          var H_ = (l) => {
            var m = Tp.length;
            return Tp.push(l), m;
          }, q_ = (l, m) => {
            for (var E = Array(l), I = 0; I < l; ++I) E[I] = bp(d()[m + 4 * I >>> 2 >>> 0], "parameter " + I);
            return E;
          }, Mm = (l, m) => Object.defineProperty(m, "name", { value: l });
          function X_(l, m, E) {
            var I = (m = q_(l, m >>> 0)).shift();
            l--;
            var G = `return function (obj, func, destructorsRef, args) {
`, we = 0, Ke = [];
            E === 0 && Ke.push("obj");
            for (var tt = ["retType"], bt = [I], kt = 0; kt < l; ++kt) Ke.push("arg" + kt), tt.push("argType" + kt), bt.push(m[kt]), G += `  var arg${kt} = argType${kt}.readValueFromPointer(args${we ? "+" + we : ""});
`, we += m[kt].argPackAdvance;
            return G += `  var rv = ${E === 1 ? "new func" : "func.call"}(${Ke.join(", ")});
`, I.Ub || (tt.push("emval_returnValue"), bt.push(mm), G += `  return emval_returnValue(retType, destructorsRef, rv);
`), tt.push(G + `};
`), l = function(Vt) {
              var pr = Function;
              if (!(pr instanceof Function)) throw new TypeError(`new_ called with constructor type ${typeof pr} which is not a function`);
              var Mr = Mm(pr.name || "unknownFunctionName", function() {
              });
              return Mr.prototype = pr.prototype, Mr = new Mr(), (Vt = pr.apply(Mr, Vt)) instanceof Object ? Vt : Mr;
            }(tt)(...bt), E = `methodCaller<(${m.map((Vt) => Vt.name).join(", ")}) => ${I.name}>`, H_(Mm(E, l));
          }
          function Q_(l) {
            return l = Ep(l >>> 0), Ls(u[l]);
          }
          function Y_(l, m) {
            return m >>>= 0, l = Cs(l >>> 0), m = Cs(m), Ls(l[m]);
          }
          function J_(l) {
            9 < (l >>>= 0) && (En[l + 1] += 1);
          }
          function Z_() {
            return Ls([]);
          }
          function ef(l) {
            l = Cs(l >>> 0);
            for (var m = Array(l.length), E = 0; E < l.length; E++) m[E] = l[E];
            return Ls(m);
          }
          function tf(l) {
            return Ls(Ep(l >>> 0));
          }
          function rf() {
            return Ls({});
          }
          function sf(l) {
            for (var m = Cs(l >>>= 0); m.length; ) {
              var E = m.pop();
              m.pop()(E);
            }
            oh(l);
          }
          function nf(l, m, E) {
            m >>>= 0, E >>>= 0, l = Cs(l >>> 0), m = Cs(m), E = Cs(E), l[m] = E;
          }
          function of(l, m) {
            return m >>>= 0, l = (l = bp(l >>> 0, "_emval_take_value")).readValueFromPointer(m), Ls(l);
          }
          function af(l, m) {
            l = -9007199254740992 > l || 9007199254740992 < l ? NaN : Number(l), m >>>= 0, l = new Date(1e3 * l), o()[m >>> 2 >>> 0] = l.getUTCSeconds(), o()[m + 4 >>> 2 >>> 0] = l.getUTCMinutes(), o()[m + 8 >>> 2 >>> 0] = l.getUTCHours(), o()[m + 12 >>> 2 >>> 0] = l.getUTCDate(), o()[m + 16 >>> 2 >>> 0] = l.getUTCMonth(), o()[m + 20 >>> 2 >>> 0] = l.getUTCFullYear() - 1900, o()[m + 24 >>> 2 >>> 0] = l.getUTCDay(), l = (l.getTime() - Date.UTC(l.getUTCFullYear(), 0, 1, 0, 0, 0, 0)) / 864e5 | 0, o()[m + 28 >>> 2 >>> 0] = l;
          }
          var ga = (l) => l % 4 == 0 && (l % 100 != 0 || l % 400 == 0), bm = [0, 31, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335], vm = [0, 31, 59, 90, 120, 151, 181, 212, 243, 273, 304, 334];
          function lf(l, m) {
            l = -9007199254740992 > l || 9007199254740992 < l ? NaN : Number(l), m >>>= 0, l = new Date(1e3 * l), o()[m >>> 2 >>> 0] = l.getSeconds(), o()[m + 4 >>> 2 >>> 0] = l.getMinutes(), o()[m + 8 >>> 2 >>> 0] = l.getHours(), o()[m + 12 >>> 2 >>> 0] = l.getDate(), o()[m + 16 >>> 2 >>> 0] = l.getMonth(), o()[m + 20 >>> 2 >>> 0] = l.getFullYear() - 1900, o()[m + 24 >>> 2 >>> 0] = l.getDay();
            var E = (ga(l.getFullYear()) ? bm : vm)[l.getMonth()] + l.getDate() - 1 | 0;
            o()[m + 28 >>> 2 >>> 0] = E, o()[m + 36 >>> 2 >>> 0] = -60 * l.getTimezoneOffset(), E = new Date(l.getFullYear(), 6, 1).getTimezoneOffset();
            var I = new Date(l.getFullYear(), 0, 1).getTimezoneOffset();
            l = 0 | (E != I && l.getTimezoneOffset() == Math.min(I, E)), o()[m + 32 >>> 2 >>> 0] = l;
          }
          function uf(l) {
            l >>>= 0;
            var m = new Date(o()[l + 20 >>> 2 >>> 0] + 1900, o()[l + 16 >>> 2 >>> 0], o()[l + 12 >>> 2 >>> 0], o()[l + 8 >>> 2 >>> 0], o()[l + 4 >>> 2 >>> 0], o()[l >>> 2 >>> 0], 0), E = o()[l + 32 >>> 2 >>> 0], I = m.getTimezoneOffset(), G = new Date(m.getFullYear(), 6, 1).getTimezoneOffset(), we = new Date(m.getFullYear(), 0, 1).getTimezoneOffset(), Ke = Math.min(we, G);
            return 0 > E ? o()[l + 32 >>> 2 >>> 0] = +(G != we && Ke == I) : 0 < E != (Ke == I) && (G = Math.max(we, G), m.setTime(m.getTime() + 6e4 * ((0 < E ? Ke : G) - I))), o()[l + 24 >>> 2 >>> 0] = m.getDay(), E = (ga(m.getFullYear()) ? bm : vm)[m.getMonth()] + m.getDate() - 1 | 0, o()[l + 28 >>> 2 >>> 0] = E, o()[l >>> 2 >>> 0] = m.getSeconds(), o()[l + 4 >>> 2 >>> 0] = m.getMinutes(), o()[l + 8 >>> 2 >>> 0] = m.getHours(), o()[l + 12 >>> 2 >>> 0] = m.getDate(), o()[l + 16 >>> 2 >>> 0] = m.getMonth(), o()[l + 20 >>> 2 >>> 0] = m.getYear(), l = m.getTime(), BigInt(isNaN(l) ? -1 : l / 1e3);
          }
          function xm(l, m, E, I, G, we, Ke) {
            return Z ? Fr(16, 1, l, m, E, I, G, we, Ke) : -52;
          }
          function Tm(l, m, E, I, G, we) {
            if (Z) return Fr(17, 1, l, m, E, I, G, we);
          }
          function df(l, m, E, I) {
            l >>>= 0, m >>>= 0, E >>>= 0, I >>>= 0;
            var G = (/* @__PURE__ */ new Date()).getFullYear(), we = new Date(G, 0, 1), Ke = new Date(G, 6, 1);
            G = we.getTimezoneOffset();
            var tt = Ke.getTimezoneOffset(), bt = Math.max(G, tt);
            d()[l >>> 2 >>> 0] = 60 * bt, o()[m >>> 2 >>> 0] = +(G != tt), we = (l = (kt) => kt.toLocaleTimeString(void 0, { hour12: !1, timeZoneName: "short" }).split(" ")[1])(we), Ke = l(Ke), tt < G ? (fa(we, E, 17), fa(Ke, I, 17)) : (fa(we, I, 17), fa(Ke, E, 17));
          }
          var ch = [], Em = (l, m) => {
            ch.length = 0;
            for (var E; E = n()[l++ >>> 0]; ) {
              var I = E != 105;
              m += (I &= E != 112) && m % 8 ? 4 : 0, ch.push(E == 112 ? d()[m >>> 2 >>> 0] : E == 106 ? Hr[m >>> 3] : E == 105 ? o()[m >>> 2 >>> 0] : h()[m >>> 3 >>> 0]), m += I ? 8 : 4;
            }
            return ch;
          };
          function cf(l, m, E) {
            return l >>>= 0, m = Em(m >>> 0, E >>> 0), Yp[l](...m);
          }
          function pf(l, m, E) {
            return l >>>= 0, m = Em(m >>> 0, E >>> 0), Yp[l](...m);
          }
          var hf = () => {
          }, mf = () => Date.now();
          function _f(l, m) {
            return Ct(zr(l >>> 0, m >>> 0));
          }
          var Pm, ff = () => {
            throw Fi += 1, "unwind";
          };
          function gf() {
            return 4294901760;
          }
          Pm = () => performance.timeOrigin + performance.now();
          var wf = () => navigator.hardwareConcurrency;
          function yf() {
            return Vn("Cannot use emscripten_pc_get_function without -sUSE_OFFSET_CONVERTER"), 0;
          }
          function Mf(l) {
            l >>>= 0;
            var m = n().length;
            if (l <= m || 4294901760 < l) return !1;
            for (var E = 1; 4 >= E; E *= 2) {
              var I = m * (1 + 0.2 / E);
              I = Math.min(I, l + 100663296);
              var G = Math;
              I = Math.max(l, I);
              e: {
                G = (G.min.call(G, 4294901760, I + (65536 - I % 65536) % 65536) - jt.buffer.byteLength + 65535) / 65536;
                try {
                  jt.grow(G), Er();
                  var we = 1;
                  break e;
                } catch {
                }
                we = void 0;
              }
              if (we) return !0;
            }
            return !1;
          }
          var Pp = () => (Vn("Cannot use convertFrameToPC (needed by __builtin_return_address) without -sUSE_OFFSET_CONVERTER"), 0), Tc = {}, Cm = (l) => {
            l.forEach((m) => {
              Pp();
            });
          };
          function bf() {
            var l = Error().stack.toString().split(`
`);
            return l[0] == "Error" && l.shift(), Cm(l), Tc.Qb = Pp(), Tc.fc = l, Tc.Qb;
          }
          function vf(l, m, E) {
            if (l >>>= 0, m >>>= 0, Tc.Qb == l) var I = Tc.fc;
            else (I = Error().stack.toString().split(`
`))[0] == "Error" && I.shift(), Cm(I);
            for (var G = 3; I[G] && Pp() != l; ) ++G;
            for (l = 0; l < E && I[l + G]; ++l) o()[m + 4 * l >>> 2 >>> 0] = Pp();
            return l;
          }
          var ph, hh = {}, km = () => {
            if (!ph) {
              var l, m = { USER: "web_user", LOGNAME: "web_user", PATH: "/", PWD: "/", HOME: "/home/web_user", LANG: (typeof navigator == "object" && navigator.languages && navigator.languages[0] || "C").replace("-", "_") + ".UTF-8", _: Ae };
              for (l in hh) hh[l] === void 0 ? delete m[l] : m[l] = hh[l];
              var E = [];
              for (l in m) E.push(`${l}=${m[l]}`);
              ph = E;
            }
            return ph;
          };
          function Sm(l, m) {
            if (Z) return Fr(18, 1, l, m);
            l >>>= 0, m >>>= 0;
            var E = 0;
            return km().forEach((I, G) => {
              var we = m + E;
              for (G = d()[l + 4 * G >>> 2 >>> 0] = we, we = 0; we < I.length; ++we) r()[G++ >>> 0] = I.charCodeAt(we);
              r()[G >>> 0] = 0, E += I.length + 1;
            }), 0;
          }
          function $m(l, m) {
            if (Z) return Fr(19, 1, l, m);
            l >>>= 0, m >>>= 0;
            var E = km();
            d()[l >>> 2 >>> 0] = E.length;
            var I = 0;
            return E.forEach((G) => I += G.length + 1), d()[m >>> 2 >>> 0] = I, 0;
          }
          function Am(l) {
            return Z ? Fr(20, 1, l) : 52;
          }
          function Im(l, m, E, I) {
            return Z ? Fr(21, 1, l, m, E, I) : 52;
          }
          function Fm(l, m, E, I) {
            return Z ? Fr(22, 1, l, m, E, I) : 70;
          }
          var xf = [null, [], []];
          function Om(l, m, E, I) {
            if (Z) return Fr(23, 1, l, m, E, I);
            m >>>= 0, E >>>= 0, I >>>= 0;
            for (var G = 0, we = 0; we < E; we++) {
              var Ke = d()[m >>> 2 >>> 0], tt = d()[m + 4 >>> 2 >>> 0];
              m += 8;
              for (var bt = 0; bt < tt; bt++) {
                var kt = n()[Ke + bt >>> 0], Vt = xf[l];
                kt === 0 || kt === 10 ? ((l === 1 ? Yt : Ct)(Xh(Vt, 0)), Vt.length = 0) : Vt.push(kt);
              }
              G += tt;
            }
            return d()[I >>> 2 >>> 0] = G, 0;
          }
          var Dm = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], Lm = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], Tf = (l, m) => {
            r().set(l, m >>> 0);
          };
          function zm(l, m, E, I) {
            function G(We, ar, Br) {
              for (We = typeof We == "number" ? We.toString() : We || ""; We.length < ar; ) We = Br[0] + We;
              return We;
            }
            function we(We, ar) {
              return G(We, ar, "0");
            }
            function Ke(We, ar) {
              function Br(Ym) {
                return 0 > Ym ? -1 : 0 < Ym ? 1 : 0;
              }
              var Di;
              return (Di = Br(We.getFullYear() - ar.getFullYear())) === 0 && (Di = Br(We.getMonth() - ar.getMonth())) === 0 && (Di = Br(We.getDate() - ar.getDate())), Di;
            }
            function tt(We) {
              switch (We.getDay()) {
                case 0:
                  return new Date(We.getFullYear() - 1, 11, 29);
                case 1:
                  return We;
                case 2:
                  return new Date(We.getFullYear(), 0, 3);
                case 3:
                  return new Date(We.getFullYear(), 0, 2);
                case 4:
                  return new Date(We.getFullYear(), 0, 1);
                case 5:
                  return new Date(We.getFullYear() - 1, 11, 31);
                case 6:
                  return new Date(We.getFullYear() - 1, 11, 30);
              }
            }
            function bt(We) {
              var ar = We.Cb;
              for (We = new Date(new Date(We.Db + 1900, 0, 1).getTime()); 0 < ar; ) {
                var Br = We.getMonth(), Di = (ga(We.getFullYear()) ? Dm : Lm)[Br];
                if (!(ar > Di - We.getDate())) {
                  We.setDate(We.getDate() + ar);
                  break;
                }
                ar -= Di - We.getDate() + 1, We.setDate(1), 11 > Br ? We.setMonth(Br + 1) : (We.setMonth(0), We.setFullYear(We.getFullYear() + 1));
              }
              return Br = new Date(We.getFullYear() + 1, 0, 4), ar = tt(new Date(We.getFullYear(), 0, 4)), Br = tt(Br), 0 >= Ke(ar, We) ? 0 >= Ke(Br, We) ? We.getFullYear() + 1 : We.getFullYear() : We.getFullYear() - 1;
            }
            l >>>= 0, m >>>= 0, E >>>= 0, I >>>= 0;
            var kt = d()[I + 40 >>> 2 >>> 0];
            for (var Vt in I = { lc: o()[I >>> 2 >>> 0], kc: o()[I + 4 >>> 2 >>> 0], Ib: o()[I + 8 >>> 2 >>> 0], Mb: o()[I + 12 >>> 2 >>> 0], Jb: o()[I + 16 >>> 2 >>> 0], Db: o()[I + 20 >>> 2 >>> 0], vb: o()[I + 24 >>> 2 >>> 0], Cb: o()[I + 28 >>> 2 >>> 0], sc: o()[I + 32 >>> 2 >>> 0], jc: o()[I + 36 >>> 2 >>> 0], mc: kt ? zr(kt) : "" }, E = zr(E), kt = { "%c": "%a %b %d %H:%M:%S %Y", "%D": "%m/%d/%y", "%F": "%Y-%m-%d", "%h": "%b", "%r": "%I:%M:%S %p", "%R": "%H:%M", "%T": "%H:%M:%S", "%x": "%m/%d/%y", "%X": "%H:%M:%S", "%Ec": "%c", "%EC": "%C", "%Ex": "%m/%d/%y", "%EX": "%H:%M:%S", "%Ey": "%y", "%EY": "%Y", "%Od": "%d", "%Oe": "%e", "%OH": "%H", "%OI": "%I", "%Om": "%m", "%OM": "%M", "%OS": "%S", "%Ou": "%u", "%OU": "%U", "%OV": "%V", "%Ow": "%w", "%OW": "%W", "%Oy": "%y" }) E = E.replace(new RegExp(Vt, "g"), kt[Vt]);
            var pr = "Sunday Monday Tuesday Wednesday Thursday Friday Saturday".split(" "), Mr = "January February March April May June July August September October November December".split(" ");
            for (Vt in kt = { "%a": (We) => pr[We.vb].substring(0, 3), "%A": (We) => pr[We.vb], "%b": (We) => Mr[We.Jb].substring(0, 3), "%B": (We) => Mr[We.Jb], "%C": (We) => we((We.Db + 1900) / 100 | 0, 2), "%d": (We) => we(We.Mb, 2), "%e": (We) => G(We.Mb, 2, " "), "%g": (We) => bt(We).toString().substring(2), "%G": bt, "%H": (We) => we(We.Ib, 2), "%I": (We) => ((We = We.Ib) == 0 ? We = 12 : 12 < We && (We -= 12), we(We, 2)), "%j": (We) => {
              for (var ar = 0, Br = 0; Br <= We.Jb - 1; ar += (ga(We.Db + 1900) ? Dm : Lm)[Br++]) ;
              return we(We.Mb + ar, 3);
            }, "%m": (We) => we(We.Jb + 1, 2), "%M": (We) => we(We.kc, 2), "%n": () => `
`, "%p": (We) => 0 <= We.Ib && 12 > We.Ib ? "AM" : "PM", "%S": (We) => we(We.lc, 2), "%t": () => "	", "%u": (We) => We.vb || 7, "%U": (We) => we(Math.floor((We.Cb + 7 - We.vb) / 7), 2), "%V": (We) => {
              var ar = Math.floor((We.Cb + 7 - (We.vb + 6) % 7) / 7);
              if (2 >= (We.vb + 371 - We.Cb - 2) % 7 && ar++, ar) ar == 53 && ((Br = (We.vb + 371 - We.Cb) % 7) == 4 || Br == 3 && ga(We.Db) || (ar = 1));
              else {
                ar = 52;
                var Br = (We.vb + 7 - We.Cb - 1) % 7;
                (Br == 4 || Br == 5 && ga(We.Db % 400 - 1)) && ar++;
              }
              return we(ar, 2);
            }, "%w": (We) => We.vb, "%W": (We) => we(Math.floor((We.Cb + 7 - (We.vb + 6) % 7) / 7), 2), "%y": (We) => (We.Db + 1900).toString().substring(2), "%Y": (We) => We.Db + 1900, "%z": (We) => {
              var ar = 0 <= (We = We.jc);
              return We = Math.abs(We) / 60, (ar ? "+" : "-") + ("0000" + (We / 60 * 100 + We % 60)).slice(-4);
            }, "%Z": (We) => We.mc, "%%": () => "%" }, E = E.replace(/%%/g, "\0\0"), kt) E.includes(Vt) && (E = E.replace(new RegExp(Vt, "g"), kt[Vt](I)));
            return Vt = function(We) {
              var ar = Array(rh(We) + 1);
              return Jh(We, ar, 0, ar.length), ar;
            }(E = E.replace(/\0\0/g, "%")), Vt.length > m ? 0 : (Tf(Vt, l), Vt.length - 1);
          }
          function Ef(l, m, E, I) {
            return zm(l >>> 0, m >>> 0, E >>> 0, I >>> 0);
          }
          Z || function() {
            for (var l = u.numThreads - 1; l--; ) Gh();
            Wn.unshift(() => {
              vc++, function(m) {
                Z ? m() : Promise.all(Gn.map(Vh)).then(m);
              }(() => Fh());
            });
          }();
          for (var Bm = Array(256), Cp = 0; 256 > Cp; ++Cp) Bm[Cp] = String.fromCharCode(Cp);
          dm = Bm, Kn = u.BindingError = class extends Error {
            constructor(l) {
              super(l), this.name = "BindingError";
            }
          }, u.InternalError = class extends Error {
            constructor(l) {
              super(l), this.name = "InternalError";
            }
          }, En.push(0, 1, void 0, 1, null, 1, !0, 1, !1, 1), u.count_emval_handles = () => En.length / 2 - 5 - ih.length;
          var Pf = [eh, Nh, Kh, Qh, Yh, Zh, em, tm, rm, sm, nm, im, om, am, lm, um, xm, Tm, Sm, $m, Am, Im, Fm, Om], Rt = function() {
            function l(E, I) {
              return Rt = E.exports, Rt = function() {
                var G = Rt, we = {};
                for (let [Ke, tt] of Object.entries(G)) we[Ke] = typeof tt == "function" ? (...bt) => {
                  xp.push(Ke);
                  try {
                    return tt(...bt);
                  } finally {
                    Ir || (xp.pop(), pn && qn === 1 && xp.length === 0 && (qn = 0, Fi += 1, vp(Hm), typeof Fibers < "u" && Fibers.tc()));
                  }
                } : tt;
                return we;
              }(), Rt = function() {
                var G = Rt, we = (tt) => (bt) => tt(bt) >>> 0, Ke = (tt) => () => tt() >>> 0;
                return (G = Object.assign({}, G)).Da = we(G.Da), G.gb = Ke(G.gb), G.ib = we(G.ib), G.emscripten_main_runtime_thread_id = Ke(G.emscripten_main_runtime_thread_id), G.tb = we(G.tb), G.ub = Ke(G.ub), G;
              }(), jh.push(Rt.jb), bc.unshift(Rt.Ca), vr = I, Fh(), Rt;
            }
            var m = Bh();
            if (vc++, u.instantiateWasm) try {
              return u.instantiateWasm(m, l);
            } catch (E) {
              Ct(`Module.instantiateWasm callback failed with error: ${E}`), S(E);
            }
            return Qp || (Qp = u.locateFile ? Oh("ort-wasm-simd-threaded.jsep.wasm") ? "ort-wasm-simd-threaded.jsep.wasm" : u.locateFile ? u.locateFile("ort-wasm-simd-threaded.jsep.wasm", Ge) : Ge + "ort-wasm-simd-threaded.jsep.wasm" : new URL(
              /* asset import */
              s(
                /*! ort-wasm-simd-threaded.jsep.wasm */
                "./node_modules/onnxruntime-web/dist/ort-wasm-simd-threaded.jsep.wasm"
              ),
              s.b
            ).href), function(E, I) {
              var G = Qp;
              return lt || typeof WebAssembly.instantiateStreaming != "function" || Oh(G) || Dh(G) || typeof fetch != "function" ? zh(G, E, I) : fetch(G, { credentials: "same-origin" }).then((we) => WebAssembly.instantiateStreaming(we, E).then(I, function(Ke) {
                return Ct(`wasm streaming compile failed: ${Ke}`), Ct("falling back to ArrayBuffer instantiation"), zh(G, E, I);
              }));
            }(m, function(E) {
              l(E.instance, E.module);
            }).catch(S), {};
          }(), Rm = (l) => (Rm = Rt.Da)(l), Nm = () => (Nm = Rt.Ea)();
          u._OrtInit = (l, m) => (u._OrtInit = Rt.Fa)(l, m), u._OrtGetLastError = (l, m) => (u._OrtGetLastError = Rt.Ga)(l, m), u._OrtCreateSessionOptions = (l, m, E, I, G, we, Ke, tt, bt, kt) => (u._OrtCreateSessionOptions = Rt.Ha)(l, m, E, I, G, we, Ke, tt, bt, kt), u._OrtAppendExecutionProvider = (l, m) => (u._OrtAppendExecutionProvider = Rt.Ia)(l, m), u._OrtAddFreeDimensionOverride = (l, m, E) => (u._OrtAddFreeDimensionOverride = Rt.Ja)(l, m, E), u._OrtAddSessionConfigEntry = (l, m, E) => (u._OrtAddSessionConfigEntry = Rt.Ka)(l, m, E), u._OrtReleaseSessionOptions = (l) => (u._OrtReleaseSessionOptions = Rt.La)(l), u._OrtCreateSession = (l, m, E) => (u._OrtCreateSession = Rt.Ma)(l, m, E), u._OrtReleaseSession = (l) => (u._OrtReleaseSession = Rt.Na)(l), u._OrtGetInputOutputCount = (l, m, E) => (u._OrtGetInputOutputCount = Rt.Oa)(l, m, E), u._OrtGetInputName = (l, m) => (u._OrtGetInputName = Rt.Pa)(l, m), u._OrtGetOutputName = (l, m) => (u._OrtGetOutputName = Rt.Qa)(l, m), u._OrtFree = (l) => (u._OrtFree = Rt.Ra)(l), u._OrtCreateTensor = (l, m, E, I, G, we) => (u._OrtCreateTensor = Rt.Sa)(l, m, E, I, G, we), u._OrtGetTensorData = (l, m, E, I, G) => (u._OrtGetTensorData = Rt.Ta)(l, m, E, I, G), u._OrtReleaseTensor = (l) => (u._OrtReleaseTensor = Rt.Ua)(l), u._OrtCreateRunOptions = (l, m, E, I) => (u._OrtCreateRunOptions = Rt.Va)(l, m, E, I), u._OrtAddRunConfigEntry = (l, m, E) => (u._OrtAddRunConfigEntry = Rt.Wa)(l, m, E), u._OrtReleaseRunOptions = (l) => (u._OrtReleaseRunOptions = Rt.Xa)(l), u._OrtCreateBinding = (l) => (u._OrtCreateBinding = Rt.Ya)(l), u._OrtBindInput = (l, m, E) => (u._OrtBindInput = Rt.Za)(l, m, E), u._OrtBindOutput = (l, m, E, I) => (u._OrtBindOutput = Rt._a)(l, m, E, I), u._OrtClearBoundOutputs = (l) => (u._OrtClearBoundOutputs = Rt.$a)(l), u._OrtReleaseBinding = (l) => (u._OrtReleaseBinding = Rt.ab)(l), u._OrtRunWithBinding = (l, m, E, I, G) => (u._OrtRunWithBinding = Rt.bb)(l, m, E, I, G), u._OrtRun = (l, m, E, I, G, we, Ke, tt) => (u._OrtRun = Rt.cb)(l, m, E, I, G, we, Ke, tt), u._OrtEndProfiling = (l) => (u._OrtEndProfiling = Rt.db)(l), u._JsepOutput = (l, m, E) => (u._JsepOutput = Rt.eb)(l, m, E), u._JsepGetNodeName = (l) => (u._JsepGetNodeName = Rt.fb)(l);
          var kp, wa = () => (wa = Rt.gb)(), hn = u._free = (l) => (hn = u._free = Rt.hb)(l), Sp = u._malloc = (l) => (Sp = u._malloc = Rt.ib)(l), mh = (l, m, E, I, G, we) => (mh = Rt.lb)(l, m, E, I, G, we), jm = () => (jm = Rt.mb)(), Um = (l, m, E, I, G) => (Um = Rt.nb)(l, m, E, I, G), _h = (l) => (_h = Rt.ob)(l), $p = (l) => ($p = Rt.pb)(l), Wm = () => (Wm = Rt.qb)(), Vm = (l, m) => (Vm = Rt.rb)(l, m), Ap = (l) => (Ap = Rt.sb)(l), fh = (l) => (fh = Rt.tb)(l), gh = () => (gh = Rt.ub)(), Gm = u.dynCall_ii = (l, m) => (Gm = u.dynCall_ii = Rt.wb)(l, m), Km = (l) => (Km = Rt.xb)(l), Hm = () => (Hm = Rt.yb)(), qm = (l) => (qm = Rt.zb)(l), Xm = () => (Xm = Rt.Ab)();
          function Qm() {
            0 < vc || (Z ? (k(u), Z || yp(bc), startWorker(u)) : (yp(Wn), 0 < vc || kp || (kp = !0, u.calledRun = !0, Ir || (Z || yp(bc), k(u), Z || yp(Xp)))));
          }
          return u.___start_em_js = 889994, u.___stop_em_js = 890240, u.stackSave = () => gh(), u.stackRestore = (l) => Ap(l), u.stackAlloc = (l) => fh(l), u.setValue = function(l, m, E = "i8") {
            switch (E.endsWith("*") && (E = "*"), E) {
              case "i1":
              case "i8":
                r()[l >>> 0] = m;
                break;
              case "i16":
                i()[l >>> 1 >>> 0] = m;
                break;
              case "i32":
                o()[l >>> 2 >>> 0] = m;
                break;
              case "i64":
                Hr[l >>> 3] = BigInt(m);
                break;
              case "float":
                p()[l >>> 2 >>> 0] = m;
                break;
              case "double":
                h()[l >>> 3 >>> 0] = m;
                break;
              case "*":
                d()[l >>> 2 >>> 0] = m;
                break;
              default:
                Vn(`invalid type for setValue: ${E}`);
            }
          }, u.getValue = function(l, m = "i8") {
            switch (m.endsWith("*") && (m = "*"), m) {
              case "i1":
              case "i8":
                return r()[l >>> 0];
              case "i16":
                return i()[l >>> 1 >>> 0];
              case "i32":
                return o()[l >>> 2 >>> 0];
              case "i64":
                return Hr[l >>> 3];
              case "float":
                return p()[l >>> 2 >>> 0];
              case "double":
                return h()[l >>> 3 >>> 0];
              case "*":
                return d()[l >>> 2 >>> 0];
              default:
                Vn(`invalid type for getValue: ${m}`);
            }
          }, u.UTF8ToString = zr, u.stringToUTF8 = fa, u.lengthBytesUTF8 = rh, xc = function l() {
            kp || Qm(), kp || (xc = l);
          }, Qm(), u.PTR_SIZE = 4, B;
        }), Ar = kr, ((e = globalThis.self) == null ? void 0 : e.name) === "em-pthread" && kr();
      }), is, Xs, zs, Ms, Nt, Qs, ks, Bs, Ss = w(() => {
        var e, t;
        st(), is = import.meta.url ?? (typeof document < "u" ? (e = document.currentScript) == null ? void 0 : e.src : typeof self < "u" ? (t = self.location) == null ? void 0 : t.href : void 0), Xs = typeof location > "u" ? void 0 : location.origin, zs = (r, n) => {
          try {
            let i = n ?? is;
            return (i ? new URL(r, i) : new URL(r)).origin === Xs;
          } catch {
            return !1;
          }
        }, Ms = async (r) => {
          let n = await (await fetch(r, { credentials: "same-origin" })).blob();
          return URL.createObjectURL(n);
        }, Nt = (At(), M(pt)).default, Qs = async () => {
          if (!is) throw new Error("Failed to load proxy worker: cannot determine the script source URL.");
          if (zs(is)) return [void 0, Nt()];
          let r = await Ms(is);
          return [r, Nt(r)];
        }, ks = (Qr(), M(nr)).default, Bs = async (r, n, i) => [void 0, ks];
      }), os, $s, cs, As, Ys, as, nt, _t, Ft = w(() => {
        Ss(), $s = !1, cs = !1, As = !1, Ys = () => {
          if (typeof SharedArrayBuffer > "u") return !1;
          try {
            return typeof MessageChannel < "u" && new MessageChannel().port1.postMessage(new SharedArrayBuffer(1)), WebAssembly.validate(new Uint8Array([0, 97, 115, 109, 1, 0, 0, 0, 1, 4, 1, 96, 0, 0, 3, 2, 1, 0, 5, 4, 1, 3, 1, 1, 10, 11, 1, 9, 0, 65, 0, 254, 16, 2, 0, 26, 11]));
          } catch {
            return !1;
          }
        }, as = () => {
          try {
            return WebAssembly.validate(new Uint8Array([0, 97, 115, 109, 1, 0, 0, 0, 1, 4, 1, 96, 0, 0, 3, 2, 1, 0, 10, 30, 1, 28, 0, 65, 0, 253, 15, 253, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 253, 186, 1, 26, 11]));
          } catch {
            return !1;
          }
        }, nt = async (e) => {
          if ($s) return Promise.resolve();
          if (cs) throw new Error("multiple calls to 'initializeWebAssembly()' detected.");
          if (As) throw new Error("previous call to 'initializeWebAssembly()' failed.");
          cs = !0;
          let t = e.initTimeout, r = e.numThreads;
          if (!as()) throw new Error("WebAssembly SIMD is not supported in the current environment.");
          let n = Ys();
          r > 1 && !n && (typeof self < "u" && !self.crossOriginIsolated && console.warn("env.wasm.numThreads is set to " + r + ", but this will not work unless you enable crossOriginIsolated mode. See https://web.dev/cross-origin-isolation-guide/ for more info."), console.warn("WebAssembly multi-threading is not supported in the current environment. Falling back to single-threading."), e.numThreads = r = 1);
          let i = e.wasmPaths, a = typeof i == "string" ? i : void 0, o = i == null ? void 0 : i.mjs, d = (o == null ? void 0 : o.href) ?? o, p = i == null ? void 0 : i.wasm, h = (p == null ? void 0 : p.href) ?? p, k = e.wasmBinary, [S, u] = await Bs(d, a, r > 1), B = !1, R = [];
          if (t > 0 && R.push(new Promise((N) => {
            setTimeout(() => {
              B = !0, N();
            }, t);
          })), R.push(new Promise((N, Z) => {
            let te = { numThreads: r };
            k ? te.wasmBinary = k : (h || a) && (te.locateFile = (Q, _e) => h ?? (a ?? _e) + Q), u(te).then((Q) => {
              cs = !1, $s = !0, os = Q, N(), S && URL.revokeObjectURL(S);
            }, (Q) => {
              cs = !1, As = !0, Z(Q);
            });
          })), await Promise.race(R), B) throw new Error(`WebAssembly backend initializing failed due to timeout: ${t}ms`);
        }, _t = () => {
          if ($s && os) return os;
          throw new Error("WebAssembly is not initialized yet.");
        };
      }), lr, bs, tr, ts = w(() => {
        Ft(), lr = (e, t) => {
          let r = _t(), n = r.lengthBytesUTF8(e) + 1, i = r._malloc(n);
          return r.stringToUTF8(e, i, n), t.push(i), i;
        }, bs = (e, t, r, n) => {
          if (typeof e == "object" && e !== null) {
            if (r.has(e)) throw new Error("Circular reference in options");
            r.add(e);
          }
          Object.entries(e).forEach(([i, a]) => {
            let o = t ? t + i : i;
            if (typeof a == "object") bs(a, o + ".", r, n);
            else if (typeof a == "string" || typeof a == "number") n(o, a.toString());
            else if (typeof a == "boolean") n(o, a ? "1" : "0");
            else throw new Error(`Can't handle extra config type: ${typeof a}`);
          });
        }, tr = (e) => {
          let t = _t(), r = t.stackSave();
          try {
            let n = t.PTR_SIZE, i = t.stackAlloc(2 * n);
            t._OrtGetLastError(i, i + n);
            let a = Number(t.getValue(i, n === 4 ? "i32" : "i64")), o = t.getValue(i + n, "*"), d = o ? t.UTF8ToString(o) : "";
            throw new Error(`${e} ERROR_CODE: ${a}, ERROR_MESSAGE: ${d}`);
          } finally {
            t.stackRestore(r);
          }
        };
      }), Rs, Js = w(() => {
        Ft(), ts(), Rs = (e) => {
          let t = _t(), r = 0, n = [], i = e || {};
          try {
            if ((e == null ? void 0 : e.logSeverityLevel) === void 0) i.logSeverityLevel = 2;
            else if (typeof e.logSeverityLevel != "number" || !Number.isInteger(e.logSeverityLevel) || e.logSeverityLevel < 0 || e.logSeverityLevel > 4) throw new Error(`log serverity level is not valid: ${e.logSeverityLevel}`);
            if ((e == null ? void 0 : e.logVerbosityLevel) === void 0) i.logVerbosityLevel = 0;
            else if (typeof e.logVerbosityLevel != "number" || !Number.isInteger(e.logVerbosityLevel)) throw new Error(`log verbosity level is not valid: ${e.logVerbosityLevel}`);
            (e == null ? void 0 : e.terminate) === void 0 && (i.terminate = !1);
            let a = 0;
            return (e == null ? void 0 : e.tag) !== void 0 && (a = lr(e.tag, n)), r = t._OrtCreateRunOptions(i.logSeverityLevel, i.logVerbosityLevel, !!i.terminate, a), r === 0 && tr("Can't create run options."), (e == null ? void 0 : e.extra) !== void 0 && bs(e.extra, "", /* @__PURE__ */ new WeakSet(), (o, d) => {
              let p = lr(o, n), h = lr(d, n);
              t._OrtAddRunConfigEntry(r, p, h) !== 0 && tr(`Can't set a run config entry: ${o} - ${d}.`);
            }), [r, n];
          } catch (a) {
            throw r !== 0 && t._OrtReleaseRunOptions(r), n.forEach((o) => t._free(o)), a;
          }
        };
      }), Ns, vs, Pn, js, Cn, Xn = w(() => {
        Ft(), ts(), Ns = (e) => {
          switch (e) {
            case "disabled":
              return 0;
            case "basic":
              return 1;
            case "extended":
              return 2;
            case "all":
              return 99;
            default:
              throw new Error(`unsupported graph optimization level: ${e}`);
          }
        }, vs = (e) => {
          switch (e) {
            case "sequential":
              return 0;
            case "parallel":
              return 1;
            default:
              throw new Error(`unsupported execution mode: ${e}`);
          }
        }, Pn = (e) => {
          e.extra || (e.extra = {}), e.extra.session || (e.extra.session = {});
          let t = e.extra.session;
          t.use_ort_model_bytes_directly || (t.use_ort_model_bytes_directly = "1"), e.executionProviders && e.executionProviders.some((r) => (typeof r == "string" ? r : r.name) === "webgpu") && (e.enableMemPattern = !1);
        }, js = (e, t, r) => {
          for (let n of t) {
            let i = typeof n == "string" ? n : n.name;
            switch (i) {
              case "webnn":
                if (i = "WEBNN", typeof n != "string") {
                  let o = n == null ? void 0 : n.deviceType;
                  if (o) {
                    let d = lr("deviceType", r), p = lr(o, r);
                    _t()._OrtAddSessionConfigEntry(e, d, p) !== 0 && tr(`Can't set a session config entry: 'deviceType' - ${o}.`);
                  }
                }
                break;
              case "webgpu":
                if (i = "JS", typeof n != "string") {
                  let o = n;
                  if (o != null && o.preferredLayout) {
                    if (o.preferredLayout !== "NCHW" && o.preferredLayout !== "NHWC") throw new Error(`preferredLayout must be either 'NCHW' or 'NHWC': ${o.preferredLayout}`);
                    let d = lr("preferredLayout", r), p = lr(o.preferredLayout, r);
                    _t()._OrtAddSessionConfigEntry(e, d, p) !== 0 && tr(`Can't set a session config entry: 'preferredLayout' - ${o.preferredLayout}.`);
                  }
                }
                break;
              case "wasm":
              case "cpu":
                continue;
              default:
                throw new Error(`not supported execution provider: ${i}`);
            }
            let a = lr(i, r);
            _t()._OrtAppendExecutionProvider(e, a) !== 0 && tr(`Can't append execution provider: ${i}.`);
          }
        }, Cn = (e) => {
          let t = _t(), r = 0, n = [], i = e || {};
          Pn(i);
          try {
            let a = Ns(i.graphOptimizationLevel ?? "all"), o = vs(i.executionMode ?? "sequential"), d = typeof i.logId == "string" ? lr(i.logId, n) : 0, p = i.logSeverityLevel ?? 2;
            if (!Number.isInteger(p) || p < 0 || p > 4) throw new Error(`log serverity level is not valid: ${p}`);
            let h = i.logVerbosityLevel ?? 0;
            if (!Number.isInteger(h) || h < 0 || h > 4) throw new Error(`log verbosity level is not valid: ${h}`);
            let k = typeof i.optimizedModelFilePath == "string" ? lr(i.optimizedModelFilePath, n) : 0;
            if (r = t._OrtCreateSessionOptions(a, !!i.enableCpuMemArena, !!i.enableMemPattern, o, !!i.enableProfiling, 0, d, p, h, k), r === 0 && tr("Can't create session options."), i.executionProviders && js(r, i.executionProviders, n), i.enableGraphCapture !== void 0) {
              if (typeof i.enableGraphCapture != "boolean") throw new Error(`enableGraphCapture must be a boolean value: ${i.enableGraphCapture}`);
              let S = lr("enableGraphCapture", n), u = lr(i.enableGraphCapture.toString(), n);
              t._OrtAddSessionConfigEntry(r, S, u) !== 0 && tr(`Can't set a session config entry: 'enableGraphCapture' - ${i.enableGraphCapture}.`);
            }
            if (i.freeDimensionOverrides) for (let [S, u] of Object.entries(i.freeDimensionOverrides)) {
              if (typeof S != "string") throw new Error(`free dimension override name must be a string: ${S}`);
              if (typeof u != "number" || !Number.isInteger(u) || u < 0) throw new Error(`free dimension override value must be a non-negative integer: ${u}`);
              let B = lr(S, n);
              t._OrtAddFreeDimensionOverride(r, B, u) !== 0 && tr(`Can't set a free dimension override: ${S} - ${u}.`);
            }
            return i.extra !== void 0 && bs(i.extra, "", /* @__PURE__ */ new WeakSet(), (S, u) => {
              let B = lr(S, n), R = lr(u, n);
              t._OrtAddSessionConfigEntry(r, B, R) !== 0 && tr(`Can't set a session config entry: ${S} - ${u}.`);
            }), [r, n];
          } catch (a) {
            throw r !== 0 && t._OrtReleaseSessionOptions(r) !== 0 && tr("Can't release session options."), n.forEach((o) => t._free(o)), a;
          }
        };
      }), Us, xs, ls, mn, Zs, _n, en, Ts, zt = w(() => {
        Us = (e) => {
          switch (e) {
            case "int8":
              return 3;
            case "uint8":
              return 2;
            case "bool":
              return 9;
            case "int16":
              return 5;
            case "uint16":
              return 4;
            case "int32":
              return 6;
            case "uint32":
              return 12;
            case "float16":
              return 10;
            case "float32":
              return 1;
            case "float64":
              return 11;
            case "string":
              return 8;
            case "int64":
              return 7;
            case "uint64":
              return 13;
            case "int4":
              return 22;
            case "uint4":
              return 21;
            default:
              throw new Error(`unsupported data type: ${e}`);
          }
        }, xs = (e) => {
          switch (e) {
            case 3:
              return "int8";
            case 2:
              return "uint8";
            case 9:
              return "bool";
            case 5:
              return "int16";
            case 4:
              return "uint16";
            case 6:
              return "int32";
            case 12:
              return "uint32";
            case 10:
              return "float16";
            case 1:
              return "float32";
            case 11:
              return "float64";
            case 8:
              return "string";
            case 7:
              return "int64";
            case 13:
              return "uint64";
            case 22:
              return "int4";
            case 21:
              return "uint4";
            default:
              throw new Error(`unsupported data type: ${e}`);
          }
        }, ls = (e, t) => {
          let r = [-1, 4, 1, 1, 2, 2, 4, 8, -1, 1, 2, 8, 4, 8, -1, -1, -1, -1, -1, -1, -1, 0.5, 0.5][e], n = typeof t == "number" ? t : t.reduce((i, a) => i * a, 1);
          return r > 0 ? Math.ceil(n * r) : void 0;
        }, mn = (e) => {
          switch (e) {
            case "float16":
              return typeof Float16Array < "u" && Float16Array.from ? Float16Array : Uint16Array;
            case "float32":
              return Float32Array;
            case "uint8":
              return Uint8Array;
            case "int8":
              return Int8Array;
            case "uint16":
              return Uint16Array;
            case "int16":
              return Int16Array;
            case "int32":
              return Int32Array;
            case "bool":
              return Uint8Array;
            case "float64":
              return Float64Array;
            case "uint32":
              return Uint32Array;
            case "int64":
              return BigInt64Array;
            case "uint64":
              return BigUint64Array;
            default:
              throw new Error(`unsupported type: ${e}`);
          }
        }, Zs = (e) => {
          switch (e) {
            case "verbose":
              return 0;
            case "info":
              return 1;
            case "warning":
              return 2;
            case "error":
              return 3;
            case "fatal":
              return 4;
            default:
              throw new Error(`unsupported logging level: ${e}`);
          }
        }, _n = (e) => e === "float32" || e === "float16" || e === "int32" || e === "int64" || e === "uint32" || e === "uint8" || e === "bool" || e === "uint4" || e === "int4", en = (e) => e === "float32" || e === "float16" || e === "int32" || e === "int64" || e === "uint32" || e === "uint64" || e === "int8" || e === "uint8" || e === "bool" || e === "uint4" || e === "int4", Ts = (e) => {
          switch (e) {
            case "none":
              return 0;
            case "cpu":
              return 1;
            case "cpu-pinned":
              return 2;
            case "texture":
              return 3;
            case "gpu-buffer":
              return 4;
            case "ml-tensor":
              return 5;
            default:
              throw new Error(`unsupported data location: ${e}`);
          }
        };
      }), fn, kn = w(() => {
        st(), fn = async (e) => {
          if (typeof e == "string") {
            let t = await fetch(e);
            if (!t.ok) throw new Error(`failed to load external data file: ${e}`);
            let r = t.headers.get("Content-Length"), n = r ? parseInt(r, 10) : 0;
            if (n < 1073741824) return new Uint8Array(await t.arrayBuffer());
            {
              if (!t.body) throw new Error(`failed to load external data file: ${e}, no response body.`);
              let i = t.body.getReader(), a;
              try {
                a = new ArrayBuffer(n);
              } catch (d) {
                if (d instanceof RangeError) {
                  let p = Math.ceil(n / 65536);
                  a = new WebAssembly.Memory({ initial: p, maximum: p }).buffer;
                } else throw d;
              }
              let o = 0;
              for (; ; ) {
                let { done: d, value: p } = await i.read();
                if (d) break;
                let h = p.byteLength;
                new Uint8Array(a, o, h).set(p), o += h;
              }
              return new Uint8Array(a, 0, n);
            }
          } else return e instanceof Blob ? new Uint8Array(await e.arrayBuffer()) : e instanceof Uint8Array ? e : new Uint8Array(e);
        };
      }), Sn, $n, Ws, An, gn, In, or, Yr = w(() => {
        zt(), Sn = ["V", "I", "W", "E", "F"], $n = (e, t) => {
          console.log(`[${Sn[e]},${(/* @__PURE__ */ new Date()).toISOString()}]${t}`);
        }, gn = (e, t) => {
          Ws = e, An = t;
        }, In = (e, t) => {
          let r = Zs(e), n = Zs(Ws);
          r >= n && $n(r, typeof t == "function" ? t() : t);
        }, or = (...e) => {
          An && In(...e);
        };
      }), wn, Fn = w(() => {
        zt(), wn = (e, t) => new (mn(t))(e);
      }), yn = w(() => {
      }), Mn, Te, P, H, ae, Me, Pe, He, ct, yt = w(() => {
        Yr(), yn(), Mn = /* @__PURE__ */ new Map([[64, 250], [128, 200], [256, 200], [512, 200], [2048, 230], [4096, 200], [8192, 50], [16384, 50], [32768, 50], [65536, 50], [131072, 50], [262144, 50], [524288, 50], [1048576, 50], [2097152, 30], [4194304, 20], [8388608, 10], [12582912, 10], [16777216, 10], [26214400, 15], [33554432, 22], [44236800, 2], [58982400, 6], [67108864, 6], [134217728, 6], [167772160, 6]]), Te = [], P = (e) => Math.ceil(Number(e) / 16) * 16, H = (e) => {
          for (let t = 0; t < Te.length; t++) {
            let r = Te[t];
            if (e <= r) return r;
          }
          return Math.ceil(e / 16) * 16;
        }, ae = 1, Me = () => ae++, Pe = async (e, t, r, n) => {
          let i = P(r), a = e.device.createBuffer({ size: i, usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ });
          try {
            let o = e.getCommandEncoder();
            e.endComputePass(), o.copyBufferToBuffer(t, 0, a, 0, i), e.flush(), await a.mapAsync(GPUMapMode.READ);
            let d = a.getMappedRange();
            if (n) {
              let p = n();
              return p.set(new Uint8Array(d, 0, r)), p;
            } else return new Uint8Array(d.slice(0, r));
          } finally {
            a.destroy();
          }
        }, He = class {
          constructor(e) {
            this.backend = e, this.storageCache = /* @__PURE__ */ new Map(), this.freeBuffers = /* @__PURE__ */ new Map(), this.freeUniformBuffers = /* @__PURE__ */ new Map(), this.buffersPending = [], this.capturedPendingBuffers = /* @__PURE__ */ new Map();
            for (let [t] of Mn) Te.push(t), this.freeBuffers.set(t, []), this.freeUniformBuffers.set(t, []);
            this.sessionCount = 0;
          }
          upload(e, t) {
            let r = t.buffer, n = t.byteOffset, i = t.byteLength, a = P(i), o = this.storageCache.get(e);
            if (!o) throw new Error("gpu data for uploading does not exist");
            if (Number(o.originalSize) !== i) throw new Error(`inconsistent data size. gpu data size=${o.originalSize}, data size=${i}`);
            let d = this.backend.device.createBuffer({ mappedAtCreation: !0, size: a, usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC }), p = d.getMappedRange();
            new Uint8Array(p).set(new Uint8Array(r, n, i)), d.unmap();
            let h = this.backend.device.createCommandEncoder();
            h.copyBufferToBuffer(d, 0, o.gpuData.buffer, 0, a), this.backend.device.queue.submit([h.finish()]), d.destroy(), or("verbose", () => `[WebGPU] GpuDataManager.upload(id=${e})`);
          }
          memcpy(e, t) {
            let r = this.storageCache.get(e);
            if (!r) throw new Error("source gpu data for memcpy does not exist");
            let n = this.storageCache.get(t);
            if (!n) throw new Error("destination gpu data for memcpy does not exist");
            if (r.originalSize !== n.originalSize) throw new Error("inconsistent source and destination gpu data size");
            let i = P(r.originalSize), a = this.backend.getCommandEncoder();
            this.backend.endComputePass(), a.copyBufferToBuffer(r.gpuData.buffer, 0, n.gpuData.buffer, 0, i);
          }
          registerExternalBuffer(e, t, r) {
            let n;
            if (r) {
              if (n = r[0], e === r[1]) return or("verbose", () => `[WebGPU] GpuDataManager.registerExternalBuffer(size=${t}) => id=${n}, buffer is the same, skip.`), n;
              if (this.backend.capturedCommandList.has(this.backend.currentSessionId)) throw new Error(`Registering a different external buffer under graph capture mode is not supported yet.
             Please use the previous external buffer!`);
            } else n = Me();
            return this.storageCache.set(n, { gpuData: { id: n, type: 0, buffer: e }, originalSize: t }), or("verbose", () => `[WebGPU] GpuDataManager.registerExternalBuffer(size=${t}) => id=${n}, registered.`), n;
          }
          unregisterExternalBuffer(e) {
            e !== void 0 && (this.storageCache.delete(e), or("verbose", () => `[WebGPU] GpuDataManager.unregisterExternalBuffer() => id=${e}`));
          }
          create(e, t = GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST) {
            let r = H(e), n, i = (t & GPUBufferUsage.STORAGE) === GPUBufferUsage.STORAGE, a = (t & GPUBufferUsage.UNIFORM) === GPUBufferUsage.UNIFORM;
            if (i || a) {
              let d = (i ? this.freeBuffers : this.freeUniformBuffers).get(r);
              d ? d.length > 0 ? n = d.pop() : n = this.backend.device.createBuffer({ size: r, usage: t }) : n = this.backend.device.createBuffer({ size: r, usage: t });
            } else n = this.backend.device.createBuffer({ size: r, usage: t });
            let o = { id: Me(), type: 0, buffer: n };
            return this.storageCache.set(o.id, { gpuData: o, originalSize: Number(e) }), or("verbose", () => `[WebGPU] GpuDataManager.create(size=${e}) => id=${o.id}`), o;
          }
          get(e) {
            var t;
            return (t = this.storageCache.get(e)) == null ? void 0 : t.gpuData;
          }
          release(e) {
            let t = typeof e == "bigint" ? Number(e) : e, r = this.storageCache.get(t);
            if (!r) {
              if (this.storageCache.size === 0) return 0;
              throw new Error("releasing data does not exist");
            }
            return or("verbose", () => `[WebGPU] GpuDataManager.release(id=${t}), gpuDataId=${r.gpuData.id}`), this.storageCache.delete(t), this.buffersPending.push(r.gpuData.buffer), r.originalSize;
          }
          async download(e, t) {
            let r = this.storageCache.get(Number(e));
            if (!r) throw new Error("data does not exist");
            await Pe(this.backend, r.gpuData.buffer, r.originalSize, t);
          }
          refreshPendingBuffers() {
            if (this.buffersPending.length !== 0) if (this.backend.sessionStatus === "default") {
              for (let e of this.buffersPending) {
                let t = Mn.get(e.size);
                if ((e.usage & GPUBufferUsage.STORAGE) === GPUBufferUsage.STORAGE) {
                  let r = this.freeBuffers.get(e.size) || [];
                  t === void 0 || r.length >= t ? e.destroy() : r.push(e);
                } else if ((e.usage & GPUBufferUsage.UNIFORM) === GPUBufferUsage.UNIFORM) {
                  let r = this.freeUniformBuffers.get(e.size) || [];
                  t === void 0 || r.length >= t ? e.destroy() : r.push(e);
                } else e.destroy();
              }
              this.buffersPending = [];
            } else {
              let e = this.capturedPendingBuffers.get(this.backend.currentSessionId);
              e || (e = [], this.capturedPendingBuffers.set(this.backend.currentSessionId, e));
              for (let t of this.buffersPending) e.push(t);
              this.buffersPending = [];
            }
          }
          dispose() {
            this.freeBuffers.forEach((e) => {
              e.forEach((t) => {
                t.destroy();
              });
            }), this.freeUniformBuffers.forEach((e) => {
              e.forEach((t) => {
                t.destroy();
              });
            }), this.storageCache.forEach((e) => {
              e.gpuData.buffer.destroy();
            }), this.capturedPendingBuffers.forEach((e) => {
              e.forEach((t) => {
                t.destroy();
              });
            }), this.storageCache = /* @__PURE__ */ new Map(), this.freeBuffers = /* @__PURE__ */ new Map(), this.freeUniformBuffers = /* @__PURE__ */ new Map(), this.capturedPendingBuffers = /* @__PURE__ */ new Map();
          }
          onCreateSession() {
            this.sessionCount += 1;
          }
          onReleaseSession(e) {
            let t = this.capturedPendingBuffers.get(e);
            t && (t.forEach((r) => {
              r.destroy();
            }), this.capturedPendingBuffers.delete(e)), this.sessionCount -= 1, this.sessionCount === 0 && (or("warning", () => "[WebGPU] Clearing webgpu buffer cache"), this.storageCache.forEach((r) => {
              r.gpuData.buffer.destroy();
            }), this.storageCache = /* @__PURE__ */ new Map());
          }
        }, ct = (...e) => new He(...e);
      }), ht, ot, Pt = w(() => {
        ht = class {
          constructor(e) {
            Object.assign(this, e);
          }
          get cacheKey() {
            return this.key || (this.key = Object.getOwnPropertyNames(this).sort().map((e) => `${this[e]}`).join(";")), this.key;
          }
        }, ot = (e) => new ht(e);
      }), hr, rr, $e, wr, Rr, Jr, Zr, Bt = w(() => {
        hr = class {
          static calcMatMulShape(e, t) {
            return e[1] !== t[0] ? void 0 : [e[0], t[1]];
          }
        }, rr = class {
          static calcShape(e, t, r = !1) {
            let n = e.length, i = t.length;
            if (n === 0) return t;
            if (i === 0) return e;
            let a = Math.max(e.length, t.length), o = new Array(a);
            if (r) {
              if (n < 2 || i < 2) return;
              let d = hr.calcMatMulShape([e[n - 2], e[n - 1]], [t[i - 2], t[i - 1]]);
              if (d === void 0) return;
              [o[a - 2], o[a - 1]] = d;
            }
            for (let d = r ? 3 : 1; d <= a; d++) {
              let p = n - d < 0 ? 1 : e[n - d], h = i - d < 0 ? 1 : t[i - d];
              if (p !== h && p > 1 && h > 1) return;
              let k = Math.max(p, h);
              if (p && h) o[a - d] = Math.max(p, h);
              else {
                if (k > 1) return;
                o[a - d] = 0;
              }
            }
            return o;
          }
          static isValidBroadcast(e, t) {
            let r = e.length, n = t.length;
            if (r > n) return !1;
            for (let i = 1; i <= r; i++) if (e[r - i] !== 1 && e[r - i] !== t[n - i]) return !1;
            return !0;
          }
        }, $e = class Op {
          static size(t) {
            return Op.getSizeFromDimensionRange(t, 0, t.length);
          }
          static convertShape(t, r = 4) {
            let n = t.length;
            if (n === 0) return [];
            let i = new Array(n), a = n - 1;
            for (; a >= 0; ) {
              if (t[a] % r === 0) {
                i[a] = t[a] / r;
                break;
              }
              if (r % t[a] !== 0) throw new Error("cannot convert shape");
              i[a] = 1, r /= t[a], a--;
            }
            for (a--; a >= 0; a--) i[a] = t[a];
            return i;
          }
          static sizeFromDimension(t, r) {
            if (r < 0 || r > t.length) throw new Error(`invalid dimension of ${r} for sizeFromDimension as Tensor has ${t.length} dimensions.`);
            return Op.getSizeFromDimensionRange(t, r, t.length);
          }
          static sizeToDimension(t, r) {
            if (r < 0 || r > t.length) throw new Error(`invalid dimension of ${r} for sizeToDimension as Tensor has ${t.length} dimensions.`);
            return Op.getSizeFromDimensionRange(t, 0, r);
          }
          static getSizeFromDimensionRange(t, r, n) {
            let i = 1;
            for (let a = r; a < n; a++) {
              if (t[a] < 0) throw new Error("cannot get valid size from specified dimension range. Most likely the range contains negative values in them.");
              i *= Number(t[a]);
            }
            return i;
          }
          static computeStrides(t) {
            let r = t.length;
            if (r === 0) return [];
            if (r === 1) return [1];
            let n = new Array(r);
            n[r - 1] = 1, n[r - 2] = t[r - 1];
            for (let i = r - 3; i >= 0; --i) n[i] = n[i + 1] * t[i + 1];
            return n;
          }
          static normalizeAxis(t, r) {
            if (t < -r && t >= r) throw new Error("unsupported axis for this operation.");
            return t < 0 ? t + r : t;
          }
          static normalizeAxes(t, r) {
            return t.map((n) => this.normalizeAxis(n, r ?? t.length));
          }
          static sortBasedOnPerm(t, r) {
            return r ? r.map((n) => t[n]) : t.slice().reverse();
          }
          static padShape(t, r) {
            let n = t.length;
            return t.map((i, a) => i + r[a] + r[a + n]);
          }
          static areEqual(t, r) {
            return t.length !== r.length ? !1 : t.every((n, i) => n === r[i]);
          }
        }, wr = class Cc {
          static adjustPoolAttributes(t, r, n, i, a, o) {
            if (!t && n.length !== r.length - 2) throw new Error("length of specified kernel shapes should be 2 less than length of input dimensions");
            if (t) for (let d = 0; d < r.length - 2; d++) d >= n.length ? n.push(r[d + 2]) : n[d] = r[d + 2];
            for (let d = 0; d < n.length; d++) if (d < i.length) {
              if (i[d] < 0) throw new Error("strides should be greater than or equal to 1");
            } else i.push(1);
            for (let d = 0; d < n.length; d++) if (d < a.length) {
              if (a[d] < 0) throw new Error("dilations should be greater than or equal to 1");
            } else a.push(1);
            for (let d = 0; d < n.length * 2; d++) if (d < o.length) {
              if (o[d] < 0) throw new Error("pad should be greater than or equal to 1");
            } else o.push(0);
            for (let d = 0; d < n.length; d++) {
              if (n[d] <= 0) throw new Error("kernel shapes need to be greater than 0");
              if (o[d] >= n[d] || o[d + n.length] >= n[d]) throw new Error("pads should be smaller than kernel");
            }
          }
          static adjustPadsBasedOnAutoPad(t, r, n, i, a, o, d) {
            if (d) {
              if (a.length !== 2 * (t.length - 2)) throw new Error("length of pads should be twice the length of data dimensions");
              if (r.length !== t.length - 2) throw new Error("length of strides should be the length of data dimensions");
              if (i.length !== t.length - 2) throw new Error("length of kernel shapes should be the length of data dimensions");
              for (let p = 0; p < t.length - 2; p++) Cc.adjustPadAndReturnShape(t[p + (o ? 1 : 2)], r[p], n[p], i[p], a, p, p + t.length - 2, d);
            }
          }
          static computePoolOutputShape(t, r, n, i, a, o, d) {
            if (r.length <= 0) throw new Error("input shape must be of size greater than 0");
            let p = [r[0], r[1]];
            return Cc.computeShapeHelper(t, r, p, n, i, a, o, d), p;
          }
          static computeConvOutputShape(t, r, n, i, a, o, d) {
            if (t.length <= 0 || r.length <= 0) throw new Error("invalid input tensor dims or invalid filter tensor dims");
            let p = [t[0], r[0]];
            return Cc.computeShapeHelper(!1, t, p, n, i, a, o, d), p;
          }
          static computeShapeHelper(t, r, n, i, a, o, d, p) {
            if (t) for (let h = 0; h < r.length - 2; h++) n.push(1);
            else for (let h = 0; h < r.length - 2; h++) n.push(Cc.adjustPadAndReturnShape(r[h + 2], i[h], a[h], o[h], d, h, h + r.length - 2, p));
          }
          static adjustPadAndReturnShape(t, r, n, i, a, o, d, p) {
            let h = n * (i - 1) + 1;
            if (p && p !== "NOTSET") switch (p) {
              case "VALID":
                return a[o] = 0, a[d] = 0, Math.floor((t - h) / r + 1);
              case "SAME_LOWER":
              case "SAME_UPPER":
                if (n !== 1) throw new Error("Dilation not supported for SAME_UPPER or SAME_LOWER");
                {
                  let k = ((t + r - 1) / r - 1) * r + i - t;
                  return a[o] = Math.floor(p === "SAME_LOWER" ? (k + 1) / 2 : k / 2), a[d] = k - a[o], Math.floor((t + k - i) / r + 1);
                }
              default:
                throw new Error("Unsupported AutoPad type");
            }
            else return Math.floor((t + a[o] + a[d] - h) / r + 1);
          }
        }, Rr = class {
          static getShapeOfGemmResult(e, t, r, n, i) {
            if (e.length !== 2 || r.length !== 2) throw new Error("shape need to be of size 2");
            let a, o, d;
            t ? (a = e[1], o = e[0]) : (a = e[0], o = e[1]);
            let p = -1;
            if (n ? (d = r[0], p = 1) : (d = r[1], p = 0), r[p] !== o) throw new Error("dimension mismatch");
            if (a <= 0 || d <= 0 || o <= 0) throw new Error("invalid shape specified");
            if (i && !rr.isValidBroadcast(i, [a, d])) throw new Error("gemm: invalid bias shape for broadcast");
            return [a, d, o];
          }
        }, Jr = -34028234663852886e22, Zr = 34028234663852886e22;
      }), Nr, ps, er, mr, vt, yr, Es, Or, qr, Mt, br, ze, wt, rs, tn, Qn, bn, Qt = w(() => {
        zt(), Bt(), Nr = 64, ps = (e, t) => {
          if (t === 3) throw new Error("vec3 has same alignment as vec4, use vec4 instead");
          switch (Number(e)) {
            case 10:
              return t > 1 ? `vec${t}<f16>` : "f16";
            case 1:
              return t > 1 ? `vec${t}<f32>` : "f32";
            case 6:
              return t > 1 ? `vec${t}<i32>` : "i32";
            case 12:
              return t > 1 ? `vec${t}<u32>` : "u32";
            case 7:
              if (t > 1) throw new Error("currently not supported vecX of uint64 yet");
              return ["vec2<u32>", "i32"];
            case 13:
              if (t > 1) throw new Error("currently not supported vecX of uint64 yet");
              return ["vec2<u32>", "u32"];
            case 9:
              if (t !== 4) throw new Error("bool must be vec4");
              return ["u32", "vec4<bool>"];
            case 22:
              return "i32";
            case 21:
              return "u32";
            default:
              throw new Error(`Unknown data type: ${e}`);
          }
        }, er = (e, t = 1) => {
          let r = ps(e, t);
          return typeof r == "string" ? r : r[0];
        }, mr = (e, t = 1) => {
          let r = ps(e, t);
          return typeof r == "string" ? r : r[1];
        }, vt = (...e) => {
          let t = [];
          return e.forEach((r) => {
            r.length !== 0 && t.push({ type: 12, data: r }, { type: 12, data: $e.computeStrides(r) });
          }), t;
        }, yr = (e) => e % 4 === 0 ? 4 : e % 2 === 0 ? 2 : 1, Es = (e = "f32", t, r = "0") => !t || t === 1 ? `${e}(${r})` : `vec${t}<${e}>(${r})`, Or = (e, t, r) => e === "f32" ? r : t === 1 ? `f32(${r})` : `vec${t}<f32>(${r})`, qr = (e, t) => t === 4 ? `(${e}.x + ${e}.y + ${e}.z + ${e}.w)` : t === 2 ? `(${e}.x + ${e}.y)` : t === 3 ? `(${e}.x + ${e}.y + ${e}.z)` : e, Mt = (e, t, r, n) => e.startsWith("uniforms.") && r > 4 ? typeof t == "string" ? n === "f16" ? `${e}[(${t}) / 8][(${t}) % 8 / 4][(${t}) % 8 % 4]` : `${e}[(${t}) / 4][(${t}) % 4]` : n === "f16" ? `${e}[${Math.floor(t / 8)}][${Math.floor(t % 8 / 4)}][${t % 8 % 4}]` : `${e}[${Math.floor(t / 4)}][${t % 4}]` : r > 1 ? `${e}[${t}]` : e, br = (e, t, r, n, i) => {
          let a = typeof r == "number", o = a ? r : r.length, d = [...new Array(o).keys()], p = o < 2 ? "u32" : o <= 4 ? `vec${o}<u32>` : `array<u32, ${o}>`, h = ps(t, i), k = typeof h == "string" ? h : h[1], S = typeof h == "string" ? h : h[0], u = { indices: p, value: k, storage: S, tensor: t }, B = (it) => typeof it == "string" ? it : `${it}u`, R = { offsetToIndices: !1, indicesToOffset: !1, broadcastedIndicesToOffset: !1, set: !1, setByIndices: !1, get: !1, getByIndices: !1 }, N = a ? "uniforms." : "", Z = `${N}${e}_shape`, te = `${N}${e}_strides`, Q = "";
          for (let it = 0; it < o - 1; it++) Q += `
    let dim${it} = current / ${Mt(te, it, o)};
    let rest${it} = current % ${Mt(te, it, o)};
    indices[${it}] = dim${it};
    current = rest${it};
    `;
          Q += `indices[${o - 1}] = current;`;
          let _e = o < 2 ? "" : `
  fn o2i_${e}(offset: u32) -> ${u.indices} {
    var indices: ${u.indices};
    var current = offset;
    ${Q}
    return indices;
  }`, me = (it) => (R.offsetToIndices = !0, o < 2 ? it : `o2i_${e}(${it})`), ye = [];
          if (o >= 2) for (let it = o - 1; it >= 0; it--) ye.push(`${Mt(te, it, o)} * (indices[${it}])`);
          let Ae = o < 2 ? "" : `
  fn i2o_${e}(indices: ${u.indices}) -> u32 {
    return ${ye.join("+")};
  }`, Ie = (it) => (R.indicesToOffset = !0, o < 2 ? it : `i2o_${e}(${it})`), Ge = (...it) => o === 0 ? "0u" : `${u.indices}(${it.map(B).join(",")})`, lt = (it, Et) => o < 2 ? `${it}` : `${Mt(it, Et, o)}`, Tt = (it, Et, cr) => o < 2 ? `${it}=${cr};` : `${Mt(it, Et, o)}=${cr};`, Kt = {}, Yt = (it, Et) => {
            R.broadcastedIndicesToOffset = !0;
            let cr = `${Et.name}broadcastedIndicesTo${e}Offset`;
            if (cr in Kt) return `${cr}(${it})`;
            let Lr = [];
            for (let ys = o - 1; ys >= 0; ys--) {
              let Hr = Et.indicesGet("outputIndices", ys + Et.rank - o);
              Lr.push(`${lt(te, ys)} * (${Hr} % ${lt(Z, ys)})`);
            }
            return Kt[cr] = `fn ${cr}(outputIndices: ${Et.type.indices}) -> u32 {
             return ${Lr.length > 0 ? Lr.join("+") : "0u"};
           }`, `${cr}(${it})`;
          }, Ct = (it, Et) => (() => {
            if (u.storage === u.value) return `${e}[${it}]=${Et};`;
            if (u.storage === "vec2<u32>" && u.value === "i32") return `${e}[${it}]=vec2<u32>(u32(${Et}), select(0u, 0xFFFFFFFFu, ${Et} < 0));`;
            if (u.storage === "vec2<u32>" && u.value === "u32") return `${e}[${it}]=vec2<u32>(u32(${Et}), 0u);`;
            if (u.storage === "u32" && u.value === "vec4<bool>") return `${e}[${it}]=dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(${Et}));`;
            throw new Error(`not supported combination of storage type ${u.storage} and value type ${u.value} yet`);
          })(), Jt = (it) => (() => {
            if (u.storage === u.value) return `${e}[${it}]`;
            if (u.storage === "vec2<u32>" && u.value === "i32") return `i32(${e}[${it}].x)`;
            if (u.storage === "vec2<u32>" && u.value === "u32") return `u32(${e}[${it}].x)`;
            if (u.storage === "u32" && u.value === "vec4<bool>") return `vec4<bool>(bool(${e}[${it}] & 0xFFu), bool(${e}[${it}] & 0xFF00u), bool(${e}[${it}] & 0xFF0000u), bool(${e}[${it}] & 0xFF000000u))`;
            throw new Error(`not supported combination of storage type ${u.storage} and value type ${u.value} yet`);
          })(), $t = o < 2 ? "" : `
  fn get_${e}ByIndices(indices: ${u.indices}) -> ${k} {
    return ${Jt(`i2o_${e}(indices)`)};
  }`, jt = o < 2 ? "" : (() => {
            let it = d.map((cr) => `d${cr}: u32`).join(", "), Et = d.map((cr) => `d${cr}`).join(", ");
            return `
  fn get_${e}(${it}) -> ${k} {
    return get_${e}ByIndices(${Ge(Et)});
  }`;
          })(), vr = (...it) => {
            if (it.length !== o) throw new Error(`indices length must be ${o}`);
            let Et = it.map(B).join(",");
            return o === 0 ? Jt("0u") : o === 1 ? Jt(Et[0]) : (R.get = !0, R.getByIndices = !0, R.indicesToOffset = !0, `get_${e}(${Et})`);
          }, Ht = (it) => o < 2 ? Jt(it) : (R.getByIndices = !0, R.indicesToOffset = !0, `get_${e}ByIndices(${it})`), Gt = o < 2 ? "" : `
  fn set_${e}ByIndices(indices: ${u.indices}, value: ${k}) {
    ${Ct(`i2o_${e}(indices)`, "value")}
  }`, Cr = o < 2 ? "" : (() => {
            let it = d.map((cr) => `d${cr}: u32`).join(", "), Et = d.map((cr) => `d${cr}`).join(", ");
            return `
  fn set_${e}(${it}, value: ${k}) {
    set_${e}ByIndices(${Ge(Et)}, value);
  }`;
          })();
          return { impl: () => {
            let it = [], Et = !1;
            return R.offsetToIndices && (it.push(_e), Et = !0), R.indicesToOffset && (it.push(Ae), Et = !0), R.broadcastedIndicesToOffset && (Object.values(Kt).forEach((cr) => it.push(cr)), Et = !0), R.set && (it.push(Cr), Et = !0), R.setByIndices && (it.push(Gt), Et = !0), R.get && (it.push(jt), Et = !0), R.getByIndices && (it.push($t), Et = !0), !a && Et && it.unshift(`const ${Z} = ${u.indices}(${r.join(",")});`, `const ${te} = ${u.indices}(${$e.computeStrides(r).join(",")});`), it.join(`
`);
          }, type: u, offsetToIndices: me, indicesToOffset: Ie, broadcastedIndicesToOffset: Yt, indices: Ge, indicesGet: lt, indicesSet: Tt, set: (...it) => {
            if (it.length !== o + 1) throw new Error(`indices length must be ${o}`);
            let Et = it[o];
            if (typeof Et != "string") throw new Error("value must be string");
            let cr = it.slice(0, o).map(B).join(",");
            return o === 0 ? Ct("0u", Et) : o === 1 ? Ct(cr[0], Et) : (R.set = !0, R.setByIndices = !0, R.indicesToOffset = !0, `set_${e}(${cr}, ${Et})`);
          }, setByOffset: Ct, setByIndices: (it, Et) => o < 2 ? Ct(it, Et) : (R.setByIndices = !0, R.indicesToOffset = !0, `set_${e}ByIndices(${it}, ${Et});`), get: vr, getByOffset: Jt, getByIndices: Ht, usage: n, name: e, strides: te, shape: Z, rank: o };
        }, ze = (e, t, r, n = 1) => br(e, t, r, "input", n), wt = (e, t, r, n = 1) => br(e, t, r, "output", n), rs = (e, t, r) => br(e, t, r, "atomicOutput", 1), tn = (e, t, r, n = 1) => br(e, t, r, "internal", n), Qn = class {
          constructor(e, t) {
            this.normalizedDispatchGroup = e, this.limits = t, this.internalVariables = [], this.variables = [], this.uniforms = [], this.variableIndex = 0;
          }
          guardAgainstOutOfBoundsWorkgroupSizes(e) {
            return `if (global_idx >= ${typeof e == "number" ? `${e}u` : e}) { return; }`;
          }
          mainStart(e = Nr) {
            let t = typeof e == "number" ? e : e[0], r = typeof e == "number" ? 1 : e[1], n = typeof e == "number" ? 1 : e[2];
            if (t > this.limits.maxComputeWorkgroupSizeX || r > this.limits.maxComputeWorkgroupSizeY || n > this.limits.maxComputeWorkgroupSizeZ) throw new Error(`workgroup size [${t}, ${r}, ${n}] exceeds the maximum workgroup size [${this.limits.maxComputeWorkgroupSizeX}, ${this.limits.maxComputeWorkgroupSizeY}, ${this.limits.maxComputeWorkgroupSizeZ}].`);
            if (t * r * n > this.limits.maxComputeInvocationsPerWorkgroup) throw new Error(`workgroup size [${t}, ${r}, ${n}] exceeds the maximum workgroup invocations ${this.limits.maxComputeInvocationsPerWorkgroup}.`);
            let i = this.normalizedDispatchGroup[1] === 1 && this.normalizedDispatchGroup[2] === 1, a = i ? `@builtin(global_invocation_id) global_id : vec3<u32>,
    @builtin(workgroup_id) workgroup_id : vec3<u32>,
    @builtin(local_invocation_index) local_idx : u32,
    @builtin(local_invocation_id) local_id : vec3<u32>` : `@builtin(global_invocation_id) global_id : vec3<u32>,
                                             @builtin(local_invocation_id) local_id : vec3<u32>,
    @builtin(local_invocation_index) local_idx : u32,
    @builtin(workgroup_id) workgroup_id : vec3<u32>,
    @builtin(num_workgroups) num_workgroups : vec3<u32>`, o = i ? `let global_idx = global_id.x;
         let workgroup_index = workgroup_id.x;` : `let workgroup_index = workgroup_id.z * num_workgroups[0] * num_workgroups[1] +
             workgroup_id.y * num_workgroups[0] + workgroup_id.x;
         let global_idx = workgroup_index * ${t * r * n}u + local_idx;`;
            return `@compute @workgroup_size(${t}, ${r}, ${n})
  fn main(${a}) {
    ${o}
  `;
          }
          appendVariableUniforms(e) {
            e.rank !== 0 && (e.shape.startsWith("uniforms.") && this.uniforms.push({ name: e.shape.replace("uniforms.", ""), type: "u32", length: e.rank }), e.strides.startsWith("uniforms.") && this.uniforms.push({ name: e.strides.replace("uniforms.", ""), type: "u32", length: e.rank }));
          }
          declareVariable(e, t) {
            if (e.usage === "internal") throw new Error("cannot use internal variable with declareVariable(). use registerInternalVariables() instead.");
            this.variables.push(e), this.appendVariableUniforms(e);
            let r = e.usage === "input" ? "read" : "read_write", n = e.usage === "atomicOutput" ? "atomic<i32>" : e.type.storage;
            return `@group(0) @binding(${t}) var<storage, ${r}> ${e.name}: array<${n}>;`;
          }
          declareVariables(...e) {
            return e.map((t) => this.declareVariable(t, this.variableIndex++)).join(`
`);
          }
          registerInternalVariable(e) {
            if (e.usage !== "internal") throw new Error("cannot use input or output variable with registerInternalVariable(). use declareVariables() instead.");
            this.internalVariables.push(e), this.appendVariableUniforms(e);
          }
          registerInternalVariables(...e) {
            return e.forEach((t) => this.registerInternalVariable(t)), this;
          }
          registerUniform(e, t, r = 1) {
            return this.uniforms.push({ name: e, type: t, length: r }), this;
          }
          registerUniforms(e) {
            return this.uniforms = this.uniforms.concat(e), this;
          }
          uniformDeclaration() {
            if (this.uniforms.length === 0) return "";
            let e = [];
            for (let { name: t, type: r, length: n } of this.uniforms) if (n && n > 4) r === "f16" ? e.push(`@align(16) ${t}:array<mat2x4<${r}>, ${Math.ceil(n / 8)}>`) : e.push(`${t}:array<vec4<${r}>, ${Math.ceil(n / 4)}>`);
            else {
              let i = n == null || n === 1 ? r : `vec${n}<${r}>`;
              e.push(`${t}:${i}`);
            }
            return `
      struct Uniforms { ${e.join(", ")} };
      @group(0) @binding(${this.variableIndex}) var<uniform> uniforms: Uniforms;`;
          }
          get additionalImplementations() {
            return this.uniformDeclaration() + this.variables.map((e) => e.impl()).join(`
`) + this.internalVariables.map((e) => e.impl()).join(`
`);
          }
          get variablesInfo() {
            if (this.uniforms.length === 0) return;
            let e = (t) => [12, 10, 1, 6][["u32", "f16", "f32", "i32"].indexOf(t)];
            return this.uniforms.map((t) => [e(t.type), t.length ?? 1]);
          }
        }, bn = (e, t) => new Qn(e, t);
      }), ya, zi, Ma, ba, Bi, va, ss, Ri, xa, Vs = w(() => {
        zt(), Bt(), Pt(), Qt(), ya = (e) => {
          if (!e || e.length !== 1) throw new Error("Transpose requires 1 input.");
        }, zi = (e, t) => t && t.length !== e ? [...new Array(e).keys()].reverse() : t, Ma = (e, t) => $e.sortBasedOnPerm(e, zi(e.length, t)), ba = (e, t, r, n) => {
          let i = `fn perm(i: ${n.type.indices}) -> ${r.type.indices} {
    var a: ${r.type.indices};`;
          for (let a = 0; a < t; ++a) i += `a[${e[a]}]=i[${a}];`;
          return i += "return a;}";
        }, Bi = (e, t) => {
          let r = [], n = [];
          for (let i = 0; i < e.length; ++i) e[i] !== 1 && r.push(e[i]), e[t[i]] !== 1 && n.push(t[i]);
          return { newShape: r, newPerm: n };
        }, va = (e, t) => {
          let r = 0;
          for (let n = 0; n < e.length; ++n) if (t[e[n]] !== 1) {
            if (e[n] < r) return !1;
            r = e[n];
          }
          return !0;
        }, ss = (e, t) => {
          let r = e.dataType, n = e.dims.length, i = zi(n, t), a = Ma(e.dims, i), o = e.dims, d = a, p = n < 2 || va(i, e.dims), h;
          if (p) return h = (R) => {
            let N = ze("input", r, o, 4), Z = wt("output", r, d, 4);
            return `
  ${R.registerUniform("output_size", "u32").declareVariables(N, Z)}
  ${R.mainStart()}
    ${R.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    output[global_idx] = input[global_idx];
  }`;
          }, { name: "TransposeCopy", shaderCache: { inputDependencies: ["type"] }, getRunData: () => {
            let R = $e.size(a);
            return { outputs: [{ dims: a, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(R / 64 / 4) }, programUniforms: [{ type: 12, data: Math.ceil(R / 4) }] };
          }, getShaderSource: h };
          let { newShape: k, newPerm: S } = Bi(e.dims, i), u = $e.areEqual(S, [2, 3, 1]), B = $e.areEqual(S, [3, 1, 2]);
          if (k.length === 2 || u || B) {
            o = u ? [k[0], k[1] * k[2]] : B ? [k[0] * k[1], k[2]] : k, d = [o[1], o[0]];
            let R = 16;
            return h = (N) => {
              let Z = ze("a", r, o.length), te = wt("output", r, d.length);
              return `
  ${N.registerUniform("output_size", "u32").declareVariables(Z, te)}
  var<workgroup> tile : array<array<${te.type.value}, ${R + 1}>, ${R}>;
  ${N.mainStart([R, R, 1])}
    let stride = (uniforms.output_shape[1] - 1) / ${R} + 1;
    let workgroup_id_x = workgroup_index % stride;
    let workgroup_id_y = workgroup_index / stride;
    let input_col = workgroup_id_y * ${R}u + local_id.x;
    let input_row = workgroup_id_x * ${R}u + local_id.y;
    if (input_row < uniforms.a_shape[0] && input_col < uniforms.a_shape[1]) {
      tile[local_id.y][local_id.x] = ${Z.getByIndices(`${Z.type.indices}(input_row, input_col)`)};
    }
    workgroupBarrier();

    let output_col = workgroup_id_x * ${R}u + local_id.x;
    let output_row = workgroup_id_y * ${R}u + local_id.y;
    if (output_row < uniforms.output_shape[0] && output_col < uniforms.output_shape[1]) {
      ${te.setByIndices(`${te.type.indices}(output_row, output_col)`, "tile[local_id.x][local_id.y]")}
    }
  }`;
            }, { name: "TransposeShared", shaderCache: { inputDependencies: ["type"] }, getRunData: () => {
              let N = $e.size(a);
              return { outputs: [{ dims: a, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(d[1] / R), y: Math.ceil(d[0] / R) }, programUniforms: [{ type: 12, data: N }, ...vt(o, d)] };
            }, getShaderSource: h };
          }
          return h = (R) => {
            let N = ze("a", r, o.length), Z = wt("output", r, d.length);
            return `
  ${R.registerUniform("output_size", "u32").declareVariables(N, Z)}

  ${ba(i, n, N, Z)}

  ${R.mainStart()}
    ${R.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let indices = ${Z.offsetToIndices("global_idx")};
    let aIndices = perm(indices);

    ${Z.setByOffset("global_idx", N.getByIndices("aIndices"))}
  }`;
          }, { name: "Transpose", shaderCache: { hint: `${t}`, inputDependencies: ["rank"] }, getRunData: () => {
            let R = $e.size(a);
            return { outputs: [{ dims: a, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(R / 64) }, programUniforms: [{ type: 12, data: R }, ...vt(o, d)] };
          }, getShaderSource: h };
        }, Ri = (e, t) => {
          ya(e.inputs), e.compute(ss(e.inputs[0], t.perm));
        }, xa = (e) => ot({ perm: e.perm });
      }), Ni, $c, Ta, ji, Ea, Yn, Pa, Ca, ka, Sa, hs, $a, Aa, Ui, Ia, Wi, rn, Fa, Ac, Oa, Ic, Fc = w(() => {
        zt(), Bt(), Qt(), Yi(), Vs(), Ni = { max: "select(bestValue, candidate, candidate > bestValue)", min: "select(bestValue, candidate, candidate < bestValue)", mean: "bestValue + candidate", sum: "bestValue + candidate", prod: "bestValue * candidate", sumSquare: "bestValue + candidate * candidate", logSumExp: "bestValue + exp(candidate)", l1: "bestValue + abs(candidate)", l2: "bestValue + candidate * candidate", logSum: "bestValue + candidate" }, $c = { max: "select(bestValue, candidate, candidate > bestValue)", min: "select(bestValue, candidate, candidate < bestValue)", mean: "bestValue + candidate", sum: "bestValue + candidate", prod: "bestValue * candidate", sumSquare: "bestValue + candidate", logSumExp: "bestValue + candidate", l1: "bestValue + candidate", l2: "bestValue + candidate", logSum: "bestValue + candidate" }, Ta = { max: "_A[offset]", min: "_A[offset]", mean: "0", sum: "0", prod: "1", sumSquare: "0", logSumExp: "0", l1: "0", l2: "0", logSum: "0" }, ji = { max: "bestValue", min: "bestValue", sum: "bestValue", prod: "bestValue", sumSquare: "bestValue", logSumExp: "log(bestValue)", l1: "bestValue", l2: "sqrt(bestValue)", logSum: "log(bestValue)" }, Ea = (e, t) => {
          let r = [];
          for (let n = t - e; n < t; ++n) r.push(n);
          return r;
        }, Yn = (e, t) => {
          let r = [], n = e.length;
          for (let a = 0; a < n; a++) t.indexOf(a) === -1 && r.push(e[a]);
          let i = t.map((a) => e[a]);
          return [r, i];
        }, Pa = (e, t) => {
          let r = e.length + t.length, n = [], i = 0;
          for (let a = 0; a < r; a++) t.indexOf(a) === -1 ? n.push(e[i++]) : n.push(1);
          return n;
        }, Ca = (e, t) => {
          for (let r = 0; r < e.length; ++r) if (e[e.length - r - 1] !== t - 1 - r) return !1;
          return !0;
        }, ka = (e, t) => {
          let r = [];
          if (!Ca(e, t)) {
            for (let n = 0; n < t; ++n) e.indexOf(n) === -1 && r.push(n);
            e.forEach((n) => r.push(n));
          }
          return r;
        }, Sa = (e, t, r, n, i, a, o) => {
          let d = r[0].dims, p = $e.size(a), h = $e.size(o), k = ze("_A", r[0].dataType, d), S = wt("output", i, a), u = 64;
          p === 1 && (u = 256);
          let B = `
          var<workgroup> aBestValues : array<f32, ${u}>;
       `, R = (N) => `
        ${N.registerUniform("reduceSize", "u32").declareVariables(k, S)}
        ${B}
        fn DIV_CEIL(a : u32, b : u32) -> u32 {
          return ((a - 1u) / b + 1u);
         }
         ${N.mainStart(u)}

          let outputIndex = global_idx / ${u};
          let offset = outputIndex * uniforms.reduceSize;

          var bestValue = f32(${Ta[n]});
          let Length = uniforms.reduceSize;
          for (var k = local_idx; k < Length; k = k + ${u}) {
           let candidate = f32(${k.getByOffset("offset + k")});
           bestValue = ${Ni[n]};
          }
          aBestValues[local_idx] = bestValue;
          workgroupBarrier();

         var reduceSize = min(Length, ${u}u);
         for (var currentSize = reduceSize / 2u; reduceSize > 1u;
             currentSize = reduceSize / 2u) {
           let interval = DIV_CEIL(reduceSize, 2u);
           if (local_idx < currentSize) {
            let candidate = aBestValues[local_idx + interval];
            bestValue = ${$c[n]};
            aBestValues[local_idx] = bestValue;
           }
           reduceSize = interval;
           workgroupBarrier();
         }

         if (local_idx == 0u) {
          ${S.setByOffset("outputIndex", `${n === "mean" ? `${S.type.storage}(bestValue / f32(uniforms.reduceSize))` : `${S.type.storage}(${ji[n]})`}`)};
         }
        }`;
          return { name: e, shaderCache: { hint: `${t};${u}`, inputDependencies: ["type"] }, getShaderSource: R, getRunData: () => ({ outputs: [{ dims: a, dataType: i }], dispatchGroup: { x: p }, programUniforms: [{ type: 12, data: h }] }) };
        }, hs = (e, t, r, n) => {
          let i = e.inputs.length === 1 ? r : Vi(e.inputs, r), a = i.axes;
          a.length === 0 && !i.noopWithEmptyAxes && (a = e.inputs[0].dims.map((B, R) => R));
          let o = $e.normalizeAxes(a, e.inputs[0].dims.length), d = o, p = e.inputs[0], h = ka(d, e.inputs[0].dims.length);
          h.length > 0 && (p = e.compute(ss(e.inputs[0], h), { inputs: [0], outputs: [-1] })[0], d = Ea(d.length, p.dims.length));
          let [k, S] = Yn(p.dims, d), u = k;
          i.keepDims && (u = Pa(k, o)), e.compute(Sa(t, i.cacheKey, [p], n, e.inputs[0].dataType, u, S), { inputs: [p] });
        }, $a = (e, t) => {
          hs(e, "ReduceMeanShared", t, "mean");
        }, Aa = (e, t) => {
          hs(e, "ReduceL1Shared", t, "l1");
        }, Ui = (e, t) => {
          hs(e, "ReduceL2Shared", t, "l2");
        }, Ia = (e, t) => {
          hs(e, "ReduceLogSumExpShared", t, "logSumExp");
        }, Wi = (e, t) => {
          hs(e, "ReduceMaxShared", t, "max");
        }, rn = (e, t) => {
          hs(e, "ReduceMinShared", t, "min");
        }, Fa = (e, t) => {
          hs(e, "ReduceProdShared", t, "prod");
        }, Ac = (e, t) => {
          hs(e, "ReduceSumShared", t, "sum");
        }, Oa = (e, t) => {
          hs(e, "ReduceSumSquareShared", t, "sumSquare");
        }, Ic = (e, t) => {
          hs(e, "ReduceLogSumShared", t, "logSum");
        };
      }), us, Da, Jn, Vi, ms, La, Zn, za, Ba, Ra, Gi, Na, ja, Ki, Ua, _s, Hi, Wa, Va, qi, Ga, Ka, Xi, Ha, qa, Qi, Yi = w(() => {
        zt(), Bt(), Pt(), Qt(), Fc(), us = (e) => {
          if (!e || e.length === 0 || e.length > 2) throw new Error("Reduce op requires 1 or 2 inputs.");
          if (e.length === 2 && e[1].dims.length !== 1) throw new Error("Invalid axes input dims.");
        }, Da = (e) => ["", "", `var value = ${e.getByIndices("input_indices")};`, ""], Jn = (e, t, r, n, i, a, o = !1, d = !1) => {
          let p = [], h = r[0].dims, k = h.length, S = $e.normalizeAxes(i, k), u = !d && S.length === 0;
          h.forEach((N, Z) => {
            u || S.indexOf(Z) >= 0 ? o && p.push(1) : p.push(N);
          });
          let B = p.length, R = $e.size(p);
          return { name: e, shaderCache: t, getShaderSource: (N) => {
            let Z = [], te = ze("_A", r[0].dataType, k), Q = wt("output", a, B), _e = n(te, Q, S), me = _e[2];
            for (let ye = 0, Ae = 0; ye < k; ye++) u || S.indexOf(ye) >= 0 ? (o && Ae++, me = `for(var j${ye}: u32 = 0; j${ye} < ${h[ye]}; j${ye}++) {
                  ${_e[2].includes("last_index") ? `let last_index = j${ye};` : ""}
                  ${te.indicesSet("input_indices", ye, `j${ye}`)}
                  ${me}
                }`) : (Z.push(`${te.indicesSet("input_indices", ye, Q.indicesGet("output_indices", Ae))};`), Ae++);
            return `

        ${N.registerUniform("output_size", "u32").declareVariables(te, Q)}

        ${N.mainStart()}
          ${N.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          var input_indices: ${te.type.indices};
          let output_indices = ${Q.offsetToIndices("global_idx")};

          ${Z.join(`
`)}
          ${_e[0]}       // init ops for reduce max/min
          ${_e[1]}
          ${me}
          ${_e[3]}
          ${_e.length === 4 ? Q.setByOffset("global_idx", "value") : _e.slice(4).join(`
`)}
        }`;
          }, getRunData: () => ({ outputs: [{ dims: p, dataType: a }], dispatchGroup: { x: Math.ceil(R / 64) }, programUniforms: [{ type: 12, data: R }, ...vt(h, p)] }) };
        }, Vi = (e, t) => {
          let r = [];
          return e[1].dims[0] > 0 && e[1].getBigInt64Array().forEach((n) => r.push(Number(n))), ot({ axes: r, keepDims: t.keepDims, noopWithEmptyAxes: t.noopWithEmptyAxes });
        }, ms = (e, t, r, n) => {
          let i = e.inputs, a = i.length === 1 ? r : Vi(i, r);
          e.compute(Jn(t, { hint: a.cacheKey, inputDependencies: ["rank"] }, [i[0]], a.noopWithEmptyAxes && a.axes.length === 0 ? Da : n, a.axes, i[0].dataType, a.keepDims, a.noopWithEmptyAxes), { inputs: [0] });
        }, La = (e, t) => {
          us(e.inputs), ms(e, "ReduceLogSum", t, (r, n) => [`var value = ${n.type.storage}(0);`, "", `value += ${r.getByIndices("input_indices")};`, "value = log(value);"]);
        }, Zn = (e, t) => {
          us(e.inputs), ms(e, "ReduceL1", t, (r, n) => [`var value = ${n.type.storage}(0);`, "", `value += abs(${r.getByIndices("input_indices")});`, ""]);
        }, za = (e, t) => {
          us(e.inputs), ms(e, "ReduceL2", t, (r, n) => [`var t = ${n.type.value}(0); var value = ${n.type.value}(0);`, "", `t = ${r.getByIndices("input_indices")}; value += (t * t);`, "value = sqrt(value);"]);
        }, Ba = (e, t) => {
          us(e.inputs), ms(e, "ReduceLogSumExp", t, (r, n) => [`var value = ${n.type.storage}(0);`, "", `value += exp(${r.getByIndices("input_indices")});`, "value = log(value);"]);
        }, Ra = (e, t) => {
          us(e.inputs), ms(e, "ReduceMax", t, (r, n, i) => {
            let a = [];
            for (let o = 0; o < r.rank; o++) (i.indexOf(o) >= 0 || i.length === 0) && a.push(r.indicesSet("input_indices", o, 0));
            return [`${a.join(`
`)}`, `var value = ${r.getByIndices("input_indices")};`, `value = max(value, ${r.getByIndices("input_indices")});`, ""];
          });
        }, Gi = (e, t) => {
          us(e.inputs), ms(e, "ReduceMean", t, (r, n, i) => {
            let a = 1;
            for (let o = 0; o < r.rank; o++) (i.indexOf(o) >= 0 || i.length === 0) && (a *= e.inputs[0].dims[o]);
            return ["var sum = f32(0);", "", `sum += f32(${r.getByIndices("input_indices")});`, `let value = ${n.type.value}(sum / ${a});`];
          });
        }, Na = (e, t) => {
          us(e.inputs), ms(e, "ReduceMin", t, (r, n, i) => {
            let a = [];
            for (let o = 0; o < r.rank; o++) (i.indexOf(o) >= 0 || i.length === 0) && a.push(`input_indices[${o}] = 0;`);
            return [`${a.join(`
`)}`, `var value = ${r.getByIndices("input_indices")};`, `value = min(value, ${r.getByIndices("input_indices")});`, ""];
          });
        }, ja = (e, t) => {
          us(e.inputs), ms(e, "ReduceProd", t, (r, n) => [`var value = ${n.type.storage}(1);`, "", `value *= ${r.getByIndices("input_indices")};`, ""]);
        }, Ki = (e, t) => {
          us(e.inputs), ms(e, "ReduceSum", t, (r, n) => [`var value = ${n.type.storage}(0);`, "", `value += ${r.getByIndices("input_indices")};`, ""]);
        }, Ua = (e, t) => {
          us(e.inputs), ms(e, "ReduceSumSquare", t, (r, n) => [`var t = ${n.type.value}(0); var value = ${n.type.value}(0);`, "", `t = ${r.getByIndices("input_indices")}; value += t * t;`, ""]);
        }, _s = (e, t, r) => {
          if (t.length === 0) return r;
          let n = 1, i = 1;
          for (let a = 0; a < t.length; a++) t.indexOf(a) === -1 ? n *= e[a] : i *= e[a];
          return i < 32 && n > 1024;
        }, Hi = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Gi(e, t) : $a(e, t);
        }, Wa = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Zn(e, t) : Aa(e, t);
        }, Va = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? za(e, t) : Ui(e, t);
        }, qi = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Ba(e, t) : Ia(e, t);
        }, Ga = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Ra(e, t) : Wi(e, t);
        }, Ka = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Na(e, t) : rn(e, t);
        }, Xi = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? ja(e, t) : Fa(e, t);
        }, Ha = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Ki(e, t) : Ac(e, t);
        }, qa = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? Ua(e, t) : Oa(e, t);
        }, Qi = (e, t) => {
          _s(e.inputs[0].dims, t.axes, t.noopWithEmptyAxes) ? La(e, t) : Ic(e, t);
        };
      }), Ji, Zi, Xa, eo, Qa = w(() => {
        zt(), Pt(), Yi(), Ji = (e) => {
          if (!e || e.length === 0 || e.length > 2) throw new Error("ArgMinMaxOp op requires 1 or 2 inputs.");
          if (e[0].dataType !== 1) throw new Error("Invalid input type.");
        }, Zi = (e, t) => {
          Ji(e.inputs);
          let r = (n, i, a) => {
            let o = [];
            for (let d = 0; d < n.rank; d++) (a.indexOf(d) >= 0 || a.length === 0) && o.push(`input_indices[${d}] = 0;`);
            return [`${o.join(`
`)}`, `var value = ${n.getByIndices("input_indices")};
var best_index : i32 = 0;`, `if (${n.getByIndices("input_indices")} ${t.selectLastIndex > 0 ? "<=" : "<"} value) {
         value = ${n.getByIndices("input_indices")};
         best_index = i32(last_index);
       }`, "", i.setByOffset("global_idx", "best_index")];
          };
          e.compute(Jn("ArgMin", { hint: t.cacheKey, inputDependencies: ["rank"] }, [e.inputs[0]], r, [t.axis], 7, t.keepDims), { inputs: [0] });
        }, Xa = (e, t) => {
          Ji(e.inputs);
          let r = (n, i, a) => {
            let o = [];
            for (let d = 0; d < n.rank; d++) (a.indexOf(d) >= 0 || a.length === 0) && o.push(`input_indices[${d}] = 0;`);
            return [`${o.join(`
`)}`, `var value = ${n.getByIndices("input_indices")};
var best_index : i32 = 0;`, `if (${n.getByIndices("input_indices")} ${t.selectLastIndex > 0 ? ">=" : ">"} value) {
         value = ${n.getByIndices("input_indices")};
         best_index = i32(last_index);
       }`, "", i.setByOffset("global_idx", "best_index")];
          };
          e.compute(Jn("argMax", { hint: t.cacheKey, inputDependencies: ["rank"] }, [e.inputs[0]], r, [t.axis], 7, t.keepDims), { inputs: [0] });
        }, eo = (e) => ot(e);
      }), Ya, ei, to, Ja, Za, vn, el, tl, ti = w(() => {
        zt(), Bt(), yn(), Qt(), Ya = (e, t) => {
          let r = e[0], n = e[1], i = e[2], a = e[3], o = e[4], d = e[5];
          if (o && d) throw new Error("Attention cannot have both past and attention_bias");
          if (r.dims.length !== 3) throw new Error('Input "input" must have 3 dimensions');
          let p = r.dims[0], h = r.dims[1], k = r.dims[2];
          if (i.dims.length !== 1) throw new Error('Input "bias" is expected to have 1 dimensions');
          if (n.dims.length !== 2) throw new Error('Input "weights" is expected to have 2 dimensions');
          if (n.dims[0] !== k) throw new Error("Input 1 dimension 0 should have same length as dimension 2 of input 0");
          if (i.dims[0] !== n.dims[1]) throw new Error('Input "bias" dimension 0 should have same length as dimension 1 of input "weights"');
          let S = i.dims[0] / 3, u = S, B = u;
          if (t.qkvHiddenSizes.length > 0) {
            if (t.qkvHiddenSizes.length !== 3) throw new Error("qkv_hidden_sizes attribute should have 3 elements");
            for (let _e of t.qkvHiddenSizes) if (_e % t.numHeads !== 0) throw new Error("qkv_hidden_sizes should be divisible by num_heads");
            S = t.qkvHiddenSizes[0], u = t.qkvHiddenSizes[1], B = t.qkvHiddenSizes[2];
          }
          let R = h;
          if (S !== u) throw new Error("qkv_hidden_sizes first element should be same as the second");
          if (i.dims[0] !== S + u + B) throw new Error('Input "bias" dimension 0 should have same length as sum of Q/K/V hidden sizes');
          let N = 0;
          if (o) {
            if (u !== B) throw new Error('Input "past" expect k_hidden_size == v_hidden_size');
            if (o.dims.length !== 5) throw new Error('Input "past" must have 5 dimensions');
            if (o.dims[0] !== 2) throw new Error('Input "past" first dimension must be 2');
            if (o.dims[1] !== p) throw new Error('Input "past" second dimension must be batch_size');
            if (o.dims[2] !== t.numHeads) throw new Error('Input "past" third dimension must be num_heads');
            if (o.dims[4] !== u / t.numHeads) throw new Error('Input "past" fifth dimension must be k_hidden_size / num_heads');
            t.pastPresentShareBuffer || (N = o.dims[3]);
          }
          let Z = R + N, te = -1, Q = 0;
          if (a) throw new Error("Mask not supported");
          if (o) throw new Error("past is not supported");
          if (d) {
            if (d.dims.length !== 4) throw new Error('Input "attention_bias" must have 4 dimensions');
            if (d.dims[0] !== p || d.dims[1] !== t.numHeads || d.dims[2] !== h || d.dims[3] !== Z) throw new Error('Expect "attention_bias" shape (batch_size, num_heads, sequence_length, total_sequence_length)');
          }
          return { batchSize: p, sequenceLength: h, pastSequenceLength: N, kvSequenceLength: R, totalSequenceLength: Z, maxSequenceLength: te, inputHiddenSize: k, hiddenSize: S, vHiddenSize: B, headSize: Math.floor(S / t.numHeads), vHeadSize: Math.floor(B / t.numHeads), numHeads: t.numHeads, isUnidirectional: !1, pastPresentShareBuffer: !1, maskFilterValue: t.maskFilterValue, maskType: Q, scale: t.scale, broadcastResPosBias: !1, passPastInKv: !1, qkvFormat: 1 };
        }, ei = (e, t, r) => t && e ? `
      let total_sequence_length_input = u32(${t.getByOffset("0")});
      let present_sequence_length = max(total_sequence_length_input, uniforms.past_sequence_length);
      let is_subsequent_prompt: bool = sequence_length > 1 && sequence_length != total_sequence_length_input;
      let is_first_prompt: bool = is_subsequent_prompt == false && sequence_length == total_sequence_length_input;
      total_sequence_length = u32(${e == null ? void 0 : e.getByOffset("batchIdx")}) + 1;
      var past_sequence_length: u32 = 0;
      if (is_first_prompt == false) {
        past_sequence_length = total_sequence_length - sequence_length;
      }
       ` : `
    ${r ? "let past_sequence_length = uniforms.past_sequence_length" : ""};
    let present_sequence_length = total_sequence_length;
    `, to = (e, t, r, n, i, a, o, d) => {
          let p = yr(o ? 1 : a), h = 64, k = a / p;
          k < h && (h = 32);
          let S = Math.ceil(a / p / h), u = [{ type: 12, data: t }, { type: 12, data: r }, { type: 12, data: n }, { type: 12, data: i }, { type: 12, data: k }, { type: 12, data: S }], B = er(e.dataType, p), R = mr(1, p), N = ["type"];
          o && N.push("type"), d && N.push("type");
          let Z = (te) => {
            let Q = wt("x", e.dataType, e.dims, p), _e = [Q], me = o ? ze("seq_lens", o.dataType, o.dims) : void 0;
            me && _e.push(me);
            let ye = d ? ze("total_sequence_length_input", d.dataType, d.dims) : void 0;
            ye && _e.push(ye);
            let Ae = mr(e.dataType), Ie = [{ name: "batch_size", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "past_sequence_length", type: "u32" }, { name: "sequence_length", type: "u32" }, { name: "total_sequence_length", type: "u32" }, { name: "elements_per_thread", type: "u32" }];
            return `
  var<workgroup> thread_max: array<f32, ${h}>;
  var<workgroup> thread_sum: array<f32, ${h}>;
  ${te.registerUniforms(Ie).declareVariables(..._e)}
  ${te.mainStart([h, 1, 1])}
    let batchIdx = workgroup_id.z / uniforms.num_heads;
    let headIdx = workgroup_id.z % uniforms.num_heads;
    let sequence_length = uniforms.sequence_length;
    var total_sequence_length = uniforms.total_sequence_length;
    ${ei(me, ye, !1)}
    let local_offset = local_idx * uniforms.elements_per_thread;
    let offset = (global_idx / ${h}) * uniforms.total_sequence_length + local_offset;
    let seq_causal_length = ${o ? "u32(past_sequence_length + workgroup_id.y + 1)" : "total_sequence_length"};
    var thread_max_vector = ${R}(-3.402823e+38f);
    for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
      thread_max_vector = max(${R}(x[offset + i]), thread_max_vector);
    }
    thread_max[local_idx] = ${(() => {
              switch (p) {
                case 1:
                  return "thread_max_vector";
                case 2:
                  return "max(thread_max_vector.x, thread_max_vector.y)";
                case 4:
                  return "max(max(thread_max_vector.x, thread_max_vector.y), max(thread_max_vector.z, thread_max_vector.w))";
                default:
                  throw new Error(`Unsupported components: ${p}`);
              }
            })()};
    workgroupBarrier();

    var max_value =  f32(-3.402823e+38f);
    for (var i = 0u; i < ${h}; i++) {
      max_value = max(thread_max[i], max_value);
    }

    var sum_vector = ${R}(0);
    for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
      sum_vector += exp(${R}(x[offset + i]) - max_value);
    }
    thread_sum[local_idx] = ${(() => {
              switch (p) {
                case 1:
                  return "sum_vector";
                case 2:
                  return "sum_vector.x + sum_vector.y";
                case 4:
                  return "sum_vector.x + sum_vector.y + sum_vector.z + sum_vector.w";
                default:
                  throw new Error(`Unsupported components: ${p}`);
              }
            })()};
    workgroupBarrier();

    var sum: f32 = 0;
    for (var i = 0u; i < ${h}; i++) {
      sum += thread_sum[i];
    }

    if (sum == 0) {
      for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
        x[offset + i] = ${Q.type.value}(${Ae}(1.0) / ${Ae}(seq_causal_length));
      }
    } else {
      for (var i: u32 = 0; i < uniforms.elements_per_thread && i + local_offset < seq_causal_length; i++) {
        var f32input = ${R}(x[offset + i]);
        x[offset + i] = ${Q.type.value}(exp(f32input - max_value) / sum);
      }
    }
      ${o ? `
        for (var total_seq_id: u32 = seq_causal_length; total_seq_id + local_offset < uniforms.total_sequence_length; total_seq_id++) {
          x[offset + total_seq_id] = ${Q.type.value}(${Ae}(0));
        }` : ""};
  }`;
          };
          return { name: "AttentionProbsSoftmax", shaderCache: { hint: `${h};${B};${p}`, inputDependencies: N }, getShaderSource: Z, getRunData: () => ({ outputs: [], dispatchGroup: { x: Math.ceil(a / h), y: i, z: t * r }, programUniforms: u }) };
        }, Ja = (e, t, r, n, i, a, o, d, p) => {
          let h = o + a.kvSequenceLength, k = [a.batchSize, a.numHeads, a.sequenceLength, h], S = e > 1 && n, u = a.kvNumHeads ? a.kvNumHeads : a.numHeads, B = S ? [a.batchSize, u, h, a.headSize] : void 0, R = a.nReps ? a.nReps : 1, N = a.scale === 0 ? 1 / Math.sqrt(a.headSize) : a.scale, Z = yr(a.headSize), te = a.headSize / Z, Q = 12, _e = { x: Math.ceil(h / Q), y: Math.ceil(a.sequenceLength / Q), z: a.batchSize * a.numHeads }, me = [{ type: 12, data: a.sequenceLength }, { type: 12, data: te }, { type: 12, data: h }, { type: 12, data: a.numHeads }, { type: 12, data: a.headSize }, { type: 1, data: N }, { type: 12, data: o }, { type: 12, data: a.kvSequenceLength }, { type: 12, data: R }], ye = S && n && $e.size(n.dims) > 0, Ae = ["type", "type"];
          ye && Ae.push("type"), i && Ae.push("type"), d && Ae.push("type"), p && Ae.push("type");
          let Ie = [{ dims: k, dataType: t.dataType, gpuDataType: 0 }];
          S && Ie.push({ dims: B, dataType: t.dataType, gpuDataType: 0 });
          let Ge = (lt) => {
            let Tt = ze("q", t.dataType, t.dims, Z), Kt = ze("key", r.dataType, r.dims, Z), Yt = [Tt, Kt];
            if (ye) {
              let Gt = ze("past_key", n.dataType, n.dims, Z);
              Yt.push(Gt);
            }
            i && Yt.push(ze("attention_bias", i.dataType, i.dims));
            let Ct = d ? ze("seq_lens", d.dataType, d.dims) : void 0;
            Ct && Yt.push(Ct);
            let Jt = p ? ze("total_sequence_length_input", p.dataType, p.dims) : void 0;
            Jt && Yt.push(Jt);
            let $t = wt("output", t.dataType, k), jt = [$t];
            S && jt.push(wt("present_key", t.dataType, B, Z));
            let vr = mr(1, Z), Ht = [{ name: "M", type: "u32" }, { name: "K", type: "u32" }, { name: "N", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "head_size", type: "u32" }, { name: "alpha", type: "f32" }, { name: "past_sequence_length", type: "u32" }, { name: "kv_sequence_length", type: "u32" }, { name: "n_reps", type: "u32" }];
            return `
  const TILE_SIZE = ${Q}u;

  var<workgroup> tileQ: array<${Tt.type.storage}, ${Q * Q}>;
  var<workgroup> tileK: array<${Tt.type.storage}, ${Q * Q}>;
  ${lt.registerUniforms(Ht).declareVariables(...Yt, ...jt)}
  ${lt.mainStart([Q, Q, 1])}
    // x holds the N and y holds the M
    let headIdx = workgroup_id.z % uniforms.num_heads;
    let kvHeadIdx = ${R === 1 ? "headIdx" : "headIdx / uniforms.n_reps"};
    let kv_num_heads = ${R === 1 ? "uniforms.num_heads" : "uniforms.num_heads / uniforms.n_reps"};
    let batchIdx = workgroup_id.z / uniforms.num_heads;
    let m = workgroup_id.y * TILE_SIZE;
    let n = workgroup_id.x * TILE_SIZE;
    let sequence_length = uniforms.M;
    var total_sequence_length = uniforms.N;
    ${ei(Ct, Jt, !0)}
    let absKvHeadIdx = batchIdx * kv_num_heads + kvHeadIdx;
    let qOffset = workgroup_id.z * uniforms.M * uniforms.K + m * uniforms.K;
    ${ye && S ? "let pastKeyOffset = absKvHeadIdx * uniforms.past_sequence_length * uniforms.K;" : ""};
    let kOffset = absKvHeadIdx * uniforms.kv_sequence_length * uniforms.K;
    ${S ? "let presentKeyOffset = absKvHeadIdx * uniforms.N * uniforms.K;" : ""}
    var value = ${vr}(0);
    for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (global_id.y < uniforms.M && w + local_id.x < uniforms.K) {
        tileQ[TILE_SIZE * local_id.y + local_id.x] = q[qOffset + local_id.y * uniforms.K + w + local_id.x];
      }
      if (n + local_id.y < uniforms.N && w + local_id.x < uniforms.K) {
        var idx = TILE_SIZE * local_id.y + local_id.x;
      ${ye && S ? `
              if (n + local_id.y < past_sequence_length) {
                tileK[idx] = past_key[pastKeyOffset + (n + local_id.y) * uniforms.K + w + local_id.x];
              } else if (n + local_id.y - past_sequence_length < uniforms.kv_sequence_length) {
                tileK[idx] = key[kOffset + (n + local_id.y - past_sequence_length) * uniforms.K + w + local_id.x];
              }` : `
          if (n + local_id.y < uniforms.kv_sequence_length) {
            tileK[idx] = key[kOffset + (n + local_id.y) * uniforms.K + w + local_id.x];
          }`}
      ${S ? `if (n + local_id.y < present_sequence_length) {
        present_key[presentKeyOffset + (n + local_id.y) * uniforms.K + w + local_id.x] = tileK[idx];
      }` : ""}
      }
      workgroupBarrier();

      for (var k: u32 = 0u; k < TILE_SIZE && w+k < uniforms.K; k++) {
          value += ${vr}(tileQ[TILE_SIZE * local_id.y + k] * tileK[TILE_SIZE * local_id.x + k]);
      }

      workgroupBarrier();
    }

    if (global_id.y < uniforms.M && global_id.x < total_sequence_length) {
      let headOffset = workgroup_id.z * uniforms.M * uniforms.N;
      let outputIdx = headOffset + global_id.y * uniforms.N + global_id.x;
      var sum: f32 = ${(() => {
              switch (Z) {
                case 1:
                  return "value";
                case 2:
                  return "value.x + value.y";
                case 4:
                  return "value.x + value.y + value.z + value.w";
                default:
                  throw new Error(`Unsupported components: ${Z}`);
              }
            })()};
        output[outputIdx] = ${$t.type.value} (sum * uniforms.alpha) + ${i ? "attention_bias[outputIdx]" : "0.0"};
    }
  }`;
          };
          return { name: "AttentionProbs", shaderCache: { hint: `${Z};${i !== void 0};${n !== void 0};${e}`, inputDependencies: Ae }, getRunData: () => ({ outputs: Ie, dispatchGroup: _e, programUniforms: me }), getShaderSource: Ge };
        }, Za = (e, t, r, n, i, a, o = void 0, d = void 0) => {
          let p = a + i.kvSequenceLength, h = i.nReps ? i.nReps : 1, k = i.vHiddenSize * h, S = e > 1 && n, u = i.kvNumHeads ? i.kvNumHeads : i.numHeads, B = S ? [i.batchSize, u, p, i.headSize] : void 0, R = [i.batchSize, i.sequenceLength, k], N = 12, Z = { x: Math.ceil(i.vHeadSize / N), y: Math.ceil(i.sequenceLength / N), z: i.batchSize * i.numHeads }, te = [{ type: 12, data: i.sequenceLength }, { type: 12, data: p }, { type: 12, data: i.vHeadSize }, { type: 12, data: i.numHeads }, { type: 12, data: i.headSize }, { type: 12, data: k }, { type: 12, data: a }, { type: 12, data: i.kvSequenceLength }, { type: 12, data: h }], Q = S && n && $e.size(n.dims) > 0, _e = ["type", "type"];
          Q && _e.push("type"), o && _e.push("type"), d && _e.push("type");
          let me = [{ dims: R, dataType: t.dataType, gpuDataType: 0 }];
          S && me.push({ dims: B, dataType: t.dataType, gpuDataType: 0 });
          let ye = (Ae) => {
            let Ie = ze("probs", t.dataType, t.dims), Ge = ze("v", r.dataType, r.dims), lt = [Ie, Ge];
            Q && lt.push(ze("past_value", n.dataType, n.dims));
            let Tt = o ? ze("seq_lens", o.dataType, o.dims) : void 0;
            o && lt.push(Tt);
            let Kt = d ? ze("total_sequence_length_input", d.dataType, d.dims) : void 0;
            d && lt.push(Kt);
            let Yt = [wt("output", t.dataType, R)];
            S && Yt.push(wt("present_value", t.dataType, B));
            let Ct = [{ name: "M", type: "u32" }, { name: "K", type: "u32" }, { name: "N", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "head_size", type: "u32" }, { name: "v_hidden_size", type: "u32" }, { name: "past_sequence_length", type: "u32" }, { name: "kv_sequence_length", type: "u32" }, { name: "n_reps", type: "u32" }];
            return `
  const TILE_SIZE = ${N}u;
  var<workgroup> tileQ: array<${Ie.type.value}, ${N * N}>;
  var<workgroup> tileV: array<${Ie.type.value}, ${N * N}>;
  ${Ae.registerUniforms(Ct).declareVariables(...lt, ...Yt)}
  ${Ae.mainStart([N, N, 1])}
   let headIdx = workgroup_id.z % uniforms.num_heads;
   let batchIdx = workgroup_id.z / uniforms.num_heads;
   let kvHeadIdx = ${h === 1 ? "headIdx" : "headIdx / uniforms.n_reps"};
   let kv_num_heads = ${h === 1 ? "uniforms.num_heads" : "uniforms.num_heads / uniforms.n_reps"};
   let m = global_id.y;
   let n = global_id.x;
   let sequence_length = uniforms.M;
   var total_sequence_length = uniforms.K;
   ${ei(Tt, Kt, !0)}
   let offsetA = workgroup_id.z * uniforms.M * uniforms.K + m * uniforms.K;
   let absKvHeadIdx = batchIdx * kv_num_heads + kvHeadIdx; // kvHeadIdx is relative to the batch
   ${Q && S ? "let pastValueOffset = absKvHeadIdx * uniforms.N * uniforms.past_sequence_length + n;" : ""};
   let vOffset = absKvHeadIdx * uniforms.N * uniforms.kv_sequence_length + n;
   ${S ? "let presentValueOffset = absKvHeadIdx * uniforms.N * uniforms.K + n;" : ""}
   var value = ${Ie.type.storage}(0);
   for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (m < uniforms.M && w + local_id.x < uniforms.K) {
        tileQ[TILE_SIZE * local_id.y + local_id.x] = probs[offsetA + w + local_id.x];
      }
      if (n < uniforms.N && w + local_id.y < uniforms.K) {
        var idx = TILE_SIZE * local_id.y + local_id.x;
        ${Q && S ? `
        if (w + local_id.y < past_sequence_length) {
          tileV[idx] = past_value[pastValueOffset + (w + local_id.y) * uniforms.N];
        } else if (w + local_id.y - past_sequence_length < uniforms.kv_sequence_length) {
          tileV[idx] = v[vOffset + (w + local_id.y - past_sequence_length) * uniforms.N];
        }
      ` : `
            if (w + local_id.y < uniforms.kv_sequence_length) {
              tileV[idx] = v[vOffset + (w + local_id.y) * uniforms.N];
            }`}
        ${S ? `
            if (w + local_id.y < present_sequence_length) {
          present_value[presentValueOffset + (w + local_id.y) * uniforms.N] = tileV[idx];
        }` : ""}
      }
     workgroupBarrier();
     for (var k: u32 = 0u; k < TILE_SIZE && w+k < total_sequence_length; k++) {
       value += tileQ[TILE_SIZE * local_id.y + k] * tileV[TILE_SIZE * k + local_id.x];
     }
     workgroupBarrier();
   }

   // we need to transpose output from BNSH_v to BSND_v
   if (m < uniforms.M && n < uniforms.N) {
     let outputIdx = batchIdx * uniforms.M * uniforms.v_hidden_size + m * uniforms.v_hidden_size
       + headIdx * uniforms.N + n;
     output[outputIdx] = value;
   }
  }`;
          };
          return { name: "AttentionScore", shaderCache: { hint: `${n !== void 0};${e}`, inputDependencies: _e }, getRunData: () => ({ outputs: me, dispatchGroup: Z, programUniforms: te }), getShaderSource: ye };
        }, vn = (e, t, r, n, i, a, o, d, p, h, k = void 0, S = void 0) => {
          let u = Math.min(e.outputCount, 1 + (o ? 1 : 0) + (d ? 1 : 0)), B = u > 1 ? h.pastSequenceLength : 0, R = B + h.kvSequenceLength, N = p && $e.size(p.dims) > 0 ? p : void 0, Z = [t, r];
          u > 1 && o && $e.size(o.dims) > 0 && Z.push(o), N && Z.push(N), k && Z.push(k), S && Z.push(S);
          let te = e.compute(Ja(u, t, r, o, N, h, B, k, S), { inputs: Z, outputs: u > 1 ? [-1, 1] : [-1] })[0];
          e.compute(to(te, h.batchSize, h.numHeads, B, h.sequenceLength, R, k, S), { inputs: k && S ? [te, k, S] : [te], outputs: [] });
          let Q = [te, n];
          u > 1 && d && $e.size(d.dims) > 0 && Q.push(d), k && Q.push(k), S && Q.push(S), e.compute(Za(u, te, n, d, h, B, k, S), { inputs: Q, outputs: u > 1 ? [0, 2] : [0] });
        }, el = (e, t) => {
          let r = [t.batchSize, t.numHeads, t.sequenceLength, t.headSize], n = t.sequenceLength, i = t.inputHiddenSize, a = t.headSize, o = 12, d = { x: Math.ceil(t.headSize / o), y: Math.ceil(t.sequenceLength / o), z: t.batchSize * t.numHeads }, p = [e.inputs[0], e.inputs[1], e.inputs[2]], h = [{ type: 12, data: n }, { type: 12, data: i }, { type: 12, data: a }, { type: 12, data: t.numHeads }, { type: 12, data: t.headSize }, { type: 12, data: t.hiddenSize }, { type: 12, data: t.hiddenSize + t.hiddenSize + t.vHiddenSize }], k = (S) => {
            let u = wt("output_q", p[0].dataType, r), B = wt("output_k", p[0].dataType, r), R = wt("output_v", p[0].dataType, r), N = ze("input", p[0].dataType, p[0].dims), Z = ze("weight", p[1].dataType, p[1].dims), te = ze("bias", p[2].dataType, p[2].dims), Q = N.type.storage, _e = [{ name: "M", type: "u32" }, { name: "K", type: "u32" }, { name: "N", type: "u32" }, { name: "num_heads", type: "u32" }, { name: "head_size", type: "u32" }, { name: "hidden_size", type: "u32" }, { name: "ldb", type: "u32" }];
            return `
  const TILE_SIZE = ${o}u;
  var<workgroup> tileInput: array<${Q}, ${o * o}>;
  var<workgroup> tileWeightQ: array<${Q}, ${o * o}>;
  var<workgroup> tileWeightK: array<${Q}, ${o * o}>;
  var<workgroup> tileWeightV: array<${Q}, ${o * o}>;
  ${S.registerUniforms(_e).declareVariables(N, Z, te, u, B, R)}
  ${S.mainStart([o, o, 1])}
    let batchIndex = workgroup_id.z / uniforms.num_heads;
    let headNumber = workgroup_id.z % uniforms.num_heads;
    let m = global_id.y;
    let n = global_id.x;

    let inputOffset = batchIndex * (uniforms.M * uniforms.K) + m * uniforms.K;
    let biasOffsetQ = headNumber * uniforms.head_size;
    let biasOffsetK = uniforms.hidden_size + biasOffsetQ;
    let biasOffsetV = uniforms.hidden_size + biasOffsetK;

    var valueQ = ${Q}(0);
    var valueK = ${Q}(0);
    var valueV = ${Q}(0);
    for (var w: u32 = 0u; w < uniforms.K; w += TILE_SIZE) {
      if (m < uniforms.M && w + local_id.x < uniforms.K) {
        tileInput[TILE_SIZE * local_id.y + local_id.x] = input[inputOffset + w + local_id.x];
      }
      if (n < uniforms.N && w + local_id.y < uniforms.K) {
        let offset = n + (w + local_id.y) * uniforms.ldb;
        tileWeightQ[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetQ + offset];
        tileWeightK[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetK + offset];
        tileWeightV[TILE_SIZE * local_id.y + local_id.x] = weight[biasOffsetV + offset];
      }
      workgroupBarrier();
      for (var k: u32 = 0u; k<TILE_SIZE && w+k < uniforms.K; k++) {
        let inputTileOffset = TILE_SIZE * local_id.y + k;
        let weightTileOffset = TILE_SIZE * k + local_id.x;
        valueQ += tileInput[inputTileOffset] * tileWeightQ[weightTileOffset];
        valueK += tileInput[inputTileOffset] * tileWeightK[weightTileOffset];
        valueV += tileInput[inputTileOffset] * tileWeightV[weightTileOffset];
      }

      workgroupBarrier();
    }

    let headOffset = (m * uniforms.N + n) % uniforms.head_size;
    valueQ += bias[headOffset + biasOffsetQ];
    valueK += bias[headOffset + biasOffsetK];
    valueV += bias[headOffset + biasOffsetV];

    let offset = workgroup_id.z * uniforms.M * uniforms.N;
    if (m < uniforms.M && n < uniforms.N) {
      let outputIdx = offset + m * uniforms.N + n;
      output_q[outputIdx] = valueQ;
      output_k[outputIdx] = valueK;
      output_v[outputIdx] = valueV;
    }
  }`;
          };
          return e.compute({ name: "AttentionPrepare", shaderCache: { inputDependencies: ["type", "type", "type"] }, getRunData: () => ({ outputs: [{ dims: r, dataType: e.inputs[0].dataType, gpuDataType: 0 }, { dims: r, dataType: e.inputs[0].dataType, gpuDataType: 0 }, { dims: r, dataType: e.inputs[0].dataType, gpuDataType: 0 }], dispatchGroup: d, programUniforms: h }), getShaderSource: k }, { inputs: p, outputs: [-1, -1, -1] });
        }, tl = (e, t) => {
          let r = Ya(e.inputs, t), [n, i, a] = el(e, r);
          return vn(e, n, i, a, e.inputs[4], void 0, void 0, void 0, e.inputs[5], r);
        };
      }), rl, sl, ro, nl, Oc = w(() => {
        Qe(), zt(), Bt(), Pt(), Qt(), rl = (e, t) => {
          if (!e || e.length !== 5) throw new Error("BatchNormalization requires 5 inputs");
          let r = (n, i, a) => {
            let o = i.length;
            if (o !== n.length) throw new Error(`${a}: num dimensions != ${o}`);
            i.forEach((d, p) => {
              if (d !== n[p]) throw new Error(`${a}: dim[${p}] do not match`);
            });
          };
          if (e[0].dims.length > 1) {
            let n = t.format === "NHWC" ? t.spatial ? e[0].dims.slice(-1) : e[0].dims.slice(-1).concat(e[0].dims.slice(1, e[0].dims.length - 1)) : e[0].dims.slice(1, t.spatial ? 2 : void 0);
            r(e[1].dims, n, "Invalid input scale"), r(e[2].dims, n, "Invalid input B"), r(e[3].dims, n, "Invalid input mean"), r(e[4].dims, n, "Invalid input var");
          } else r(e[1].dims, [1], "Invalid input scale"), r(e[2].dims, [1], "Invalid input B"), r(e[3].dims, [1], "Invalid input mean"), r(e[4].dims, [1], "Invalid input var");
        }, sl = (e, t) => {
          let { epsilon: r, spatial: n, format: i } = t, a = e[0].dims, o = n ? yr(a[a.length - 1]) : 1, d = i === "NHWC" && a.length > 1 ? o : 1, p = $e.size(a) / o, h = n, k = h ? a.length : a, S = ze("x", e[0].dataType, e[0].dims, o), u = ze("scale", e[1].dataType, e[1].dims, d), B = ze("bias", e[2].dataType, e[2].dims, d), R = ze("inputMean", e[3].dataType, e[3].dims, d), N = ze("inputVar", e[4].dataType, e[4].dims, d), Z = wt("y", e[0].dataType, k, o), te = () => {
            let _e = "";
            if (n) _e = `let cOffset = ${a.length === 1 ? "0u" : i === "NHWC" ? `outputIndices[${a.length - 1}] / ${o}` : "outputIndices[1]"};`;
            else if (i === "NCHW") _e = `
            ${Z.indicesSet("outputIndices", "0", "0")}
            let cOffset = ${Z.indicesToOffset("outputIndices")};`;
            else {
              _e = `var cIndices = ${u.type.indices}(0);
                       cIndices[0] = outputIndices[${a.length - 1}];`;
              for (let me = 1; me < u.rank; me++) _e += `cIndices[${me}] = outputIndices[${me}];`;
              _e += `let cOffset = ${u.indicesToOffset("cIndices")};`;
            }
            return _e;
          }, Q = (_e) => `
  const epsilon = ${r};
  ${_e.registerUniform("outputSize", "u32").declareVariables(S, u, B, R, N, Z)}
  ${_e.mainStart()}
  ${_e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
    var outputIndices = ${Z.offsetToIndices(`global_idx * ${o}`)};
    ${te()}
    let scale = ${u.getByOffset("cOffset")};
    let bias = ${B.getByOffset("cOffset")};
    let inputMean = ${R.getByOffset("cOffset")};
    let inputVar = ${N.getByOffset("cOffset")};
    let x = ${S.getByOffset("global_idx")};
    let value = (x - inputMean) * inverseSqrt(inputVar + epsilon) * scale + bias;
    ${Z.setByOffset("global_idx", "value")}
  }`;
          return { name: "BatchNormalization", shaderCache: { hint: `${t.epsilon}_${t.format}_${n}_${o}`, inputDependencies: h ? ["rank", "type", "type", "type", "type"] : void 0 }, getShaderSource: Q, getRunData: () => ({ outputs: [{ dims: e[0].dims, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: h ? [{ type: 12, data: p }, ...vt(a)] : [{ type: 12, data: p }] }) };
        }, ro = (e) => ot(e), nl = (e, t) => {
          let { inputs: r, outputCount: n } = e, i = ro({ ...t, outputCount: n });
          if (v.webgpu.validateInputContent && rl(r, i), t.trainingMode) throw new Error("BatchNormalization trainingMode is not supported yet.");
          e.compute(sl(r, i));
        };
      }), so, il, ol, al = w(() => {
        Bt(), Qt(), so = (e) => {
          if (e[0].dims.length !== 3) throw new Error("input should have 3 dimensions");
          if (![320, 640, 1280].includes(e[0].dims[2])) throw new Error("number of channels should be 320, 640 or 1280");
          if (e[1].dims.length !== 1) throw new Error("bias is expected to have 1 dimensions");
          if (e[0].dims[2] !== e[1].dims[0]) throw new Error("last dimension of input and bias are not the same");
        }, il = (e) => {
          let t = e[0].dims, r = e[0].dims[2], n = $e.size(t) / 4, i = e[0].dataType, a = ze("input", i, t, 4), o = ze("bias", i, [r], 4), d = ze("residual", i, t, 4), p = wt("output", i, t, 4);
          return { name: "BiasAdd", getRunData: () => ({ outputs: [{ dims: t, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(n / 64) } }), getShaderSource: (h) => `
  const channels = ${r}u / 4;
  ${h.declareVariables(a, o, d, p)}

  ${h.mainStart()}
    ${h.guardAgainstOutOfBoundsWorkgroupSizes(n)}
    let value = ${a.getByOffset("global_idx")}
      + ${o.getByOffset("global_idx % channels")} + ${d.getByOffset("global_idx")};
    ${p.setByOffset("global_idx", "value")}
  }` };
        }, ol = (e) => {
          so(e.inputs), e.compute(il(e.inputs));
        };
      }), ll, ur, no, ul, dl, io, cl, pl, oo, hl, ml, ao, _l, fl, lo, gl, On, wl, ri, uo, yl, Ml, co, bl, vl, po, xl, Tl, ho, El, Pl, mo, Cl, kl, _o, fo, Sl, si, go, $l, Al, Il, wo, Fl, Ol, yo = w(() => {
        zt(), Bt(), Pt(), Qt(), ll = (e, t, r, n, i, a, o) => {
          let d = Math.ceil(t / 4), p = "";
          typeof i == "string" ? p = `${i}(a)` : p = i("a");
          let h = ze("inputData", r, [d], 4), k = wt("outputData", n, [d], 4), S = [{ name: "vec_size", type: "u32" }];
          return o && S.push(...o), `
      ${e.registerUniforms(S).declareVariables(h, k)}

  ${a ?? ""}

  ${e.mainStart()}
    ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}

    let a = ${h.getByOffset("global_idx")};
    ${k.setByOffset("global_idx", p)}
  }`;
        }, ur = (e, t, r, n, i, a = e.dataType, o, d) => {
          let p = [{ type: 12, data: Math.ceil($e.size(e.dims) / 4) }];
          return o && p.push(...o), { name: t, shaderCache: { hint: i, inputDependencies: ["type"] }, getShaderSource: (h) => ll(h, $e.size(e.dims), e.dataType, a, r, n, d), getRunData: (h) => ({ outputs: [{ dims: e.dims, dataType: a }], dispatchGroup: { x: Math.ceil($e.size(h[0].dims) / 64 / 4) }, programUniforms: p }) };
        }, no = (e) => {
          e.compute(ur(e.inputs[0], "Abs", "abs"));
        }, ul = (e) => {
          e.compute(ur(e.inputs[0], "Acos", "acos"));
        }, dl = (e) => {
          e.compute(ur(e.inputs[0], "Acosh", "acosh"));
        }, io = (e) => {
          e.compute(ur(e.inputs[0], "Asin", "asin"));
        }, cl = (e) => {
          e.compute(ur(e.inputs[0], "Asinh", "asinh"));
        }, pl = (e) => {
          e.compute(ur(e.inputs[0], "Atan", "atan"));
        }, oo = (e) => {
          e.compute(ur(e.inputs[0], "Atanh", "atanh"));
        }, hl = (e) => ot(e), ml = (e, t) => {
          let r;
          switch (t.to) {
            case 10:
              r = "vec4<f16>";
              break;
            case 1:
              r = "vec4<f32>";
              break;
            case 12:
              r = "vec4<u32>";
              break;
            case 6:
              r = "vec4<i32>";
              break;
            case 9:
              r = "vec4<bool>";
              break;
            default:
              throw new RangeError(`not supported type (specified in attribute 'to' from 'Cast' operator): ${t.to}`);
          }
          e.compute(ur(e.inputs[0], "Cast", r, void 0, t.cacheKey, t.to));
        }, ao = (e) => {
          let t, r, n = e.length >= 2 && e[1].data !== 0, i = e.length >= 3 && e[2].data !== 0;
          switch (e[0].dataType) {
            case 1:
              t = n ? e[1].getFloat32Array()[0] : -34028234663852886e22, r = i ? e[2].getFloat32Array()[0] : 34028234663852886e22;
              break;
            case 10:
              t = n ? e[1].getUint16Array()[0] : 64511, r = i ? e[2].getUint16Array()[0] : 31743;
              break;
            default:
              throw new Error("Unsupport data type");
          }
          return ot({ min: t, max: r });
        }, _l = (e, t) => {
          let r = t || ao(e.inputs), n = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "Clip", (i) => `clamp(${i}, vec4<${n}>(uniforms.min), vec4<${n}>(uniforms.max))`, void 0, r.cacheKey, void 0, [{ type: e.inputs[0].dataType, data: r.min }, { type: e.inputs[0].dataType, data: r.max }], [{ name: "min", type: n }, { name: "max", type: n }]), { inputs: [0] });
        }, fl = (e) => {
          e.compute(ur(e.inputs[0], "Ceil", "ceil"));
        }, lo = (e) => {
          e.compute(ur(e.inputs[0], "Cos", "cos"));
        }, gl = (e) => {
          e.compute(ur(e.inputs[0], "Cosh", "cosh"));
        }, On = (e) => ot(e), wl = (e, t) => {
          let r = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "Elu", (n) => `elu_vf32(${n})`, `
  const elu_alpha_ = ${r}(${t.alpha});

  fn elu_f32(a: ${r}) -> ${r} {
  return select((exp(a) - 1.0) * elu_alpha_, a, a >= 0.0);
  }

  fn elu_vf32(v: vec4<${r}>) -> vec4<${r}> {
  return vec4(elu_f32(v.x), elu_f32(v.y), elu_f32(v.z), elu_f32(v.w));
  }`, t.cacheKey));
        }, ri = (e = "f32") => `
const r0: ${e} = 0.3275911;
const r1: ${e} = 0.254829592;
const r2: ${e} = -0.284496736;
const r3: ${e} = 1.421413741;
const r4: ${e} = -1.453152027;
const r5: ${e} = 1.061405429;

fn erf_vf32(v: vec4<${e}>) -> vec4<${e}> {
  let absv = abs(v);
  let x = 1.0 / (1.0 + r0 * absv);
  return sign(v) * (1.0 - ((((r5 * x + r4) * x + r3) * x + r2) * x + r1) * x * exp(-absv * absv));
}`, uo = (e) => {
          let t = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "Erf", (r) => `erf_vf32(${r})`, ri(t)));
        }, yl = (e) => {
          e.compute(ur(e.inputs[0], "Exp", "exp"));
        }, Ml = (e) => {
          e.compute(ur(e.inputs[0], "Floor", "floor"));
        }, co = (e) => {
          let t = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "Gelu", (r) => `0.5 * ${r} * (1.0 + erf_vf32(${r} * 0.7071067811865475))`, ri(t)));
        }, bl = (e, t) => {
          let r = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "LeakyRelu", (n) => `select(leaky_relu_alpha_ * ${n}, ${n}, ${n} >= vec4<${r}>(0.0))`, `const leaky_relu_alpha_ = ${r}(${t.alpha});`, t.cacheKey));
        }, vl = (e) => {
          e.compute(ur(e.inputs[0], "Not", (t) => `!${t}`));
        }, po = (e) => {
          e.compute(ur(e.inputs[0], "Neg", (t) => `-${t}`));
        }, xl = (e) => {
          e.compute(ur(e.inputs[0], "Reciprocal", (t) => `1.0/${t}`));
        }, Tl = (e) => {
          let t = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "Relu", (r) => `select(vec4<${t}>(0.0), ${r}, ${r} > vec4<${t}>(0.0))`));
        }, ho = (e) => {
          e.compute(ur(e.inputs[0], "Sigmoid", (t) => `(1.0 / (1.0 + exp(-${t})))`));
        }, El = (e) => ot(e), Pl = (e, t) => {
          let r = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "HardSigmoid", (n) => `max(vec4<${r}>(0.0), min(vec4<${r}>(1.0), ${t.alpha} * ${n} + vec4<${r}>(${t.beta})))`, void 0, t.cacheKey));
        }, mo = (e) => {
          e.compute(ur(e.inputs[0], "Sin", "sin"));
        }, Cl = (e) => {
          e.compute(ur(e.inputs[0], "Sinh", "sinh"));
        }, kl = (e) => {
          e.compute(ur(e.inputs[0], "Sqrt", "sqrt"));
        }, _o = (e) => {
          e.compute(ur(e.inputs[0], "Tan", "tan"));
        }, fo = (e) => `sign(${e}) * (1 - exp(-2 * abs(${e}))) / (1 + exp(-2 * abs(${e})))`, Sl = (e) => {
          e.compute(ur(e.inputs[0], "Tanh", fo));
        }, si = (e = "f32") => `
const fast_gelu_a: ${e} = 0.5;
const fast_gelu_b: ${e} = 0.7978845608028654;
const fast_gelu_c: ${e} = 0.035677408136300125;

fn tanh_v(v: vec4<${e}>) -> vec4<${e}> {
  return ${fo("v")};
}
`, go = (e) => `(fast_gelu_a + fast_gelu_a * tanh_v(${e} * (fast_gelu_c * ${e} * ${e} + fast_gelu_b))) * ${e}`, $l = (e) => {
          let t = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "FastGelu", go, si(t), void 0, e.inputs[0].dataType));
        }, Al = (e, t) => {
          let r = mr(e.inputs[0].dataType);
          return e.compute(ur(e.inputs[0], "ThresholdedRelu", (n) => `select(vec4<${r}>(0.0), ${n}, ${n} > thresholded_relu_alpha_)`, `const thresholded_relu_alpha_ = vec4<${r}>(${t.alpha});`, t.cacheKey)), 0;
        }, Il = (e) => {
          e.compute(ur(e.inputs[0], "Log", "log"));
        }, wo = (e, t) => `
const alpha = vec4<${e}>(${t});
const one = ${e}(1.0);
const zero = ${e}(0.0);

fn quick_gelu_impl(x: vec4<${e}>) -> vec4<${e}> {
  let v = x *alpha;
  var x1 : vec4<${e}>;
  for (var i = 0; i < 4; i = i + 1) {
    if (v[i] >= zero) {
      x1[i] = one / (one + exp(-v[i]));
    } else {
      x1[i] = one - one / (one + exp(v[i]));
    }
  }
  return x * x1;
}
`, Fl = (e) => `quick_gelu_impl(${e})`, Ol = (e, t) => {
          let r = mr(e.inputs[0].dataType);
          e.compute(ur(e.inputs[0], "QuickGelu", Fl, wo(r, t.alpha), t.cacheKey, e.inputs[0].dataType));
        };
      }), Dl, Mo, Ll, Dc = w(() => {
        Bt(), Qt(), yo(), Dl = (e) => {
          if (e[0].dims.length !== 3) throw new Error("input should have 3 dimensions");
          if (![2560, 5120, 10240].includes(e[0].dims[2])) throw new Error("hidden state should be 2560, 5120 or 10240");
          if (e[1].dims.length !== 1) throw new Error("bias is expected to have 1 dimensions");
          if (e[0].dims[2] !== e[1].dims[0]) throw new Error("last dimension of input and bias are not the same");
        }, Mo = (e) => {
          let t = e[0].dims.slice();
          t[2] = t[2] / 2;
          let r = ze("input", e[0].dataType, e[0].dims, 4), n = ze("bias", e[0].dataType, [e[0].dims[2]], 4), i = wt("output", e[0].dataType, t, 4), a = $e.size(t) / 4, o = er(e[0].dataType);
          return { name: "BiasSplitGelu", getRunData: () => ({ outputs: [{ dims: t, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(a / 64) } }), getShaderSource: (d) => `
  const M_SQRT2 = sqrt(2.0);
  const halfChannels = ${e[0].dims[2] / 4 / 2}u;

  ${d.declareVariables(r, n, i)}

  ${ri(o)}

  ${d.mainStart()}
    ${d.guardAgainstOutOfBoundsWorkgroupSizes(a)}
    let biasIdx = global_idx % halfChannels;
    let batchIndex = global_idx / halfChannels;
    let inputOffset = biasIdx + batchIndex * halfChannels * 2;
    let valueLeft = input[inputOffset] + bias[biasIdx];
    let valueRight = input[inputOffset + halfChannels] + bias[biasIdx + halfChannels];
    let geluRight = valueRight * 0.5 * (erf_vf32(valueRight / M_SQRT2) + 1);

    ${i.setByOffset("global_idx", "valueLeft * geluRight")}
  }` };
        }, Ll = (e) => {
          Dl(e.inputs), e.compute(Mo(e.inputs));
        };
      }), zl, Bl, ds, Rl, Nl, jl, Ul, bo, Wl, Vl, vo, Gl, Kl, Hl = w(() => {
        zt(), Bt(), Qt(), zl = (e, t, r, n, i, a, o, d, p, h, k, S) => {
          let u, B;
          typeof d == "string" ? u = B = (Q, _e) => `${d}((${Q}),(${_e}))` : typeof d == "function" ? u = B = d : (u = d.scalar, B = d.vector);
          let R = wt("outputData", k, n.length, 4), N = ze("aData", p, t.length, 4), Z = ze("bData", h, r.length, 4), te;
          if (i) if (a) {
            let Q = $e.size(t) === 1, _e = $e.size(r) === 1, me = t.length > 0 && t[t.length - 1] % 4 === 0, ye = r.length > 0 && r[r.length - 1] % 4 === 0;
            Q || _e ? te = R.setByOffset("global_idx", B(Q ? `${N.type.value}(${N.getByOffset("0")}.x)` : N.getByOffset("global_idx"), _e ? `${Z.type.value}(${Z.getByOffset("0")}.x)` : Z.getByOffset("global_idx"))) : te = `
            let outputIndices = ${R.offsetToIndices("global_idx * 4u")};
            let offsetA = ${N.broadcastedIndicesToOffset("outputIndices", R)};
            let offsetB = ${Z.broadcastedIndicesToOffset("outputIndices", R)};
            ${R.setByOffset("global_idx", B(o || me ? N.getByOffset("offsetA / 4u") : `${N.type.value}(${N.getByOffset("offsetA / 4u")}[offsetA % 4u])`, o || ye ? Z.getByOffset("offsetB / 4u") : `${Z.type.value}(${Z.getByOffset("offsetB / 4u")}[offsetB % 4u])`))}
          `;
          } else te = R.setByOffset("global_idx", B(N.getByOffset("global_idx"), Z.getByOffset("global_idx")));
          else {
            if (!a) throw new Error("no necessary to use scalar implementation for element-wise binary op implementation.");
            let Q = (_e, me, ye = "") => {
              let Ae = `aData[indexA${me}][componentA${me}]`, Ie = `bData[indexB${me}][componentB${me}]`;
              return `
            let outputIndices${me} = ${R.offsetToIndices(`global_idx * 4u + ${me}u`)};
            let offsetA${me} = ${N.broadcastedIndicesToOffset(`outputIndices${me}`, R)};
            let offsetB${me} = ${Z.broadcastedIndicesToOffset(`outputIndices${me}`, R)};
            let indexA${me} = offsetA${me} / 4u;
            let indexB${me} = offsetB${me} / 4u;
            let componentA${me} = offsetA${me} % 4u;
            let componentB${me} = offsetB${me} % 4u;
            ${_e}[${me}] = ${ye}(${u(Ae, Ie)});
          `;
            };
            k === 9 ? te = `
            var data = vec4<u32>(0);
            ${Q("data", 0, "u32")}
            ${Q("data", 1, "u32")}
            ${Q("data", 2, "u32")}
            ${Q("data", 3, "u32")}
            outputData[global_idx] = dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(data));` : te = `
            ${Q("outputData[global_idx]", 0)}
            ${Q("outputData[global_idx]", 1)}
            ${Q("outputData[global_idx]", 2)}
            ${Q("outputData[global_idx]", 3)}
          `;
          }
          return `
        ${e.registerUniform("vec_size", "u32").declareVariables(N, Z, R)}

        ${S ?? ""}

        ${e.mainStart()}
        ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
        ${te}
      }`;
        }, Bl = (e, t, r, n, i, a, o = r.dataType) => {
          let d = r.dims.map((N) => Number(N) ?? 1), p = n.dims.map((N) => Number(N) ?? 1), h = !$e.areEqual(d, p), k = d, S = $e.size(d), u = !1, B = !1, R = [h];
          if (h) {
            let N = rr.calcShape(d, p, !1);
            if (!N) throw new Error("Can't perform binary op on the given tensors");
            k = N.slice(), S = $e.size(k);
            let Z = $e.size(d) === 1, te = $e.size(p) === 1, Q = d.length > 0 && d[d.length - 1] % 4 === 0, _e = p.length > 0 && p[p.length - 1] % 4 === 0;
            R.push(Z), R.push(te), R.push(Q), R.push(_e);
            let me = 1;
            for (let ye = 1; ye < k.length; ye++) {
              let Ae = d[d.length - ye], Ie = p[p.length - ye];
              if (Ae === Ie) me *= Ae;
              else break;
            }
            me % 4 === 0 ? (B = !0, u = !0) : (Z || te || Q || _e) && (u = !0);
          } else u = !0;
          return R.push(u), { name: e, shaderCache: { hint: t + R.map((N) => N.toString()).join("_"), inputDependencies: ["rank", "rank"] }, getShaderSource: (N) => zl(N, d, p, k, u, h, B, i, r.dataType, n.dataType, o, a), getRunData: () => ({ outputs: [{ dims: k, dataType: o }], dispatchGroup: { x: Math.ceil(S / 64 / 4) }, programUniforms: [{ type: 12, data: Math.ceil($e.size(k) / 4) }, ...vt(d, p, k)] }) };
        }, ds = (e, t, r, n, i, a) => {
          e.compute(Bl(t, i ?? "", e.inputs[0], e.inputs[1], r, n, a));
        }, Rl = (e) => {
          ds(e, "Add", (t, r) => `${t}+${r}`);
        }, Nl = (e) => {
          ds(e, "Div", (t, r) => `${t}/${r}`);
        }, jl = (e) => {
          ds(e, "Equal", { scalar: (t, r) => `u32(${t}==${r})`, vector: (t, r) => `vec4<u32>(${t}==${r})` }, void 0, void 0, 9);
        }, Ul = (e) => {
          ds(e, "Mul", (t, r) => `${t}*${r}`);
        }, bo = (e) => {
          let t = ze("input", e.inputs[0].dataType, e.inputs[0].dims).type.value;
          ds(e, "Pow", { scalar: (r, n) => `pow_custom(${r},${n})`, vector: (r, n) => `pow_vector_custom(${r},${n})` }, `
    fn pow_custom(a : ${t}, b : ${t}) -> ${t} {
      if (b == ${t}(0.0)) {
        return ${t}(1.0);
      } else if (a < ${t}(0.0) && f32(b) != floor(f32(b))) {
        return ${t}(pow(f32(a), f32(b))); // NaN
      }
      return select(sign(a), ${t}(1.0), round(f32(abs(b) % ${t}(2.0))) != 1.0) * ${t}(${t === "i32" ? "round" : ""}(pow(f32(abs(a)), f32(b))));
    }
    fn pow_vector_custom(a : vec4<${t}>, b : vec4<${t}>) -> vec4<${t}> {
      // TODO: implement vectorized pow
      return vec4<${t}>(pow_custom(a.x, b.x), pow_custom(a.y, b.y), pow_custom(a.z, b.z), pow_custom(a.w, b.w));
    }
      `);
        }, Wl = (e) => {
          ds(e, "Sub", (t, r) => `${t}-${r}`);
        }, Vl = (e) => {
          ds(e, "Greater", { scalar: (t, r) => `u32(${t}>${r})`, vector: (t, r) => `vec4<u32>(${t}>${r})` }, void 0, void 0, 9);
        }, vo = (e) => {
          ds(e, "Less", { scalar: (t, r) => `u32(${t}<${r})`, vector: (t, r) => `vec4<u32>(${t}<${r})` }, void 0, void 0, 9);
        }, Gl = (e) => {
          ds(e, "GreaterOrEqual", { scalar: (t, r) => `u32(${t}>=${r})`, vector: (t, r) => `vec4<u32>(${t}>=${r})` }, void 0, void 0, 9);
        }, Kl = (e) => {
          ds(e, "LessOrEqual", { scalar: (t, r) => `u32(${t}<=${r})`, vector: (t, r) => `vec4<u32>(${t}<=${r})` }, void 0, void 0, 9);
        };
      }), ql, Xl, xo, Ql, Yl, To, Lc = w(() => {
        zt(), Bt(), Pt(), Qt(), ql = (e, t) => {
          if (!e || e.length < 1) throw new Error("too few inputs");
          let r = 0, n = e[r], i = n.dataType, a = n.dims.length;
          e.forEach((o, d) => {
            if (d !== r) {
              if (o.dataType !== i) throw new Error("input tensors should be one type");
              if (o.dims.length !== a) throw new Error("input tensors should have the same shape");
              o.dims.forEach((p, h) => {
                if (h !== t && p !== n.dims[h]) throw new Error("non concat dimensions must match");
              });
            }
          });
        }, Xl = (e, t) => `
  fn calculateInputIndex(index: u32) -> u32 {
    let sizeInConcatAxis = array<u32, ${e}u>(${t});
    for (var i: u32 = 0u; i < ${e}; i += 1u ) {
      if (index < sizeInConcatAxis[i]) {
        return i;
      }
    }
    return ${e}u;
  }`, xo = (e, t) => {
          let r = e.length, n = [];
          for (let i = 0; i < r; ++i) {
            let a = t.setByOffset("global_idx", e[i].getByIndices("indices"));
            r === 1 ? n.push(a) : i === 0 ? n.push(`if (inputIndex == ${i}u) { ${a} }`) : i === r - 1 ? n.push(`else { ${a} }`) : n.push(`else if (inputIndex == ${i}) { ${a} }`);
          }
          return n.join(`
`);
        }, Ql = (e, t, r, n) => {
          let i = $e.size(r), a = new Array(e.length), o = new Array(e.length), d = 0, p = [], h = [], k = [{ type: 12, data: i }];
          for (let N = 0; N < e.length; ++N) d += e[N].dims[t], a[N] = d, h.push(e[N].dims.length), o[N] = ze(`input${N}`, n, h[N]), p.push("rank"), k.push({ type: 12, data: a[N] });
          for (let N = 0; N < e.length; ++N) k.push(...vt(e[N].dims));
          k.push(...vt(r));
          let S = wt("output", n, r.length), u = S.indicesGet("indices", t), B = Array.from(Array(a.length).keys()).map((N) => `uniforms.sizeInConcatAxis${N}`).join(","), R = (N) => `

  ${(() => {
            N.registerUniform("outputSize", "u32");
            for (let Z = 0; Z < e.length; Z++) N.registerUniform(`sizeInConcatAxis${Z}`, "u32");
            return N.declareVariables(...o, S);
          })()}

  ${Xl(a.length, B)}

  ${N.mainStart()}
    ${N.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

    var indices = ${S.offsetToIndices("global_idx")};

    let inputIndex = calculateInputIndex(${u});
    if (inputIndex != 0u) {
      let sizeInConcatAxis = array<u32, ${a.length}u>(${B});
      ${u} -= sizeInConcatAxis[inputIndex - 1u];
    }

    ${xo(o, S)}
  }`;
          return { name: "Concat", shaderCache: { hint: `${t}`, inputDependencies: p }, getRunData: () => ({ outputs: [{ dims: r, dataType: n }], dispatchGroup: { x: Math.ceil(i / 64) }, programUniforms: k }), getShaderSource: R };
        }, Yl = (e, t) => {
          let r = e.inputs, n = r[0].dims, i = $e.normalizeAxis(t.axis, n.length);
          ql(r, i);
          let a = n.slice();
          a[i] = r.reduce((d, p) => d + (p.dims.length > i ? p.dims[i] : 0), 0);
          let o = r.filter((d) => $e.size(d.dims) > 0);
          e.compute(Ql(o, i, a, r[0].dataType), { inputs: o });
        }, To = (e) => ot({ axis: e.axis });
      }), sn, Is, nn, Eo, Gs = w(() => {
        zt(), Bt(), sn = (e, t, r = "f32") => {
          switch (e.activation) {
            case "Relu":
              return `value = max(value, ${t}(0.0));`;
            case "Sigmoid":
              return `value = (${t}(1.0) / (${t}(1.0) + exp(-value)));`;
            case "Clip":
              return `value = clamp(value, ${t}(${r}(uniforms.clip_min)), ${t}(${r}(uniforms.clip_max)));`;
            case "HardSigmoid":
              return `value = max(${t}(0.0), min(${t}(1.0), ${r}(uniforms.alpha) * value + ${r}(uniforms.beta)));`;
            case "LeakyRelu":
              return `value = select(${r}(uniforms.alpha) * value, value, value >= ${t}(0.0));`;
            case "Tanh":
              return `let e2x = exp(-2.0 * abs(value));
              value = sign(value) * (1.0 - e2x) / (1.0 + e2x);
        `;
            case "":
              return "";
            default:
              throw new Error(`Unsupported activation ${e.activation}`);
          }
        }, Is = (e, t) => {
          e.activation === "Clip" ? t.push({ type: 1, data: e.clipMax }, { type: 1, data: e.clipMin }) : e.activation === "HardSigmoid" ? t.push({ type: 1, data: e.alpha }, { type: 1, data: e.beta }) : e.activation === "LeakyRelu" && t.push({ type: 1, data: e.alpha });
        }, nn = (e, t) => {
          e.activation === "Clip" ? t.push({ name: "clip_max", type: "f32" }, { name: "clip_min", type: "f32" }) : e.activation === "HardSigmoid" ? t.push({ name: "alpha", type: "f32" }, { name: "beta", type: "f32" }) : e.activation === "LeakyRelu" && t.push({ name: "alpha", type: "f32" });
        }, Eo = (e) => {
          let t = (e == null ? void 0 : e.activation) || "";
          if (t === "HardSigmoid") {
            let [r, n] = (e == null ? void 0 : e.activation_params) || [0.2, 0.5];
            return { activation: t, alpha: r, beta: n };
          } else if (t === "Clip") {
            let [r, n] = (e == null ? void 0 : e.activation_params) || [Jr, Zr];
            return { activation: t, clipMax: n, clipMin: r };
          } else if (t === "LeakyRelu") {
            let [r] = (e == null ? void 0 : e.activation_params) || [0.01];
            return { activation: t, alpha: r };
          }
          return { activation: t };
        };
      }), Wr, Jl, ni = w(() => {
        Wr = (e, t) => {
          switch (e) {
            case 1:
              return t;
            case 2:
              return `vec2<${t}>`;
            case 3:
              return `vec3<${t}>`;
            case 4:
              return `vec4<${t}>`;
            default:
              throw new Error(`${e}-component is not supported.`);
          }
        }, Jl = (e) => `
      ${e ? "value = value + getBiasByOutputCoords(coords);" : ""}
      `;
      }), Zl, zc = w(() => {
        Zl = (e) => `
fn getIndexFromCoords4D(coords : vec4<i32>, shape : vec4<i32>) -> i32 {
  return dot(coords, vec4<i32>(
      shape.y * shape.z * shape.w, shape.z * shape.w, shape.w, 1));
}
fn getOutputIndexFromCoords(coords : vec4<i32>) -> i32 {
  return dot(coords, vec4<i32>(
    i32(${e}.x), i32(${e}.y), i32(${e}.z), 1));
}
`;
      }), Dn, ii, Po = w(() => {
        zt(), Bt(), Qt(), Gs(), Dn = (e, t, r, n, i) => {
          let a = n - r;
          return `
      ${Array.from({ length: r }).map((o, d) => `
      if (${Mt(t.shape, d, t.rank)} != 1) {
        ${t.indicesSet(e, d, Mt(i, d + a, n))}
      } else {
        ${t.indicesSet(e, d, 0)}
      }`).join("")}
`;
        }, ii = (e, t, r, n, i = !1, a) => {
          let o = e[0].dims, d = e[1].dims, p = o[o.length - 2], h = d[d.length - 1], k = o[o.length - 1], S = yr(h), u = yr(k), B = yr(p), R = $e.size(r) / S / B, N = e.length > 2, Z = n ? n.slice(0, -2) : r.slice(0, -2), te = [$e.size(Z), p, h], Q = [{ type: 12, data: R }, { type: 12, data: p }, { type: 12, data: h }, { type: 12, data: k }];
          Is(t, Q), Q.push(...vt(Z, o, d)), N && Q.push(...vt(e[2].dims)), Q.push(...vt(te));
          let _e = (me) => {
            let ye = tn("batch_dims", e[0].dataType, Z.length), Ae = ze("a", e[0].dataType, o.length, u), Ie = ze("b", e[1].dataType, d.length, S), Ge = wt("output", e[0].dataType, te.length, S), lt = er(Ge.type.tensor), Tt = sn(t, Ge.type.value, lt), Kt = [Ae, Ie], Yt = "";
            if (N) {
              let $t = i ? S : 1;
              Kt.push(ze("bias", e[2].dataType, e[2].dims.length, $t)), Yt = `${i ? `value += bias[col / ${$t}];` : `value += ${Ge.type.value}(bias[row + i]);`}`;
            }
            let Ct = [{ name: "output_size", type: "u32" }, { name: "M", type: "u32" }, { name: "N", type: "u32" }, { name: "K", type: "u32" }];
            nn(t, Ct);
            let Jt = () => {
              let $t = `var a_data: ${Ae.type.value};`;
              for (let jt = 0; jt < u; jt++) $t += `
              let b_data${jt} = b[(b_offset + (k + ${jt}) * uniforms.N + col) / ${S}];`;
              for (let jt = 0; jt < B; jt++) {
                $t += `a_data = a[(a_offset + (row + ${jt}) * uniforms.K + k) / ${u}];`;
                for (let vr = 0; vr < u; vr++) $t += `
            values[${jt}] = fma(${Ie.type.value}(a_data${u === 1 ? "" : `[${vr}]`}), b_data${vr}, values[${jt}]);
`;
              }
              return $t;
            };
            return `
  ${me.registerUniforms(Ct).registerInternalVariables(ye).declareVariables(...Kt, Ge)}
  ${me.mainStart()}
    ${me.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let col = (global_idx % (uniforms.N / ${S})) * ${S};
    var index1 = global_idx / (uniforms.N / ${S});
    let stride1 = uniforms.M / ${B};
    let row = (index1 % stride1) * ${B};
    let batch = index1 / stride1;

    ${r.length === 2 ? "" : `let batch_indices = ${ye.offsetToIndices("batch")};`}

    var a_indices: ${Ae.type.indices};
    ${Dn("a_indices", Ae, Ae.rank - 2, ye.rank, "batch_indices")}
    ${Ae.indicesSet("a_indices", Ae.rank - 2, 0)}
    ${Ae.indicesSet("a_indices", Ae.rank - 1, 0)}
    let a_offset = ${Ae.indicesToOffset("a_indices")};

    var b_indices: ${Ie.type.indices};
    ${Dn("b_indices", Ie, Ie.rank - 2, ye.rank, "batch_indices")}
    ${Ie.indicesSet("b_indices", Ie.rank - 2, 0)}
    ${Ie.indicesSet("b_indices", Ie.rank - 1, 0)}
    let b_offset = ${Ie.indicesToOffset("b_indices")};
    var values: array<${Ge.type.value}, ${B}>;
    for (var k: u32 = 0u; k < uniforms.K; k = k + ${u}) {
      ${Jt()}
    }
    for (var i = 0u; i < ${B}u; i++) {
      var value = values[i];
      ${Yt}
      ${Tt}
      let cur_indices = ${Ge.type.indices}(batch, row + i, col);
      let offset = ${Ge.indicesToOffset("cur_indices")};
      ${Ge.setByOffset(`offset / ${S}`, "value")};
    }
  }
  `;
          };
          return { name: "MatMulNaive", shaderCache: { hint: `${t.activation};${S};${u};${B};${i}`, inputDependencies: N ? ["rank", "rank", "rank"] : ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: a ? a(r) : r, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(R / 64) }, programUniforms: Q }), getShaderSource: _e };
        };
      }), eu, tu, oi, Co, ru, ai, su, li, ui = w(() => {
        zt(), Bt(), Qt(), Gs(), Po(), ni(), eu = (e, t) => e ? `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          kStart + inputRow,
          globalRowStart / innerElementSize + inputCol${t ? ", batchIndices" : ""});
        ` : `
        mm_Asub[inputRow][inputCol] = mm_readA(batch,
          globalRow + innerRow,
          kStart / innerElementSize + inputCol${t ? ", batchIndices" : ""});
        `, tu = (e, t) => e ? `
        let ACached0 = mm_Asub[k * innerElementSize][localRow];
        let ACached1 = mm_Asub[k * innerElementSize + 1][localRow];
        let ACached2 = mm_Asub[k * innerElementSize + 2][localRow];
        ${t === 3 ? "" : "let ACached3 = mm_Asub[k * innerElementSize + 3][localRow];"}
        for (var i = 0; i < rowPerThread; i = i + 1) {
          acc[i] = BCached0 * ACached0[i] + acc[i];
          acc[i] = BCached1 * ACached1[i] + acc[i];
          acc[i] = BCached2 * ACached2[i] + acc[i];
          ${t === 3 ? "" : "acc[i] = BCached3 * ACached3[i] + acc[i];"}
        }` : `
        for (var i = 0; i < rowPerThread; i = i + 1) {
          let ACached = mm_Asub[tileRow + i][k];
          acc[i] = BCached0 * ACached.x + acc[i];
          acc[i] = BCached1 * ACached.y + acc[i];
          acc[i] = BCached2 * ACached.z + acc[i];
          ${t === 3 ? "" : "acc[i] = BCached3 * ACached.w + acc[i];"}
        }`, oi = (e, t, r = "f32", n, i = !1, a = 32, o = !1, d = 32) => {
          let p = t[1] * e[1], h = t[0] * e[0], k = i ? p : a, S = i ? a : p, u = k / t[0], B = a / t[1];
          if (!((i && u === 4 && e[1] === 4 || !i && (u === 3 || u === 4)) && k % t[0] === 0 && a % t[1] === 0 && e[0] === 4)) throw new Error(`If transposeA ${i} is true, innerElementSize ${u} and workPerThread[1] ${e[1]} must be 4.
      Otherwise, innerElementSize ${u} must be 3 or 4.
  tileAWidth ${k} must be divisible by workgroupSize[0]${t[0]}. tileInner ${a} must be divisible by workgroupSize[1] ${t[1]}. colPerThread ${e[0]} must be 4.`);
          return `
var<workgroup> mm_Asub: array<array<vec${u}<${r}>, ${k / u}>, ${S}>;
var<workgroup> mm_Bsub: array<array<vec4<${r}>, ${h / e[0]}>, ${a}>;

const rowPerThread = ${e[1]};
const colPerThread = ${e[0]};
const innerElementSize = ${u};
const tileInner = ${a};

@compute @workgroup_size(${t[0]}, ${t[1]}, ${t[2]})
fn main(@builtin(local_invocation_id) localId : vec3<u32>,
        @builtin(global_invocation_id) globalId : vec3<u32>,
        @builtin(workgroup_id) workgroupId : vec3<u32>) {
  let localRow = i32(localId.y);
  let tileRow = localRow * rowPerThread;
  let tileCol = i32(localId.x);

  let globalRow =i32(globalId.y) * rowPerThread;
  let globalCol = i32(globalId.x);
  let batch = ${o ? "0" : "i32(globalId.z)"};
  ${n ? `let batchIndices = ${n.offsetToIndices("u32(batch)")};` : ""}
  let globalRowStart = i32(workgroupId.y) * ${p};

  let num_tiles = ${o ? `${Math.ceil(d / a)}` : "(uniforms.dim_inner - 1) / tileInner + 1"};
  var kStart = ${o ? `i32(globalId.z) * ${d}` : "0"};

  var acc: array<vec4<${r}>, rowPerThread>;

  // Loop over shared dimension.
  let tileRowB = localRow * ${B};
  for (var t = 0; t < num_tiles; t = t + 1) {
      // Load one tile of A into local memory.
      for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
          let inputRow = tileRow + innerRow;
          let inputCol = tileCol;
          ${eu(i, n)}
      }

      // Load one tile of B into local memory.
      for (var innerRow = 0; innerRow < ${B}; innerRow = innerRow + 1) {
          let inputRow = tileRowB + innerRow;
          let inputCol = tileCol;
          mm_Bsub[inputRow][inputCol] = mm_readB(batch, kStart + inputRow, globalCol${n ? ", batchIndices" : ""});
      }
      kStart = kStart + tileInner;
      workgroupBarrier();

      // Compute acc values for a single thread.
      for (var k = 0; k < tileInner / innerElementSize; k = k + 1) {
          let BCached0 = mm_Bsub[k * innerElementSize][tileCol];
          let BCached1 = mm_Bsub[k * innerElementSize + 1][tileCol];
          let BCached2 = mm_Bsub[k * innerElementSize + 2][tileCol];
          ${u === 3 ? "" : "let BCached3 = mm_Bsub[k * innerElementSize + 3][tileCol];"}

          ${tu(i, u)}
      }

      workgroupBarrier();
  }

  for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      mm_write(batch, globalRow + innerRow, globalCol, acc[innerRow]);
  }
}`;
        }, Co = (e, t) => e ? `
            mm_Asub[inputRow][inputCol] = mm_readA(batch,
              kStart + inputRow,
              globalRowStart + inputCol${t ? ", batchIndices" : ""});
            ` : `
            mm_Asub[inputRow][inputCol] = mm_readA(batch,
              globalRowStart + inputRow,
              kStart + inputCol${t ? ", batchIndices" : ""});
            `, ru = (e) => e ? "let ACached = mm_Asub[k][tileRow + innerRow];" : "let ACached = mm_Asub[tileRow + innerRow][k];", ai = (e, t, r = "f32", n, i = !1, a = 32, o = !1, d = 32, p = !1) => {
          let h = e[1] * t[1], k = e[0] * t[0], S = i ? h : a, u = i ? a : h;
          if (!(u % t[1] === 0 && S % t[0] === 0 && a % t[1] === 0)) throw new Error(`tileAHight ${u} must be divisible by workgroupSize[1]${t[1]}, tileAWidth ${S} must be divisible by workgroupSize[0]${t[0]}, tileInner ${a} must be divisible by workgroupSize[1]${t[1]}`);
          let B = u / t[1], R = S / t[0], N = a / t[1], Z = p ? `
    let localRow = i32(localId.y);
    let localCol = i32(localId.x);
    let globalRowStart = i32(workgroupId.y) * ${h};
    let globalColStart = i32(workgroupId.x) * ${k};

    // Loop over shared dimension.
    for (var t = 0; t < num_tiles; t = t + 1) {
      // Load one tile of A into local memory.
      for (var inputRow = localRow; inputRow < ${u}; inputRow = inputRow + ${t[1]}) {
        for (var inputCol = localCol; inputCol < ${S}; inputCol = inputCol + ${t[0]}) {
          ${Co(i, n)}
        }
      }
      // Load one tile of B into local memory.
      for (var inputRow = localRow; inputRow < ${a}; inputRow = inputRow + ${t[1]}) {
            for (var inputCol = localCol; inputCol < ${k}; inputCol = inputCol + ${t[0]}) {
          mm_Bsub[inputRow][inputCol] = mm_readB(batch,
            kStart + inputRow,
            globalColStart + inputCol${n ? ", batchIndices" : ""});
        }
      }
      kStart = kStart + tileInner;
      workgroupBarrier();

      // Compute acc values for a single thread.
      var BCached : array<${r}, colPerThread>;
      for (var k = 0; k < tileInner; k = k + 1) {
        for (var inner = 0; inner < colPerThread; inner = inner + 1) {
          BCached[inner] = mm_Bsub[k][localCol + inner * ${t[0]}];
        }
        for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
          let ACached = ${i ? `mm_Asub[k][localRow + innerRow * ${t[1]}];` : `mm_Asub[localRow + innerRow * ${t[1]}][k];`}
          for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
            acc[innerRow][innerCol] = acc[innerRow][innerCol] +
                ACached * BCached[innerCol];
          }
        }
      }
      workgroupBarrier();
    }
    for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      let gRow = globalRowStart + localRow + innerRow * ${t[1]};
      for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
        let gCol = globalColStart + localCol + innerCol * ${t[0]};
        mm_write(batch, gRow, gCol, acc[innerRow][innerCol]);
      }
    }
    ` : `
let tileRow = i32(localId.y) * rowPerThread;
let tileCol = i32(localId.x) * colPerThread;

let globalRow = i32(globalId.y) * rowPerThread;
let globalCol = i32(globalId.x) * colPerThread;
let globalRowStart = i32(workgroupId.y) * ${h};

let tileRowA = i32(localId.y) * ${B};
let tileColA = i32(localId.x) * ${R};
let tileRowB = i32(localId.y) * ${N};
// Loop over shared dimension.
for (var t = 0; t < num_tiles; t = t + 1) {
  // Load one tile of A into local memory.
  for (var innerRow = 0; innerRow < ${B}; innerRow = innerRow + 1) {
    for (var innerCol = 0; innerCol < ${R}; innerCol = innerCol + 1) {
      let inputRow = tileRowA + innerRow;
      let inputCol = tileColA + innerCol;
      ${Co(i, n)}
    }
  }

  // Load one tile of B into local memory.
  for (var innerRow = 0; innerRow < ${N}; innerRow = innerRow + 1) {
    for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
      let inputRow = tileRowB + innerRow;
      let inputCol = tileCol + innerCol;
      mm_Bsub[inputRow][inputCol] = mm_readB(batch,
        kStart + inputRow,
        globalCol + innerCol${n ? ", batchIndices" : ""});
    }
  }
  kStart = kStart + tileInner;
  workgroupBarrier();

  // Compute acc values for a single thread.
  var BCached : array<${r}, colPerThread>;
  for (var k = 0; k < tileInner; k = k + 1) {
    for (var inner = 0; inner < colPerThread; inner = inner + 1) {
      BCached[inner] = mm_Bsub[k][tileCol + inner];
    }

    for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
      ${ru(i)}
      for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
        acc[innerRow][innerCol] = acc[innerRow][innerCol] + ACached * BCached[innerCol];
      }
    }
  }

  workgroupBarrier();
}

for (var innerRow = 0; innerRow < rowPerThread; innerRow = innerRow + 1) {
  for (var innerCol = 0; innerCol < colPerThread; innerCol = innerCol + 1) {
    mm_write(batch, globalRow + innerRow, globalCol + innerCol,
        acc[innerRow][innerCol]);
  }
}
`;
          return `
  var<workgroup> mm_Asub : array<array<${r}, ${S}>, ${u}>;
  var<workgroup> mm_Bsub : array<array<${r}, ${k}>, ${a}>;
  const rowPerThread = ${e[1]};
  const colPerThread = ${e[0]};
  const tileInner = ${a};

@compute @workgroup_size(${t[0]}, ${t[1]}, ${t[2]})
fn main(@builtin(local_invocation_id) localId : vec3<u32>,
        @builtin(global_invocation_id) globalId : vec3<u32>,
        @builtin(workgroup_id) workgroupId : vec3<u32>) {
    let batch = ${o ? "0" : "i32(globalId.z)"};
    ${n ? `let batchIndices = ${n.offsetToIndices("u32(batch)")};` : ""}
    let num_tiles = ${o ? `${Math.ceil(d / a)}` : "(uniforms.dim_inner - 1) / tileInner + 1"};
    var kStart = ${o ? `i32(globalId.z) * ${d}` : "0"};

    var acc : array<array<${r}, colPerThread>, rowPerThread>;
    ${Z}
  }
`;
        }, su = (e, t, r, n, i = !1) => {
          let [a, o, d, p] = n, h = er(n[0].type.tensor);
          return `
    fn mm_readA(batch: i32, row: i32, colIn: i32, batchIndices: ${a.type.indices}) -> ${Wr(e, h)} {
      var value = ${Wr(e, h)}(0.0);
      let col = colIn * ${e};
      if(row < uniforms.dim_a_outer && col < uniforms.dim_inner)
      {
        var aIndices: ${o.type.indices};
        ${Dn("aIndices", o, o.rank - 2, a.rank, "batchIndices")}
        ${o.indicesSet("aIndices", o.rank - 2, "u32(row)")}
        ${o.indicesSet("aIndices", o.rank - 1, "u32(colIn)")}
        value = ${o.getByIndices("aIndices")};
      }
      return value;
    }

    fn mm_readB(batch: i32, row: i32, colIn: i32, batchIndices: ${a.type.indices}) -> ${Wr(e, h)} {
      var value = ${Wr(e, h)}(0.0);
      let col = colIn * ${e};
      if(row < uniforms.dim_inner && col < uniforms.dim_b_outer)
      {
        var bIndices: ${d.type.indices};
        ${Dn("bIndices", d, d.rank - 2, a.rank, "batchIndices")}
        ${d.indicesSet("bIndices", d.rank - 2, "u32(row)")}
        ${d.indicesSet("bIndices", d.rank - 1, "u32(colIn)")}
        value = ${d.getByIndices("bIndices")};
      }
      return value;
    }

    fn mm_write(batch: i32, row: i32, colIn: i32, valueIn: ${Wr(e, h)}) {
      let col = colIn * ${e};
      if (row < uniforms.dim_a_outer && col < uniforms.dim_b_outer) {
        var value = valueIn;
        let coords = vec3<i32>(batch, row, colIn);
        ${t ? `value = value + ${i ? "bias[colIn]" : `${Wr(e, h)}(bias[row])`};` : ""}
        ${r}
        ${p.setByIndices("vec3<u32>(coords)", "value")}
      }
    }
    `;
        }, li = (e, t, r, n, i = !1, a) => {
          let o = e[0].dims, d = e[1].dims, p = o.slice(0, -2), h = d.slice(0, -2), k = n ? n.slice(0, -2) : r.slice(0, -2), S = $e.size(k), u = o[o.length - 2], B = o[o.length - 1], R = d[d.length - 1], N = B % 4 === 0 && R % 4 === 0, Z = u <= 8 ? [4, 1, 1] : [4, 4, 1], te = [8, 8, 1], Q = [Math.ceil(R / te[0] / Z[0]), Math.ceil(u / te[1] / Z[1]), Math.ceil(S / te[2] / Z[2])], _e = N ? 4 : 1, me = [...p, u, B / _e], ye = me.length, Ae = [...h, B, R / _e], Ie = Ae.length, Ge = [S, u, R / _e], lt = [{ type: 6, data: u }, { type: 6, data: R }, { type: 6, data: B }];
          Is(t, lt), lt.push(...vt(k, me, Ae));
          let Tt = ["rank", "rank"], Kt = e.length > 2;
          Kt && (lt.push(...vt(e[2].dims)), Tt.push("rank")), lt.push(...vt(Ge));
          let Yt = (Ct) => {
            let Jt = k.length, $t = tn("batchDims", e[0].dataType, Jt, 1), jt = er(e[0].dataType), vr = ze("a", e[0].dataType, ye, _e), Ht = ze("b", e[1].dataType, Ie, _e), Gt = wt("result", e[0].dataType, Ge.length, _e), Cr = [vr, Ht];
            if (Kt) {
              let ys = i ? _e : 1;
              Cr.push(ze("bias", e[2].dataType, e[2].dims.length, ys));
            }
            let it = [{ name: "dim_a_outer", type: "i32" }, { name: "dim_b_outer", type: "i32" }, { name: "dim_inner", type: "i32" }];
            nn(t, it);
            let Et = er(Gt.type.tensor), cr = sn(t, Gt.type.value, Et), Lr = su(_e, Kt, cr, [$t, vr, Ht, Gt], i);
            return `
  ${Ct.registerUniforms(it).registerInternalVariables($t).declareVariables(...Cr, Gt)}
  ${Lr}
  ${N ? oi(Z, te, jt, $t) : ai(Z, te, jt, $t)}
                   `;
          };
          return { name: "MatMul", shaderCache: { hint: `${Z};${t.activation};${N};${i}`, inputDependencies: Tt }, getRunData: () => ({ outputs: [{ dims: a ? a(r) : r, dataType: e[0].dataType }], dispatchGroup: { x: Q[0], y: Q[1], z: Q[2] }, programUniforms: lt }), getShaderSource: Yt };
        };
      }), nu, iu, ou = w(() => {
        zt(), Yr(), Qt(), Gs(), ni(), zc(), ui(), nu = (e, t, r, n, i = !1, a, o = 4, d = 4, p = 4, h = "f32") => {
          let k = (lt) => {
            switch (lt) {
              case 1:
                return "resData = x[xIndex];";
              case 3:
                return `resData = vec3<${h}>(x[xIndex], x[xIndex + 1], x[xIndex + 2]);`;
              case 4:
                return "resData = x[xIndex / 4];";
              default:
                throw new Error(`innerElementSize ${lt} is not supported.`);
            }
          }, S = (lt) => {
            switch (lt) {
              case 1:
                return "return w[row * i32(uniforms.w_shape[3]) + colIn];";
              case 4:
                return "return w[row * i32(uniforms.w_shape[3]) / 4 + colIn];";
              default:
                throw new Error(`innerElementSize ${lt} is not supported.`);
            }
          }, u = e ? `
    let coord = vec4<i32>(batch, xRow, xCol, xCh);
    ` : `
    let coord = vec4<i32>(batch, xCh, xRow, xCol);
    `, B = e ? `
    let coords = vec4<i32>(
      batch,
      row / outWidth,
      row % outWidth,
      col);
    ` : `
    let coords = vec4<i32>(
      batch,
      row,
      col / outWidth,
      col % outWidth);
    `, R = e ? "i32(uniforms.x_shape[1])" : "i32(uniforms.x_shape[2])", N = e ? "i32(uniforms.x_shape[2])" : "i32(uniforms.x_shape[3])", Z = e ? "row" : "col", te = e ? "col" : "row", Q = `
    let inChannels = i32(uniforms.w_shape[2]);
    let outWidth = ${e ? "i32(uniforms.result_shape[2])" : "i32(uniforms.result_shape[3])"};
    let outRow = ${Z} / outWidth;
    let outCol = ${Z} % outWidth;

    let WRow = ${te} / (i32(uniforms.w_shape[1]) * inChannels);
    let WCol = ${te} / inChannels % i32(uniforms.w_shape[1]);
    let xRow = outRow * uniforms.stride[0] + uniforms.dilation[0] * WRow - uniforms.pad[0];
    let xCol = outCol * uniforms.stride[1] + uniforms.dilation[1] * WCol - uniforms.pad[1];
    let xCh = ${te} % inChannels;
    var resData = ${Wr(o, h)}(0.0);
    // The bounds checking is always needed since we use it to pad zero for
    // the 'same' padding type.
    if (xRow >= 0 && xRow < ${R} && xCol >= 0 && xCol < ${N}) {
      ${u}
      let xIndex = getIndexFromCoords4D(coord, vec4<i32>(uniforms.x_shape));
      ${k(o)}
    }
    return resData;`, _e = e ? t && n ? `
    let col = colIn * ${o};
    ${Q}` : `
    let col = colIn * ${o};
    if (row < uniforms.dim_a_outer && col < uniforms.dim_inner) {
      ${Q}
    }
    return ${Wr(o, h)}(0.0);` : n && r ? `
    let col = colIn * ${o};
    ${Q}` : `
    let col = colIn * ${o};
    if (row < uniforms.dim_inner && col < uniforms.dim_b_outer) {
      ${Q}
    }
    return ${Wr(o, h)}(0.0);`, me = `${S(d)}`, ye = Wr(p, h), Ae = Wr(e ? o : d, h), Ie = Wr(e ? d : o, h), Ge = sn(a, ye, h);
          return `
    fn mm_readA(batch: i32, row : i32, colIn : i32) -> ${Ae} {
      ${e ? _e : me}
    }

    fn mm_readB(batch: i32, row : i32, colIn : i32) -> ${Ie} {
      ${e ? me : _e}
    }

    fn mm_write(batch: i32, row : i32, colIn : i32, valueIn : ${ye}) {
      let col = colIn * ${p};
      if (row < uniforms.dim_a_outer && col < uniforms.dim_b_outer)
      {
      var value = valueIn;
      let outWidth = ${e ? "i32(uniforms.result_shape[2])" : "i32(uniforms.result_shape[3])"};
      ${B}
      ${Jl(i)}
      ${Ge}
      setOutputAtCoords(coords[0], coords[1], coords[2], coords[3], value);
      }
    }`;
        }, iu = (e, t, r, n, i, a, o, d, p) => {
          let h = t.format === "NHWC", k = h ? e[0].dims[3] : e[0].dims[1], S = r[0], u = h ? r[2] : r[3], B = h ? r[1] : r[2], R = h ? r[3] : r[1], N = h && (k % 4 === 0 || k % 3 === 0) && R % 4 === 0, Z = h ? R : u * B, te = h ? u * B : R, Q = [8, 8, 1], _e = n <= 8 ? [4, 1, 1] : [4, 4, 1], me = [Math.ceil(Z / Q[0] / _e[0]), Math.ceil(te / Q[1] / _e[1]), Math.ceil(S / Q[2] / _e[2])];
          or("verbose", () => `[conv2d_mm_webgpu] dispatch = ${me}`);
          let ye = N ? h && k % 4 !== 0 ? 3 : 4 : 1, Ae = Q[1] * _e[1], Ie = Q[0] * _e[0], Ge = Math.max(Q[0] * ye, Q[1]), lt = n % Ae === 0, Tt = i % Ie === 0, Kt = a % Ge === 0, Yt = N ? [ye, 4, 4] : [1, 1, 1], Ct = [{ type: 6, data: n }, { type: 6, data: i }, { type: 6, data: a }, { type: 6, data: [t.pads[0], t.pads[1]] }, { type: 6, data: t.strides }, { type: 6, data: t.dilations }];
          Is(t, Ct), Ct.push(...vt(e[0].dims, e[1].dims));
          let Jt = ["rank", "rank"];
          o && (Ct.push(...vt(e[2].dims)), Jt.push("rank")), Ct.push(...vt(r));
          let $t = (jt) => {
            let vr = [{ name: "dim_a_outer", type: "i32" }, { name: "dim_b_outer", type: "i32" }, { name: "dim_inner", type: "i32" }, { name: "pad", type: "i32", length: 2 }, { name: "stride", type: "i32", length: 2 }, { name: "dilation", type: "i32", length: 2 }];
            nn(t, vr);
            let Ht = N ? 4 : 1, Gt = er(e[0].dataType), Cr = `
      fn setOutputAtIndex(flatIndex : i32, value : ${N ? `vec4<${Gt}>` : Gt}) {
        result[flatIndex] = ${N ? `vec4<${Gt}>` : Gt}(value);
      }
      fn setOutputAtCoords(d0 : i32, d1 : i32, d2 : i32, d3 : i32, value : ${N ? `vec4<${Gt}>` : Gt}) {
        let flatIndex = getOutputIndexFromCoords(vec4<i32>(d0, d1, d2, d3));
        setOutputAtIndex(flatIndex ${N ? "/ 4" : ""}, value);
      }`, it = ze("x", e[0].dataType, e[0].dims.length, ye === 3 ? 1 : ye), Et = ze("w", e[1].dataType, e[1].dims.length, Ht), cr = [it, Et], Lr = wt("result", e[0].dataType, r.length, Ht);
            if (o) {
              let ys = ze("bias", e[2].dataType, e[2].dims.length, Ht);
              cr.push(ys), Cr += `
        fn getBiasByOutputCoords(coords : vec4<i32>) -> ${N ? `vec4<${Gt}>` : Gt} {
          return bias[coords.${h ? "w" : "y"}${N ? "/ 4" : ""}];
        }`;
            }
            return `
        ${Zl("uniforms.result_strides")}
        //struct Uniforms { xShape : vec4<i32>, wShape : vec4<i32>, outShape : vec4<i32>,
        //  outShapeStrides: vec3<i32>, filterDims : vec2<i32>, pad : vec2<i32>, stride : vec2<i32>,
        //  dilation : vec2<i32>, dimAOuter : i32, dimBOuter : i32, dimInner : i32 };
        ${jt.registerUniforms(vr).declareVariables(...cr, Lr)}
        ${Cr}
        ${nu(h, lt, Tt, Kt, o, t, Yt[0], Yt[1], Yt[2], Gt)}
        ${N ? oi(_e, Q, Gt, void 0, !h, Ge) : ai(_e, Q, Gt, void 0, !h, Ge, !1, void 0, d)}`;
          };
          return { name: "Conv2DMatMul", shaderCache: { hint: `${t.cacheKey};${ye};${N};${lt};${Tt};${Kt};${Ae};${Ie};${Ge}`, inputDependencies: Jt }, getRunData: () => ({ outputs: [{ dims: p ? p(r) : r, dataType: e[0].dataType }], dispatchGroup: { x: me[0], y: me[1], z: me[2] }, programUniforms: Ct }), getShaderSource: $t };
        };
      }), au, ko, xn, lu, So, $o, uu, du, Bc = w(() => {
        zt(), Yr(), Bt(), Qt(), Gs(), ni(), au = (e) => {
          let t = 1;
          for (let r = 0; r < e.length; r++) t *= e[r];
          return t;
        }, ko = (e) => typeof e == "number" ? [e, e, e] : e, xn = (e, t) => t <= 1 ? e : e + (e - 1) * (t - 1), lu = (e, t, r, n = 1) => {
          let i = xn(t, n);
          return Math.floor((e[0] * (r - 1) - r + i) / 2);
        }, So = (e, t, r, n, i) => {
          i == null && (i = lu(e, t[0], n[0]));
          let a = [0, 0, 0, r];
          for (let o = 0; o < 3; o++) e[o] + 2 * i >= t[o] && (a[o] = Math.trunc((e[o] - t[o] + 2 * i) / n[o] + 1));
          return a;
        }, $o = (e, t, r, n, i, a, o, d, p, h) => {
          let k, S, u, B;
          if (e === "VALID" && (e = 0), typeof e == "number") {
            k = { top: e, bottom: e, left: e, right: e, front: e, back: e };
            let R = So([t, r, n, 1], [d, p, h], 1, [i, a, o], e);
            S = R[0], u = R[1], B = R[2];
          } else if (Array.isArray(e)) {
            if (!e.every((N, Z, te) => N === te[0])) throw Error(`Unsupported padding parameter: ${e}`);
            k = { top: e[0], bottom: e[1], left: e[2], right: e[3], front: e[4], back: e[5] };
            let R = So([t, r, n, 1], [d, p, h], 1, [i, a, o], e[0]);
            S = R[0], u = R[1], B = R[2];
          } else if (e === "SAME_UPPER") {
            S = Math.ceil(t / i), u = Math.ceil(r / a), B = Math.ceil(n / o);
            let R = (S - 1) * i + d - t, N = (u - 1) * a + p - r, Z = (B - 1) * o + h - n, te = Math.floor(R / 2), Q = R - te, _e = Math.floor(N / 2), me = N - _e, ye = Math.floor(Z / 2), Ae = Z - ye;
            k = { top: _e, bottom: me, left: ye, right: Ae, front: te, back: Q };
          } else throw Error(`Unknown padding parameter: ${e}`);
          return { padInfo: k, outDepth: S, outHeight: u, outWidth: B };
        }, uu = (e, t, r, n, i, a = !1, o = "channelsLast") => {
          let d, p, h, k, S;
          if (o === "channelsLast") [d, p, h, k, S] = e;
          else if (o === "channelsFirst") [d, S, p, h, k] = e;
          else throw new Error(`Unknown dataFormat ${o}`);
          let [u, , B, R, N] = t, [Z, te, Q] = ko(r), [_e, me, ye] = ko(n), Ae = xn(B, _e), Ie = xn(R, me), Ge = xn(N, ye), { padInfo: lt, outDepth: Tt, outHeight: Kt, outWidth: Yt } = $o(i, p, h, k, Z, te, Q, Ae, Ie, Ge), Ct = a ? u * S : u, Jt = [0, 0, 0, 0, 0];
          return o === "channelsFirst" ? Jt = [d, Ct, Tt, Kt, Yt] : o === "channelsLast" && (Jt = [d, Tt, Kt, Yt, Ct]), { batchSize: d, dataFormat: o, inDepth: p, inHeight: h, inWidth: k, inChannels: S, outDepth: Tt, outHeight: Kt, outWidth: Yt, outChannels: Ct, padInfo: lt, strideDepth: Z, strideHeight: te, strideWidth: Q, filterDepth: B, filterHeight: R, filterWidth: N, effectiveFilterDepth: Ae, effectiveFilterHeight: Ie, effectiveFilterWidth: Ge, dilationDepth: _e, dilationHeight: me, dilationWidth: ye, inShape: e, outShape: Jt, filterShape: t };
        }, du = (e, t, r, n, i, a) => {
          let o = a === "channelsLast";
          o ? e[0].dims[3] : e[0].dims[1];
          let d = [64, 1, 1], p = { x: r.map((Z, te) => te) }, h = [Math.ceil(au(p.x.map((Z) => r[Z])) / d[0]), 1, 1];
          or("verbose", () => `[conv3d_naive_webgpu] dispatch = ${h}`);
          let k = 1, S = $e.size(r), u = [{ type: 12, data: S }, { type: 12, data: n }, { type: 12, data: i }, { type: 12, data: t.strides }, { type: 12, data: t.dilations }];
          Is(t, u), u.push(...vt(e[0].dims, e[1].dims));
          let B = ["rank", "rank"], R = e.length === 3;
          R && (u.push(...vt(e[2].dims)), B.push("rank")), u.push(...vt(r));
          let N = (Z) => {
            let te = [{ name: "output_size", type: "u32" }, { name: "filter_dims", type: "u32", length: n.length }, { name: "pads", type: "u32", length: i.length }, { name: "strides", type: "u32", length: t.strides.length }, { name: "dilations", type: "u32", length: t.dilations.length }];
            nn(t, te);
            let Q = 1, _e = er(e[0].dataType), me = ze("x", e[0].dataType, e[0].dims.length, k), ye = ze("W", e[1].dataType, e[1].dims.length, Q), Ae = [me, ye], Ie = wt("result", e[0].dataType, r.length, Q), Ge = "";
            if (R) {
              let Kt = ze("bias", e[2].dataType, e[2].dims.length, Q);
              Ae.push(Kt), Ge += `
        fn getBiasByOutputCoords(coords : array<u32, 5>) -> ${_e} {
          return bias[${o ? Mt("coords", 4, 5) : Mt("coords", 1, 5)}];
        }`;
            }
            let lt = Wr(k, _e), Tt = sn(t, lt, _e);
            return `
            ${Ge}
            fn getX(d0 : u32, d1 : u32, d2 : u32, d3 : u32, d4 : u32) -> f32 {
              let aIndices = array<u32, 5>(d0, d1, d2, d3, d4);
              return ${me.getByIndices("aIndices")};
            }
            fn getW(d0 : u32, d1 : u32, d2 : u32, d3 : u32, d4 : u32) -> f32 {
              let aIndices = array<u32, 5>(d0, d1, d2, d3, d4);
              return ${ye.getByIndices("aIndices")};
            }
          ${Z.registerUniforms(te).declareVariables(...Ae, Ie)}
          ${Z.mainStart()}
          ${Z.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
              let coords = ${Ie.offsetToIndices("global_idx")};
              let batch = ${Mt("coords", 0, me.rank)};
              let d2 = ${o ? Mt("coords", me.rank - 1, me.rank) : Mt("coords", 1, me.rank)};
              let xFRCCorner = vec3<u32>(${o ? Mt("coords", 1, me.rank) : Mt("coords", 2, me.rank)},
              ${o ? Mt("coords", 2, me.rank) : Mt("coords", 3, me.rank)},
              ${o ? Mt("coords", 3, me.rank) : Mt("coords", 4, me.rank)}) * uniforms.strides - uniforms.pads;
              let xFCorner = xFRCCorner.x;
              let xRCorner = xFRCCorner.y;
              let xCCorner = xFRCCorner.z;
              let xShapeY = ${o ? Mt("uniforms.x_shape", 1, me.rank) : Mt("uniforms.x_shape", 2, me.rank)};
              let xShapeZ = ${o ? Mt("uniforms.x_shape", 2, me.rank) : Mt("uniforms.x_shape", 3, me.rank)};
              let xShapeW = ${o ? Mt("uniforms.x_shape", 3, me.rank) : Mt("uniforms.x_shape", 4, me.rank)};
              let xShapeU = ${o ? Mt("uniforms.x_shape", 4, me.rank) : Mt("uniforms.x_shape", 1, me.rank)};
              let inputDepthNearestVec4 = (xShapeU / 4) * 4;
              let inputDepthVec4Remainder = xShapeU % 4;

              var value = 0.0;
              for (var wF = 0u; wF < uniforms.filter_dims[0]; wF++) {
                let xF = xFCorner + wF * uniforms.dilations[0];
                if (xF < 0 || xF >= xShapeY) {
                  continue;
                }

                for (var wR = 0u; wR < uniforms.filter_dims[1]; wR++) {
                  let xR = xRCorner + wR * uniforms.dilations[1];
                  if (xR < 0 || xR >= xShapeZ) {
                    continue;
                  }

                  for (var wC = 0u; wC < uniforms.filter_dims[2]; wC++) {
                    let xC = xCCorner + wC * uniforms.dilations[2];
                    if (xC < 0 || xC >= xShapeW) {
                      continue;
                    }

                    for (var d1 = 0u; d1 < inputDepthNearestVec4; d1 += 4) {
                      ${o ? `let xValues = vec4<f32>(
                               getX(batch, xF, xR, xC, d1),
                               getX(batch, xF, xR, xC, d1 + 1),
                               getX(batch, xF, xR, xC, d1 + 2),
                               getX(batch, xF, xR, xC, d1 + 3));
                            ` : `let xValues = vec4<f32>(
                               getX(batch, d1, xF, xR, xC),
                               getX(batch, d1 + 1, xF, xR, xC),
                               getX(batch, d1 + 2, xF, xR, xC),
                               getX(batch, d1 + 3, xF, xR, xC));
                            `}
                            let wValues = vec4<f32>(
                              getW(d2, d1, wF, wR, wC),
                              getW(d2, d1 + 1, wF, wR, wC),
                              getW(d2, d1 + 2, wF, wR, wC),
                              getW(d2, d1 + 3, wF, wR, wC));
                      value += dot(xValues, wValues);
                    }
                    if (inputDepthVec4Remainder == 1) {
                        ${o ? `value += getX(batch, xF, xR, xC, inputDepthNearestVec4)
                          * getW(d2, inputDepthNearestVec4, wF, wR, wC);` : `value += getX(batch, inputDepthNearestVec4, xF, xR, xC)
                          * getW(d2, inputDepthNearestVec4, wF, wR, wC);`}
                    } else if (inputDepthVec4Remainder == 2) {
                      ${o ? `let xValues = vec2<f32>(
                        getX(batch, xF, xR, xC, inputDepthNearestVec4),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 1));
                      ` : `let xValues = vec2<f32>(
                        getX(batch, inputDepthNearestVec4, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 1, xF, xR, xC));
                    `}
                    let wValues = vec2<f32>(
                      getW(d2, inputDepthNearestVec4, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 1, wF, wR, wC));
                      value += dot(xValues, wValues);
                    } else if (inputDepthVec4Remainder == 3) {
                      ${o ? `let xValues = vec3<f32>(
                        getX(batch, xF, xR, xC, inputDepthNearestVec4),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 1),
                        getX(batch, xF, xR, xC, inputDepthNearestVec4 + 2));
                      ` : `let xValues = vec3<f32>(
                        getX(batch, inputDepthNearestVec4, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 1, xF, xR, xC),
                        getX(batch, inputDepthNearestVec4 + 2, xF, xR, xC));
                    `}
                    let wValues = vec3<f32>(
                      getW(d2, inputDepthNearestVec4, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 1, wF, wR, wC),
                      getW(d2, inputDepthNearestVec4 + 2, wF, wR, wC));
                      value += dot(xValues, wValues);
                    }
                  }
                }
              }
              ${R ? "value = value + getBiasByOutputCoords(coords)" : ""};
              ${Tt}
              result[global_idx] = f32(value);
          }`;
          };
          return { name: "Conv3DNaive", shaderCache: { hint: `${t.cacheKey};${o};${k};${R}`, inputDependencies: B }, getRunData: () => ({ outputs: [{ dims: r, dataType: e[0].dataType }], dispatchGroup: { x: h[0], y: h[1], z: h[2] }, programUniforms: u }), getShaderSource: N };
        };
      }), cu, di, Rc = w(() => {
        zt(), Bt(), Qt(), Gs(), cu = (e, t, r, n) => {
          let i = e.length > 2, a = i ? "value += b[output_channel];" : "", o = e[0].dims, d = e[1].dims, p = t.format === "NHWC", h = p ? r[3] : r[1], k = h / t.group, S = p && k >= 4 ? yr(h) : 1, u = $e.size(r) / S, B = [{ type: 12, data: u }, { type: 12, data: t.dilations }, { type: 12, data: [t.strides[0], t.strides[1]] }, { type: 12, data: [t.pads[0], t.pads[1]] }, { type: 12, data: k }];
          Is(t, B), B.push(...vt(o, [d[0], d[1], d[2], d[3] / S]));
          let R = i ? ["rank", "rank", "rank"] : ["rank", "rank"];
          B.push(...vt([r[0], r[1], r[2], r[3] / S]));
          let N = (Z) => {
            let te = wt("output", e[0].dataType, r.length, S), Q = er(te.type.tensor), _e = sn(t, te.type.value, Q), me = ze("x", e[0].dataType, o.length), ye = ze("w", e[1].dataType, d.length, S), Ae = [me, ye];
            i && Ae.push(ze("b", e[2].dataType, e[2].dims, S));
            let Ie = [{ name: "output_size", type: "u32" }, { name: "dilations", type: "u32", length: t.dilations.length }, { name: "strides", type: "u32", length: 2 }, { name: "pads", type: "u32", length: 2 }, { name: "output_channels_per_group", type: "u32" }];
            nn(t, Ie);
            let Ge = p ? `
      for (var wHeight: u32 = 0u; wHeight < uniforms.w_shape[0]; wHeight++) {
        let xHeight = xRCCorner.x + wHeight * uniforms.dilations[0];

        if (xHeight < 0u || xHeight >= uniforms.x_shape[1]) {
          continue;
        }

        for (var wWidth: u32 = 0u; wWidth < uniforms.w_shape[1]; wWidth++) {
          let xWidth = xRCCorner.y + wWidth * uniforms.dilations[1];
          if (xWidth < 0u || xWidth >= uniforms.x_shape[2]) {
            continue;
          }

          for (var wInChannel: u32 = 0u; wInChannel < uniforms.w_shape[2]; wInChannel++) {
            let input_channel = in_channel_offset + wInChannel;
            let xVal = ${me.get("batch", "xHeight", "xWidth", "input_channel")};
            let wVal = ${ye.get("wHeight", "wWidth", "wInChannel", "output_channel")};
            value += xVal * wVal;
          }
        }
      }
      ` : `
      for (var wInChannel: u32 = 0u; wInChannel < uniforms.w_shape[1]; wInChannel++) {
        let input_channel = in_channel_offset + wInChannel;
        for (var wHeight: u32 = 0u; wHeight < uniforms.w_shape[2]; wHeight++) {
          let xHeight = xRCCorner.x + wHeight * uniforms.dilations[0];

          if (xHeight < 0u || xHeight >= uniforms.x_shape[2]) {
            continue;
          }

          for (var wWidth: u32 = 0u; wWidth < uniforms.w_shape[3]; wWidth++) {
            let xWidth = xRCCorner.y + wWidth * uniforms.dilations[1];
            if (xWidth < 0u || xWidth >= uniforms.x_shape[3]) {
              continue;
            }

            let xVal = ${me.get("batch", "input_channel", "xHeight", "xWidth")};
            let wVal = ${ye.get("output_channel", "wInChannel", "wHeight", "wWidth")};
            value += xVal * wVal;
          }
        }
      }
      `;
            return `
  ${Z.registerUniforms(Ie).declareVariables(...Ae, te)}

  ${Z.mainStart()}
    ${Z.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let outputIndices = ${te.offsetToIndices("global_idx")};
    let batch: u32 = outputIndices[0];
    let output_channel: u32 = outputIndices[${p ? 3 : 1}];
    let xRCCorner: vec2<u32> = vec2<u32>(outputIndices[${p ? 1 : 2}], outputIndices[${p ? 2 : 3}]) * uniforms.strides - uniforms.pads;
    let group_id: u32 = output_channel * ${S} / uniforms.output_channels_per_group;
    var in_channel_offset = group_id * uniforms.w_shape[${p ? 2 : 1}];

    var value: ${te.type.value} = ${te.type.value}(0);
    ${Ge}
    ${a}
    ${_e}
    ${te.setByOffset("global_idx", "value")}
  }`;
          };
          return { name: "GroupedConv", shaderCache: { hint: `${t.cacheKey}_${S}`, inputDependencies: R }, getRunData: () => ({ outputs: [{ dims: n ? n(r) : r, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(u / 64) }, programUniforms: B }), getShaderSource: N };
        }, di = (e, t, r, n) => {
          let i = e.length > 2, a = yr(r[3]), o = yr(r[2]), d = $e.size(r) / a / o, p = [e[0].dims[0], e[0].dims[1], e[0].dims[2], e[0].dims[3] / a], h = [e[1].dims[0], e[1].dims[1], e[1].dims[2], e[1].dims[3] / a], k = [r[0], r[1], r[2], r[3] / a], S = [{ type: 12, data: d }, { type: 6, data: [t.strides[0], t.strides[1]] }, { type: 6, data: [t.pads[0], t.pads[1]] }];
          Is(t, S), S.push(...vt(p, h, k));
          let u = (o - 1) * t.strides[1] + h[1], B = (R) => {
            let N = wt("output", e[0].dataType, k.length, a), Z = er(N.type.tensor), te = sn(t, N.type.value, Z), Q = ze("x", e[0].dataType, p.length, a), _e = ze("w", e[1].dataType, h.length, a), me = [Q, _e];
            i && me.push(ze("b", e[2].dataType, e[2].dims, a));
            let ye = i ? "value += b[output_channel];" : "", Ae = [{ name: "output_size", type: "u32" }, { name: "strides", type: "i32", length: 2 }, { name: "pads", type: "i32", length: 2 }];
            return nn(t, Ae), `
  ${R.registerUniforms(Ae).declareVariables(...me, N)}
  ${R.mainStart()}
    ${R.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let width0 = uniforms.output_shape[3];
    let output_channel = global_idx % width0;
    var index1 = global_idx / width0;
    let width1 = uniforms.output_shape[2] / ${o}u;
    let col = (index1 % width1) * ${o}u;
    index1 = index1 / width1;
    let row = index1 % uniforms.output_shape[1];
    let batch = index1 / uniforms.output_shape[1];

    let x_corner = vec2<i32>(i32(row), i32(col)) * uniforms.strides - uniforms.pads;

    var x_vals: array<${Q.type.value}, ${u}>;
    var values: array<${N.type.value}, ${o}>;
    let input_channel = output_channel;
    // Use constant instead of uniform can give better performance for w's height/width.
    for (var w_height: u32 = 0u; w_height < ${h[0]}; w_height++) {
      let x_height = x_corner.x + i32(w_height);
      if (x_height >= 0 && u32(x_height) < uniforms.x_shape[1]) {
        for (var i = 0; i < ${u}; i++) {
          let x_width = x_corner.y + i;
          if (x_width >= 0 && u32(x_width) < uniforms.x_shape[2]) {
            x_vals[i] = ${Q.get("batch", "u32(x_height)", "u32(x_width)", "input_channel")};
          } else {
            x_vals[i] = ${Q.type.value}(0);
          }
        }
        for (var w_width: u32 = 0u; w_width < ${h[1]}; w_width++) {
          let w_val = ${_e.get("w_height", "w_width", "0", "output_channel")};
          for (var i = 0u; i < ${o}u; i++) {
            values[i] = fma(x_vals[i * u32(uniforms.strides[1]) + w_width], w_val, values[i]);
          }
        }
      }
    }

    for (var i = 0u; i < ${o}u; i++) {
      var value = values[i];
      ${ye}
      ${te}
      ${N.set("batch", "row", "col + i", "output_channel", "value")};
    }
  }`;
          };
          return { name: "GroupedConv-Vectorize", shaderCache: { hint: `${t.cacheKey};${a};${o};${u};${h[0]};${h[1]}`, inputDependencies: i ? ["rank", "rank", "type"] : ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: n ? n(r) : r, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(d / 64) }, programUniforms: S }), getShaderSource: B };
        };
      }), pu, ci, hu, pi, hi, Ao, mu, Io, Fo, Nc = w(() => {
        Bt(), ou(), Bc(), ui(), Rc(), Gs(), Po(), Vs(), pu = (e, t, r, n, i, a) => {
          let o = e[0], d = e.slice(a ? 1 : 2, a ? 3 : 4), p = d.length, h = t[0], k = t.slice(2).map((u, B) => u + (u - 1) * (r[B] - 1)), S = d.map((u, B) => u + n[B] + n[B + p]).map((u, B) => Math.floor((u - k[B] + i[B]) / i[B]));
          return S.splice(0, 0, o), S.splice(a ? 3 : 1, 0, h), S;
        }, ci = [2, 3, 1, 0], hu = (e, t) => {
          if (!e || e.length !== 2 && e.length !== 3) throw new Error("Conv requires 2 or 3 inputs");
          if (e[0].dims.length > 5) throw new Error("greater than 5D is not supported");
          if (e[0].dims.length !== e[1].dims.length) throw new Error("filter does not have same dimension as input");
          let r = e[0].dims[t.format === "NHWC" ? e[0].dims.length - 1 : 1], n = e[1].dims[1] * t.group;
          if (r !== n) throw new Error("FILTER_IN_CHANNEL should be equal to DATA_CHANNEL");
          if (e.length === 3 && (e[2].dims.length !== 1 || e[1].dims[0] !== e[2].dims[0])) throw new Error("invalid bias");
          let i = e[0].dims.length - 2;
          if (t.dilations.length !== i) throw new Error(`dilations should be ${i}D`);
          if (t.strides.length !== i) throw new Error(`strides should be ${i}D`);
          if (t.pads.length !== i * 2) throw new Error(`pads should be ${i * 2}D`);
          if (t.kernelShape.length !== 0 && t.kernelShape.length !== e[1].dims.length - 2) throw new Error("invalid kernel shape");
        }, pi = (e, t) => {
          let r = e.kernelShape.slice();
          r.length < t[1].dims.length - 2 && r.push(...Array(t[1].dims.length - 2 - r.length).fill(0));
          for (let a = 2; a < t[1].dims.length; ++a) r[a - 2] === 0 && (r[a - 2] = t[1].dims[a]);
          let n = e.pads.slice();
          wr.adjustPadsBasedOnAutoPad(t[0].dims, e.strides, e.dilations, r, n, e.format === "NHWC", e.autoPad);
          let i = Object.assign({}, e);
          return Object.assign(i, { kernelShape: r, pads: n }), i;
        }, hi = (e) => {
          let t = Eo(e), r = e.format, n = ["NOTSET", "VALID", "SAME_UPPER", "SAME_LOWER"][e.auto_pad], i = e.dilations, a = e.group, o = e.kernel_shape, d = e.pads, p = e.strides, h = e.w_is_const();
          return { autoPad: n, format: r, dilations: i, group: a, kernelShape: o, pads: d, strides: p, wIsConst: h, ...t, cacheKey: `${e.format};${t.activation};` };
        }, Ao = (e, t, r, n) => {
          let i = r.format === "NHWC", a = pu(t[0].dims, t[1].dims, r.dilations, r.pads, r.strides, i);
          if (r.group !== 1) {
            let Ae = [t[0]];
            if (i) {
              let Ie = e.kernelCustomData.wT ?? e.compute(ss(t[1], ci), { inputs: [1], outputs: [r.wIsConst ? -2 : -1] })[0];
              r.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = Ie), Ae.push(Ie);
            } else Ae.push(t[1]);
            t.length === 3 && Ae.push(t[2]), !e.adapterInfo.isArchitecture("ampere") && i && t[1].dims[0] === r.group && t[1].dims[1] === 1 && r.dilations[0] === 1 && r.dilations[1] === 1 ? e.compute(di(Ae, r, a, n), { inputs: Ae }) : e.compute(cu(Ae, r, a, n), { inputs: Ae });
            return;
          }
          let o = t.length === 3, d = t[0].dims[i ? 1 : 2], p = t[0].dims[i ? 2 : 3], h = t[0].dims[i ? 3 : 1], k = t[1].dims[2], S = t[1].dims[3], u = a[i ? 1 : 2], B = a[i ? 2 : 3], R = a[i ? 3 : 1], N = i && k === d && S === p && r.pads[0] === 0 && r.pads[1] === 0;
          if (N || k === 1 && S === 1 && r.dilations[0] === 1 && r.dilations[1] === 1 && r.strides[0] === 1 && r.strides[1] === 1 && r.pads[0] === 0 && r.pads[1] === 0) {
            let Ae = a[0], Ie, Ge, lt, Tt = [];
            if (i) {
              let Ct = e.kernelCustomData.wT ?? e.compute(ss(t[1], ci), { inputs: [1], outputs: [r.wIsConst ? -2 : -1] })[0];
              if (r.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = Ct), N) {
                let Jt = d * p * h;
                Ie = t[0].reshape([1, Ae, Jt]), Ge = Ct.reshape([1, Jt, R]), lt = [1, Ae, R];
              } else Ie = t[0].reshape([Ae, d * p, h]), Ge = Ct.reshape([1, h, R]), lt = [Ae, u * B, R];
              Tt.push(Ie), Tt.push(Ge);
            } else Ie = t[0].reshape([Ae, h, d * p]), Ge = t[1].reshape([1, R, h]), lt = [Ae, R, u * B], Tt.push(Ge), Tt.push(Ie);
            o && Tt.push(t[2]);
            let Kt = lt[2], Yt = Tt[0].dims[Tt[0].dims.length - 1];
            Kt < 8 && Yt < 8 ? e.compute(ii(Tt, r, a, lt, i, n), { inputs: Tt }) : e.compute(li(Tt, r, a, lt, i, n), { inputs: Tt });
            return;
          }
          let Z = !0, te = e.kernelCustomData.wT ?? e.compute(ss(t[1], ci), { inputs: [1], outputs: [r.wIsConst ? -2 : -1] })[0];
          r.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = te);
          let Q = [t[0], te];
          o && Q.push(t[2]);
          let _e = i ? u * B : R, me = i ? R : u * B, ye = k * S * h;
          e.compute(iu(Q, r, a, _e, me, ye, o, Z, n), { inputs: Q });
        }, mu = (e, t) => {
          let r = t.format === "NHWC", n = [e.inputs[0].reshape(r ? [e.inputs[0].dims[0], 1, e.inputs[0].dims[1], e.inputs[0].dims[2]] : [e.inputs[0].dims[0], e.inputs[0].dims[1], 1, e.inputs[0].dims[2]]), e.inputs[1].reshape([e.inputs[1].dims[0], e.inputs[1].dims[1], 1, e.inputs[1].dims[2]])];
          e.inputs.length === 3 && n.push(e.inputs[2]);
          let i = [0, t.pads[0], 0, t.pads[1]], a = [1].concat(t.strides), o = [1].concat(t.dilations), d = [1].concat(t.kernelShape), p = pi({ ...t, pads: i, strides: a, dilations: o, kernelShape: d }, n);
          Ao(e, n, p, (h) => r ? [h[0], h[2], h[3]] : [h[0], h[1], h[3]]);
        }, Io = (e, t, r) => {
          let n = r.format === "NHWC" ? "channelsLast" : "channelsFirst", i = pi(r, t), a = r.autoPad === "NOTSET" ? r.pads : r.autoPad, o = uu(t[0].dims, t[1].dims, r.strides, r.dilations, a, !1, n);
          e.compute(du(t, i, o.outShape, [o.filterDepth, o.filterHeight, o.filterWidth], [o.padInfo.front, o.padInfo.top, o.padInfo.left], n));
        }, Fo = (e, t) => {
          if (hu(e.inputs, t), e.inputs[0].dims.length === 3) mu(e, t);
          else if (e.inputs[0].dims.length === 5) Io(e, e.inputs, t);
          else {
            let r = pi(t, e.inputs);
            Ao(e, e.inputs, r);
          }
        };
      }), _u, jc = w(() => {
        zt(), Yr(), Bt(), Qt(), _u = (e, t, r) => {
          let n = e.length > 2, i = t.outputShape, a = t.format === "NHWC", o = t.group, d = e[1].dims, p = d[2] / o, h = d[3], k = a ? yr(h) : 1, S = $e.size(i) / k, u = [Math.ceil(S / 64), 1, 1];
          or("verbose", () => `[conv2d_backprop_webgpu] dispatch = ${u}`);
          let B = ["rank", "rank"], R = [t.strides[0], t.strides[1]], N = [t.kernelShape[a ? 1 : 2], t.kernelShape[a ? 2 : 3]], Z = [t.dilations[0], t.dilations[1]], te = [N[0] + (t.dilations[0] <= 1 ? 0 : (t.kernelShape[a ? 1 : 2] - 1) * (t.dilations[0] - 1)), N[1] + (t.dilations[1] <= 1 ? 0 : (t.kernelShape[a ? 2 : 3] - 1) * (t.dilations[1] - 1))], Q = [te[0] - 1 - Math.floor((t.pads[0] + t.pads[2]) / 2), te[1] - 1 - Math.floor((t.pads[1] + t.pads[3]) / 2)], _e = [{ type: 12, data: S }, { type: 12, data: R }, { type: 12, data: N }, { type: 12, data: Z }, { type: 12, data: te }, { type: 6, data: Q }, { type: 12, data: p }, { type: 12, data: h }, ...vt(e[0].dims, e[1].dims)];
          n && (_e.push(...vt(e[2].dims)), B.push("rank")), _e.push(...vt(i));
          let me = (ye) => {
            let Ae = [{ name: "output_size", type: "u32" }, { name: "strides", type: "u32", length: R.length }, { name: "filter_dims", type: "u32", length: N.length }, { name: "dilations", type: "u32", length: N.length }, { name: "effective_filter_dims", type: "u32", length: te.length }, { name: "pads", type: "i32", length: Q.length }, { name: "input_channels_per_group", type: "u32" }, { name: "output_channels_per_group", type: "u32" }], Ie = er(e[0].dataType), Ge = a ? 1 : 2, lt = a ? 2 : 3, Tt = a ? 3 : 1, Kt = ze("W", e[1].dataType, e[1].dims.length, k), Yt = ze("Dy", e[0].dataType, e[0].dims.length), Ct = [Yt, Kt];
            n && Ct.push(ze("bias", e[2].dataType, [i[Tt]].length, k));
            let Jt = wt("result", e[0].dataType, i.length, k), $t = `
            let outputIndices = ${Jt.offsetToIndices(`global_idx * ${k}`)};
            let batch = ${Jt.indicesGet("outputIndices", 0)};
            let d1 = ${Jt.indicesGet("outputIndices", Tt)};
            let r = ${Jt.indicesGet("outputIndices", Ge)};
            let c = ${Jt.indicesGet("outputIndices", lt)};
            let dyCorner = vec2<i32>(i32(r), i32(c)) - uniforms.pads;
            let dyRCorner = dyCorner.x;
            let dyCCorner = dyCorner.y;
            let groupId = d1 / uniforms.output_channels_per_group;
            let wOutChannel = d1 - groupId * uniforms.output_channels_per_group;
            // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).
            // ? = to be determined. : = across all values in that axis.
            var dotProd = ${Jt.type.value}(0.0);
            for (var wR: u32 = 0; wR < uniforms.effective_filter_dims.x; wR = wR + 1) {
              if (wR % uniforms.dilations.x != 0) {
                continue;
              }
              let dyR = (${Ie}(dyRCorner) + ${Ie}(wR)) / ${Ie}(uniforms.strides[0]);
              let wRPerm = uniforms.filter_dims.x - 1 - wR / uniforms.dilations.x;
              if (dyR < 0.0 || dyR >= ${Ie}(uniforms.Dy_shape[${Ge}]) || fract(dyR) > 0.0 ||
                  wRPerm < 0) {
                continue;
              }
              let idyR: u32 = u32(dyR);

              for (var wC: u32 = 0; wC < uniforms.effective_filter_dims.y; wC = wC + 1) {
                if (wC % uniforms.dilations.y != 0) {
                  continue;
                }
                let dyC = (${Ie}(dyCCorner) + ${Ie}(wC)) / ${Ie}(uniforms.strides.y);
                let wCPerm = uniforms.filter_dims.y - 1 - wC / uniforms.dilations.y;
                if (dyC < 0.0 || dyC >= ${Ie}(uniforms.Dy_shape[${lt}]) ||
                    fract(dyC) > 0.0 || wCPerm < 0) {
                  continue;
                }
                let idyC: u32 = u32(dyC);
                var inputChannel = groupId * uniforms.input_channels_per_group;
                for (var d2: u32 = 0; d2 < uniforms.input_channels_per_group; d2 = d2 + 1) {
                  let xValue = ${a ? Yt.get("batch", "idyR", "idyC", "inputChannel") : Yt.get("batch", "inputChannel", "idyR", "idyC")};
                  let w_offset = ${Kt.indicesToOffset(`${Kt.type.indices}(u32(wRPerm), u32(wCPerm), inputChannel, wOutChannel)`)};
                  let wValue = ${Kt.getByOffset(`w_offset / ${k}`)};
                  dotProd = dotProd + xValue * wValue;
                  inputChannel = inputChannel + 1;
                }
              }
            }
            let value = dotProd${n ? ` + bias[d1 / ${k}]` : ""};
            ${Jt.setByOffset("global_idx", "value")};
          `;
            return `
    ${ye.registerUniforms(Ae).declareVariables(...Ct, Jt)}
      ${ye.mainStart()}
      ${ye.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")};
    ${$t}}`;
          };
          return { name: "ConvTranspose2D", shaderCache: { hint: `${t.cacheKey};${k}`, inputDependencies: B }, getRunData: () => ({ dispatchGroup: { x: u[0], y: u[1], z: u[2] }, outputs: [{ dims: r ? r(i) : i, dataType: e[0].dataType }], programUniforms: _e }), getShaderSource: me };
        };
      }), Oo, fu, gu, mi, wu, yu, _i, Mu, bu, vu = w(() => {
        jc(), Gs(), Vs(), Oo = (e, t, r, n, i, a) => (e - 1) * t + r + (n - 1) * i + 1 - a, fu = (e, t, r, n, i) => {
          let a = Math.floor(e / 2);
          t === "SAME_UPPER" ? (r[n] = a, r[i] = e - a) : t === "SAME_LOWER" && (r[n] = e - a, r[i] = a);
        }, gu = (e, t, r, n, i, a, o, d, p, h) => {
          let k = e.length - 2, S = h.length === 0;
          p.length < k && p.push(...Array(k - p.length).fill(0));
          let u = e[0], B = t[d ? 3 : 1] * i;
          for (let R = 0, N = e.length - k - (d ? 1 : 0); R < k; ++R, ++N) {
            let Z = e[N], te = S ? Z * o[R] : h[R], Q = Oo(Z, o[R], a[R], t[N], r[R], te);
            fu(Q, n, a, R, R + k), S && h.push(o[R] * (Z - 1) + p[R] + (t[N] - 1) * r[R] + 1 - a[R] - a[R + k]);
          }
          h.splice(0, 0, u), h.splice(d ? 3 : 1, 0, B);
        }, mi = (e, t) => {
          let r = e.kernelShape.slice();
          if (e.kernelShape.length === 0 || e.kernelShape.reduce((S, u) => S * u, 1) === 0) {
            r.length = 0;
            for (let S = 2; S < t[1].dims.length; ++S) r.push(t[1].dims[S]);
          }
          let n = e.format === "NHWC";
          r.splice(0, 0, t[1].dims[0]), r.splice(n ? 3 : 1, 0, t[1].dims[1]);
          let i = e.pads.slice(), a = e.outputShape.slice(), o = e.outputPadding.slice(), d = t[0].dims, p = e.dilations.slice();
          if (p.reduce((S, u) => S + u, 0) === 0) {
            let S = t[0].dims.length - 2;
            p = new Array(S).fill(1);
          }
          let h = e.strides.slice();
          if (h.reduce((S, u) => S + u, 0) === 0) {
            let S = t[0].dims.length - 2;
            h = new Array(S).fill(1);
          }
          gu(d, r, p, e.autoPad, e.group, i, h, n, o, a);
          let k = Object.assign({}, e);
          return Object.assign(k, { kernelShape: r, pads: i, outputPadding: o, outputShape: a, dilations: p, strides: h }), k;
        }, wu = (e) => {
          let t = Eo(e), r = e.format, n = ["NOTSET", "VALID", "SAME_UPPER", "SAME_LOWER"][typeof e.autoPad > "u" ? 0 : e.autoPad], i = e.dilations, a = e.group, o = e.kernelShape, d = e.pads, p = e.strides, h = e.wIsConst(), k = e.outputPadding, S = e.outputShape;
          return { autoPad: n, format: r, dilations: i, group: a, kernelShape: o, outputPadding: k, outputShape: S, pads: d, strides: p, wIsConst: h, ...t, cacheKey: `${e.format};${t.activation};` };
        }, yu = (e, t) => {
          if (!e || e.length !== 2 && e.length !== 3) throw new Error("Conv requires 2 or 3 inputs");
          if (e[0].dims.length !== 4 && e[0].dims.length !== 3) throw new Error("currently only support 2-dimensional conv");
          if (e[0].dims.length !== e[1].dims.length) throw new Error("filter does not have same dimension as input");
          let r = e[0].dims[t.format === "NHWC" ? e[0].dims.length - 1 : 1], n = e[1].dims[0];
          if (r !== n) throw new Error("FILTER_IN_CHANNEL should be equal to DATA_CHANNEL");
          let i = e[1].dims[1] * t.group;
          if (e.length === 3 && (e[2].dims.length !== 1 || e[2].dims[0] !== i)) throw new Error("invalid bias");
          let a = e[0].dims.length - 2;
          if (t.dilations.reduce((o, d) => o + d, 0) > 0 && t.dilations.length !== a) throw new Error(`dilations should be ${a}D`);
          if (t.strides.reduce((o, d) => o + d, 0) > 0 && t.strides.length !== a) throw new Error(`strides should be ${a}D`);
          if (t.pads.reduce((o, d) => o + d, 0) > 0 && t.pads.length !== a * 2) throw new Error(`pads should be ${a * 2}D`);
          if (t.outputPadding.length !== a && t.outputPadding.length !== 0) throw new Error(`output_padding should be ${a}D`);
          if (t.kernelShape.reduce((o, d) => o + d, 0) > 0 && t.kernelShape.length !== 0 && t.kernelShape.length !== e[1].dims.length - 2) throw new Error("invalid kernel shape");
          if (t.outputShape.length !== 0 && t.outputShape.length !== e[0].dims.length - 2) throw new Error("invalid output shape");
        }, _i = (e, t, r, n) => {
          let i = e.kernelCustomData.wT ?? e.compute(ss(t[1], [2, 3, 0, 1]), { inputs: [1], outputs: [r.wIsConst ? -2 : -1] })[0];
          r.wIsConst && !e.kernelCustomData.wT && (e.kernelCustomData.wT = i);
          let a = [t[0], i];
          t.length === 3 && a.push(t[2]), e.compute(_u(a, r, n), { inputs: a });
        }, Mu = (e, t) => {
          let r = t.format === "NHWC", n = [e.inputs[0].reshape(r ? [e.inputs[0].dims[0], 1, e.inputs[0].dims[1], e.inputs[0].dims[2]] : [e.inputs[0].dims[0], e.inputs[0].dims[1], 1, e.inputs[0].dims[2]]), e.inputs[1].reshape([e.inputs[1].dims[0], e.inputs[1].dims[1], 1, e.inputs[1].dims[2]])];
          e.inputs.length === 3 && n.push(e.inputs[2]);
          let i = t.kernelShape;
          (i.length === 0 || i[0] === 0) && (i = [e.inputs[1].dims[2]]);
          let a = t.dilations;
          (a.length === 0 || a[0] === 0) && (a = [1]);
          let o = t.strides;
          (o.length === 0 || o[0] === 0) && (o = [1]);
          let d = t.pads;
          d.length === 0 && (d = [0, 0]), d = [0, d[0], 0, d[1]], o = [1].concat(o), a = [1].concat(a), i = [1].concat(i);
          let p = mi({ ...t, pads: d, strides: o, dilations: a, kernelShape: i }, n);
          _i(e, n, p, (h) => r ? [h[0], h[2], h[3]] : [h[0], h[1], h[3]]);
        }, bu = (e, t) => {
          if (yu(e.inputs, t), e.inputs[0].dims.length === 3) Mu(e, t);
          else {
            let r = mi(t, e.inputs);
            _i(e, e.inputs, r);
          }
        };
      }), xu, Tu, Eu, Uc = w(() => {
        zt(), Bt(), Pt(), Qt(), xu = (e, t, r, n) => {
          let i = $e.size(t), a = t.length, o = ze("input", e, a), d = wt("output", e, a), p = r.dataType === 6 ? r.getInt32Array()[0] : Number(r.getBigInt64Array()[0]), h = $e.normalizeAxis(p, a), k = (S) => {
            let u = ` i32(${o.indicesGet("inputIndices", "uniforms.axis")}) `, B = Mt("uniforms.input_shape", "uniforms.axis", a), R = n.reverse ? u + (n.exclusive ? " + 1" : "") : "0", N = n.reverse ? B : u + (n.exclusive ? "" : " + 1");
            return `
                ${S.registerUniform("outputSize", "u32").registerUniform("axis", "u32").declareVariables(o, d)}
                ${S.mainStart()}
                  ${S.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
                  var inputIndices = ${d.offsetToIndices("global_idx")};
                  var sum = ${d.type.value}(0);
                  let first : i32 = ${R};
                  let last : i32 = ${N};
                  for (var i : i32 = first; i < last; i++) {
                    ${o.indicesSet("inputIndices", "uniforms.axis", "u32(i)")};
                    sum = sum + ${o.getByIndices("inputIndices")};
                  }
                  ${d.setByOffset("global_idx", "sum")};
                }`;
          };
          return { name: "CumSum", shaderCache: { hint: n.cacheKey, inputDependencies: ["rank"] }, getRunData: () => ({ outputs: [{ dims: t, dataType: e }], dispatchGroup: { x: Math.ceil(i / 64) }, programUniforms: [{ type: 12, data: i }, { type: 12, data: h }, ...vt(t, t)] }), getShaderSource: k };
        }, Tu = (e, t) => {
          let r = e.inputs[0].dims, n = e.inputs[0].dataType, i = e.inputs[1];
          e.compute(xu(n, r, i, t), { inputs: [0] });
        }, Eu = (e) => {
          let t = e.exclusive === 1, r = e.reverse === 1;
          return ot({ exclusive: t, reverse: r });
        };
      }), Pu, Cu, Do, ku, Su, $u = w(() => {
        zt(), Bt(), Pt(), Qt(), Pu = (e) => {
          if (!e || e.length !== 1) throw new Error("DepthToSpace requires 1 input.");
          if (e[0].dims.length !== 4) throw new Error("DepthToSpace requires 4D input.");
        }, Cu = (e, t, r, n) => {
          let i = [];
          i.push(`fn perm(i: ${n.type.indices}) -> ${r.type.indices} {
    var a: ${r.type.indices};`);
          for (let a = 0; a < t; ++a) i.push(r.indicesSet("a", e[a], `i[${a}]`));
          return i.push("return a;}"), i.join(`
`);
        }, Do = (e, t) => {
          let r, n, i, a, o, d, p = t.format === "NHWC", h = t.blocksize, k = t.mode === "DCR";
          p ? ([r, n, i, a] = e.dims, o = k ? [r, n, i, h, h, a / h ** 2] : [r, n, i, a / h ** 2, h, h], d = k ? [0, 1, 3, 2, 4, 5] : [0, 1, 4, 2, 5, 3]) : ([r, n, i, a] = [e.dims[0], e.dims[2], e.dims[3], e.dims[1]], o = k ? [r, h, h, a / h ** 2, n, i] : [r, a / h ** 2, h, h, n, i], d = k ? [0, 3, 4, 1, 5, 2] : [0, 1, 4, 2, 5, 3]);
          let S = e.reshape(o), u = S.dims.length, B = e.dataType, R = ze("a", B, u), N = wt("output", B, u), Z = (te) => `
  ${te.registerUniform("output_size", "u32").declareVariables(R, N)}

  ${Cu(d, u, R, N)}

  ${te.mainStart()}
    ${te.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let indices = ${N.offsetToIndices("global_idx")};
    let aIndices = perm(indices);

    ${N.setByOffset("global_idx", R.getByIndices("aIndices"))}
  }`;
          return { name: "DepthToSpace", shaderCache: { hint: `${e.dims};${t.blocksize};${t.mode}`, inputDependencies: ["rank"] }, getRunData: (te) => {
            let Q = p ? [r, n * h, i * h, a / h ** 2] : [r, a / h ** 2, n * h, i * h], _e = $e.size(Q), me = S.dims, ye = $e.sortBasedOnPerm(me, d);
            return { outputs: [{ dims: Q, dataType: te[0].dataType }], dispatchGroup: { x: Math.ceil(_e / 64) }, programUniforms: [{ type: 12, data: _e }, ...vt(me, ye)] };
          }, getShaderSource: Z };
        }, ku = (e, t) => {
          Pu(e.inputs), e.compute(Do(e.inputs[0], t));
        }, Su = (e) => ot({ blocksize: e.blocksize, mode: e.mode, format: e.format });
      }), fi, Ln, Fs, Au, Iu, Fu, Ou, gi, Du, Lu, zu, Wc = w(() => {
        zt(), Bt(), Pt(), Qt(), fi = "[a-zA-Z]|\\.\\.\\.", Ln = "(" + fi + ")+", Fs = "^" + Ln + "$", Au = "(" + Ln + ",)*" + Ln, Iu = "^" + Au + "$", Fu = class {
          constructor(e = -1) {
            this.symbolToIndices = /* @__PURE__ */ new Map(), this.inputIndex = e;
          }
          addSymbol(e, t) {
            let r = this.symbolToIndices.get(e);
            r === void 0 ? r = [t] : r.push(t), this.symbolToIndices.set(e, r);
          }
        }, Ou = class {
          constructor(e, t) {
            var i;
            this.equation = t, this.hasEllipsis = !1, this.symbolToInfo = /* @__PURE__ */ new Map(), this.lhs = new Array(), this.outputDims = [];
            let [r, n] = t.includes("->") ? t.split("->", 2) : [t, ""];
            if (!r.match(RegExp(Iu))) throw new Error("Invalid LHS term");
            if (r.split(",").forEach((a, o) => {
              let d = e[o].dims.slice();
              if (!a.match(RegExp(Fs))) throw new Error("Invalid LHS term");
              let p = this.processTerm(a, !0, d, o);
              this.lhs.push(p);
            }), n === "") n += [...this.symbolToInfo.entries()].filter(([a, o]) => o.count === 1 || a === "...").map(([a]) => a).join("");
            else if (!n.match(RegExp(Ln))) throw new Error("Invalid RHS");
            (i = n.match(RegExp(fi, "g"))) == null || i.forEach((a) => {
              if (a === "...") this.outputDims = this.outputDims.concat(this.ellipsisDims);
              else {
                let o = this.symbolToInfo.get(a);
                if (o === void 0) throw new Error("Invalid RHS symbol");
                this.outputDims.push(o.dimValue);
              }
            }), this.rhs = this.processTerm(n, !1, this.outputDims);
          }
          addSymbol(e, t, r) {
            let n = this.symbolToInfo.get(e);
            if (n !== void 0) {
              if (n.dimValue !== t && n.count !== 1) throw new Error("Dimension mismatch");
              n.count++, n.inputIndices.push(r);
            } else n = { count: 1, dimValue: t, inputIndices: [r] };
            this.symbolToInfo.set(e, n);
          }
          processTerm(e, t, r, n = -1) {
            let i = r.length, a = !1, o = [], d = 0;
            if (!e.match(RegExp(Fs)) && !t && e !== "") throw new Error("Invalid LHS term");
            let p = e.match(RegExp(fi, "g")), h = new Fu(n);
            return p == null || p.forEach((k, S) => {
              if (k === "...") {
                if (a) throw new Error("Only one ellipsis is allowed per input term");
                a = !0;
                let u = i - p.length + 1;
                if (u < 0) throw new Error("Ellipsis out of bounds");
                if (o = r.slice(d, d + u), this.hasEllipsis) {
                  if (this.ellipsisDims.length !== o.length || this.ellipsisDims.toString() !== o.toString()) throw new Error("Ellipsis dimensions mismatch");
                } else if (t) this.hasEllipsis = !0, this.ellipsisDims = o;
                else throw new Error("Ellipsis must be specified in the LHS");
                for (let B = 0; B < o.length; B++) {
                  let R = String.fromCharCode(48 + B);
                  h.addSymbol(R, S + B), this.addSymbol(R, r[d++], n);
                }
              } else h.addSymbol(k, S + (this.hasEllipsis ? this.ellipsisDims.length - 1 : 0)), this.addSymbol(k, r[d++], n);
            }), h;
          }
        }, gi = (e) => e + "_max", Du = (e, t, r, n) => {
          let i = e.map((h) => h.length).map((h, k) => ze(`input${k}`, t, h)), a = $e.size(n), o = wt("output", t, n.length), d = [...r.symbolToInfo.keys()].filter((h) => !r.rhs.symbolToIndices.has(h)), p = (h) => {
            let k = [], S = "var prod = 1.0;", u = "var sum = 0.0;", B = "sum += prod;", R = [], N = [], Z = [], te = [], Q = r.symbolToInfo.size === r.rhs.symbolToIndices.size;
            r.symbolToInfo.forEach((me, ye) => {
              var Ae;
              if (r.rhs.symbolToIndices.has(ye)) {
                let Ie = (Ae = r.rhs.symbolToIndices.get(ye)) == null ? void 0 : Ae[0];
                Ie !== void 0 && r.lhs.forEach((Ge, lt) => {
                  if (me.inputIndices.includes(lt)) {
                    let Tt = Ge.symbolToIndices.get(ye);
                    if (Tt === void 0) throw new Error("Invalid symbol error");
                    Tt.forEach((Kt) => {
                      k.push(`${i[lt].indicesSet(`input${lt}Indices`, Kt, o.indicesGet("outputIndices", Ie))}`);
                    });
                  }
                });
              } else r.lhs.forEach((Ie, Ge) => {
                if (me.inputIndices.includes(Ge)) {
                  let lt = Ie.symbolToIndices.get(ye);
                  if (lt === void 0) throw new Error("Invalid symbol error");
                  lt.forEach((Tt) => {
                    R.push(`${i[Ge].indicesSet(`input${Ge}Indices`, Tt, `${ye}`)}`);
                  }), te.push(`prod *= ${i[Ge].getByIndices(`input${Ge}Indices`)};`);
                }
              }), N.push(`for(var ${ye}: u32 = 0; ${ye} < uniforms.${gi(ye)}; ${ye}++) {`), Z.push("}");
            });
            let _e = Q ? [...k, `let sum = ${i.map((me, ye) => me.getByIndices(`input${ye}Indices`)).join(" * ")};`] : [...k, u, ...N, ...R, S, ...te, B, ...Z];
            return `
            ${h.registerUniforms(d.map((me) => ({ name: `${gi(me)}`, type: "u32" }))).registerUniform("outputSize", "u32").declareVariables(...i, o)}

            ${h.mainStart()}
            ${h.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
            var outputIndices = ${o.offsetToIndices("global_idx")};
            ${i.map((me, ye) => `var input${ye}Indices: ${i[ye].type.indices};`).join(`
`)}
            ${_e.join(`
`)};
            ${o.setByOffset("global_idx", "sum")};
          }`;
          };
          return { name: "Einsum", shaderCache: { hint: r.equation, inputDependencies: e.map(() => "rank") }, getRunData: () => {
            let h = d.filter((S) => r.symbolToInfo.has(S)).map((S) => {
              var u;
              return { type: 12, data: ((u = r.symbolToInfo.get(S)) == null ? void 0 : u.dimValue) || 0 };
            });
            h.push({ type: 12, data: a });
            let k = e.map((S, u) => [...vt(S)]).reduce((S, u) => S.concat(u), h);
            return k.push(...vt(n)), { outputs: [{ dims: n, dataType: t }], dispatchGroup: { x: Math.ceil(a / 64) }, programUniforms: k };
          }, getShaderSource: p };
        }, Lu = (e, t) => {
          let r = new Ou(e.inputs, t.equation), n = r.outputDims, i = e.inputs.map((a, o) => a.dims);
          e.compute(Du(i, e.inputs[0].dataType, r, n));
        }, zu = (e) => {
          let t = e.equation.replace(/\s+/g, "");
          return ot({ equation: t });
        };
      }), wi, Lo, Bu, Ru, zn, Vc = w(() => {
        zt(), Bt(), Qt(), wi = (e) => {
          if (!e || e.length !== 2) throw new Error("Expand requires 2 input.");
          let t = e[0].dims, r = Array.from(e[1].getBigInt64Array(), Number), n = r.length < t.length ? 0 : r.length - t.length, i = t.length < r.length ? 0 : t.length - r.length;
          for (; n < r.length && i < t.length; ++n, ++i) if (r[n] !== t[i] && r[n] !== 1 && t[i] !== 1) throw new Error("Expand requires shape to be broadcastable to input");
        }, Lo = (e, t) => {
          let r = e.length - t.length, n = [];
          for (let i = 0; i < r; ++i) n.push(e[i]);
          for (let i = 0; i < t.length; ++i) n.push(t[i] === 1 ? e[i + r] : t[i]);
          return n;
        }, Bu = (e, t) => e.length > t.length ? Lo(e, t) : Lo(t, e), Ru = (e) => {
          let t = e[0].dims, r = Array.from(e[1].getBigInt64Array(), Number), n = Bu(t, r), i = e[0].dataType, a = i === 9 || $e.size(t) === 1, o = i === 9 || t.length > 0 && t[t.length - 1] % 4 === 0 ? 4 : 1, d = a || n.length > 0 && n[n.length - 1] % 4 === 0 ? 4 : 1, p = Math.ceil($e.size(n) / d), h = (S) => {
            let u = ze("input", i, t.length, o), B = wt("output", i, n.length, d), R;
            if (i === 9) {
              let N = (Z, te, Q = "") => `
          let outputIndices${te} = ${B.offsetToIndices(`outputOffset + ${te}u`)};
          let offset${te} = ${u.broadcastedIndicesToOffset(`outputIndices${te}`, B)};
          let index${te} = offset${te} / 4u;
          let component${te} = offset${te} % 4u;
          ${Z}[${te}] = ${Q}(${u.getByOffset(`index${te}`)}[component${te}]);
        `;
              R = `
        let outputOffset = global_idx * ${d};
        var data = vec4<u32>(0);
        ${N("data", 0, "u32")}
        ${N("data", 1, "u32")}
        ${N("data", 2, "u32")}
        ${N("data", 3, "u32")}
        ${B.setByOffset("global_idx", "data")}
      }`;
            } else R = `
        let outputIndices = ${B.offsetToIndices(`global_idx * ${d}`)};
        let inputOffset = ${u.broadcastedIndicesToOffset("outputIndices", B)};
        let data = ${B.type.value}(${u.getByOffset(`inputOffset / ${o}`)});
        ${B.setByOffset("global_idx", "data")}
      }`;
            return `
    ${S.registerUniform("vec_size", "u32").declareVariables(u, B)}
    ${S.mainStart()}
    ${S.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
    ${R}`;
          }, k = [{ type: 12, data: p }, ...vt(t, n)];
          return { name: "Expand", shaderCache: { hint: `${n.length};${o}${d}`, inputDependencies: ["rank"] }, getShaderSource: h, getRunData: () => ({ outputs: [{ dims: n, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: k }) };
        }, zn = (e) => {
          wi(e.inputs), e.compute(Ru(e.inputs), { inputs: [0] });
        };
      }), Nu, ju, Gc = w(() => {
        zt(), Bt(), Qt(), yo(), Nu = (e) => {
          let t = e[0].dataType, r = $e.size(e[0].dims), n = $e.size(e[1].dims), i = n % 4 === 0, a = (o) => {
            let d = ze("x", t, [1], 4), p = ze("bias", t, [1], 4), h = wt("y", t, [1], 4), k = [{ name: "output_vec_size", type: "u32" }, { name: "bias_size", type: "u32" }], S = (B) => `
      let bias${B}_offset: u32 = (global_idx * 4 + ${B}) % uniforms.bias_size;
      let bias${B} = ${p.getByOffset(`bias${B}_offset / 4`)}[bias${B}_offset % 4];`, u = i ? `
      let bias = ${p.getByOffset("global_idx % (uniforms.bias_size / 4)")};` : `${S(0)}${S(1)}${S(2)}${S(3)}
      let bias = ${d.type.value}(bias0, bias1, bias2, bias3);`;
            return `${o.registerUniforms(k).declareVariables(d, p, h)}

    ${si(mr(t))}

    ${o.mainStart(Nr)}
      ${o.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_vec_size")}

      let x = ${d.getByOffset("global_idx")};
      ${u}
      let x_in = x + bias;
      ${h.setByOffset("global_idx", go("x_in"))}
    }`;
          };
          return { name: "FastGeluWithBias", shaderCache: { hint: `${i}`, inputDependencies: ["type", "type"] }, getShaderSource: a, getRunData: (o) => ({ outputs: [{ dims: o[0].dims, dataType: o[0].dataType }], programUniforms: [{ type: 12, data: Math.ceil(r / 4) }, { type: 12, data: n }], dispatchGroup: { x: Math.ceil(r / Nr / 4) } }) };
        }, ju = (e) => {
          e.inputs.length < 2 || $e.size(e.inputs[1].dims) === 0 ? $l(e) : e.compute(Nu(e.inputs));
        };
      }), yi, Uu, Wu, Vu, Dp = w(() => {
        zt(), Bt(), Pt(), Qt(), yi = (e) => {
          if (!e || e.length !== 2) throw new Error("Gather requires 2 inputs.");
        }, Uu = (e, t) => {
          let r = e[0].dims, n = e[1].dims, i = r.length, a = $e.normalizeAxis(t.axis, i), o = r.slice(0);
          o.splice(a, 1, ...n);
          let d = r[a], p = e[0].dataType === 9 ? 4 : 1, h = Math.ceil($e.size(o) / p), k = [{ type: 12, data: h }, { type: 6, data: d }, { type: 12, data: a }, ...vt(e[0].dims, e[1].dims, o)], S = (u) => {
            let B = ze("data", e[0].dataType, e[0].dims.length, p), R = ze("inputIndices", e[1].dataType, e[1].dims.length), N = wt("output", e[0].dataType, o.length, p), Z = (Q) => {
              let _e = n.length, me = `var indicesIndices${Q}  = ${R.type.indices}(0);`;
              for (let ye = 0; ye < _e; ye++) me += `${_e > 1 ? `indicesIndices${Q}[${ye}]` : `indicesIndices${Q}`} = ${o.length > 1 ? `outputIndices${Q}[uniforms.axis + ${ye}]` : `outputIndices${Q}`};`;
              me += `
          var idx${Q} = ${R.getByIndices(`indicesIndices${Q}`)};
          if (idx${Q} < 0) {
            idx${Q} = idx${Q} + uniforms.axisDimLimit;
          }
          var dataIndices${Q} : ${B.type.indices};
        `;
              for (let ye = 0, Ae = 0; ye < i; ye++) ye === a ? (me += `${i > 1 ? `dataIndices${Q}[${ye}]` : `dataIndices${Q}`} = u32(idx${Q});`, Ae += _e) : (me += `${i > 1 ? `dataIndices${Q}[${ye}]` : `dataIndices${Q}`} = ${o.length > 1 ? `outputIndices${Q}[${Ae}]` : `outputIndices${Q}`};`, Ae++);
              return me;
            }, te;
            if (e[0].dataType === 9) {
              let Q = (_e, me, ye = "") => `
          let outputIndices${me} = ${N.offsetToIndices(`outputOffset + ${me}u`)};
          ${Z(me)};
          let offset${me} = ${B.indicesToOffset(`dataIndices${me}`)};
          let index${me} = offset${me} / 4u;
          let component${me} = offset${me} % 4u;
          ${_e}[${me}] = ${ye}(${B.getByOffset(`index${me}`)}[component${me}]);
        `;
              te = `
        let outputOffset = global_idx * ${p};
        var value = vec4<u32>(0);
        ${Q("value", 0, "u32")}
        ${Q("value", 1, "u32")}
        ${Q("value", 2, "u32")}
        ${Q("value", 3, "u32")}
        ${N.setByOffset("global_idx", "value")}
      `;
            } else te = `
      let outputIndices = ${N.offsetToIndices("global_idx")};
      ${Z("")};
      let value = ${B.getByIndices("dataIndices")};
      ${N.setByOffset("global_idx", "value")};
      `;
            return `
      ${u.registerUniform("outputSize", "u32").registerUniform("axisDimLimit", "i32").registerUniform("axis", "u32").declareVariables(B, R, N)}
      ${u.mainStart()}
        ${u.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
        ${te}
      }`;
          };
          return { name: "Gather", shaderCache: { hint: t.cacheKey, inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: o, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(h / 64) }, programUniforms: k }), getShaderSource: S };
        }, Wu = (e) => ot({ axis: e.axis }), Vu = (e, t) => {
          let r = e.inputs;
          yi(r), e.compute(Uu(e.inputs, t));
        };
      }), Gu, Ku, Hu, Bn = w(() => {
        zt(), Bt(), Qt(), Gu = (e, t, r, n, i, a, o, d, p) => {
          let h = [{ type: 12, data: a }, { type: 12, data: n }, { type: 12, data: i }, { type: 12, data: r }, { type: 12, data: o }, { type: 12, data: d }, { type: 12, data: p }], k = [a];
          h.push(...vt(t.dims, k));
          let S = (u) => {
            let B = ze("indices_data", t.dataType, t.dims.length), R = wt("input_slice_offsets_data", 12, 1, 1), N = [B, R], Z = [{ name: "output_size", type: "u32" }, { name: "batch_dims", type: "u32" }, { name: "input_dims", type: "u32", length: i.length }, { name: "sizes_from_slice_dims_data", type: "u32", length: r.length }, { name: "num_slices_per_batch", type: "u32" }, { name: "input_batch_stride", type: "u32" }, { name: "num_slice_dims", type: "u32" }];
            return `
  ${u.registerUniforms(Z).declareVariables(...N)}
  ${u.mainStart()}
    ${u.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let batch_idx = global_idx / uniforms.num_slices_per_batch;
    let base_offset = batch_idx * uniforms.input_batch_stride;

    let slice_indices_base_offset = global_idx * uniforms.num_slice_dims;
    var relative_slice_offset = 0;
    for (var dim_idx = 0u; dim_idx < uniforms.num_slice_dims; dim_idx ++) {
      var index = i32(indices_data[dim_idx + slice_indices_base_offset].x);
      let input_dim_idx = uniforms.batch_dims + dim_idx;
      if (index < 0) {
        ${i.length === 1 ? "index += i32(uniforms.input_dims);" : "index += i32(uniforms.input_dims[input_dim_idx]);"}
      }
      ${r.length === 1 ? "relative_slice_offset += index * i32(uniforms.sizes_from_slice_dims_data);" : "relative_slice_offset += index * i32(uniforms.sizes_from_slice_dims_data[dim_idx]);"}
    }

    input_slice_offsets_data[global_idx] =  base_offset + u32(relative_slice_offset);
  }`;
          };
          return e.compute({ name: "computeSliceOffsets", shaderCache: { hint: `${i.length}_${r.length}`, inputDependencies: ["rank"] }, getRunData: () => ({ outputs: [{ dims: k, dataType: e.inputs[1].dataType }], dispatchGroup: { x: Math.ceil(a / 64) }, programUniforms: h }), getShaderSource: S }, { inputs: [t], outputs: [-1] })[0];
        }, Ku = (e, t) => {
          let r = e.inputs, n = r[0].dims, i = r[0].dataType, a = r[1].dims, o = a[a.length - 1], d = $e.sizeToDimension(a, a.length - 1), p = $e.sizeFromDimension(n, t.batchDims + o), h = $e.sizeToDimension(n, t.batchDims), k = $e.sizeFromDimension(n, t.batchDims), S = d / h, u = new Array(o), B = p;
          for (let me = 0; me < o; ++me) u[o - 1 - me] = B, B *= n[t.batchDims + o - 1 - me];
          let R = Gu(e, r[1], u, t.batchDims, n, d, S, k, o), N = t.batchDims + o;
          if (N > n.length) throw new Error("last dimension of indices must not be larger than rank of input tensor");
          let Z = a.slice(0, -1).concat(n.slice(N)), te = $e.size(Z), Q = [{ type: 12, data: te }, { type: 12, data: p }, ...vt(r[0].dims, R.dims, Z)], _e = (me) => {
            let ye = ze("data", r[0].dataType, r[0].dims.length), Ae = ze("slice_offsets", 12, R.dims.length), Ie = wt("output", r[0].dataType, Z.length);
            return `
          ${me.registerUniform("output_size", "u32").registerUniform("slice_size", "u32").declareVariables(ye, Ae, Ie)}
            ${me.mainStart()}
            ${me.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          let slice_offset = slice_offsets[global_idx / uniforms.slice_size];
          output[global_idx] = data[u32(slice_offset) + global_idx % uniforms.slice_size];
        }`;
          };
          e.compute({ name: "GatherND", shaderCache: { hint: t.cacheKey, inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: Z, dataType: i }], dispatchGroup: { x: Math.ceil(te / 64) }, programUniforms: Q }), getShaderSource: _e }, { inputs: [r[0], R] });
        }, Hu = (e) => ({ batchDims: e.batch_dims, cacheKey: "" });
      }), qu, Xu, Qu, Yu, Kc = w(() => {
        zt(), Bt(), Pt(), Qt(), qu = (e, t) => {
          if (e.length < 3 || e.length > 4) throw new Error("GatherBlockQuantized requires 3 or 4 inputs.");
          let r = $e.normalizeAxis(t.quantizeAxis, e[0].dims.length), n = t.blockSize, i = e[0], a = e[2], o = e.length === 4 ? e[3] : void 0;
          if (a.dims.length !== i.dims.length || !i.dims.map((d, p) => p === r ? Math.ceil(d / n) === a.dims[p] : d === a.dims[p]).reduce((d, p) => d && p, !0)) throw new Error("Scales must have the same rank as the input tensor and the dims should match except on gatherAxis.");
          if (o) {
            if (o.dataType !== i.dataType) throw new Error("Zero point must have the same data type as the input tensor.");
            if (o.dims.length !== a.dims.length || !o.dims.map((d, p) => d === a.dims[p]).reduce((d, p) => d && p, !0)) throw new Error("Zero point must have the same rank as the input tensor and the dims should match except on quantizeAxis.");
          }
        }, Xu = (e, t) => {
          let r = e[0].dims, n = e[1].dims, i = r.length, a = $e.normalizeAxis(t.gatherAxis, i), o = $e.normalizeAxis(t.quantizeAxis, i), d = r.slice(0);
          d.splice(a, 1, ...n);
          let p = $e.size(d), h = e[2].dataType, k = e[0].dataType === 22, S = [{ type: 12, data: p }, { type: 12, data: o }, { type: 12, data: a }, { type: 12, data: t.blockSize }, ...vt(...e.map((B, R) => B.dims), d)], u = (B) => {
            let R = ze("data", e[0].dataType, e[0].dims.length), N = ze("inputIndices", e[1].dataType, e[1].dims.length), Z = ze("scales", e[2].dataType, e[2].dims.length), te = e.length > 3 ? ze("zeroPoint", e[3].dataType, e[3].dims.length) : void 0, Q = wt("output", h, d.length), _e = [R, N, Z];
            te && _e.push(te);
            let me = [{ name: "output_size", type: "u32" }, { name: "quantize_axis", type: "u32" }, { name: "gather_axis", type: "u32" }, { name: "block_size", type: "u32" }];
            return `
        ${B.registerUniforms(me).declareVariables(..._e, Q)}
        ${B.mainStart()}
        let output_indices = ${Q.offsetToIndices("global_idx")};
        var indices_indices = ${N.type.indices}(0);
        ${n.length > 1 ? `
          for (var i: u32 = 0; i < ${n.length}; i++) {
            let index = ${Q.indicesGet("output_indices", "uniforms.gather_axis + i")};
            ${N.indicesSet("indices_indices", "i", "index")};
          }` : `indices_indices = ${Q.indicesGet("output_indices", "uniforms.gather_axis")};`};
        var data_indices = ${R.type.indices}(0);
        for (var i: u32 = 0; i < uniforms.gather_axis; i++) {
          let index = ${Q.indicesGet("output_indices", "i")};
          ${R.indicesSet("data_indices", "i", "index")};
        }
        var index_from_indices = ${N.getByIndices("indices_indices")};
        if (index_from_indices < 0) {
          index_from_indices += ${r[a]};
        }
        ${R.indicesSet("data_indices", "uniforms.gather_axis", "u32(index_from_indices)")};
        for (var i = uniforms.gather_axis + 1; i < ${d.length}; i++) {
          let index = ${Q.indicesGet("output_indices", `i + ${n.length} - 1`)};
          ${R.indicesSet("data_indices", "i", "index")};
        }
        let data_offset = ${R.indicesToOffset("data_indices")};
        let data_index = data_offset % 8;
        // Convert 4-bit packed data to 8-bit packed data.
        let packed_4bit_quantized_data = ${R.getByOffset("data_offset / 8")};
        let packed_8bit_quantized_data = (packed_4bit_quantized_data >> (4 * (data_index % 2))) & 0x0f0f0f0f;
        let quantized_data_vec = ${k ? "unpack4xI8" : "unpack4xU8"}(u32(packed_8bit_quantized_data));
        let quantized_data = quantized_data_vec[data_index / 2];
        var scale_indices = data_indices;
        let quantize_axis_index = ${Z.indicesGet("data_indices", "uniforms.quantize_axis")} / uniforms.block_size;
        ${Z.indicesSet("scale_indices", "uniforms.quantize_axis", "quantize_axis_index")};
        var scale = ${Z.getByIndices("scale_indices")};
        ${te ? `
              let zero_point_indices = scale_indices;
              let zero_point_offset = ${te.indicesToOffset("zero_point_indices")};
              let zero_point_index = zero_point_offset % 8;
              let packed_4bit_zero_points = ${te.getByOffset("zero_point_offset / 8")};
              let packed_8bit_zero_points = (packed_4bit_zero_points >> (4 * (zero_point_index % 2))) & 0x0f0f0f0f;
              let zero_point_vec = ${k ? "unpack4xI8" : "unpack4xU8"}(u32(packed_8bit_zero_points));
              let zero_point = zero_point_vec[zero_point_index / 2];` : "var zero_point = 0"};
        let dequantized_data = ${mr(h)}(quantized_data - zero_point) * scale;
        ${Q.setByOffset("global_idx", "dequantized_data")};
    }`;
          };
          return { name: "GatherBlockQuantized", shaderCache: { hint: `${t.cacheKey};${e.filter((B, R) => R !== 1).map((B) => B.dims.join("_")).join(";")}`, inputDependencies: Array.from({ length: e.length }, (B, R) => "rank") }, getRunData: () => ({ outputs: [{ dims: d, dataType: h }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: S }), getShaderSource: u };
        }, Qu = (e, t) => {
          let r = e.inputs;
          qu(r, t), e.compute(Xu(e.inputs, t));
        }, Yu = (e) => ot({ blockSize: e.blockSize, gatherAxis: e.gatherAxis, quantizeAxis: e.quantizeAxis });
      }), Mi, Hc, Ju, Zu, qc = w(() => {
        zt(), Bt(), Pt(), Qt(), Mi = (e) => {
          if (!e || e.length !== 2) throw new Error("GatherElements requires 2 inputs.");
          if (e[0].dims.length < 1) throw new Error("GatherElements requires that the data input be rank >= 1.");
          if (e[0].dims.length !== e[1].dims.length) throw new Error(`GatherElements requires that the data input and
                     indices input tensors be of same rank.`);
        }, Hc = (e, t) => {
          let r = e[0].dims, n = e[0].dataType, i = r.length, a = e[1].dims, o = e[1].dataType, d = $e.normalizeAxis(t.axis, i), p = r[d], h = a.slice(0), k = $e.size(h), S = ze("input", n, i), u = ze("indicesInput", o, a.length), B = wt("output", n, h.length), R = [{ type: 12, data: k }, { type: 6, data: p }, { type: 12, data: d }];
          return R.push(...vt(r, a, h)), { name: "GatherElements", shaderCache: { inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: h, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(k / 64) }, programUniforms: R }), getShaderSource: (N) => `
      ${N.registerUniform("outputSize", "u32").registerUniform("axisDimLimit", "i32").registerUniform("axis", "u32").declareVariables(S, u, B)}
      ${N.mainStart()}
      ${N.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

      let outputIndices = ${B.offsetToIndices("global_idx")};

      var idx = ${u.getByOffset("global_idx")};
      if (idx < 0) {
        idx = idx + uniforms.axisDimLimit;
      }
      var inputIndices = ${S.type.indices}(outputIndices);
      ${S.indicesSet("inputIndices", "uniforms.axis", "u32(idx)")};
      let value = ${S.getByIndices("inputIndices")};

      ${B.setByOffset("global_idx", "value")};
  }` };
        }, Ju = (e) => ot({ axis: e.axis }), Zu = (e, t) => {
          let r = e.inputs;
          Mi(r), e.compute(Hc(e.inputs, t));
        };
      }), ed, td, zo, rd, Xc = w(() => {
        zt(), Bt(), Qt(), ed = (e) => {
          if (!e) throw new Error("Input is missing");
          if (e.length < 2 || e.length > 3) throw new Error("Invaid input number.");
          if (e.length === 3 && e[2].dims.length > 2) throw new Error("Invalid input shape of C");
          if (e[0].dataType !== e[1].dataType || e.length === 3 && e[0].dataType !== e[2].dataType) throw new Error("Input types are mismatched");
        }, td = (e, t) => {
          let r = e[0].dims.slice(), n = e[1].dims.slice(), [i, a, o] = Rr.getShapeOfGemmResult(r, t.transA, n, t.transB, e.length === 3 ? e[2].dims : void 0), d = [i, a];
          if (!d) throw new Error("Can't use gemm on the given tensors");
          let p = 16, h = Math.ceil(a / p), k = Math.ceil(i / p), S = !0, u = $e.size(d), B = [{ type: 12, data: S ? h : u }, { type: 12, data: i }, { type: 12, data: a }, { type: 12, data: o }, { type: 1, data: t.alpha }, { type: 1, data: t.beta }], R = ["type", "type"];
          e.length === 3 && (B.push(...vt(e[2].dims)), R.push("rank")), B.push(...vt(d));
          let N = (te) => {
            let Q = "";
            t.transA && t.transB ? Q = "value += a[k * uniforms.M + m] * b[n * uniforms.K + k];" : t.transA && !t.transB ? Q = "value += a[k * uniforms.M + m] * b[k * uniforms.N + n];" : !t.transA && t.transB ? Q = "value += a[m * uniforms.K + k] * b[n * uniforms.K + k];" : !t.transA && !t.transB && (Q = "value += a[m * uniforms.K + k] * b[k * uniforms.N + n];");
            let _e = t.alpha === 1 ? "" : "value *= uniforms.alpha;", me = ze("a", e[0].dataType, e[0].dims), ye = ze("b", e[1].dataType, e[1].dims), Ae = me.type.value, Ie = null, Ge = [me, ye];
            e.length === 3 && (Ie = ze("c", e[2].dataType, e[2].dims.length), Ge.push(Ie));
            let lt = wt("output", e[0].dataType, d.length);
            Ge.push(lt);
            let Tt = [{ name: "output_size", type: "u32" }, { name: "M", type: "u32" }, { name: "N", type: "u32" }, { name: "K", type: "u32" }, { name: "alpha", type: "f32" }, { name: "beta", type: "f32" }];
            return `
  ${te.registerUniforms(Tt).declareVariables(...Ge)}

  ${te.mainStart()}
    ${te.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

    let m = global_idx / uniforms.N;
    let n = global_idx % uniforms.N;

    var value = ${Ae}(0);
    for (var k: u32 = 0u; k < uniforms.K; k++) {
      ${Q}
    }

    ${_e}
    ${Ie != null ? `let cOffset = ${Ie.broadcastedIndicesToOffset("vec2(m, n)", lt)}; value += ${Ae}(uniforms.beta) * ${Ie.getByOffset("cOffset")};` : ""}
    output[global_idx] = value;
  }`;
          }, Z = (te) => {
            let Q = ze("a", e[0].dataType, e[0].dims), _e = ze("b", e[1].dataType, e[1].dims), me = null, ye = [Q, _e];
            e.length === 3 && (me = ze("c", e[2].dataType, e[2].dims.length), ye.push(me));
            let Ae = wt("output", e[0].dataType, d.length);
            ye.push(Ae);
            let Ie = [{ name: "num_tile_n", type: "u32" }, { name: "M", type: "u32" }, { name: "N", type: "u32" }, { name: "K", type: "u32" }, { name: "alpha", type: "f32" }, { name: "beta", type: "f32" }], Ge = "", lt = "";
            t.transA && t.transB ? (lt = `
      var col = tile_row_start + local_id.x;
      var row = k_start + local_id.y;
      if (col < uniforms.M && row < uniforms.K) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.M + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${Q.type.value}(0);
      }

      col = k_start + local_id.x;
      row = tile_col_start + local_id.y;
      if (col < uniforms.K && row < uniforms.N) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.K + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${_e.type.value}(0);
      }
      `, Ge = "value += tile_a[k][local_id.y] * tile_b[local_id.x][k];") : t.transA && !t.transB ? (lt = `
      var col = tile_row_start + local_id.x;
      var row = k_start + local_id.y;
      if (col < uniforms.M && row < uniforms.K) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.M + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${Q.type.value}(0);
      }

      col = tile_col_start + local_id.x;
      row = k_start + local_id.y;
      if (col < uniforms.N && row < uniforms.K) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.N + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${_e.type.value}(0);
      }
      `, Ge = "value += tile_a[k][local_id.y] * tile_b[k][local_id.x];") : !t.transA && t.transB ? (lt = `
      var col = k_start + local_id.x;
      var row = tile_row_start + local_id.y;
      if (col < uniforms.K && row < uniforms.M) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.K + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${Q.type.value}(0);
      }

      col = k_start + local_id.x;
      row = tile_col_start + local_id.y;
      if (col < uniforms.K && row < uniforms.N) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.K + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${_e.type.value}(0);
      }
      `, Ge = "value += tile_a[local_id.y][k] * tile_b[local_id.x][k];") : !t.transA && !t.transB && (lt = `
      var col = k_start + local_id.x;
      var row = tile_row_start + local_id.y;
      if (col < uniforms.K && row < uniforms.M) {
        tile_a[local_id.y][local_id.x] = a[row * uniforms.K + col];
      } else {
        tile_a[local_id.y][local_id.x] = ${Q.type.value}(0);
      }

      col = tile_col_start + local_id.x;
      row = k_start + local_id.y;
      if (col < uniforms.N && row < uniforms.K) {
        tile_b[local_id.y][local_id.x] = b[row * uniforms.N + col];
      } else {
        tile_b[local_id.y][local_id.x] = ${_e.type.value}(0);
      }
      `, Ge = "value += tile_a[local_id.y][k] * tile_b[k][local_id.x];");
            let Tt = t.alpha === 1 ? "" : "value *= uniforms.alpha;";
            return `
  ${te.registerUniforms(Ie).declareVariables(...ye)}
  var<workgroup> tile_a: array<array<${Q.type.storage}, ${p}>, ${p}>;
  var<workgroup> tile_b: array<array<${_e.type.storage}, ${p}>, ${p}>;
  ${te.mainStart([p, p, 1])}
    let tile_col_start = (workgroup_index % uniforms.num_tile_n) * ${p};
    let tile_row_start = (workgroup_index / uniforms.num_tile_n) * ${p};
    let num_tiles = (uniforms.K - 1) / ${p} + 1;
    var k_start = 0u;
    var value = ${Ae.type.value}(0);
    for (var t: u32 = 0u; t < num_tiles; t++) {
      ${lt}
      k_start = k_start + ${p};
      workgroupBarrier();

      for (var k: u32 = 0u; k < ${p}; k++) {
        ${Ge}
      }
      workgroupBarrier();
    }

    ${Tt}
    let m = tile_row_start + local_id.y;
    let n = tile_col_start + local_id.x;
    ${me != null ? `let cOffset = ${me.broadcastedIndicesToOffset("vec2(m, n)", Ae)}; value += ${Ae.type.value}(uniforms.beta) * ${me.getByOffset("cOffset")};` : ""}
    if (m < uniforms.M && n < uniforms.N) {
      output[m * uniforms.N + n] = value;
    }
  }`;
          };
          return S ? { name: "GemmShared", shaderCache: { hint: `${t.cacheKey}`, inputDependencies: R }, getRunData: () => ({ outputs: [{ dims: d, dataType: e[0].dataType }], dispatchGroup: { x: h * k }, programUniforms: B }), getShaderSource: Z } : { name: "Gemm", shaderCache: { hint: `${t.cacheKey}`, inputDependencies: R }, getRunData: () => ({ outputs: [{ dims: d, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(u / 64) }, programUniforms: B }), getShaderSource: N };
        }, zo = (e) => {
          let t = e.transA, r = e.transB, n = e.alpha, i = e.beta;
          return { transA: t, transB: r, alpha: n, beta: i, cacheKey: `${e.transA};${e.transB};${e.alpha === 1}` };
        }, rd = (e, t) => {
          ed(e.inputs), e.compute(td(e.inputs, t));
        };
      }), fs, Os, on, Ks, sd, nd, bi, id, od, ad, ld, Bo, vi, Qc, Yc = w(() => {
        zt(), Bt(), Pt(), Qt(), [fs, Os, on, Ks] = [0, 1, 2, 3], sd = (e) => {
          if (e[0].dims.length !== 4) throw new Error("only 4-D tensor is supported.");
          if (e[0].dims.length !== e[1].dims.length) throw new Error("input dimensions must be equal to grid dimensions");
          if (e[0].dims.length - 2 !== e[1].dims[e[1].dims.length - 1]) throw new Error(`last dimension of grid must be equal to ${e[0].dims.length - 2}`);
          if (e[0].dims[0] !== e[1].dims[0]) throw new Error("grid batch size must match input batch size");
        }, nd = `
  fn gs_get_cubic_coeffs(x: f32) -> vec4<f32> {
    let cubic_alpha = -0.75f;
    let x_abs = abs(x);
    var coeffs: vec4<f32>;
    coeffs[0] = (((cubic_alpha * (x_abs + 1) - 5 * cubic_alpha) * (x_abs + 1) + 8 * cubic_alpha) * (x_abs + 1) - 4 * cubic_alpha);
    coeffs[1] = (((cubic_alpha + 2) * x_abs - (cubic_alpha + 3)) * x_abs * x_abs + 1);
    coeffs[2] = (((cubic_alpha + 2) * (1 - x_abs) - (cubic_alpha + 3)) * (1 - x_abs) * (1 - x_abs) + 1);
    coeffs[3] = (((cubic_alpha * (2 - x_abs) - 5 * cubic_alpha) * (2 - x_abs) + 8 * cubic_alpha) * (2 - x_abs) - 4 * cubic_alpha);
    return coeffs;
  }
`, bi = (e) => `
  fn gs_bicubic_interpolate(p: mat4x4<${e}>, x: f32, y: f32) -> ${e} {
    var v: vec4<f32>;
    var coeffs = gs_get_cubic_coeffs(x);
    for (var i = 0; i < 4; i++) {
      v[i] = coeffs[0] * p[i][0] + coeffs[1] * p[i][1] + coeffs[2] * p[i][2] + coeffs[3] * p[i][3];
    }
    coeffs = gs_get_cubic_coeffs(y);
    let pixel = ${e}(coeffs[0] * v[0] + coeffs[1] * v[1] + coeffs[2] * v[2] + coeffs[3] * v[3]);
    return pixel;
  }
`, id = (e) => `
  fn gs_denormalize(n: f32, length: i32) -> f32 {
    ${e.alignCorners === 0 ? `
    // alignCorners: false => [-1, 1] to [-0.5, length - 0.5]
    return ((n + 1.0) * f32(length) - 1.0) / 2.0;
    ` : `
    // alignCorners: true => [-1, 1] to [0, length - 1]
    return (n + 1.0) / 2.0 * (f32(length - 1));
    `}
  }
`, od = (e) => `
  ${e.paddingMode === "reflection" ? `
      fn gs_reflect(x: i32, x_min: f32, x_max: f32) -> u32 {
        var dx = 0.0;
        var fx = f32(x);
        let range = x_max - x_min;
        if (fx < x_min) {
          dx = x_min - fx;
          let n = u32(dx / range);
          let r = dx - f32(n) * range;
          if (n % 2 == 0) {
            fx = x_min + r;
          } else {
            fx = x_max - r;
          }
        } else if (fx > x_max) {
          dx = fx - x_max;
          let n = u32(dx / range);
          let r = dx - f32(n) * range;
          if (n % 2 == 0) {
            fx = x_max - r;
          } else {
            fx = x_min + r;
          }
        }
        return u32(fx);
      }` : ""}
`, ad = (e, t, r) => `
  fn pixel_at_grid(r: i32, c: i32, H: i32, W: i32, batch: u32, channel: u32, border: vec4<f32>) -> ${t} {
     var pixel = ${t}(0);
     var indices = vec4<u32>(0);
     indices[${fs}] = batch;
     indices[${Os}] = channel;` + (() => {
          switch (r.paddingMode) {
            case "zeros":
              return `
          if (r >= 0 && r < H && c >=0 && c < W) {
            indices[${on}] = u32(r);
            indices[${Ks}] = u32(c);
          }
        `;
            case "border":
              return `
          indices[${on}] = u32(clamp(r, 0, H - 1));
          indices[${Ks}] = u32(clamp(c, 0, W - 1));
        `;
            case "reflection":
              return `
          indices[${on}] = gs_reflect(r, border[1], border[3]);
          indices[${Ks}] = gs_reflect(c, border[0], border[2]);
        `;
            default:
              throw new Error(`padding mode ${r.paddingMode} is not supported`);
          }
        })() + `
    return ${e.getByIndices("indices")};
  }
`, ld = (e, t, r) => (() => {
          switch (r.mode) {
            case "nearest":
              return `
          let result = pixel_at_grid(i32(round(y)), i32(round(x)), H_in, W_in, indices[${fs}], indices[${Os}], border);
        `;
            case "bilinear":
              return `
          let x1 = i32(floor(x));
          let y1 = i32(floor(y));
          let x2 = x1 + 1;
          let y2 = y1 + 1;

          let p11 = pixel_at_grid(y1, x1, H_in, W_in, indices[${fs}], indices[${Os}], border);
          let p12 = pixel_at_grid(y1, x2, H_in, W_in, indices[${fs}], indices[${Os}], border);
          let p21 = pixel_at_grid(y2, x1, H_in, W_in, indices[${fs}], indices[${Os}], border);
          let p22 = pixel_at_grid(y2, x2, H_in, W_in, indices[${fs}], indices[${Os}], border);

          let dx2 = ${t}(f32(x2) - x);
          let dx1 = ${t}(x - f32(x1));
          let dy2 = ${t}(f32(y2) - y);
          let dy1 = ${t}(y - f32(y1));
          let result = dy2 * (dx2 * p11 + dx1 * p12) + dy1 * (dx2 * p21 + dx1 * p22);
        `;
            case "bicubic":
              return `
          let x0 = i32(floor(x)) - 1;
          let y0 = i32(floor(y)) - 1;
          var p: mat4x4<${t}>;
          for (var h = 0; h < 4; h++) {
            for (var w = 0; w < 4; w++) {
              p[h][w] = pixel_at_grid(h + y0, w + x0, H_in, W_in, indices[${fs}], indices[${Os}], border);
            }
          }

          let dx = x - f32(x0 + 1);
          let dy = y - f32(y0 + 1);
          let result = gs_bicubic_interpolate(p, dx, dy);
        `;
            default:
              throw new Error(`mode ${r.mode} is not supported`);
          }
        })() + `${e.setByOffset("global_idx", "result")}`, Bo = (e, t) => {
          let r = ze("x", e[0].dataType, e[0].dims.length), n = [e[1].dims[0], e[1].dims[1], e[1].dims[2]], i = ze("grid", e[1].dataType, n.length, 2), a = [e[0].dims[0], e[0].dims[1], e[1].dims[1], e[1].dims[2]];
          t.format === "NHWC" && (a = [e[0].dims[0], e[1].dims[1], e[1].dims[2], e[0].dims[3]], [fs, Os, on, Ks] = [0, 3, 1, 2]);
          let o = wt("output", e[0].dataType, a.length), d = r.type.value, p = $e.size(a), h = [{ type: 12, data: p }, ...vt(e[0].dims, n, a)], k = (S) => `
  ${S.registerUniform("output_size", "u32").declareVariables(r, i, o)}
  ${nd}
  ${bi(d)}
  ${id(t)}
  ${od(t)}
  ${ad(r, d, t)}

  ${S.mainStart()}
    ${S.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let H_in = i32(uniforms.x_shape[${on}]);
      let W_in = i32(uniforms.x_shape[${Ks}]);

      ${t.alignCorners === 0 ? `
      let x_min = -0.5;
      let x_max = f32(W_in) - 0.5;
      let y_min = -0.5;
      let y_max = f32(H_in) - 0.5;
      ` : `
      let x_min = 0.0;
      let x_max = f32(W_in) - 1.0;
      let y_min = 0.0;
      let y_max = f32(H_in) - 1.0;
      `};
      let border = vec4<f32>(x_min, y_min, x_max, y_max);

      let indices = ${o.offsetToIndices("global_idx")};
      var grid_indices = vec3<u32>(indices[${fs}], indices[${on}], indices[${Ks}]);
      let nxy = ${i.getByIndices("grid_indices")};
      var x = gs_denormalize(f32(nxy[0]), W_in);
      var y = gs_denormalize(f32(nxy[1]), H_in);

      ${ld(o, d, t)}
  }`;
          return { name: "GridSample", shaderCache: { hint: `${t.cacheKey}`, inputDependencies: ["type", "type"] }, getRunData: (S) => {
            let u = $e.size(a);
            return { outputs: [{ dims: a, dataType: S[0].dataType }], dispatchGroup: { x: Math.ceil(u / 64) }, programUniforms: h };
          }, getShaderSource: k };
        }, vi = (e, t) => {
          sd(e.inputs), e.compute(Bo(e.inputs, t));
        }, Qc = (e) => ot({ alignCorners: e.align_corners, mode: e.mode, paddingMode: e.padding_mode, format: e.format });
      }), es, Ro, ud, No, jo, Rn, dd, Uo = w(() => {
        zt(), Bt(), Pt(), yn(), ti(), Qt(), Vs(), es = (e, t) => e.length > t && e[t].dims.length > 0 ? e[t] : void 0, Ro = (e, t) => {
          let r = e[0], n = es(e, 1), i = es(e, 2), a = es(e, 3), o = es(e, 4), d = es(e, 5), p = es(e, 6), h = es(e, 7);
          if (r.dims.length !== 3 && r.dims.length !== 5) throw new Error("Input query is expected to have 3 or 5 dimensions");
          let k = r.dims[0], S = r.dims[1], u = r.dims.length === 3 ? r.dims[2] : t.numHeads * r.dims[4], B = S, R = 0, N = 0, Z = Math.floor(u / t.numHeads);
          if (p && h && $e.size(p.dims) && $e.size(h.dims)) {
            if (p.dims.length !== 4) throw new Error('Input "past_key" is expected to have 4 dimensions');
            if (p.dims[0] !== k || p.dims[1] !== t.numHeads || p.dims[3] !== Z) throw new Error('Input "past_key" shape (batch_size, num_heads, past_sequence_length, head_size)');
            if (h.dims[0] !== k || h.dims[1] !== t.numHeads || h.dims[3] !== Z) throw new Error('Input "past_value" shape (batch_size, num_heads, past_sequence_length, head_size)');
            if (p.dims[2] !== h.dims[2]) throw new Error('Input "past_key" and "past_value" shall have same dim 2 (past_sequence_length)');
            if (h.dims.length !== 4) throw new Error('Input "past_value" is expected to have 4 dimensions');
            R = p.dims[2], N = p.dims[2];
          } else if (p && $e.size(p.dims) || h && $e.size(h.dims)) throw new Error('Input "past_key" and "past_value" shall be both present or both absent');
          let te;
          if (n && $e.size(n.dims) > 0) {
            if (r.dims.length !== 3) throw new Error('Input "query" is expected to have 3 dimensions when key is given');
            if (n.dims.length < 3 || n.dims.length > 5) throw new Error('Input "key" is expected to have 3, 4, or 5 dimensions');
            if (r.dims[0] !== n.dims[0]) throw new Error('Input "query" and "key" shall have same dim 0 (batch size)');
            if (n.dims.length === 3) {
              if (n.dims[2] !== r.dims[2]) throw new Error('Input "query" and "key" shall have same dim 2 (hidden_size)');
              te = 2, B = n.dims[1];
            } else if (n.dims.length === 5) {
              if (n.dims[2] !== t.numHeads || n.dims[3] !== 2 || n.dims[4] !== Z) throw new Error('Expect "key" shape (batch_size, kv_sequence_length, num_heads, 2, head_size) for packed kv');
              if (i) throw new Error('Expect "value" be none when "key" has packed kv format.');
              te = 5, B = n.dims[1];
            } else {
              if (n.dims[1] !== t.numHeads || n.dims[3] !== Z) throw new Error('Expect "key" shape (batch_size, num_heads, kv_sequence_length, head_size) for past_key');
              te = 0, B = n.dims[2];
            }
          } else {
            if (r.dims.length !== 5) throw new Error('Input "query" is expected to have 5 dimensions when key is empty');
            if (r.dims[2] !== t.numHeads || r.dims[3] !== 3) throw new Error('Expect "query" shape (batch_size, kv_sequence_length, num_heads, 3, head_size) for packed kv');
            te = 3;
          }
          if (a && $e.size(a.dims) > 0) {
            if (a.dims.length !== 1) throw new Error('Input "bias" is expected to have 1 dimension');
            if (n && n.dims.length === 5 && n.dims[3] === 2) throw new Error("bias is not allowed for packed kv.");
          }
          let Q = R + B, _e = 0;
          if (o && $e.size(o.dims) > 0) {
            _e = 8;
            let Ie = o.dims;
            throw Ie.length === 1 ? Ie[0] === k ? _e = 1 : Ie[0] === 3 * k + 2 && (_e = 3) : Ie.length === 2 && Ie[0] === k && Ie[1] === Q && (_e = 5), _e === 8 ? new Error('Input "key_padding_mask" shape shall be (batch_size) or (batch_size, total_sequence_length)') : new Error("Mask not supported");
          }
          let me = !1, ye = u;
          if (i && $e.size(i.dims) > 0) {
            if (i.dims.length !== 3 && i.dims.length !== 4) throw new Error('Input "value" is expected to have 3 or 4 dimensions');
            if (r.dims[0] !== i.dims[0]) throw new Error('Input "query" and "value" shall have same dim 0 (batch_size)');
            if (i.dims.length === 3) {
              if (B !== i.dims[1]) throw new Error('Input "key" and "value" shall have the same dim 1 (kv_sequence_length)');
              ye = i.dims[2];
            } else {
              if (B !== i.dims[2]) throw new Error('Input "key" and "value" shall have the same dim 2 (kv_sequence_length)');
              ye = i.dims[1] * i.dims[3], me = !0;
            }
          }
          let Ae = !1;
          if (o && $e.size(o.dims) > 0) throw new Error("Key padding mask is not supported");
          if (d && $e.size(d.dims) > 0) {
            if (d.dims.length !== 4) throw new Error('Input "attention_bias" is expected to have 4 dimensions');
            if (d.dims[0] !== k || d.dims[1] !== t.numHeads || d.dims[2] !== S || d.dims[3] !== Q) throw new Error('Expect "attention_bias" shape (batch_size, num_heads, sequence_length, total_sequence_length)');
          }
          return { batchSize: k, sequenceLength: S, pastSequenceLength: R, kvSequenceLength: B, totalSequenceLength: Q, maxSequenceLength: N, inputHiddenSize: 0, hiddenSize: u, vHiddenSize: ye, headSize: Z, vHeadSize: Math.floor(ye / t.numHeads), numHeads: t.numHeads, isUnidirectional: !1, pastPresentShareBuffer: !1, maskFilterValue: t.maskFilterValue, maskType: _e, scale: t.scale, broadcastResPosBias: Ae, passPastInKv: me, qkvFormat: te };
        }, ud = (e) => ot({ ...e }), No = ot({ perm: [0, 2, 1, 3] }), jo = (e, t, r, n, i, a, o) => {
          let d = [n, i, a], p = $e.size(d), h = [{ type: 12, data: p }, { type: 12, data: o }, { type: 12, data: a }], k = (S) => {
            let u = wt("qkv_with_bias", t.dataType, d), B = ze("qkv", t.dataType, d), R = ze("bias", r.dataType, d), N = [{ name: "output_size", type: "u32" }, { name: "bias_offset", type: "u32" }, { name: "hidden_size", type: "u32" }];
            return `
  ${S.registerUniforms(N).declareVariables(B, R, u)}
  ${S.mainStart()}
    ${S.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
    let bias_offset_idx = (global_idx % uniforms.hidden_size) + uniforms.bias_offset;

    qkv_with_bias[global_idx] = qkv[global_idx] + bias[bias_offset_idx];
  }`;
          };
          return e.compute({ name: "MultiHeadAttentionAddBias", shaderCache: { inputDependencies: ["type", "type"] }, getRunData: () => ({ outputs: [{ dims: d, dataType: t.dataType, gpuDataType: 0 }], dispatchGroup: { x: Math.ceil(p / 64) }, programUniforms: h }), getShaderSource: k }, { inputs: [t, r], outputs: [-1] })[0];
        }, Rn = (e, t, r, n, i, a, o, d) => {
          let p = a;
          if (o && $e.size(o.dims) > 0) {
            if (n === 1) throw new Error("AddBiasReshape is not implemented. Please export your model with packed QKV or KV");
            return p = jo(e, a, o, t, n, r * i, d), p = p.reshape([t, n, r, i]), r === 1 || n === 1 ? p : e.compute(ss(p, No.perm), { inputs: [p], outputs: [-1] })[0];
          } else return a.dims.length === 3 && (p = a.reshape([t, n, r, i])), r === 1 || n === 1 ? p : e.compute(ss(p, No.perm), { inputs: [p], outputs: [-1] })[0];
        }, dd = (e, t) => {
          let r = Ro(e.inputs, t), n = e.inputs[0], i = es(e.inputs, 1), a = es(e.inputs, 2), o = es(e.inputs, 3), d = es(e.inputs, 4), p = es(e.inputs, 5), h = es(e.inputs, 6), k = es(e.inputs, 7);
          if (n.dims.length === 5) throw new Error("Packed QKV is not implemented");
          if ((i == null ? void 0 : i.dims.length) === 5) throw new Error("Packed KV is not implemented");
          let S = i && a && i.dims.length === 4 && a.dims.length === 4, u = Rn(e, r.batchSize, r.numHeads, r.sequenceLength, r.headSize, n, o, 0);
          if (S) return vn(e, u, i, a, d, void 0, h, k, p, r);
          if (!i || !a) throw new Error("key and value must be provided");
          let B = Rn(e, r.batchSize, r.numHeads, r.kvSequenceLength, r.headSize, i, o, r.hiddenSize), R = Rn(e, r.batchSize, r.numHeads, r.kvSequenceLength, r.vHeadSize, a, o, 2 * r.hiddenSize);
          vn(e, u, B, R, d, void 0, h, k, p, r);
        };
      }), Jc, Zc, Wo, Vo, Go, cd, Ko, pd = w(() => {
        zt(), Bt(), Pt(), Qt(), Jc = (e) => {
          if (!e || e.length < 1) throw new Error("too few inputs");
        }, Zc = (e, t) => {
          let r = [], n = t.numOutputs;
          return e[1].dims[0] > 0 && (e[1].getBigInt64Array().forEach((i) => r.push(Number(i))), n = r.length), ot({ numOutputs: n, axis: t.axis, splitSizes: r });
        }, Wo = (e) => `
fn calculateOutputIndex(index: u32) -> u32 {
    for (var i: u32 = 0u; i < ${e}u; i += 1u ) {
    if (index < ${Mt("uniforms.size_in_split_axis", "i", e)}) {
        return i;
    }
    }
    return ${e}u;
}`, Vo = (e) => {
          let t = e.length, r = [];
          for (let n = 0; n < t; ++n) {
            let i = e[n].setByIndices("indices", "input[global_idx]");
            t === 1 ? r.push(i) : n === 0 ? r.push(`if (output_number == ${n}u) { ${i} }`) : n === t - 1 ? r.push(`else { ${i} }`) : r.push(`else if (output_number == ${n}) { ${i} }`);
          }
          return `
      fn writeBufferData(output_number: u32, indices: ${e[0].type.indices}, global_idx: u32) {
        ${r.join(`
`)}
      }`;
        }, Go = (e, t) => {
          let r = e[0].dims, n = $e.size(r), i = e[0].dataType, a = $e.normalizeAxis(t.axis, r.length), o = new Array(t.numOutputs), d = ze("input", i, r.length), p = new Array(t.numOutputs), h = [], k = [], S = 0, u = [{ type: 12, data: n }];
          for (let R = 0; R < t.numOutputs; R++) {
            S += t.splitSizes[R], p[R] = S;
            let N = r.slice();
            N[a] = t.splitSizes[R], k.push(N), o[R] = wt(`output${R}`, i, N.length), h.push({ dims: k[R], dataType: e[0].dataType });
          }
          u.push({ type: 12, data: p }, ...vt(r, ...k));
          let B = (R) => `
  ${R.registerUniform("input_size", "u32").registerUniform("size_in_split_axis", "u32", p.length).declareVariables(d, ...o)}
  ${Wo(p.length)}
  ${Vo(o)}

  ${R.mainStart()}
    ${R.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.input_size")}

    var indices = ${d.offsetToIndices("global_idx")};
    var index = ${d.indicesGet("indices", a)};
    let output_number = calculateOutputIndex(index);
    if (output_number != 0) {
      index -= ${Mt("uniforms.size_in_split_axis", "output_number - 1u", p.length)};
      ${d.indicesSet("indices", a, "index")};
    }
    writeBufferData(output_number, indices, global_idx);
  }`;
          return { name: "Split", shaderCache: { hint: t.cacheKey, inputDependencies: ["rank"] }, getShaderSource: B, getRunData: () => ({ outputs: h, dispatchGroup: { x: Math.ceil(n / 64) }, programUniforms: u }) };
        }, cd = (e, t) => {
          Jc(e.inputs);
          let r = e.inputs.length === 1 ? t : Zc(e.inputs, t);
          e.compute(Go(e.inputs, r), { inputs: [0] });
        }, Ko = (e) => {
          let t = e.axis, r = e.splitSizes, n = e.numOutputs < 0 ? r.length : e.numOutputs;
          if (n !== r.length) throw new Error("numOutputs and splitSizes lengh must be equal");
          return ot({ axis: t, numOutputs: n, splitSizes: r });
        };
      }), hd, Ho, qo, md, _d = w(() => {
        Pt(), ti(), Uo(), pd(), Vs(), hd = (e, t) => {
          if (t.doRotary && e.length <= 7) throw new Error("cos_cache and sin_cache inputs are required if do_rotary is specified");
          let r = e[0], n = e[1], i = e[2], a = e[3], o = e[4];
          if (t.localWindowSize !== -1) throw new Error("Local attention is not supported");
          if (t.softcap !== 0) throw new Error("Softcap is not supported");
          if (t.rotaryInterleaved !== 0) throw new Error("Rotary interleaved is not supported");
          if (t.smoothSoftmax) throw new Error("Smooth softmax is not supported");
          if (r.dims.length !== 3 && r.dims.length !== 5) throw new Error("Input query is expected to have 3 or 5 dimensions");
          let d = !1, p = r.dims[0], h = r.dims[1], k = r.dims.length === 3 ? d ? r.dims[2] / 3 : r.dims[2] : t.numHeads * r.dims[4], S = h, u = 0, B = !n || n.dims.length === 0, R = Math.floor(B ? k / (t.numHeads + 2 * t.kvNumHeads) : k / t.numHeads);
          B && (k = R * t.numHeads);
          let N = a && a.dims.length !== 0, Z = o && o.dims.length !== 0;
          if (N && a.dims.length === 4 && a.dims[0] === p && a.dims[1] !== t.kvNumHeads && a.dims[2] === t.kvNumHeads && a.dims[3] === R) throw new Error("BSNH pastKey/pastValue is not supported");
          if (N && Z) {
            if (a.dims.length !== 4) throw new Error('Input "past_key" is expected to have 4 dimensions');
            if (o.dims.length !== 4) throw new Error('Input "past_value" is expected to have 4 dimensions');
            u = a.dims[2];
          } else if (N || Z) throw new Error('Input "past_key" and "past_value" shall be both present or both absent');
          let te = 1;
          if (n && n.dims.length > 0) {
            if (r.dims.length !== 3) throw new Error('Input "query" is expected to have 3 dimensions when key is given');
            if (n.dims.length < 3 || n.dims.length > 5) throw new Error('Input "key" is expected to have 3, 4, or 5 dimensions');
            if (r.dims[0] !== n.dims[0]) throw new Error('Input "query" and "key" shall have same dim 0 (batch size)');
            if (n.dims.length === 3) {
              if (r.dims[2] % n.dims[2] !== 0) throw new Error('Dimension 2 of "query" should be a multiple of "key"');
              S = n.dims[1];
            } else if (n.dims.length === 5) {
              if (n.dims[2] !== t.numHeads || n.dims[3] !== 2 || n.dims[4] !== R) throw new Error('Expect "key" shape (batch_size, kv_sequence_length, num_heads, 2, head_size) for packed kv');
              if (i) throw new Error('Expect "value" be none when "key" has packed kv format.');
              S = n.dims[1];
            } else {
              if (n.dims[1] !== t.numHeads || n.dims[3] !== R) throw new Error('Expect "key" shape (batch_size, num_heads, kv_sequence_length, head_size) for past_key');
              S = n.dims[2];
            }
          } else {
            if (r.dims.length !== 3 && r.dims.length !== 5) throw new Error('Input "query" is expected to have 3 or 5 dimensions when key is empty');
            if (r.dims.length === 5 && (r.dims[2] !== t.numHeads || r.dims[3] !== 3)) throw new Error('Expect "query" shape (batch_size, kv_sequence_length, num_heads, 3, head_size) for packed kv');
            te = 3;
          }
          let Q = 0, _e = !1, me = t.kvNumHeads ? R * t.kvNumHeads : k;
          if (i && i.dims.length > 0) {
            if (i.dims.length !== 3 && i.dims.length !== 4) throw new Error('Input "value" is expected to have 3 or 4 dimensions');
            if (r.dims[0] !== i.dims[0]) throw new Error('Input "query" and "value" shall have same dim 0 (batch_size)');
            if (i.dims.length === 3) {
              if (S !== i.dims[1]) throw new Error('Input "key" and "value" shall have the same dim 1 (kv_sequence_length)');
              me = i.dims[2];
            } else {
              if (S !== i.dims[2]) throw new Error('Input "past_key" and "past_value" shall have the same dim 2 (kv_sequence_length)');
              me = i.dims[1] * i.dims[3], _e = !0;
            }
          }
          let ye = e.length > 4 ? e[5] : void 0;
          if (ye && ye.dims.length !== 1 && ye.dims[0] !== p) throw new Error('Input "seqlens" is expected to have 1 dimension and the same dim 0 as batch_size');
          return { batchSize: p, sequenceLength: h, pastSequenceLength: u, kvSequenceLength: S, totalSequenceLength: -1, maxSequenceLength: -1, inputHiddenSize: 0, hiddenSize: k, vHiddenSize: me, headSize: R, vHeadSize: Math.floor(me / t.kvNumHeads), numHeads: t.numHeads, kvNumHeads: t.kvNumHeads, nReps: t.numHeads / t.kvNumHeads, pastPresentShareBuffer: !1, maskType: Q, scale: t.scale, broadcastResPosBias: !1, passPastInKv: _e, qkvFormat: te };
        }, Ho = ot({ perm: [0, 2, 1, 3] }), qo = (e, t, r) => {
          let n = t, i = r.kvNumHeads;
          return t.dims.length === 3 && r.kvSequenceLength !== 0 && (n = t.reshape([r.batchSize, r.kvSequenceLength, i, r.headSize]), n = e.compute(ss(n, Ho.perm), { inputs: [n], outputs: [-1] })[0]), n;
        }, md = (e, t) => {
          var Z;
          let r = hd(e.inputs, t);
          if (e.inputs[0].dims.length === 5) throw new Error("Packed QKV is not implemented");
          if (((Z = e.inputs[1]) == null ? void 0 : Z.dims.length) === 5) throw new Error("Packed KV is not implemented");
          let n = e.inputs[0], i = e.inputs[1] && e.inputs[1].dims.length > 0 ? e.inputs[1] : void 0, a = e.inputs[2] && e.inputs[2].dims.length > 0 ? e.inputs[2] : void 0, o = e.inputs[3] && e.inputs[3].dims.length !== 0 ? e.inputs[3] : void 0, d = e.inputs[4] && e.inputs[4].dims.length !== 0 ? e.inputs[4] : void 0, p = e.inputs.length > 4 ? e.inputs[5] : void 0, h = e.inputs.length > 5 ? e.inputs[6] : void 0, k = r.kvNumHeads ? r.kvNumHeads : r.numHeads, S = ot({ axis: 2, numOutputs: 3, splitSizes: [r.numHeads * r.headSize, k * r.headSize, k * r.headSize] }), [u, B, R] = !i && !a ? e.compute(Go([n], S), { inputs: [n], outputs: [-1, -1, -1] }) : [n, i, a], N = Rn(e, r.batchSize, r.numHeads, r.sequenceLength, r.headSize, u, void 0, 0);
          vn(e, N, qo(e, B, r), qo(e, R, r), void 0, void 0, o, d, void 0, r, p, h);
        };
      }), Xo, fd, gd, wd, ep = w(() => {
        zt(), Bt(), Vs(), Qt(), Xo = (e, t, r, n, i, a, o, d) => {
          let p = yr(a), h = p === 1 ? "f32" : `vec${p}f`, k = p === 1 ? "vec2f" : `mat2x${p}f`, S = i * o, u = 64;
          S === 1 && (u = 256);
          let B = [i, o, a / p], R = [i, o, 2], N = ["rank", "type", "type"], Z = [];
          Z.push(...vt(B, R));
          let te = (Q) => {
            let _e = ze("x", t.dataType, 3, p), me = ze("scale", r.dataType, r.dims), ye = ze("bias", n.dataType, n.dims), Ae = wt("output", 1, 3, 2), Ie = [_e, me, ye, Ae];
            return `
  var<workgroup> workgroup_shared : array<${k}, ${u}>;
  const workgroup_size = ${u}u;
  ${Q.declareVariables(...Ie)}
  ${Q.mainStart(u)}
    let batch = workgroup_index / uniforms.x_shape[1];
    let channel = workgroup_index % uniforms.x_shape[1];
    let hight = uniforms.x_shape[2];
    // initialize workgroup memory
    var sum = ${h}(0);
    var squared_sum = ${h}(0);
    for (var h = local_idx; h < hight; h += workgroup_size) {
      let value = ${h}(${_e.get("batch", "channel", "h")});
      sum += value;
      squared_sum += value * value;
    }
    workgroup_shared[local_idx] = ${k}(sum, squared_sum);
    workgroupBarrier();

    for (var currSize = workgroup_size >> 1;  currSize > 0; currSize = currSize >> 1) {
      if (local_idx < currSize) {
        workgroup_shared[local_idx] = workgroup_shared[local_idx] + workgroup_shared[local_idx + currSize];
      }
      workgroupBarrier();
    }
    if (local_idx == 0) {
      let sum_final = ${qr("workgroup_shared[0][0]", p)} / f32(hight * ${p});
      let squared_sum_final = ${qr("workgroup_shared[0][1]", p)} / f32(hight * ${p});

      let inv_std_dev = inverseSqrt(squared_sum_final - sum_final * sum_final + f32(${d}));
      let channel_scale = inv_std_dev * f32(scale[channel]);
      let channel_shift = f32(bias[channel]) - sum_final * channel_scale;
      output[workgroup_index] = vec2f(channel_scale, channel_shift);
    }
  }`;
          };
          return e.compute({ name: "InstanceNormComputeChannelScaleShift", shaderCache: { hint: `${p};${d};${u}`, inputDependencies: N }, getRunData: () => ({ outputs: [{ dims: R, dataType: 1 }], dispatchGroup: { x: S }, programUniforms: Z }), getShaderSource: te }, { inputs: [t, r, n], outputs: [-1] })[0];
        }, fd = (e, t, r) => {
          let n = t[0].dims, i = n, a = 2, o = n[0], d = n[1], p = $e.sizeFromDimension(n, a), h = yr(p), k = $e.size(i) / h, S = Xo(e, t[0], t[1], t[2], o, p, d, r.epsilon), u = [o, d, p / h], B = [o, d], R = ["type", "none"], N = (Z) => {
            let te = ze("x", t[0].dataType, u.length, h), Q = ze("scale_shift", 1, B.length, 2), _e = wt("output", t[0].dataType, u.length, h), me = [te, Q, _e];
            return `
  ${Z.registerUniform("output_size", "u32").declareVariables(...me)}
  ${Z.mainStart()}
  ${Z.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let outputIndices = ${_e.offsetToIndices("global_idx")};
      let batch = outputIndices[0];
      let channel = outputIndices[1];
      let scale_shift = ${Q.getByIndices("vec2<u32>(batch, channel)")};
      let value = ${te.getByOffset("global_idx")} * ${_e.type.value}(scale_shift.x) + ${_e.type.value}(scale_shift.y);
      ${_e.setByOffset("global_idx", "value")};
  }`;
          };
          e.compute({ name: "InstanceNormalization", shaderCache: { hint: `${h}`, inputDependencies: R }, getRunData: () => ({ outputs: [{ dims: i, dataType: t[0].dataType }], dispatchGroup: { x: Math.ceil(k / 64) }, programUniforms: [{ type: 12, data: k }, ...vt(u, B, u)] }), getShaderSource: N }, { inputs: [t[0], S] });
        }, gd = (e, t, r) => {
          let n = t[0].dims, i = n, a = n[0], o = n[n.length - 1], d = $e.sizeFromDimension(n, 1) / o, p = yr(o), h = $e.size(i) / p, k = [{ type: 12, data: d }, { type: 12, data: Math.floor(o / p) }], S = ["type", "type"], u = !1, B = [0, n.length - 1];
          for (let te = 0; te < n.length - 2; te++) u = u || n[te + 1] !== 1, B.push(te + 1);
          u = u && n[n.length - 1] !== 1;
          let R = u ? e.compute(ss(e.inputs[0], B), { inputs: [e.inputs[0]], outputs: [-1] })[0] : e.inputs[0].reshape(Array.from({ length: n.length }, (te, Q) => n[B[Q]])), N = Xo(e, R, t[1], t[2], a, d, o, r.epsilon), Z = (te) => {
            let Q = er(t[0].dataType), _e = p === 1 ? "vec2f" : `mat${p}x2f`, me = (Ie) => {
              let Ge = Ie === 0 ? "x" : "y", lt = p === 1 ? "f32" : `vec${p}f`;
              switch (p) {
                case 1:
                  return `${Q}(${lt}(scale.${Ge}))`;
                case 2:
                  return `vec2<${Q}>(${lt}(scale[0].${Ge}, scale[1].${Ge}))`;
                case 4:
                  return `vec4<${Q}>(${lt}(scale[0].${Ge}, scale[1].${Ge}, scale[2].${Ge}, scale[3].${Ge}))`;
                default:
                  throw new Error(`Not supported compoents ${p}`);
              }
            }, ye = ze("input", t[0].dataType, t[0].dims, p), Ae = wt("output", t[0].dataType, i, p);
            return `
  @group(0) @binding(0) var<storage, read> input : array<${ye.type.storage}>;
  @group(0) @binding(1) var<storage, read> scale_input : array<${_e}>;
  @group(0) @binding(2) var<storage, read_write> output : array<${Ae.type.storage}>;
  struct Uniforms {H: u32, C : u32};
  @group(0) @binding(3) var<uniform> uniforms: Uniforms;

  ${te.mainStart()}
    let current_image_number = global_idx / (uniforms.C * uniforms.H);
    let current_channel_number = global_idx % uniforms.C;

    let scale_offset = current_image_number * uniforms.C + current_channel_number;
    let scale = scale_input[scale_offset];
    output[global_idx] = fma(input[global_idx], ${me(0)}, ${me(1)});
  }`;
          };
          e.compute({ name: "InstanceNormalizationNHWC", shaderCache: { hint: `${p}`, inputDependencies: S }, getRunData: () => ({ outputs: [{ dims: i, dataType: t[0].dataType }], dispatchGroup: { x: Math.ceil(h / 64) }, programUniforms: k }), getShaderSource: Z }, { inputs: [t[0], N] });
        }, wd = (e, t) => {
          t.format === "NHWC" ? gd(e, e.inputs, t) : fd(e, e.inputs, t);
        };
      }), yd, Md, bd, tp = w(() => {
        zt(), Bt(), Qt(), yd = (e) => {
          if (!e || e.length < 2) throw new Error("layerNorm requires at least 2 inputs.");
        }, Md = (e, t, r) => {
          let n = t.simplified, i = e[0].dims, a = e[1], o = !n && e[2], d = i, p = $e.normalizeAxis(t.axis, i.length), h = $e.sizeToDimension(i, p), k = $e.sizeFromDimension(i, p), S = $e.size(a.dims), u = o ? $e.size(o.dims) : 0;
          if (S !== k || o && u !== k) throw new Error(`Size of X.shape()[axis:] == ${k}.
       Size of scale and bias (if provided) must match this.
       Got scale size of ${S} and bias size of ${u}`);
          let B = [];
          for (let ye = 0; ye < i.length; ++ye) ye < p ? B.push(i[ye]) : B.push(1);
          let R = yr(k), N = ["type", "type"], Z = [{ type: 12, data: h }, { type: 1, data: k }, { type: 12, data: Math.floor(k / R) }, { type: 1, data: t.epsilon }];
          o && N.push("type");
          let te = r > 1, Q = r > 2, _e = (ye) => {
            let Ae = er(e[0].dataType), Ie = [ze("x", e[0].dataType, e[0].dims, R), ze("scale", a.dataType, a.dims, R)];
            o && Ie.push(ze("bias", o.dataType, o.dims, R)), Ie.push(wt("output", e[0].dataType, d, R)), te && Ie.push(wt("mean_data_output", 1, B)), Q && Ie.push(wt("inv_std_output", 1, B));
            let Ge = [{ name: "norm_count", type: "u32" }, { name: "norm_size", type: "f32" }, { name: "norm_size_vectorized", type: "u32" }, { name: "epsilon", type: "f32" }];
            return `
  ${ye.registerUniforms(Ge).declareVariables(...Ie)}
  ${ye.mainStart()}
    ${ye.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.norm_count")}
    let offset = global_idx * uniforms.norm_size_vectorized;
    var mean_vector = ${Es("f32", R)};
    var mean_square_vector = ${Es("f32", R)};

    for (var h: u32 = 0u; h < uniforms.norm_size_vectorized; h++) {
      let value = ${Or(Ae, R, "x[h + offset]")};
      mean_vector += value;
      mean_square_vector += value * value;
    }
    let mean = ${qr("mean_vector", R)} / uniforms.norm_size;
    let inv_std_dev = inverseSqrt(${qr("mean_square_vector", R)} / uniforms.norm_size ${n ? "" : "- mean * mean"} + uniforms.epsilon);

    for (var j: u32 = 0; j < uniforms.norm_size_vectorized; j++) {
      let f32input = ${Or(Ae, R, "x[j + offset]")};
      let f32scale = ${Or(Ae, R, "scale[j]")};
      output[j + offset] = ${Ie[0].type.value}((f32input ${n ? "" : "- mean"}) * inv_std_dev * f32scale
        ${o ? `+ ${Or(Ae, R, "bias[j]")}` : ""}
      );
    }

    ${te ? "mean_data_output[global_idx] = mean" : ""};
    ${Q ? "inv_std_output[global_idx] = inv_std_dev" : ""};
  }`;
          }, me = [{ dims: d, dataType: e[0].dataType }];
          return te && me.push({ dims: B, dataType: 1 }), Q && me.push({ dims: B, dataType: 1 }), { name: "LayerNormalization", shaderCache: { hint: `${R};${r};${n}`, inputDependencies: N }, getRunData: () => ({ outputs: me, dispatchGroup: { x: Math.ceil(h / 64) }, programUniforms: Z }), getShaderSource: _e };
        }, bd = (e, t) => {
          yd(e.inputs), e.compute(Md(e.inputs, t, e.outputCount));
        };
      }), Qo, vd, rp = w(() => {
        Bt(), Po(), ui(), Qo = (e) => {
          if (!e || e.length !== 2) throw new Error("MatMul requires 2 inputs.");
          if (e[0].dims[e[0].dims.length - 1] !== e[1].dims[e[1].dims.length - 2]) throw new Error("shared dimension does not match.");
        }, vd = (e) => {
          Qo(e.inputs);
          let t = rr.calcShape(e.inputs[0].dims, e.inputs[1].dims, !0);
          if (!t) throw new Error("Can't use matmul on the given tensors");
          let r = t[t.length - 1], n = e.inputs[0].dims[e.inputs[0].dims.length - 1];
          if (r < 8 && n < 8) e.compute(ii(e.inputs, { activation: "" }, t));
          else {
            let i = t[t.length - 2], a = $e.size(e.inputs[0].dims.slice(0, -2)), o = $e.size(e.inputs[1].dims.slice(0, -2));
            if (a !== 1 && i === 1 && o === 1) {
              let d = e.inputs[0].reshape([1, a, n]), p = e.inputs[1].reshape([1, n, r]), h = [1, a, r], k = [d, p];
              e.compute(li(k, { activation: "" }, t, h), { inputs: k });
            } else e.compute(li(e.inputs, { activation: "" }, t));
          }
        };
      }), Yo, xd, Td, _r, sp, Lp = w(() => {
        zt(), Bt(), Pt(), Qt(), Yo = (e, t) => {
          if (e.length < 3 || e.length > 4) throw new Error("MatMulNBits requires 3 or 4 inputs");
          let r = e[0], n = r.dims.length;
          if (r.dims[n - 1] !== t.k) throw new Error("The last dim of input shape does not match the k value");
          let i = Math.floor((t.k + t.blockSize - 1) / t.blockSize), a = t.blockSize / 8 * t.bits, o = e[1];
          if (!$e.areEqual(o.dims, [t.n, i, a])) throw new Error("The second inputs must be 3D tensor with shape N X nBlocksPerCol X blobSize");
          let d = e[2].dims;
          if ($e.size(d) !== t.n * i) throw new Error("scales input size error.");
          if (e.length === 4) {
            let p = e[3].dims, h = t.bits > 4 ? t.n * i : t.n * Math.floor((i + 1) / 2);
            if ($e.size(p) !== h) throw new Error("zeroPoints input size error.");
          }
        }, xd = (e, t) => {
          let r = e[0].dims, n = r.length, i = r[n - 2], a = t.k, o = t.n, d = r.slice(0, n - 2), p = $e.size(d), h = e[1].dims[2] / 4, k = e[0].dataType, S = yr(t.k), u = yr(h), B = yr(o), R = d.concat([i, o]), N = i > 1 && o / B % 2 === 0 ? 2 : 1, Z = $e.size(R) / B / N, te = 64, Q = [], _e = [p, i, a / S], me = $e.convertShape(e[1].dims).slice();
          me.splice(-1, 1, h / u), Q.push(...vt(_e)), Q.push(...vt(me)), Q.push(...vt(e[2].dims)), e.length === 4 && Q.push(...vt($e.convertShape(e[3].dims)));
          let ye = [p, i, o / B];
          Q.push(...vt(ye));
          let Ae = (Ie) => {
            let Ge = _e.length, lt = ze("a", e[0].dataType, Ge, S), Tt = ze("b", 12, me.length, u), Kt = ze("scales", e[2].dataType, e[2].dims.length), Yt = [lt, Tt, Kt], Ct = e.length === 4 ? ze("zero_points", 12, e[3].dims.length) : void 0;
            Ct && Yt.push(Ct);
            let Jt = ye.length, $t = wt("output", e[0].dataType, Jt, B), jt = er(e[0].dataType), vr = (() => {
              switch (S) {
                case 1:
                  return `array<${jt}, 8>`;
                case 2:
                  return `mat4x2<${jt}>`;
                case 4:
                  return `mat2x4<${jt}>`;
                default:
                  throw new Error(`${S}-component is not supported.`);
              }
            })(), Ht = () => {
              let it = `
          // reuse a data
            var input_offset = ${lt.indicesToOffset(`${lt.type.indices}(batch, row, word_offset)`)};
            var a_data: ${vr};
            for (var j: u32 = 0; j < ${8 / S}; j++) {
              a_data[j] = ${lt.getByOffset("input_offset")};
              input_offset++;
            }
          `;
              for (let Et = 0; Et < B * N; Et++) it += `
            b_value = ${u === 1 ? `b${Et}_data` : `b${Et}_data[i]`};
            b_value_lower = unpack4xU8(b_value & b_mask);
            b_value_upper = unpack4xU8((b_value >> 4) & b_mask);
            b_quantized_values = ${vr}(${Array.from({ length: 4 }, (cr, Lr) => `${jt}(b_value_lower[${Lr}]), ${jt}(b_value_upper[${Lr}])`).join(", ")});
            b_dequantized_values = ${S === 1 ? `${vr}(${Array.from({ length: 8 }, (cr, Lr) => `(b_quantized_values[${Lr}] - ${Ct ? `zero_point${Et}` : "zero_point"}) * scale${Et}`).join(", ")});` : `(b_quantized_values - ${vr}(${Array(8).fill(`${Ct ? `zero_point${Et}` : "zero_point"}`).join(",")})) * scale${Et};`};
            workgroup_shared[local_id.x * ${N} + ${Math.floor(Et / B)}]${B > 1 ? `[${Et % B}]` : ""} += ${Array.from({ length: 8 / S }, (cr, Lr) => `${S === 1 ? `a_data[${Lr}] * b_dequantized_values[${Lr}]` : `dot(a_data[${Lr}], b_dequantized_values[${Lr}])`}`).join(" + ")};
          `;
              return it;
            }, Gt = () => {
              let it = `
            var col_index = col * ${B};
            ${Ct ? `
            let zero_point_bytes_per_col = (nBlocksPerCol + 1) / 2;
            var zero_point_byte_count: u32;
            var zero_point_word_index: u32;
            var zero_point_byte_offset: u32;
            let zero_point_nibble_offset: u32 = block & 0x1u;
            var zero_point_bits_offset: u32;
            var zero_point_word: u32;` : `
            // The default zero point is 8 for unsigned 4-bit quantization.
            let zero_point = ${jt}(8);`}
            `;
              for (let Et = 0; Et < B * N; Et++) it += `
            let scale${Et} = ${Kt.getByOffset("col_index * nBlocksPerCol + block")};
            ${Ct ? `
            zero_point_byte_count = col_index * zero_point_bytes_per_col + (block >> 0x1u);
            zero_point_word_index = zero_point_byte_count >> 0x2u;
            zero_point_byte_offset = zero_point_byte_count & 0x3u;
            zero_point_bits_offset = (zero_point_byte_offset << 3) + (zero_point_nibble_offset << 2);
            zero_point_word = ${Ct.getByOffset("zero_point_word_index")} >> zero_point_bits_offset;
            let zero_point${Et} = ${jt}((zero_point_word) & 0xFu);` : ""}
            col_index += 1;`;
              return it;
            }, Cr = () => {
              let it = `col_index = col * ${B};`;
              for (let Et = 0; Et < B * N; Et++) it += `
            let b${Et}_data = ${Tt.getByIndices(`${Tt.type.indices}(col_index, block, word)`)};
            col_index += 1;`;
              return it += `
            var b_value: u32;
            let b_mask: u32 = 0x0F0F0F0Fu;
            var b_value_lower: vec4<u32>;
            var b_value_upper: vec4<u32>;
            var b_quantized_values: ${vr};
            var b_dequantized_values: ${vr};`, it;
            };
            return `
        var<workgroup> workgroup_shared: array<${$t.type.value}, ${N * te}>;
        ${Ie.declareVariables(...Yt, $t)}
        ${Ie.mainStart([te, 1, 1])}
          let output_indices = ${$t.offsetToIndices(`(global_idx / ${te}) * ${N}`)};
          let col = output_indices[2];
          let row = output_indices[1];
          let batch = output_indices[0];
          let nBlocksPerCol = uniforms.b_shape[1];

          for (var block = local_id.x; block < nBlocksPerCol; block += ${te}) {
            //process one block
            var word_offset: u32 = block * ${t.blockSize / S};
            ${Gt()}
            for (var word: u32 = 0; word < ${h}; word += ${u}) {
              ${Cr()}
              for (var i: u32 = 0; i < ${u}; i++) {
                ${Ht()}
                word_offset += ${8 / S};
              }
            }
          }
          workgroupBarrier();

          if (local_id.x < ${N}) {
            var output_value: ${$t.type.value} = ${$t.type.value}(0);
            var workgroup_shared_offset: u32 = local_id.x;
            for (var b: u32 = 0u; b < ${te}u; b++) {
              output_value += workgroup_shared[workgroup_shared_offset];
              workgroup_shared_offset += ${N};
            }
            ${$t.setByIndices(`${$t.type.indices}(batch, row, col + local_id.x)`, "output_value")};
          }
        }`;
          };
          return { name: "MatMulNBits", shaderCache: { hint: `${t.blockSize};${t.bits};${S};${u};${B};${N};${te}`, inputDependencies: Array(e.length).fill("rank") }, getRunData: () => ({ outputs: [{ dims: R, dataType: k }], dispatchGroup: { x: Z }, programUniforms: Q }), getShaderSource: Ae };
        }, Td = (e, t) => {
          let r = e[0].dims, n = r.length, i = r[n - 2], a = t.k, o = t.n, d = r.slice(0, n - 2), p = $e.size(d), h = e[1].dims[2] / 4, k = e[0].dataType, S = yr(t.k), u = yr(h), B = d.concat([i, o]), R = 128, N = o % 8 === 0 ? 8 : o % 4 === 0 ? 4 : 1, Z = R / N, te = Z * u * 8, Q = te / S, _e = te / t.blockSize, me = $e.size(B) / N, ye = [], Ae = [p, i, a / S], Ie = $e.convertShape(e[1].dims).slice();
          Ie.splice(-1, 1, h / u), ye.push(...vt(Ae)), ye.push(...vt(Ie)), ye.push(...vt(e[2].dims)), e.length === 4 && ye.push(...vt($e.convertShape(e[3].dims)));
          let Ge = [p, i, o];
          ye.push(...vt(Ge));
          let lt = (Tt) => {
            let Kt = Ae.length, Yt = ze("a", e[0].dataType, Kt, S), Ct = ze("b", 12, Ie.length, u), Jt = ze("scales", e[2].dataType, e[2].dims.length), $t = [Yt, Ct, Jt], jt = e.length === 4 ? ze("zero_points", 12, e[3].dims.length) : void 0;
            jt && $t.push(jt);
            let vr = Ge.length, Ht = wt("output", e[0].dataType, vr), Gt = er(e[0].dataType), Cr = () => {
              switch (S) {
                case 1:
                  return `
          let a_data0 = vec4<${Gt}>(sub_a[word_offset], sub_a[word_offset + 1], sub_a[word_offset + 2], sub_a[word_offset + 3]);
          let a_data1 = vec4<${Gt}>(sub_a[word_offset + 4], sub_a[word_offset + 5], sub_a[word_offset + 6], sub_a[word_offset + 7]);`;
                case 2:
                  return `
          let a_data0 = vec4<${Gt}>(sub_a[word_offset], sub_a[word_offset + 1]);
          let a_data1 = vec4<${Gt}>(sub_a[word_offset + 2], sub_a[word_offset + 3]);`;
                case 4:
                  return `
          let a_data0 = sub_a[word_offset];
          let a_data1 = sub_a[word_offset + 1];`;
                default:
                  throw new Error(`${S}-component is not supported.`);
              }
            };
            return `
        var<workgroup> sub_a: array<${Yt.type.value}, ${Q}>;
        var<workgroup> inter_results: array<array<${Ht.type.value}, ${Z}>, ${N}>;
        ${Tt.declareVariables(...$t, Ht)}
        ${Tt.mainStart([Z, N, 1])}
          let output_indices = ${Ht.offsetToIndices(`workgroup_index * ${N}`)};
          let col = output_indices[2];
          let row = output_indices[1];
          let batch = output_indices[0];
          let n_blocks_per_col = uniforms.b_shape[1];
          let num_tiles =  (n_blocks_per_col - 1) / ${_e} + 1;

          // Loop over shared dimension.
          for (var tile: u32 = 0; tile < num_tiles; tile += 1) {
            let a_col_start = tile * ${Q};
            // load one tile A data into shared memory.
            for (var a_offset = local_idx; a_offset < ${Q}; a_offset += ${R})
            {
              let a_col = a_col_start + a_offset;
              if (a_col < uniforms.a_shape[2])
              {
                sub_a[a_offset] = ${Yt.getByIndices(`${Yt.type.indices}(batch, row, a_col)`)};
              } else {
                sub_a[a_offset] = ${Yt.type.value}(0);
              }
            }
            workgroupBarrier();

            // each thread process one block
            let b_row = col + local_id.y;
            let block = tile * ${_e} + local_id.x;
            ${jt ? `
            let zero_point_bytes_per_col = (n_blocks_per_col + 1) / 2;
            let zero_point_byte_count = b_row * zero_point_bytes_per_col + (block >> 0x1u);
            let zero_point_word_index = zero_point_byte_count >> 0x2u;
            let zero_point_byte_offset = zero_point_byte_count & 0x3u;
            let zero_point_nibble_offset: u32 = block & 0x1u;
            let zero_point_bits_offset = (zero_point_byte_offset << 3) + (zero_point_nibble_offset << 2);
            let zero_point_word = ${jt.getByOffset("zero_point_word_index")} >> zero_point_bits_offset;
            let zero_point = ${Gt}((zero_point_word) & 0xFu);` : `
            // The default zero point is 8 for unsigned 4-bit quantization.
            let zero_point = ${Gt}(8);`}
            let scale = ${Jt.getByOffset("b_row * n_blocks_per_col + block")};
            let b_data = ${Ct.getByIndices(`${Ct.type.indices}(b_row, block, 0)`)};
            var word_offset = local_id.x * ${t.blockSize / S};
            for (var i: u32 = 0; i < ${u}; i++) {
              ${Cr()}
              let b_value = ${u === 1 ? "b_data" : "b_data[i]"};
              let b_value_lower = unpack4xU8(b_value & 0x0F0F0F0Fu);
              let b_value_upper = unpack4xU8((b_value >> 4) & 0x0F0F0F0Fu);
              let b_quantized_values = mat2x4<${Gt}>(${Array.from({ length: 4 }, (it, Et) => `${Gt}(b_value_lower[${Et}]), ${Gt}(b_value_upper[${Et}])`).join(", ")});
              let b_dequantized_values = (b_quantized_values - mat2x4<${Gt}>(${Array(8).fill("zero_point").join(",")})) * scale;
              inter_results[local_id.y][local_id.x] += ${Array.from({ length: 2 }, (it, Et) => `${`dot(a_data${Et}, b_dequantized_values[${Et}])`}`).join(" + ")};
              word_offset += ${8 / S};
            }
            workgroupBarrier();
          }

          if (local_idx < ${N}) {
            var output_value: ${Ht.type.value} = ${Ht.type.value}(0);
            for (var b = 0u; b < ${Z}; b++) {
              output_value += inter_results[local_idx][b];
            }
            if (col + local_idx < uniforms.output_shape[2])
            {
              ${Ht.setByIndices(`${Ht.type.indices}(batch, row, col + local_idx)`, "output_value")}
            }
          }
        }`;
          };
          return { name: "BlockwiseMatMulNBits32", shaderCache: { hint: `${t.blockSize};${S};${u};${Z};${N}`, inputDependencies: Array(e.length).fill("rank") }, getRunData: () => ({ outputs: [{ dims: B, dataType: k }], dispatchGroup: { x: me }, programUniforms: ye }), getShaderSource: lt };
        }, _r = (e, t) => {
          Yo(e.inputs, t), t.blockSize === 32 && e.adapterInfo.isVendor("intel") && e.adapterInfo.isArchitecture("gen-12lp") ? e.compute(Td(e.inputs, t)) : e.compute(xd(e.inputs, t));
        }, sp = (e) => ot(e);
      }), np, Jo, Ed, Pd, Cd, kd, Zo, ea, ip, op = w(() => {
        zt(), Bt(), Qt(), np = (e) => {
          if (!e || e.length < 1) throw new Error("Too few inputs");
          if (e[0].dataType !== 1 && e[0].dataType !== 10) throw new Error("Input type must be float or float16.");
          if (e.length >= 2) {
            let t = e[0].dims.length * 2 === e[1].dims[0];
            if (e.length === 4 && (t = e[3].dims[0] * 2 === e[1].dims[0]), !t) throw new Error("The pads should be a 1D tensor of shape [2 * input_rank] or [2 * num_axes].");
          }
        }, Jo = (e, t, r) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
            k = i32(${e.indicesGet("indices", i)}) - ${Mt("uniforms.pads", i, r)};
            if (k < 0) {
              break;
            }
            if (k >= i32(${Mt("uniforms.x_shape", i, t)})) {
              break;
            }
            offset += k * i32(${Mt("uniforms.x_strides", i, t)});
        `;
          return `
          value = ${e.type.value}(uniforms.constant_value);
          for (var i = 0; i < 1; i++) {
            var offset = 0;
            var k = 0;
            ${n}
            value = x[offset];
          }
      `;
        }, Ed = (e, t, r) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
                k = i32(${e.indicesGet("indices", i)}) - ${Mt("uniforms.pads", i, r)};
                if (k < 0) {
                  k = -k;
                }
                {
                  let _2n_1 = 2 * (i32(${Mt("uniforms.x_shape", i, t)}) - 1);
                  k = k % _2n_1;
                  if(k >= i32(${Mt("uniforms.x_shape", i, t)})) {
                    k = _2n_1 - k;
                  }
                }
                offset += k * i32(${Mt("uniforms.x_strides", i, t)});
            `;
          return `
              var offset = 0;
              var k = 0;
              ${n}
              value = x[offset];
          `;
        }, Pd = (e, t, r) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
                k = i32(${e.indicesGet("indices", i)}) - ${Mt("uniforms.pads", i, r)};
                if (k < 0) {
                  k = 0;
                }
                if (k >= i32(${Mt("uniforms.x_shape", i, t)})) {
                  k = i32(${Mt("uniforms.x_shape", i, t)}) - 1;
                }
                offset += k * i32(${Mt("uniforms.x_strides", i, t)});
            `;
          return `
              var offset = 0;
              var k = 0;
              ${n}
              value = x[offset];
          `;
        }, Cd = (e, t, r) => {
          let n = "";
          for (let i = t - 1; i >= 0; --i) n += `
                k = i32(${e.indicesGet("indices", i)}) - ${Mt("uniforms.pads", i, r)};
                if (k < 0)  {
                  k += i32(${Mt("uniforms.x_shape", i, t)}]);
                }
                if (k >= i32(${Mt("uniforms.x_shape", i, t)})) {
                  k -= i32(${Mt("uniforms.x_shape", i, t)});
                }
                offset += k * i32(${Mt("uniforms.x_strides", i, t)});
            `;
          return `
              var offset = 0;
              var k = 0;
              ${n}
              value = x[offset];
          `;
        }, kd = (e, t, r) => {
          switch (r.mode) {
            case 0:
              return Jo(e, t, r.pads.length);
            case 1:
              return Ed(e, t, r.pads.length);
            case 2:
              return Pd(e, t, r.pads.length);
            case 3:
              return Cd(e, t, r.pads.length);
            default:
              throw new Error("Invalid mode");
          }
        }, Zo = (e, t) => {
          let r = $e.padShape(e[0].dims.slice(), t.pads), n = e[0].dims, i = $e.size(r), a = [{ type: 12, data: i }, { type: 6, data: t.pads }], o = e.length >= 3 && e[2].data;
          t.mode === 0 && a.push({ type: o ? e[2].dataType : 1, data: t.value }), a.push(...vt(e[0].dims, r));
          let d = ["rank"], p = (h) => {
            let k = wt("output", e[0].dataType, r.length), S = ze("x", e[0].dataType, n.length), u = S.type.value, B = kd(k, n.length, t), R = [{ name: "output_size", type: "u32" }, { name: "pads", type: "i32", length: t.pads.length }];
            return t.mode === 0 && R.push({ name: "constant_value", type: o ? u : "f32" }), `
            ${h.registerUniforms(R).declareVariables(S, k)}
            ${h.mainStart()}
            ${h.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}

            let indices = ${k.offsetToIndices("global_idx")};

            var value = ${u}(0);
            ${B}
            output[global_idx] = value;
        }`;
          };
          return { name: "Pad", shaderCache: { hint: `${t.mode}${o}`, inputDependencies: d }, getRunData: () => ({ outputs: [{ dims: r, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil($e.size(r) / 64) }, programUniforms: a }), getShaderSource: p };
        }, ea = (e, t) => {
          if (e.length > 1) {
            let r = e[1].getBigInt64Array(), n = e.length >= 3 && e[2].data ? e[2].dataType === 10 ? e[2].getUint16Array()[0] : e[2].getFloat32Array()[0] : 0, i = e[0].dims.length, a = new Int32Array(2 * i).fill(0);
            if (e.length >= 4) {
              let d = e[3].getBigInt64Array();
              for (let p = 0; p < d.length; p++) a[Number(d[p])] = Number(r[p]), a[Number(d[p]) + i] = Number(r[p + d.length]);
            } else r.forEach((d, p) => a[Number(p)] = Number(d));
            let o = [];
            return a.forEach((d) => o.push(d)), { mode: t.mode, value: n, pads: o };
          } else return t;
        }, ip = (e, t) => {
          np(e.inputs);
          let r = ea(e.inputs, t);
          e.compute(Zo(e.inputs, r), { inputs: [0] });
        };
      }), Nn, xi, Sd, $d, ta, Ad, Id, ra, sa, Fd, Od, na, Dd, Ld, ia, zd, ap, Bd, Rd, lp = w(() => {
        Qe(), zt(), Bt(), Qt(), Nn = (e) => {
          if (v.webgpu.validateInputContent && (!e || e.length !== 1)) throw new Error("Pool ops requires 1 input.");
        }, xi = (e, t, r) => {
          let n = t.format === "NHWC", i = e.dims.slice();
          n && i.splice(1, 0, i.pop());
          let a = Object.hasOwnProperty.call(t, "dilations"), o = t.kernelShape.slice(), d = t.strides.slice(), p = a ? t.dilations.slice() : [], h = t.pads.slice();
          wr.adjustPoolAttributes(r, i, o, d, p, h);
          let k = wr.computePoolOutputShape(r, i, d, p, o, h, t.autoPad), S = Object.assign({}, t);
          a ? Object.assign(S, { kernelShape: o, strides: d, pads: h, dilations: p, cacheKey: t.cacheKey }) : Object.assign(S, { kernelShape: o, strides: d, pads: h, cacheKey: t.cacheKey });
          let u = k.slice();
          return u.push(u.splice(1, 1)[0]), [S, n ? u : k];
        }, Sd = (e, t) => {
          let r = t.format === "NHWC", n = $e.size(e), i = $e.size(t.kernelShape), a = [{ type: 12, data: n }, { type: 12, data: i }], o = [{ name: "outputSize", type: "u32" }, { name: "kernelSize", type: "u32" }];
          if (t.kernelShape.length <= 2) {
            let d = t.kernelShape[t.kernelShape.length - 1], p = t.strides[t.strides.length - 1], h = t.pads[t.pads.length / 2 - 1], k = t.pads[t.pads.length - 1], S = !!(h + k);
            a.push({ type: 12, data: d }, { type: 12, data: p }, { type: 12, data: h }, { type: 12, data: k }), o.push({ name: "kw", type: "u32" }, { name: "sw", type: "u32" }, { name: "pwStart", type: "u32" }, { name: "pwEnd", type: "u32" });
            let u = !1;
            if (t.kernelShape.length === 2) {
              let B = t.kernelShape[t.kernelShape.length - 2], R = t.strides[t.strides.length - 2], N = t.pads[t.pads.length / 2 - 2], Z = t.pads[t.pads.length - 2];
              u = !!(N + Z), a.push({ type: 12, data: B }, { type: 12, data: R }, { type: 12, data: N }, { type: 12, data: Z }), o.push({ name: "kh", type: "u32" }, { name: "sh", type: "u32" }, { name: "phStart", type: "u32" }, { name: "phEnd", type: "u32" });
            }
            return [a, o, !0, S, u];
          } else {
            if (r) throw new Error("Pooling with kernelShape.length > 2 is not supported for NHWC format.");
            let d = $e.computeStrides(t.kernelShape);
            a.push({ type: 12, data: d }, { type: 12, data: t.pads }, { type: 12, data: t.strides }), o.push({ name: "kernelStrides", type: "u32", length: d.length }, { name: "pads", type: "u32", length: t.pads.length }, { name: "strides", type: "u32", length: t.strides.length });
            let p = t.pads.reduce((h, k) => h + k);
            return [a, o, !!p, !1, !1];
          }
        }, $d = (e, t, r, n, i, a, o, d, p, h, k, S) => {
          let u = i.format === "NHWC", B = t.type.value, R = wt("output", t.type.tensor, n);
          if (i.kernelShape.length <= 2) {
            let N = "", Z = "", te = "", Q = r - (u ? 2 : 1);
            if (k ? N = `
                for (var i: u32 = 0u; i < uniforms.kw; i++) {
                  xIndices[${Q}] = indices[${Q}] * uniforms.sw - uniforms.pwStart + i;
                  if (xIndices[${Q}] < 0 || xIndices[${Q}]
                      >= uniforms.x_shape[${Q}]) {
                    pad++;
                    continue;
                  }
                  let x_val = x[${t.indicesToOffset("xIndices")}];
                  ${a}
                }` : N = `
                for (var i: u32 = 0u; i < uniforms.kw; i++) {
                  xIndices[${Q}] = indices[${Q}] * uniforms.sw - uniforms.pwStart + i;
                  let x_val = x[${t.indicesToOffset("xIndices")}];
                  ${a}
                }`, i.kernelShape.length === 2) {
              let _e = r - (u ? 3 : 2);
              S ? Z = `
                for (var j: u32 = 0u; j < uniforms.kh; j++) {
                  xIndices[${_e}] = indices[${_e}] * uniforms.sh - uniforms.phStart + j;
                  if (xIndices[${_e}] < 0 || xIndices[${_e}] >= uniforms.x_shape[${_e}]) {
                    pad += i32(uniforms.kw);
                    continue;
                  }
              ` : Z = `
                for (var j: u32 = 0u; j < uniforms.kh; j++) {
                  xIndices[${_e}] = indices[${_e}] * uniforms.sh - uniforms.phStart + j;
                `, te = `
              }
            `;
            }
            return `
            ${e.registerUniforms(p).declareVariables(t, R)}

            ${e.mainStart()}
              ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}

              let indices = ${R.offsetToIndices("global_idx")};
              var xIndices = ${R.offsetToIndices("global_idx")};

              var value = ${B}(${d});
              var pad = 0;
              ${Z}
              ${N}
              ${te}
              ${o}

              output[global_idx] = value;
            }`;
          } else {
            if (u) throw new Error("Pooling with kernelShape.length > 2 is not supported for NHWC format.");
            let N = i.kernelShape.length, Z = i.pads.length, te = "";
            return h ? te = `
                if (xIndices[j] >= uniforms.x_shape[j]) {
                  pad++;
                  isPad = true;
                  break;
                }
              }
              if (!isPad) {
                let x_val = x[${t.indicesToOffset("xIndices")}];
                ${a}
              }` : te = `
              }
              let x_val = x[${t.indicesToOffset("xIndices")}];
              ${a}
            `, `
            ${e.registerUniforms(p).declareVariables(t, R)}

            ${e.mainStart()}
              ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
              let indices = ${R.offsetToIndices("global_idx")};
              var xIndices = ${R.offsetToIndices("global_idx")};

              var offsets: array<u32, ${N}>;

              var value = ${B}(${d});
              var pad = 0;
              var isPad = false;

              for (var i: u32 = 0u; i < uniforms.kernelSize; i++) {
                var offset = i;
                for (var j = 0u; j < ${N - 1}u; j++) {
                  offsets[j] = offset / ${Mt("uniforms.kernelStrides", "j", N)};
                  offset -= offsets[j] * ${Mt("uniforms.kernelStrides", "j", N)};
                }
                offsets[${N - 1}] = offset;

                isPad = false;
                for (var j = ${r - N}u; j < ${r}u; j++) {
                  xIndices[j] = indices[j] * ${Mt("uniforms.strides", `j - ${r - N}u`, N)}
                    + offsets[j - ${r - N}u] - ${Mt("uniforms.pads", "j - 2u", Z)};
                  ${te}
              }
              ${o}

              output[global_idx] = value;
            }`;
          }
        }, ta = (e) => `${e.format};${e.ceilMode};${e.autoPad};${e.kernelShape.length}`, Ad = (e) => `${ta(e)};${e.countIncludePad}`, Id = (e) => `${ta(e)};${e.storageOrder};${e.dilations}`, ra = (e) => ({ format: e.format, autoPad: ["NOTSET", "VALID", "SAME_UPPER", "SAME_LOWER"][e.auto_pad], ceilMode: e.ceil_mode, kernelShape: e.kernel_shape, strides: e.strides, pads: e.pads }), sa = (e, t, r, n) => {
          let [i, a] = xi(t, n, r), o = ze("x", t.dataType, t.dims.length), d = o.type.value, p = "value += x_val;", h = "";
          i.countIncludePad ? h += `value /= ${d}(uniforms.kernelSize);` : h += `value /= ${d}(i32(uniforms.kernelSize) - pad);`;
          let [k, S, u, B, R] = Sd(a, i);
          k.push(...vt(t.dims, a));
          let N = ["rank"];
          return { name: e, shaderCache: { hint: `${n.cacheKey};${u};${B};${R}`, inputDependencies: N }, getRunData: () => ({ outputs: [{ dims: a, dataType: t.dataType }], dispatchGroup: { x: Math.ceil($e.size(a) / 64) }, programUniforms: k }), getShaderSource: (Z) => $d(Z, o, t.dims.length, a.length, i, p, h, 0, S, u, B, R) };
        }, Fd = (e) => {
          let t = e.count_include_pad !== 0, r = ra(e);
          if (r.ceilMode !== 0) throw new Error("using ceil() in shape computation is not yet supported for AveragePool");
          let n = { countIncludePad: t, ...r, cacheKey: "" };
          return { ...n, cacheKey: Ad(n) };
        }, Od = (e, t) => {
          Nn(e.inputs), e.compute(sa("AveragePool", e.inputs[0], !1, t));
        }, na = { autoPad: "", ceilMode: 0, countIncludePad: !1, kernelShape: [], strides: [], pads: [], storageOrder: 0, dilations: [] }, Dd = (e) => {
          let t = e.format;
          return { format: t, ...na, cacheKey: t };
        }, Ld = (e, t) => {
          Nn(e.inputs), e.compute(sa("GlobalAveragePool", e.inputs[0], !0, t));
        }, ia = (e, t, r, n) => {
          let [i, a] = xi(t, n, r), o = `
      value = max(x_val, value);
    `, d = "", p = ze("x", t.dataType, t.dims.length), h = ["rank"], [k, S, u, B, R] = Sd(a, i);
          return k.push(...vt(t.dims, a)), { name: e, shaderCache: { hint: `${n.cacheKey};${u};${B};${R}`, inputDependencies: h }, getRunData: () => ({ outputs: [{ dims: a, dataType: t.dataType }], dispatchGroup: { x: Math.ceil($e.size(a) / 64) }, programUniforms: k }), getShaderSource: (N) => $d(N, p, t.dims.length, a.length, i, o, d, t.dataType === 10 ? -65504 : -1e5, S, u, B, R) };
        }, zd = (e, t) => {
          Nn(e.inputs), e.compute(ia("MaxPool", e.inputs[0], !1, t));
        }, ap = (e) => {
          let t = e.storage_order, r = e.dilations, n = ra(e);
          if (t !== 0) throw new Error("column major storage order is not yet supported for MaxPool");
          if (n.ceilMode !== 0) throw new Error("using ceil() in shape computation is not yet supported for MaxPool");
          let i = { storageOrder: t, dilations: r, ...n, cacheKey: "" };
          return { ...i, cacheKey: Id(i) };
        }, Bd = (e) => {
          let t = e.format;
          return { format: t, ...na, cacheKey: t };
        }, Rd = (e, t) => {
          Nn(e.inputs), e.compute(ia("GlobalMaxPool", e.inputs[0], !0, t));
        };
      }), Nd, jd, Ud, up, Wd = w(() => {
        zt(), Bt(), Pt(), Qt(), Nd = (e, t) => {
          if (e.length < 2 || e.length > 3) throw new Error("DequantizeLinear requires 2 or 3 inputs.");
          if (e.length === 3 && e[1].dims === e[2].dims) throw new Error("x-scale and x-zero-point must have the same shape.");
          if (e.length === 3 && e[0].dataType !== e[2].dataType) throw new Error("x and x-zero-point must have the same data type.");
          if (e[0].dataType === 6 && e.length > 2) throw new Error("In the case of dequantizing int32 there is no zero point.");
          if (e[1].dims.length !== 0 && e[1].dims.length !== 1 && e[1].dims.length !== e[0].dims.length) throw new Error("scale input must be a scalar, a 1D tensor, or have the same rank as the input tensor.");
          if (e.length > 2) {
            if (e[0].dataType !== e[2].dataType) throw new Error("x and x-zero-point must have the same data type.");
            if (e[1].dims.length !== e[2].dims.length) throw new Error("scale and zero-point inputs must have the same rank.");
            if (!e[1].dims.map((r, n) => r === e[2].dims[n]).reduce((r, n) => r && n, !0)) throw new Error("scale and zero-point inputs must have the same shape.");
          }
          if (t.blockSize > 0) {
            if (e[1].dims.length === 0 || e[1].dims.length === 1 && e[1].dims[0] === 1) throw new Error("blockSize must be set only for block quantization.");
            if (!e[1].dims.map((i, a) => a === t.axis || i === e[0].dims[a]).reduce((i, a) => i && a, !0)) throw new Error("For block qunatization, scale input shape to match the input shape except for the axis");
            if (e[1].dims.length !== e[0].dims.length) throw new Error("For block qunatization the scale input rank must be the same as the x rank.");
            let r = e[0].dims[t.axis], n = e[1].dims[t.axis];
            if (t.blockSize < Math.ceil(r / n) || t.blockSize > Math.ceil(r / (n - 1) - 1)) throw new Error("blockSize must be with in the range [ceil(dI / Si), ceil(dI / (Si - 1) - 1)].");
          }
        }, jd = (e, t) => {
          let r = $e.normalizeAxis(t.axis, e[0].dims.length), n = e[0].dataType, i = n === 3, a = e[0].dims, o = e[1].dataType, d = $e.size(a), p = n === 3 || n === 2, h = p ? [Math.ceil($e.size(e[0].dims) / 4)] : e[0].dims, k = e[1].dims, S = e.length > 2 ? e[2] : void 0, u = S ? p ? [Math.ceil($e.size(S.dims) / 4)] : S.dims : void 0, B = k.length === 0 || k.length === 1 && k[0] === 1, R = B === !1 && k.length === 1, N = yr(d), Z = B && (!p || N === 4), te = Z ? N : 1, Q = Z && !p ? N : 1, _e = ze("input", p ? 12 : n, h.length, Q), me = ze("scale", o, k.length), ye = S ? ze("zero_point", p ? 12 : n, u.length) : void 0, Ae = wt("output", o, a.length, te), Ie = [_e, me];
          ye && Ie.push(ye);
          let Ge = [h, k];
          S && Ge.push(u);
          let lt = [{ type: 12, data: d / te }, { type: 12, data: r }, { type: 12, data: t.blockSize }, ...vt(...Ge, a)], Tt = (Kt) => {
            let Yt = [{ name: "output_size", type: "u32" }, { name: "axis", type: "u32" }, { name: "block_size", type: "u32" }];
            return `
      ${Kt.registerUniforms(Yt).declareVariables(...Ie, Ae)}
      ${Kt.mainStart()}
          ${Kt.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
          let output_indices = ${Ae.offsetToIndices("global_idx")};

          // Set input x
          ${p ? `
            let input = ${_e.getByOffset("global_idx / 4")};
            let x_vec = ${i ? "unpack4xI8(input)" : "unpack4xU8(input)"};
            let x_value = ${te === 1 ? "x_vec[global_idx % 4]" : "x_vec"};` : `let x_value = ${_e.getByOffset("global_idx")};`};

          // Set scale input
          ${B ? `let scale_value= ${me.getByOffset("0")}` : R ? `
            let scale_index = ${Ae.indicesGet("output_indices", "uniforms.axis")};
            let scale_value= ${me.getByOffset("scale_index")};` : `
            var scale_indices: ${me.type.indices} = output_indices;
            let index = ${me.indicesGet("scale_indices", "uniforms.axis")} / uniforms.block_size;
            ${me.indicesSet("scale_indices", "uniforms.axis", "index")};
            let scale_value= ${me.getByIndices("scale_indices")};`};

          // Set zero-point input
          ${ye ? B ? p ? `
                let zero_point_input = ${ye.getByOffset("0")};
                let zero_point_vec =  ${i ? "unpack4xI8(zero_point_input)" : "unpack4xU8(zero_point_input)"};
                let zero_point_value= zero_point_vec[0]` : `let zero_point_value = ${ye.getByOffset("0")}` : R ? p ? `
                let zero_point_index = ${Ae.indicesGet("output_indices", "uniforms.axis")};
                let zero_point_input = ${ye.getByOffset("zero_point_index / 4")};
                let zero_point_vec =  ${i ? "unpack4xI8(zero_point_input)" : "unpack4xU8(zero_point_input)"};
                let zero_point_value = zero_point_vec[zero_point_index % 4]` : `
                let zero_point_index = ${Ae.indicesGet("output_indices", "uniforms.axis")};
                let zero_point_value = ${ye.getByOffset("zero_point_index")};` : p ? `
                let zero_point_offset = ${me.indicesToOffset("scale_indices")};
                let zero_point_input = ${ye.getByOffset("zero_point_offset / 4")};
                let zero_point_vec = ${i ? "unpack4xI8(zero_point_input)" : "unpack4xU8(zero_point_input)"};
                let zero_point_value = zero_point_vec[zero_point_offset % 4];` : `let zero_point_value = ${ye.getByIndices("scale_indices")};` : `let zero_point_value = ${p ? i ? "i32" : "u32" : _e.type.value}(0);`};
      // Compute and write output
      ${Ae.setByOffset("global_idx", `${Ae.type.value}(x_value - zero_point_value) * scale_value`)};
      }`;
          };
          return { name: "DequantizeLinear", shaderCache: { hint: t.cacheKey, inputDependencies: ye ? ["rank", "rank", "rank"] : ["rank", "rank"] }, getShaderSource: Tt, getRunData: () => ({ outputs: [{ dims: a, dataType: o }], dispatchGroup: { x: Math.ceil(d / te / 64), y: 1, z: 1 }, programUniforms: lt }) };
        }, Ud = (e, t) => {
          Nd(e.inputs, t), e.compute(jd(e.inputs, t));
        }, up = (e) => ot({ axis: e.axis, blockSize: e.blockSize });
      }), Vd, Gd, Kd, dp = w(() => {
        Qe(), zt(), Qt(), Vd = (e, t, r) => {
          let n = e === t, i = e < t && r < 0, a = e > t && r > 0;
          if (n || i || a) throw new Error("Range these inputs' contents are invalid.");
        }, Gd = (e, t, r, n) => {
          let i = Math.abs(Math.ceil((t - e) / r)), a = [i], o = i, d = [{ type: 12, data: o }, { type: n, data: e }, { type: n, data: r }, ...vt(a)], p = (h) => {
            let k = wt("output", n, a.length), S = k.type.value, u = [{ name: "outputSize", type: "u32" }, { name: "start", type: S }, { name: "delta", type: S }];
            return `
        ${h.registerUniforms(u).declareVariables(k)}
        ${h.mainStart()}
        ${h.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
        output[global_idx] = uniforms.start + ${S}(global_idx) * uniforms.delta;
      }`;
          };
          return { name: "Range", shaderCache: { hint: `${n}` }, getShaderSource: p, getRunData: () => ({ outputs: [{ dims: a, dataType: n }], dispatchGroup: { x: Math.ceil(o / 64) }, programUniforms: d }) };
        }, Kd = (e) => {
          let t = 0, r = 0, n = 0;
          e.inputs[0].dataType === 6 ? (t = e.inputs[0].getInt32Array()[0], r = e.inputs[1].getInt32Array()[0], n = e.inputs[2].getInt32Array()[0]) : e.inputs[0].dataType === 1 && (t = e.inputs[0].getFloat32Array()[0], r = e.inputs[1].getFloat32Array()[0], n = e.inputs[2].getFloat32Array()[0]), v.webgpu.validateInputContent && Vd(t, r, n), e.compute(Gd(t, r, n, e.inputs[0].dataType), { inputs: [] });
        };
      }), Hd, qd, Xd, Qd, cp = w(() => {
        zt(), Bt(), Pt(), Qt(), Hd = (e, t, r, n) => {
          if (e !== "none" && n !== "i32" && n !== "u32" && n !== "f32") throw new Error(`Input ${n} is not supported with reduction ${e}.`);
          let i = `{
                var oldValue = 0;
                loop {
                  let newValueF32 =`, a = `;
                  let newValue = bitcast<i32>(newValueF32);
                  let res = atomicCompareExchangeWeak(&${t}, oldValue, newValue);
                  if res.exchanged {
                    break;
                  }
                  oldValue = res.old_value;
                }
              }`;
          switch (e) {
            case "none":
              return `${t}=${r};`;
            case "add":
              return n === "i32" || n === "u32" ? `atomicAdd(&${t}, bitcast<${n}>(${r}));` : `
              ${i}bitcast<${n}>(oldValue) + (${r})${a}`;
            case "max":
              return n === "i32" || n === "u32" ? `atomicMax(&${t}, bitcast<${n}>(${r}));` : `
                ${i}max(bitcast<f32>(oldValue), (${r}))${a}`;
            case "min":
              return n === "i32" || n === "u32" ? `atomicMin(&${t}, bitcast<${n}>(${r}));` : `${i}min(bitcast<${n}>(oldValue), (${r}))${a}`;
            case "mul":
              return `${i}(bitcast<${n}>(oldValue) * (${r}))${a}`;
            default:
              throw new Error(`Reduction ${e} is not supported.`);
          }
        }, qd = (e, t) => {
          let r = e[0].dims, n = e[1].dims, i = r, a = 1, o = Math.ceil($e.size(n) / a), d = n[n.length - 1], p = $e.sizeFromDimension(r, d), h = [{ type: 12, data: o }, { type: 12, data: d }, { type: 12, data: p }, ...vt(e[1].dims, e[2].dims, i)], k = (S) => {
            let u = ze("indices", e[1].dataType, e[1].dims.length), B = ze("updates", e[2].dataType, e[2].dims.length, a), R = t.reduction !== "none" && t.reduction !== "" ? rs("output", e[0].dataType, i.length) : wt("output", e[0].dataType, i.length, a);
            return `
      ${S.registerUniform("output_size", "u32").registerUniform("last_index_dimension", "u32").registerUniform("num_updates_elements", "u32").declareVariables(u, B, R)}
      ${S.mainStart()}
        ${S.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
  var data_offset = 0u;
  let indices_start = uniforms.last_index_dimension * global_idx;
  let indices_end = indices_start + uniforms.last_index_dimension;
  for (var i = indices_start; i < indices_end; i++) {
    var index = i32(indices[i].x);
    ${e[0].dims.length === 1 ? `
    let element_count_dim = uniforms.output_strides;
    let dim_value = uniforms.output_shape;` : `
    let element_count_dim = uniforms.output_strides[i - indices_start];
    let dim_value = uniforms.output_shape[i - indices_start + uniforms.last_index_dimension];`}
    if (index >= 0) {
      if (index >= i32(dim_value)) {
        index = i32(dim_value - 1);
      }
    } else {
      if (index < -i32(dim_value)) {
        index = 0;
      } else {
        index += i32(dim_value);
      }
    }
    data_offset += u32((u32(index) * element_count_dim));
  }

  for (var i = 0u; i < uniforms.num_updates_elements; i++) {
    let value = updates[uniforms.num_updates_elements * global_idx + i];
    ${Hd(t.reduction, "output[data_offset + i]", "value", R.type.value)}
  }

      }`;
          };
          return { name: "ScatterND", shaderCache: { hint: `${t.cacheKey}_${t.reduction}`, inputDependencies: ["rank", "rank"] }, getRunData: () => ({ outputs: [{ dims: i, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(o / 64) }, programUniforms: h }), getShaderSource: k };
        }, Xd = (e) => ot({ reduction: e.reduction }), Qd = (e, t) => {
          e.compute(qd(e.inputs, t), { inputs: [e.inputs[1], e.inputs[2]], outputs: [] });
        };
      }), Yd, Jd, Zd, ec, tc, rc, sc, nc, ic, oc, ac, oa, lc, uc, dc, cc, pc, hc, mc, zp = w(() => {
        zt(), Bt(), Pt(), Qt(), Yd = (e, t) => {
          if (e.every((r) => r > 0 || (() => {
            throw new Error("Resize requires scales input values to be positive");
          })), e.length > 0) {
            if (t.mode === "linear") {
              if (!(e.length === 2 || e.length === 3 || e.length === 4 && e[0] === 1 && e[1] === 1 || e.length === 4 && e[0] === 1 && e[3] === 1 || e.length === 5 && e[0] === 1 && e[1] === 1)) throw new Error(`For linear mode, Resize requires scales to be 2D, 3D, 4D with either two outermost or one innermost and
            one outermost scale values equal to 1, or 5D with two outermost scale values equal to 1`);
            } else if (t.mode === "cubic" && !(e.length === 2 || e.length === 4 && e[0] === 1 && e[1] === 1 || e.length === 4 && e[0] === 1 && e[3] === 1)) throw new Error("Resize requires scales input size to be 2 or 4 for cubic mode");
          }
        }, Jd = (e, t, r) => {
          t.every((i) => i >= 0 && i < r || (() => {
            throw new Error("Resize requires axes input values to be positive and less than rank");
          }));
          let n = new Array(r).fill(1);
          return t.forEach((i, a) => n[i] = e[a]), n;
        }, Zd = (e, t, r, n, i, a) => {
          let [o, d, p] = r > 10 ? [1, 2, 3] : [-1, e.length > 1 ? 1 : -1, -1], h = e[0].dims.length;
          if (o > 0 && e.length > o && e[o].dims.length > 0) e[o].getFloat32Array().forEach((k) => a.push(k));
          else if (t.coordinateTransformMode === "tf_crop_and_resize") throw new Error("Resize requires RoI input to be specified when coordinateTransformMode is tfCropAndResize");
          if (d > 0 && e.length > d && e[d].dims.length === 1 && e[d].dims[0] > 0) {
            if (e[d].getFloat32Array().forEach((k) => n.push(k)), n.length !== 0 && n.length !== h && r >= 18 && n.length !== t.axes.length) throw new Error("Resize requires scales input size to be same as input rank or axes size for opset 18 and up");
            Yd(n, t), t.axes.length > 0 && Jd(n, t.axes, h).forEach((k, S) => n[S] = k);
          }
          if (p > 0 && e.length > p && e[p].dims.length === 1 && e[p].dims[0] > 0 && (e[p].getBigInt64Array().forEach((k) => i.push(Number(k))), i.length !== 0 && i.length !== h && r >= 18 && i.length !== t.axes.length)) throw new Error("Resize requires sizes input size to be same as input rank or axes size for opset 18 and up");
          if (t.axes.length > 0) {
            if (n.length !== 0 && n.length !== t.axes.length) throw new Error('Resize requires "scales" input size to be of axes rank when axes attributes is specified');
            if (i.length !== 0 && i.length !== t.axes.length) throw new Error('Resize requires "sizes" input size to be of rank axes rank when axes attributes is specified');
          }
          if (typeof n < "u" && typeof i < "u" && n.length > 0 && i.length > h) throw new Error("Resize requires only of scales or sizes to be specified");
        }, ec = (e, t) => `fn getOriginalCoordinateFromResizedCoordinate(xResized: u32, xScale: f32, lengthResized: u32,
     lengthOriginal: u32, roiStart: f32, roiEnd: f32) -> ${t} { ` + (() => {
          switch (e) {
            case "asymmetric":
              return `return ${t}(xResized) / ${t}(xScale);`;
            case "pytorch_half_pixel":
              return `if (lengthResized > 1) {
                    return (${t}(xResized) + 0.5) / ${t}(xScale) - 0.5;
                  } else {
                    return 0.0;
                  }`;
            case "tf_half_pixel_for_nn":
              return `return (${t}(xResized) + 0.5) / ${t}(xScale);`;
            case "align_corners":
              return `if (lengthResized == 1) {
                    return 0.0;
                  } else {
                    // The whole part and the fractional part are calculated separately due to inaccuracy of floating
                    // point division. As an example, f32(21) / f32(7) may evaluate to 2.99... instead of 3, causing an
                    // offset-by-one error later in floor().
                    let whole = ${t}(xResized * (lengthOriginal - 1) / (lengthResized - 1));
                    let fract =
                        ${t}(xResized * (lengthOriginal - 1) % (lengthResized - 1)) / ${t}(lengthResized - 1);
                    return whole + fract;
                  }`;
            case "tf_crop_and_resize":
              return `if (lengthResized > 1) {
                    return ${t}(roiStart) * ${t}(lengthOriginal - 1) +
                        (${t}(xResized) * ${t}(roiEnd - roiStart) * ${t}(lengthOriginal - 1)) /
                        ${t}(lengthResized - 1);
                  } else {
                    return 0.5 * ${t}(roiStart + roiEnd) * ${t}(lengthOriginal - 1);
                  }`;
            case "half_pixel_symmetric":
              return `const outputWidth = ${t}xScale * ${t}(lengthResized);
                  const adjustment = ${t}(lengthResized) / outputWidth;
                  const center = ${t}(lengthOriginal) / 2;
                  const offset = center * (1 - adjustment);
                  return offset + ((${t}(xResized) + 0.5) / ${t}(xScale)) - 0.5;`;
            case "half_pixel":
              return `return ((${t}(xResized) + 0.5) / ${t}(xScale)) - 0.5;`;
            default:
              throw new Error(`Coordinate transform mode ${e} is not supported`);
          }
        })() + "}", tc = (e, t, r) => `fn getNearestPixelFromOriginal(xOriginal: ${r}, isDownSample: bool) -> ${r} {` + (() => {
          switch (e) {
            case "round_prefer_ceil":
              return "if (fract(xOriginal) == 0.5) {             return ceil(xOriginal);           } else {             return round(xOriginal);           }";
            case "floor":
              return "return floor(xOriginal);";
            case "ceil":
              return "return ceil(xOriginal);";
            case "round_prefer_floor":
              return "if (fract(xOriginal) == 0.5) {                     return floor(xOriginal);                   } else {                     return round(xOriginal);                   }";
            case "simple":
            default:
              if (t < 11) return "if (isDownSample)                     {                       return ceil(xOriginal);                     } else {                       return xOriginal;                     }";
              throw new Error(`Nearest mode ${e} is not supported`);
          }
        })() + "}", rc = (e, t, r) => {
          let n = new Array(r).fill(0).concat(new Array(r).fill(1)), i = e.length === 0 ? n : e.slice();
          return t.length > 0 ? (t.forEach((a, o) => {
            n[a] = i[o], n[o + r] = i[t.length + o];
          }), n) : i;
        }, sc = (e, t, r, n) => {
          let i = [];
          if (r.length > 0) if (n.length > 0) {
            if (e.forEach((a) => i.push(a)), Math.max(...n) > e.length) throw new Error("axes is out of bound");
            n.forEach((a, o) => i[a] = r[o]);
          } else r.forEach((a) => i.push(a));
          else {
            if (t.length === 0) throw new Error("Resize requires either scales or sizes.");
            i = e.map((a, o) => Math.round(a * t[o]));
          }
          return i;
        }, nc = (e, t, r) => {
          let n = (() => {
            switch (r.keepAspectRatioPolicy) {
              case "not_larger":
                return r.axes.length > 0 ? Math.min(...r.axes.map((a) => t[a]), Number.MAX_VALUE) : Math.min(...t, Number.MAX_VALUE);
              case "not_smaller":
                return r.axes.length > 0 ? Math.max(...r.axes.map((a) => t[a]), Number.MIN_VALUE) : Math.max(...t, Number.MIN_VALUE);
              default:
                throw new Error(`Keep aspect ratio policy ${r.keepAspectRatioPolicy} is not supported`);
            }
          })();
          t.fill(1, 0, t.length);
          let i = e.slice();
          return r.axes.length > 0 ? (r.axes.forEach((a) => t[a] = n), r.axes.forEach((a) => i[a] = Math.round(e[a] * t[a]))) : (t.fill(n, 0, t.length), i.forEach((a, o) => i[o] = Math.round(a * t[o]))), i;
        }, ic = (e, t, r, n, i) => `
    fn calculateOriginalIndicesFromOutputIndices(output_indices: ${e.type.indices}) -> array<${e.type.value}, ${r.length}> {
      var original_indices: array<${e.type.value}, ${r.length}>;
      for (var i:u32 = 0; i < ${r.length}; i++) {
        var output_index = ${e.indicesGet("output_indices", "i")};
        var scale = ${Mt("uniforms.scales", "i", n)};
        var roi_low = ${Mt("uniforms.roi", "i", i)};
        var roi_hi = ${Mt("uniforms.roi", `i + ${t.length}`, i)};
        if (scale == 1.0) {
          original_indices[i] = ${e.type.value}(output_index);
        } else {
          var input_shape_i = ${Mt("uniforms.input_shape", "i", t.length)};
          var output_shape_i = ${Mt("uniforms.output_shape", "i", r.length)};
          original_indices[i] = getOriginalCoordinateFromResizedCoordinate(output_index, scale, output_shape_i,
                                                                           input_shape_i, roi_low, roi_hi);
        }
      }
      return original_indices;
    }`, oc = (e, t, r, n, i, a, o) => `
    fn calculateInputIndicesFromOutputIndices(output_indices: ${t.type.indices}) -> ${e.type.indices} {
      var input_indices: ${e.type.indices};
      for (var i:u32 = 0; i < ${n.length}; i++) {
        var output_index = ${t.indicesGet("output_indices", "i")};
        var input_index: u32;
        var scale = ${Mt("uniforms.scales", "i", i)};
        if (scale == 1.0) {
          input_index = output_index;
        } else {
          var roi_low = ${Mt("uniforms.roi", "i", a)};
          var roi_hi = ${Mt("uniforms.roi", `i + ${r.length}`, a)};
          var input_shape_i = ${Mt("uniforms.input_shape", "i", r.length)};
          var output_shape_i = ${Mt("uniforms.output_shape", "i", n.length)};
          var original_idx = getOriginalCoordinateFromResizedCoordinate(output_index, scale, output_shape_i,
                                                                        input_shape_i, roi_low, roi_hi);
          if (!${o} || (original_idx >= 0 && original_idx < ${t.type.value}(input_shape_i))) {
            if (original_idx < 0) {
              input_index = 0;
            } else if (original_idx > ${t.type.value}(input_shape_i - 1)) {
              input_index = input_shape_i - 1;
            } else {
              input_index = u32(getNearestPixelFromOriginal(original_idx, scale < 1));
            }
          } else {
            input_index = u32(original_idx);
          }
        }
        ${e.indicesSet("input_indices", "i", " input_index")}
      }
      return input_indices;
    }`, ac = (e, t) => `
    fn checkInputIndices(input_indices: ${e.type.indices}) -> bool {
      for (var i:u32 = 0; i < ${t.length}; i++) {
        var input_index = ${e.indicesGet("input_indices", "i")};
        if (input_index < 0 || input_index >= ${Mt("uniforms.input_shape", "i", t.length)}) {
          return false;
        }
      }
      return true;
    }`, oa = (e, t, r, n) => e.rank > n ? `
    ${e.indicesSet("input_indices", t, "channel")};
    ${e.indicesSet("input_indices", r, "batch")};
` : "", lc = (e, t, r, n, i) => {
          let [a, o, d, p] = r.length === 2 ? [-1, 0, 1, -1] : [0, 2, 3, 1], h = e.type.value;
          return `
    fn getInputValue(batch: u32, channel: u32, row: u32, col: u32) -> ${h} {
      var input_indices: ${e.type.indices};
      ${e.indicesSet("input_indices", o, `max(0, min(row, ${r[o]} - 1))`)};
      ${e.indicesSet("input_indices", d, `max(0, min(col, ${r[d]} - 1))`)};
      ${oa(e, p, a, 2)}
      return ${e.getByIndices("input_indices")};
    }

    fn bilinearInterpolation(output_indices: ${t.type.indices}) -> ${h} {
      var originalIndices = calculateOriginalIndicesFromOutputIndices(output_indices);
      var row:${h} = originalIndices[${o}];
      var col:${h} = originalIndices[${d}];
      ${n ? `if (row < 0 || row > (${r[o]} - 1) || col < 0 || col > (${r[d]} - 1)) {
        return ${i};
      }` : ""};
      row = max(0, min(row, ${r[o]} - 1));
      col = max(0, min(col, ${r[d]} - 1));
      var row1: u32 = u32(row);
      var col1: u32 = u32(col);
      var row2: u32 = u32(row + 1);
      var col2: u32 = u32(col + 1);
      var channel: u32 = ${r.length > 2 ? `u32(originalIndices[${p}])` : "0"};
      var batch: u32 =  ${r.length > 2 ? `u32(originalIndices[${a}])` : "0"};
      var x11: ${h} = getInputValue(batch, channel, row1, col1);
      var x12: ${h} = getInputValue(batch, channel, row1, col2);
      var x21: ${h} = getInputValue(batch, channel, row2, col1);
      var x22: ${h} = getInputValue(batch, channel, row2, col2);
      var dx1: ${h} = abs(row - ${h}(row1));
      var dx2: ${h} = abs(${h}(row2) - row);
      var dy1: ${h} = abs(col - ${h}(col1));
      var dy2: ${h} = abs(${h}(col2) - col);
      if (row1 == row2) {
        dx1 = 0.5;
        dx2 = 0.5;
      }
      if (col1 == col2) {
        dy1 = 0.5;
        dy2 = 0.5;
      }
      return (x11 * dx2 * dy2 + x12 * dx2 * dy1 + x21 * dx1 * dy2 + x22 * dx1 * dy1);
    }`;
        }, uc = (e, t, r, n, i, a, o, d, p, h) => {
          let k = r.length === 2, [S, u] = k ? [0, 1] : [2, 3], B = e.type.value, R = (N) => {
            let Z = N === S ? "row" : "col";
            return `
      fn ${Z}CubicInterpolation(input_indices: ${e.type.indices}, output_indices: ${t.type.indices}) -> ${B} {
        var output_index = ${t.indicesGet("output_indices", N)};
        var originalIdx: ${B} = getOriginalCoordinateFromResizedCoordinate(output_index, ${i[N]},
        ${n[N]}, ${r[N]}, ${a[N]}, ${a[N]} + ${r.length});
        var fractOriginalIdx: ${B} = originalIdx - floor(originalIdx);
        var coefs = getCubicInterpolationCoefs(fractOriginalIdx);

        if (${d} && (originalIdx < 0 || originalIdx > (${r[N]} - 1))) {
          return ${p};
        }
        var data: array<${B}, 4> = array<${B}, 4>(0.0, 0.0, 0.0, 0.0);
        for (var i: i32 = -1; i < 3; i++) {
          var ${Z}: ${B} = originalIdx + ${B}(i);
          if (${Z} < 0 || ${Z} >= ${r[N]}) {
            ${h ? `coefs[i + 1] = 0.0;
                        continue;` : d ? `return ${p};` : `${Z} = max(0, min(${Z}, ${r[N]} - 1));`};
          }
        var input_indices_copy: ${e.type.indices} = input_indices;
          ${e.indicesSet("input_indices_copy", N, `u32(${Z})`)};
          data[i + 1] = ${N === S ? e.getByIndices("input_indices_copy") : "rowCubicInterpolation(input_indices_copy, output_indices)"};
        }
        return cubicInterpolation1D(data, coefs);
      }`;
          };
          return `
    ${R(S)};
    ${R(u)};
  fn getCubicInterpolationCoefs(s: ${B}) -> array<${B}, 4> {
    var absS = abs(s);
    var coeffs: array<${B}, 4> = array<${B}, 4>(0.0, 0.0, 0.0, 0.0);
    var oneMinusAbsS: ${B} = 1.0 - absS;
    var twoMinusAbsS: ${B} = 2.0 - absS;
    var onePlusAbsS: ${B} = 1.0 + absS;
    coeffs[0] = ((${o} * onePlusAbsS - 5 * ${o}) * onePlusAbsS + 8 * ${o}) * onePlusAbsS - 4 * ${o};
    coeffs[1] = ((${o} + 2) * absS - (${o} + 3)) * absS * absS + 1;
    coeffs[2] = ((${o} + 2) * oneMinusAbsS - (${o} + 3)) * oneMinusAbsS * oneMinusAbsS + 1;
    coeffs[3] = ((${o} * twoMinusAbsS - 5 * ${o}) * twoMinusAbsS + 8 * ${o}) * twoMinusAbsS - 4 * ${o};
    return coeffs;
  }

  fn cubicInterpolation1D(x: array<${B}, 4>, coefs: array<${B}, 4>) -> ${B} {
    var coefsSum: ${B} = coefs[0] + coefs[1] + coefs[2] + coefs[3];
    return (x[0] * coefs[0] + x[1] * coefs[1]+ x[2] * coefs[2]+ x[3] * coefs[3]) / coefsSum;
  }

  fn bicubicInterpolation(output_indices: ${t.type.indices}) -> ${B} {
    var input_indices: ${e.type.indices} = output_indices;
    return colCubicInterpolation(input_indices, output_indices);
  }
    `;
        }, dc = (e, t, r, n, i) => {
          let [a, o, d, p, h] = r.length === 3 ? [-1, 0, 1, 2, -1] : [0, 2, 3, 4, 1], k = e.type.value;
          return `
    fn getInputValue(batch: u32, channel: u32, depth:u32, height: u32, width: u32) -> ${k} {
      var input_indices: ${e.type.indices};
      ${e.indicesSet("input_indices", o, `max(0, min(depth, ${r[o]} - 1))`)};
      ${e.indicesSet("input_indices", d, `max(0, min(height, ${r[d]} - 1))`)};
      ${e.indicesSet("input_indices", p, `max(0, min(width, ${r[p]} - 1))`)};
      ${oa(e, h, a, 3)}
      return ${e.getByIndices("input_indices")};
    }

    fn trilinearInterpolation(output_indices: ${t.type.indices}) -> ${k} {
      var originalIndices = calculateOriginalIndicesFromOutputIndices(output_indices);
      var depth:${k} = originalIndices[${o}];
      var height:${k} = originalIndices[${d}];
      var width:${k} = originalIndices[${p}];
      ${n ? `if (depth < 0 || depth > (${r[o]} - 1) || height < 0 || height > (${r[d]} - 1) || width < 0 || (width > ${r[p]} - 1)) {
      return ${i};
        }` : ""};

    depth = max(0, min(depth, ${r[o]} - 1));
      height = max(0, min(height, ${r[d]} - 1));
      width = max(0, min(width, ${r[p]} - 1));
      var depth1: u32 = u32(depth);
      var height1: u32 = u32(height);
      var width1: u32 = u32(width);
      var depth2: u32 = u32(depth + 1);
      var height2: u32 = u32(height + 1);
      var width2: u32 = u32(width + 1);
      var channel: u32 = ${r.length > 3 ? `u32(originalIndices[${h}])` : "0"};
      var batch: u32 =  ${r.length > 3 ? `u32(originalIndices[${a}])` : "0"};

      var x111: ${k} = getInputValue(batch, channel, depth1, height1, width1);
      var x112: ${k} = getInputValue(batch, channel, depth1, height1, width2);
      var x121: ${k} = getInputValue(batch, channel, depth1, height2, width1);
      var x122: ${k} = getInputValue(batch, channel, depth1, height2, width2);
      var x211: ${k} = getInputValue(batch, channel, depth2, height1, width1);
      var x212: ${k} = getInputValue(batch, channel, depth2, height1, width2);
      var x221: ${k} = getInputValue(batch, channel, depth2, height2, width1);
      var x222: ${k} = getInputValue(batch, channel, depth2, height2, width2);
      var dx1: ${k} = abs(depth - ${k}(depth1));
      var dx2: ${k} = abs(${k}(depth2) - depth);
      var dy1: ${k} = abs(height - ${k}(height1));
      var dy2: ${k} = abs(${k}(height2) - height);
      var dz1: ${k} = abs(width - ${k}(width1));
      var dz2: ${k} = abs(${k}(width2) - width);
      if (depth1 == depth2) {
        dx1 = 0.5;
        dx2 = 0.5;
      }
      if (height1 == height2) {
        dy1 = 0.5;
        dy2 = 0.5;
      }
      if (width1 == width2) {
        dz1 = 0.5;
        dz2 = 0.5;
      }
      return (x111 * dx2 * dy2 * dz2 + x112 * dx2 * dy2 * dz1 + x121 * dx2 * dy1 *dz2 + x122 * dx2 * dy1 * dz1 +
              x211 * dx1 * dy2 * dz2 + x212 * dx1 * dy2 * dz1 + x221 * dx1 * dy1 *dz2 + x222 * dx1 * dy1 * dz1);
    }`;
        }, cc = (e, t, r, n, i, a) => {
          let o = e.dims, d = rc(a, t.axes, o.length), p = sc(o, n, i, t.axes), h = n.slice();
          n.length === 0 && (h = o.map((Q, _e) => Q === 0 ? 1 : p[_e] / Q), t.keepAspectRatioPolicy !== "stretch" && (p = nc(o, h, t)));
          let k = wt("output", e.dataType, p.length), S = ze("input", e.dataType, o.length), u = $e.size(p), B = o.length === p.length && o.every((Q, _e) => Q === p[_e]), R = t.coordinateTransformMode === "tf_crop_and_resize", N = t.extrapolationValue, Z = S.type.value, te = (Q) => `
      ${B ? "" : `
      ${ec(t.coordinateTransformMode, Z)};
      ${(() => {
            switch (t.mode) {
              case "nearest":
                return `
              ${ac(S, o)};
              ${tc(t.nearestMode, r, Z)};
              ${oc(S, k, o, p, h.length, d.length, R)};
              `;
              case "linear":
                return `
              ${ic(k, o, p, h.length, d.length)};
              ${(() => {
                  if (o.length === 2 || o.length === 4) return `${lc(S, k, o, R, N)}`;
                  if (o.length === 3 || o.length === 5) return `${dc(S, k, o, R, N)}`;
                  throw Error("Linear mode only supports input dims 2, 3, 4 and 5 are supported in linear mode.");
                })()};
            `;
              case "cubic":
                return `
            ${(() => {
                  if (o.length === 2 || o.length === 4) return `${uc(S, k, o, p, h, d, t.cubicCoeffA, R, t.extrapolationValue, t.excludeOutside)}`;
                  throw Error("Cubic mode only supports input dims 2 and 4 are supported in linear mode.");
                })()};
            `;
              default:
                throw Error("Invalid resize mode");
            }
          })()};
      `}
      ${Q.registerUniform("output_size", "u32").registerUniform("scales", "f32", h.length).registerUniform("roi", "f32", d.length).declareVariables(S, k)}
      ${Q.mainStart()}
        ${Q.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
        ${B ? "output[global_idx] = input[global_idx];" : `
        let output_indices = ${k.offsetToIndices("global_idx")};
        var input_indices: ${S.type.indices};
        ${(() => {
            switch (t.mode) {
              case "nearest":
                return `input_indices = calculateInputIndicesFromOutputIndices(output_indices);
                if (checkInputIndices(input_indices)) {
                  output[global_idx] = ${S.getByIndices("input_indices")};
                } else {
                  output[global_idx] = ${t.extrapolationValue};
                }`;
              case "linear":
                return `output[global_idx] = ${o.length === 2 || o.length === 4 ? "bilinearInterpolation" : "trilinearInterpolation"}(output_indices);`;
              case "cubic":
                return "output[global_idx] = bicubicInterpolation(output_indices);";
              default:
                throw Error(`Unsupported resize mode: ${t.mode}`);
            }
          })()};
`}
      }`;
          return { name: "Resize", shaderCache: { hint: `${t.cacheKey}|${r}|${h.length > 0 ? h : ""}|${i.length > 0 ? i : ""}|${d.length > 0 ? d : ""}|${B}|${o}`, inputDependencies: ["rank"] }, getShaderSource: te, getRunData: () => ({ outputs: [{ dims: p, dataType: e.dataType }], dispatchGroup: { x: Math.ceil(u / 64) }, programUniforms: [{ type: 12, data: u }, { type: 1, data: h }, { type: 1, data: d }, ...vt(o, p)] }) };
        }, pc = (e) => {
          let t = e.customDataBuffer;
          return new Uint32Array(t, t.byteOffset, 1)[0];
        }, hc = (e, t) => {
          let r = [], n = [], i = [], a = pc(e);
          if (t.antialias !== 0) throw Error("Only default value (0) for Antialias attribute is supported");
          Zd(e.inputs, t, a, r, n, i), e.compute(cc(e.inputs[0], t, a, r, n, i), { inputs: [0] });
        }, mc = (e) => {
          let t = e.antialias, r = e.axes, n = e.coordinateTransformMode, i = e.cubicCoeffA, a = e.excludeOutside !== 0, o = e.extrapolationValue, d = e.keepAspectRatioPolicy, p = e.mode, h = e.nearestMode === "" ? "simple" : e.nearestMode;
          return ot({ antialias: t, axes: r, coordinateTransformMode: n, cubicCoeffA: i, excludeOutside: a, extrapolationValue: o, keepAspectRatioPolicy: d, mode: p, nearestMode: h });
        };
      }), Xt, _c, jr, Vr = w(() => {
        zt(), Bt(), Pt(), Qt(), Xt = (e, t) => {
          let [r, n, i, a] = e, { numHeads: o, rotaryEmbeddingDim: d } = t;
          if (r.dims.length !== 3 && r.dims.length !== 4) throw new Error(`Input 'x' is expected to have 3 or 4 dimensions, got ${r.dims.length}`);
          if (!$e.areEqual(n.dims, []) && !$e.areEqual(n.dims, [1]) && n.dims.length !== 2) throw new Error(`Input 'position_ids' is expected to have 0, 1, or 2 dimensions, got ${n.dims.length}`);
          if (i.dims.length !== 2) throw new Error(`Input 'cos_cache' is expected to have 2 dimensions, got ${i.dims.length}`);
          if (a.dims.length !== 2) throw new Error(`Input 'sin_cache' is expected to have 2 dimensions, got ${a.dims.length}`);
          if (!$e.areEqual(i.dims, a.dims)) throw new Error("Inputs 'cos_cache' and 'sin_cache' are expected to have the same shape");
          if (d > 0 && o === 0) throw new Error("num_heads must be provided if rotary_embedding_dim is specified");
          let p = r.dims[0], h = r.dims[r.dims.length - 2], k = i.dims[0], S = $e.sizeFromDimension(r.dims, 1) / h, u = d === 0 ? i.dims[1] * 2 : S / o;
          if (d > u) throw new Error("rotary_embedding_dim must be less than or equal to head_size");
          if (n.dims.length === 2) {
            if (p !== n.dims[0]) throw new Error(`Input 'position_ids' dimension 0 should be of size batch_size, got ${n.dims[0]}`);
            if (h !== n.dims[1]) throw new Error(`Input 'position_ids' dimension 1 should be of size sequence_length, got ${n.dims[1]}`);
          }
          if (u / 2 !== i.dims[1] && d / 2 !== i.dims[1]) throw new Error(`Input 'cos_cache' dimension 1 should be same as head_size / 2 or rotary_embedding_dim / 2, got ${i.dims[1]}`);
          if (h > k) throw new Error("Updating cos_cache and sin_cache in RotaryEmbedding is not currently supported");
        }, _c = (e, t) => {
          let { interleaved: r, numHeads: n, rotaryEmbeddingDim: i, scale: a } = t, o = e[0].dims[0], d = $e.sizeFromDimension(e[0].dims, 1), p = e[0].dims[e[0].dims.length - 2], h = d / p, k = e[2].dims[1], S = i === 0 ? k * 2 : h / n, u = new Array(o, p, h / S, S - k), B = $e.computeStrides(u), R = [{ type: 1, data: a }, { type: 12, data: u }, { type: 12, data: B }, ...e[0].dims.length === 3 ? new Array({ type: 12, data: [d, h, S, 1] }) : [], ...e[0].dims.length === 4 ? new Array({ type: 12, data: [d, S, p * S, 1] }) : [], ...vt(e[0].dims, e[1].dims, e[2].dims, e[3].dims, e[0].dims)], N = (Z) => {
            let te = ze("input", e[0].dataType, e[0].dims.length), Q = ze("position_ids", e[1].dataType, e[1].dims.length), _e = ze("cos_cache", e[2].dataType, e[2].dims.length), me = ze("sin_cache", e[3].dataType, e[3].dims.length), ye = wt("output", e[0].dataType, e[0].dims.length);
            return Z.registerUniforms([{ name: "scale", type: "f32" }, { name: "global_shape", type: "u32", length: u.length }, { name: "global_strides", type: "u32", length: B.length }, { name: "input_output_strides", type: "u32", length: B.length }]), `
        ${Z.declareVariables(te, Q, _e, me, ye)}

        ${Z.mainStart(Nr)}
          let half_rotary_emb_dim = uniforms.${_e.name}_shape[1];
          let bsnh = global_idx / uniforms.global_strides % uniforms.global_shape;
          let size = uniforms.global_shape[0] * uniforms.global_strides[0];
          ${Z.guardAgainstOutOfBoundsWorkgroupSizes("size")}

          if (bsnh[3] < half_rotary_emb_dim) {
            let position_ids_idx =
                ${Q.broadcastedIndicesToOffset("bsnh.xy", wt("", Q.type.tensor, 2))};
            let position_id =
                u32(${Q.getByOffset("position_ids_idx")}) + select(0, bsnh[1], position_ids_idx == 0);
            let i = dot(bsnh, uniforms.input_output_strides) + select(0, bsnh[3], ${r});
            let j = i + select(half_rotary_emb_dim, 1, ${r});
            let re = ${te.getByOffset("i")} * ${_e.get("position_id", "bsnh[3]")} -
                ${te.getByOffset("j")} * ${me.get("position_id", "bsnh[3]")};
            ${ye.setByOffset("i", "re")}
            let im = ${te.getByOffset("i")} * ${me.get("position_id", "bsnh[3]")} +
                ${te.getByOffset("j")} * ${_e.get("position_id", "bsnh[3]")};
            ${ye.setByOffset("j", "im")}
          } else {
            let k = dot(bsnh, uniforms.input_output_strides) + half_rotary_emb_dim;
            ${ye.setByOffset("k", te.getByOffset("k"))}
          }
        }`;
          };
          return { name: "RotaryEmbedding", shaderCache: { hint: ot({ interleaved: r }).cacheKey, inputDependencies: ["rank", "rank", "rank", "rank"] }, getShaderSource: N, getRunData: () => ({ outputs: [{ dims: e[0].dims, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil($e.size(u) / Nr) }, programUniforms: R }) };
        }, jr = (e, t) => {
          Xt(e.inputs, t), e.compute(_c(e.inputs, t));
        };
      }), Xr, an, pp, hp = w(() => {
        zt(), Bt(), Qt(), Xr = (e) => {
          if (!e || e.length < 3) throw new Error("layerNorm requires at least 3 inputs.");
          let t = e[0], r = e[1], n = e[2];
          if (t.dataType !== r.dataType || t.dataType !== n.dataType) throw new Error("All inputs must have the same data type");
          if (t.dims.length !== 3 && t.dims.length !== 2) throw new Error("Input must be 2D or 3D");
          if (r.dims.length !== 3 && r.dims.length !== 2) throw new Error("Skip must be 2D or 3D");
          let i = t.dims[t.dims.length - 1], a = t.dims[t.dims.length - 2];
          if (r.dims[r.dims.length - 1] !== i) throw new Error("Skip must have the same hidden size as input");
          if (r.dims[r.dims.length - 2] !== a) throw new Error("Skip must have the same sequence length as input");
          if (n.dims.length !== 1) throw new Error("Gamma must be 1D");
          if (n.dims[n.dims.length - 1] !== i) throw new Error("Gamma must have the same hidden size as input");
          if (e.length > 3) {
            let o = e[3];
            if (o.dims.length !== 1) throw new Error("Beta must be 1D");
            if (o.dims[o.dims.length - 1] !== i) throw new Error("Beta must have the same hidden size as input");
          }
          if (e.length > 4) {
            let o = e[4];
            if (o.dims.length !== 1) throw new Error("Bias must be 1D");
            if (o.dims[o.dims.length - 1] !== i) throw new Error("Bias must have the same hidden size as input");
          }
        }, an = (e, t, r, n) => {
          let i = t.simplified, a = e[0].dims, o = $e.size(a), d = a, p = o, h = a.slice(-1)[0], k = n ? a.slice(0, -1).concat(1) : [], S = !i && e.length > 3, u = e.length > 4, B = n && r > 1, R = n && r > 2, N = r > 3, Z = 64, te = yr(h), Q = [{ type: 12, data: p }, { type: 12, data: te }, { type: 12, data: h }, { type: 1, data: t.epsilon }], _e = (ye) => {
            let Ae = [{ name: "output_size", type: "u32" }, { name: "components", type: "u32" }, { name: "hidden_size", type: "u32" }, { name: "epsilon", type: "f32" }], Ie = [ze("x", e[0].dataType, e[0].dims, te), ze("skip", e[1].dataType, e[1].dims, te), ze("gamma", e[2].dataType, e[2].dims, te)];
            S && Ie.push(ze("beta", e[3].dataType, e[3].dims, te)), u && Ie.push(ze("bias", e[4].dataType, e[4].dims, te)), Ie.push(wt("output", e[0].dataType, d, te)), B && Ie.push(wt("mean_output", 1, k)), R && Ie.push(wt("inv_std_output", 1, k)), N && Ie.push(wt("input_skip_bias_sum", e[0].dataType, d, te));
            let Ge = er(e[0].dataType), lt = er(1, te);
            return `

      ${ye.registerUniforms(Ae).declareVariables(...Ie)}
      var<workgroup> sum_shared : array<${lt}, ${Z}>;
      var<workgroup> sum_squared_shared : array<${lt}, ${Z}>;

      ${ye.mainStart([Z, 1, 1])}
        let ix = local_id.x;
        let iy = global_id.x / ${Z};

        let hidden_size_vectorized: u32 = uniforms.hidden_size / uniforms.components;
        var stride = hidden_size_vectorized / ${Z};
        let offset = ix * stride + iy * hidden_size_vectorized;
        let offset1d = stride * ix;
        if (ix == ${Z - 1}) {
          stride = hidden_size_vectorized - stride * ix;
        }
        for (var i: u32 = 0; i < stride; i++) {
          let skip_value = skip[offset + i];
          let bias_value = ${u ? "bias[offset1d + i]" : Ge + "(0.0)"};
          let input_value = x[offset + i];
          let value = input_value + skip_value + bias_value;
          ${N ? "input_skip_bias_sum[offset + i] = value;" : ""}
          output[offset + i] = value;
          let f32_value = ${Or(Ge, te, "value")};
          sum_shared[ix] += f32_value;
          sum_squared_shared[ix] += f32_value * f32_value;
        }
        workgroupBarrier();

        var reduce_size : u32 = ${Z};
        for (var curr_size = reduce_size >> 1;  curr_size > 0; curr_size = reduce_size >> 1) {
          reduce_size = curr_size + (reduce_size & 1);
          if (ix < curr_size) {
            sum_shared[ix] += sum_shared[ix + reduce_size];
            sum_squared_shared[ix] += sum_squared_shared[ix + reduce_size];
          }
          workgroupBarrier();
        }

        let sum = sum_shared[0];
        let square_sum = sum_squared_shared[0];
        let mean = ${qr("sum", te)} / f32(uniforms.hidden_size);
        let inv_std_dev = inverseSqrt(${qr("square_sum", te)} / f32(uniforms.hidden_size) ${i ? "" : "- mean * mean"} + uniforms.epsilon);
        ${B ? "mean_output[global_idx] = mean;" : ""}
        ${R ? "inv_std_output[global_idx] = inv_std_dev;" : ""}

        for (var i: u32 = 0; i < stride; i++) {
          output[offset + i] = (output[offset + i] ${i ? "" : `- ${Ge}(mean)`}) *
            ${Ge}(inv_std_dev) * gamma[offset1d + i]
            ${S ? "+ beta[offset1d + i]" : ""};
        }
      }`;
          }, me = [{ dims: d, dataType: e[0].dataType }];
          return r > 1 && me.push({ dims: k, dataType: 1 }), r > 2 && me.push({ dims: k, dataType: 1 }), r > 3 && me.push({ dims: a, dataType: e[0].dataType }), { name: "SkipLayerNormalization", shaderCache: { hint: `${te};${B};${R};${N}`, inputDependencies: e.map((ye, Ae) => "type") }, getShaderSource: _e, getRunData: () => ({ outputs: me, dispatchGroup: { x: Math.ceil(p / h) }, programUniforms: Q }) };
        }, pp = (e, t) => {
          Xr(e.inputs);
          let r = [0];
          e.outputCount > 1 && r.push(-3), e.outputCount > 2 && r.push(-3), e.outputCount > 3 && r.push(3), e.compute(an(e.inputs, t, e.outputCount, !1), { outputs: r });
        };
      }), fc, _, T, U, be, Fe, Se, Ye, rt = w(() => {
        zt(), Bt(), Pt(), Qt(), fc = (e, t) => {
          if (!e || e.length < 1) throw new Error("too few inputs");
          if (t.axes.length !== 0) {
            if (t.axes.length !== t.starts.length || t.axes.length !== t.ends.length) throw new Error("axes, starts and ends must have the same length");
          } else if (t.starts.length !== t.ends.length) throw new Error("starts and ends must have the same length");
          e.slice(1).forEach((r, n) => {
            if (e[n + 1].dataType !== 6 && e[n + 1].dataType !== 7) throw new Error(`Input ${n} must be an array of int32 or int64`);
          });
        }, _ = (e, t) => {
          let r = [];
          if (e.length > t) if (e[t].dataType === 7) e[t].getBigInt64Array().forEach((n) => r.push(Number(n)));
          else if (e[t].dataType === 6) e[t].getInt32Array().forEach((n) => r.push(Number(n)));
          else throw new Error(`Input ${t} must be an array of int32 or int64`);
          return r;
        }, T = (e, t) => {
          if (e.length > 1) {
            let r = _(e, 1), n = _(e, 2), i = _(e, 3);
            return i.length === 0 && (i = [...Array(e[0].dims.length).keys()]), ot({ starts: r, ends: n, axes: i });
          } else return t;
        }, U = (e, t, r, n, i) => {
          let a = e;
          return e < 0 && (a += r[n[t]]), i[t] < 0 ? Math.max(0, Math.min(a, r[n[t]] - 1)) : Math.max(0, Math.min(a, r[n[t]]));
        }, be = (e, t, r) => `fn calculateInputIndices(output_indices: ${t.type.indices}) -> ${e.type.indices} {
          var input_indices: ${e.type.indices};
          var carry = 0u;
          for (var i = ${r.length}; i >= 0; i--) {
            let input_shape_i = ${Mt("uniforms.input_shape", "i", r.length)};
            let steps_i = ${Mt("uniforms.steps", "i", r.length)};
            let signs_i = ${Mt("uniforms.signs", "i", r.length)};
            let starts_i = ${Mt("uniforms.starts", "i", r.length)};
            var output_index = ${t.indicesGet("output_indices", "i")};
            var input_index = output_index * steps_i + starts_i + carry;
            carry = input_index / input_shape_i;
            input_index = input_index % input_shape_i;
            if (signs_i < 0) {
              input_index = input_shape_i - input_index - 1u + starts_i;
            }
            ${e.indicesSet("input_indices", "i", "input_index")};
          }
          return input_indices;
      }`, Fe = (e, t) => {
          let r = e[0].dims, n = $e.size(r), i = t.axes.length > 0 ? $e.normalizeAxes(t.axes, r.length) : [...Array(r.length).keys()], a = _(e, 4);
          a.forEach((te) => te !== 0 || (() => {
            throw new Error("step cannot be 0");
          })), a.length === 0 && (a = Array(i.length).fill(1));
          let o = t.starts.map((te, Q) => U(te, Q, r, i, a)), d = t.ends.map((te, Q) => U(te, Q, r, i, a));
          if (i.length !== o.length || i.length !== d.length) throw new Error("start, ends and axes should have the same number of elements");
          if (i.length !== r.length) for (let te = 0; te < r.length; ++te) i.includes(te) || (o.splice(te, 0, 0), d.splice(te, 0, r[te]), a.splice(te, 0, 1));
          let p = a.map((te) => Math.sign(te));
          a.forEach((te, Q, _e) => {
            if (te < 0) {
              let me = (d[Q] - o[Q]) / te, ye = o[Q], Ae = ye + me * a[Q];
              o[Q] = Ae, d[Q] = ye, _e[Q] = -te;
            }
          });
          let h = r.slice(0);
          i.forEach((te, Q) => {
            h[te] = Math.ceil((d[te] - o[te]) / a[te]);
          });
          let k = { dims: h, dataType: e[0].dataType }, S = wt("output", e[0].dataType, h.length), u = ze("input", e[0].dataType, e[0].dims.length), B = $e.size(h), R = [{ name: "outputSize", type: "u32" }, { name: "starts", type: "u32", length: o.length }, { name: "signs", type: "i32", length: p.length }, { name: "steps", type: "u32", length: a.length }], N = [{ type: 12, data: B }, { type: 12, data: o }, { type: 6, data: p }, { type: 12, data: a }, ...vt(e[0].dims, h)], Z = (te) => `
      ${te.registerUniforms(R).declareVariables(u, S)}
        ${be(u, S, r)}
        ${te.mainStart()}
          ${te.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.outputSize")}
          let output_indices = ${S.offsetToIndices("global_idx")};
          let input_indices = calculateInputIndices(output_indices);
          ${S.setByOffset("global_idx", u.getByIndices("input_indices"))}
      }`;
          return { name: "Slice", shaderCache: { hint: `${p.length}_${o.length}_${a.length}`, inputDependencies: ["rank"] }, getShaderSource: Z, getRunData: () => ({ outputs: [k], dispatchGroup: { x: Math.ceil(n / 64) }, programUniforms: N }) };
        }, Se = (e, t) => {
          fc(e.inputs, t);
          let r = T(e.inputs, t);
          e.compute(Fe(e.inputs, r), { inputs: [0] });
        }, Ye = (e) => {
          let t = e.starts, r = e.ends, n = e.axes;
          return ot({ starts: t, ends: r, axes: n });
        };
      }), mt, xt, Lt, Ut, Dt = w(() => {
        zt(), Bt(), Pt(), Vs(), Qt(), mt = (e) => {
          if (!e || e.length !== 1) throw new Error("Softmax op requires 1 input.");
        }, xt = (e, t) => {
          let r = e.inputs[0], n = r.dims, i = $e.size(n), a = n.length, o = $e.normalizeAxis(t.axis, a), d = o < n.length - 1, p, h = [];
          d ? (h = Array.from({ length: a }, (Ie, Ge) => Ge), h[o] = a - 1, h[a - 1] = o, p = e.compute(ss(r, h), { inputs: [r], outputs: [-1] })[0]) : p = r;
          let k = p.dims, S = k[a - 1], u = i / S, B = yr(S), R = S / B, N = 64;
          u === 1 && (N = 256);
          let Z = (Ie, Ge) => Ge === 4 ? `max(max(${Ie}.x, ${Ie}.y), max(${Ie}.z, ${Ie}.w))` : Ge === 2 ? `max(${Ie}.x, ${Ie}.y)` : Ge === 3 ? `max(max(${Ie}.x, ${Ie}.y), ${Ie}.z)` : Ie, te = ze("x", p.dataType, p.dims, B), Q = wt("result", p.dataType, p.dims, B), _e = te.type.value, me = er(p.dataType) === "f32" ? `var threadMax = ${_e}(-3.402823e+38f);` : `var threadMax = ${_e}(-65504.0h);`, ye = (Ie) => `
      var<workgroup> rowMaxShared : ${_e};
      var<workgroup> rowSumShared : ${_e};
      var<workgroup> threadShared : array<${_e}, ${N}>;

      fn getValue(row: i32, col: i32, row_stride: i32) -> ${_e} {
        let index = row * row_stride + col;
        return x[index];
      }

      fn setValue(row: i32, col: i32, row_stride: i32, value: ${_e}) {
        let index = row * row_stride + col;
        result[index] = value;
      }
      ${Ie.registerUniform("packedCols", "i32").declareVariables(te, Q)}
      ${Ie.mainStart(N)}
        let gindex = i32(global_idx);
        let lindex = i32(local_idx);
        const wg = ${N};
        let row = gindex / wg;
        let cols = uniforms.packedCols;
        let row_stride : i32 = uniforms.packedCols;

        // find the rows max
        ${me}
        for (var col = lindex; col < cols; col += wg) {
          let value = getValue(row, col, row_stride);
          threadMax = max(threadMax, value);
        }
        if (lindex < cols) {
          threadShared[lindex] = threadMax;
        }
        workgroupBarrier();

        var reduceSize = min(cols, wg);
        for (var currSize = reduceSize >> 1;  currSize > 0; currSize = reduceSize >> 1) {
          reduceSize = currSize + (reduceSize & 1);
          if (lindex < currSize) {
            threadShared[lindex] = max(threadShared[lindex], threadShared[lindex + reduceSize]);
          }
          workgroupBarrier();
        }
        if (lindex == 0) {
          rowMaxShared = ${_e}(${Z("threadShared[0]", B)});
        }
        workgroupBarrier();

        // find the rows sum
        var threadSum = ${_e}(0.0);
        for (var col = lindex; col < cols; col += wg) {
          let subExp = exp(getValue(row, col, row_stride) - rowMaxShared);
          threadSum += subExp;
        }
        threadShared[lindex] = threadSum;
        workgroupBarrier();

        for (var currSize = wg >> 1;  currSize > 0; currSize = currSize >> 1) {
          if (lindex < currSize) {
            threadShared[lindex] = threadShared[lindex] + threadShared[lindex + currSize];
          }
          workgroupBarrier();
        }
        if (lindex == 0) {
          rowSumShared = ${_e}(${qr("threadShared[0]", B)});
        }
        workgroupBarrier();

        // calculate final value for each element in the row
        for (var col = lindex; col < cols; col += wg) {
          let value = exp(getValue(row, col, row_stride) - rowMaxShared) / rowSumShared;
          setValue(row, col, row_stride, value);
        }
      }`, Ae = e.compute({ name: "Softmax", shaderCache: { hint: `${B};${N}`, inputDependencies: ["type"] }, getRunData: () => ({ outputs: [{ dims: k, dataType: p.dataType }], dispatchGroup: { x: u }, programUniforms: [{ type: 6, data: R }] }), getShaderSource: ye }, { inputs: [p], outputs: [d ? -1 : 0] })[0];
          d && e.compute(ss(Ae, h), { inputs: [Ae] });
        }, Lt = (e, t) => {
          mt(e.inputs), xt(e, t);
        }, Ut = (e) => ot({ axis: e.axis });
      }), Wt, Zt, sr, qt, ir, Sr = w(() => {
        zt(), Bt(), Qt(), Wt = (e) => Array.from(e.getBigInt64Array(), Number), Zt = (e) => {
          if (!e || e.length !== 2) throw new Error("Tile requires 2 inputs.");
          if (e[0].dataType !== 1 && e[0].dataType !== 10 && e[0].dataType !== 6 && e[0].dataType !== 12) throw new Error("Tile only support float, float16, int32, and uint32 data types");
          if (e[1].dataType !== 7) throw new Error("Tile `repeats` input should be of int64 data type");
          if (e[1].dims.length !== 1) throw new Error("Tile `repeats` input should be 1-D");
          if (Wt(e[1]).length !== e[0].dims.length) throw new Error("Tile `repeats` input should have same number of elements as rank of input data tensor");
        }, sr = (e, t) => {
          let r = [];
          for (let n = 0; n < e.length; ++n) r.push(e[n] * t[n]);
          return r;
        }, qt = (e, t) => {
          let r = e[0].dims, n = t ?? Wt(e[1]), i = sr(r, n), a = $e.size(i), o = e[0].dataType, d = ze("input", o, r.length), p = wt("output", o, i.length), h = (k) => `
      const inputShape = ${d.indices(...r)};
      ${k.registerUniform("output_size", "u32").declareVariables(d, p)}
      ${k.mainStart()}
      ${k.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.output_size")}
      let output_indices = ${p.offsetToIndices("global_idx")};
      var input_indices: ${d.type.indices};
      for (var i = 0; i < ${r.length}; i++) {
        let input_dim_i = ${d.indicesGet("uniforms.input_shape", "i")};
        let input_dim_value = ${p.indicesGet("output_indices", "i")}  % input_dim_i;

        ${d.indicesSet("input_indices", "i", "input_dim_value")}
      }
      ${p.setByOffset("global_idx", d.getByIndices("input_indices"))}
    }`;
          return { name: "Tile", shaderCache: { hint: `${n}`, inputDependencies: ["rank"] }, getRunData: () => ({ outputs: [{ dims: i, dataType: e[0].dataType }], dispatchGroup: { x: Math.ceil(a / 64) }, programUniforms: [{ type: 12, data: a }, ...vt(e[0].dims, i)] }), getShaderSource: h };
        }, ir = (e) => {
          Zt(e.inputs), e.compute(qt(e.inputs), { inputs: [0] });
        };
      }), xr, dr, Pr, $r = w(() => {
        zt(), Bt(), Qt(), xr = (e, t, r, n, i) => {
          let a = wt("output_data", i, r.length, 4), o = ze("a_data", t[1].dataType, t[1].dims.length, 4), d = ze("b_data", t[2].dataType, t[2].dims.length, 4), p = ze("c_data", t[0].dataType, t[0].dims.length, 4), h, k = (S, u, B) => `select(${u}, ${S}, ${B})`;
          if (!n) h = a.setByOffset("global_idx", k(o.getByOffset("global_idx"), d.getByOffset("global_idx"), p.getByOffset("global_idx")));
          else {
            let S = (u, B, R = "") => {
              let N = `a_data[index_a${B}][component_a${B}]`, Z = `b_data[index_b${B}][component_b${B}]`, te = `bool(c_data[index_c${B}] & (0xffu << (component_c${B} * 8)))`;
              return `
            let output_indices${B} = ${a.offsetToIndices(`global_idx * 4u + ${B}u`)};
            let offset_a${B} = ${o.broadcastedIndicesToOffset(`output_indices${B}`, a)};
            let offset_b${B} = ${d.broadcastedIndicesToOffset(`output_indices${B}`, a)};
            let offset_c${B} = ${p.broadcastedIndicesToOffset(`output_indices${B}`, a)};
            let index_a${B} = offset_a${B} / 4u;
            let index_b${B} = offset_b${B} / 4u;
            let index_c${B} = offset_c${B} / 4u;
            let component_a${B} = offset_a${B} % 4u;
            let component_b${B} = offset_b${B} % 4u;
            let component_c${B} = offset_c${B} % 4u;
            ${u}[${B}] = ${R}(${k(N, Z, te)});
          `;
            };
            i === 9 ? h = `
            var data = vec4<u32>(0);
            ${S("data", 0, "u32")}
            ${S("data", 1, "u32")}
            ${S("data", 2, "u32")}
            ${S("data", 3, "u32")}
            output_data[global_idx] = dot(vec4<u32>(0x1, 0x100, 0x10000, 0x1000000), vec4<u32>(data));` : h = `
            ${S("output_data[global_idx]", 0)}
            ${S("output_data[global_idx]", 1)}
            ${S("output_data[global_idx]", 2)}
            ${S("output_data[global_idx]", 3)}
          `;
          }
          return `
        ${e.registerUniform("vec_size", "u32").declareVariables(p, o, d, a)}
        ${e.mainStart()}
        ${e.guardAgainstOutOfBoundsWorkgroupSizes("uniforms.vec_size")}
        ${h}
      }`;
        }, dr = (e) => {
          let t = e[1].dims, r = e[2].dims, n = e[0].dims, i = e[1].dataType, a = !($e.areEqual(t, r) && $e.areEqual(r, n)), o = t, d = $e.size(t);
          if (a) {
            let h = rr.calcShape(rr.calcShape(t, r, !1), n, !1);
            if (!h) throw new Error("Can't perform where op on the given tensors");
            o = h, d = $e.size(o);
          }
          let p = Math.ceil(d / 4);
          return { name: "Where", shaderCache: { inputDependencies: ["rank", "rank", "rank"] }, getShaderSource: (h) => xr(h, e, o, a, i), getRunData: () => ({ outputs: [{ dims: o, dataType: i }], dispatchGroup: { x: Math.ceil(d / 64 / 4) }, programUniforms: [{ type: 12, data: p }, ...vt(n, t, r, o)] }) };
        }, Pr = (e) => {
          e.compute(dr(e.inputs));
        };
      }), Gr, Ur = w(() => {
        Qa(), ti(), Oc(), al(), Dc(), Hl(), Lc(), Nc(), vu(), Uc(), $u(), Wc(), Vc(), Gc(), Dp(), Bn(), Kc(), qc(), Xc(), Yc(), _d(), ep(), tp(), rp(), Lp(), Uo(), op(), lp(), Wd(), dp(), cp(), Yi(), zp(), Vr(), hp(), rt(), Dt(), pd(), Sr(), Vs(), yo(), $r(), Gr = /* @__PURE__ */ new Map([["Abs", [no]], ["Acos", [ul]], ["Acosh", [dl]], ["Add", [Rl]], ["ArgMax", [Xa, eo]], ["ArgMin", [Zi, eo]], ["Asin", [io]], ["Asinh", [cl]], ["Atan", [pl]], ["Atanh", [oo]], ["Attention", [tl]], ["AveragePool", [Od, Fd]], ["BatchNormalization", [nl]], ["BiasAdd", [ol]], ["BiasSplitGelu", [Ll]], ["Cast", [ml, hl]], ["Ceil", [fl]], ["Clip", [_l]], ["Concat", [Yl, To]], ["Conv", [Fo, hi]], ["ConvTranspose", [bu, wu]], ["Cos", [lo]], ["Cosh", [gl]], ["CumSum", [Tu, Eu]], ["DepthToSpace", [ku, Su]], ["DequantizeLinear", [Ud, up]], ["Div", [Nl]], ["Einsum", [Lu, zu]], ["Elu", [wl, On]], ["Equal", [jl]], ["Erf", [uo]], ["Exp", [yl]], ["Expand", [zn]], ["FastGelu", [ju]], ["Floor", [Ml]], ["FusedConv", [Fo, hi]], ["Gather", [Vu, Wu]], ["GatherElements", [Zu, Ju]], ["GatherBlockQuantized", [Qu, Yu]], ["GatherND", [Ku, Hu]], ["Gelu", [co]], ["Gemm", [rd, zo]], ["GlobalAveragePool", [Ld, Dd]], ["GlobalMaxPool", [Rd, Bd]], ["Greater", [Vl]], ["GreaterOrEqual", [Gl]], ["GridSample", [vi, Qc]], ["GroupQueryAttention", [md]], ["HardSigmoid", [Pl, El]], ["InstanceNormalization", [wd]], ["LayerNormalization", [bd]], ["LeakyRelu", [bl, On]], ["Less", [vo]], ["LessOrEqual", [Kl]], ["Log", [Il]], ["MatMul", [vd]], ["MatMulNBits", [_r, sp]], ["MaxPool", [zd, ap]], ["Mul", [Ul]], ["MultiHeadAttention", [dd, ud]], ["Neg", [po]], ["Not", [vl]], ["Pad", [ip]], ["Pow", [bo]], ["QuickGelu", [Ol, On]], ["Range", [Kd]], ["Reciprocal", [xl]], ["ReduceMin", [Ka]], ["ReduceMean", [Hi]], ["ReduceMax", [Ga]], ["ReduceSum", [Ha]], ["ReduceProd", [Xi]], ["ReduceL1", [Wa]], ["ReduceL2", [Va]], ["ReduceLogSum", [Qi]], ["ReduceLogSumExp", [qi]], ["ReduceSumSquare", [qa]], ["Relu", [Tl]], ["Resize", [hc, mc]], ["RotaryEmbedding", [jr]], ["ScatterND", [Qd, Xd]], ["Sigmoid", [ho]], ["Sin", [mo]], ["Sinh", [Cl]], ["Slice", [Se, Ye]], ["SkipLayerNormalization", [pp]], ["Split", [cd, Ko]], ["Sqrt", [kl]], ["Softmax", [Lt, Ut]], ["Sub", [Wl]], ["Tan", [_o]], ["Tanh", [Sl]], ["ThresholdedRelu", [Al, On]], ["Tile", [ir]], ["Transpose", [Ri, xa]], ["Where", [Pr]]]);
      }), gs, jn = w(() => {
        Qe(), Yr(), Qt(), gs = class {
          constructor(e) {
            this.backend = e, this.repo = /* @__PURE__ */ new Map(), this.attributesBound = !1;
          }
          getArtifact(e) {
            return this.repo.get(e);
          }
          setArtifact(e, t) {
            this.repo.set(e, t);
          }
          run(e, t, r, n, i) {
            Ve(e.programInfo.name);
            let a = this.backend.device, o = this.backend.getComputePassEncoder();
            this.backend.writeTimestamp(this.backend.pendingDispatchNumber * 2);
            let d = [];
            for (let h of t) d.push({ binding: d.length, resource: { buffer: h.buffer } });
            for (let h of r) d.push({ binding: d.length, resource: { buffer: h.buffer } });
            i && d.push({ binding: d.length, resource: i });
            let p = a.createBindGroup({ layout: e.computePipeline.getBindGroupLayout(0), entries: d, label: e.programInfo.name });
            if (this.backend.sessionStatus === "capturing") {
              let h = { kernelId: this.backend.currentKernelId, computePipeline: e.computePipeline, bindGroup: p, dispatchGroup: n };
              this.backend.capturedCommandList.get(this.backend.currentSessionId).push(h);
            }
            o.setPipeline(e.computePipeline), o.setBindGroup(0, p), o.dispatchWorkgroups(...n), this.backend.writeTimestamp(this.backend.pendingDispatchNumber * 2 + 1), this.backend.pendingDispatchNumber++, (this.backend.pendingDispatchNumber >= this.backend.maxDispatchNumber || this.backend.queryType === "at-passes") && this.backend.endComputePass(), this.backend.pendingDispatchNumber >= this.backend.maxDispatchNumber && this.backend.flush(), Ne(e.programInfo.name);
          }
          dispose() {
          }
          build(e, t) {
            Ve(e.name);
            let r = this.backend.device, n = [];
            [{ feature: "shader-f16", extension: "f16" }, { feature: "subgroups", extension: "subgroups" }, { feature: "subgroups-f16", extension: "subgroups_f16" }].forEach((h) => {
              r.features.has(h.feature) && n.push(`enable ${h.extension};`);
            });
            let i = bn(t, this.backend.device.limits), a = e.getShaderSource(i), o = `${n.join(`
`)}
${i.additionalImplementations}
${a}`, d = r.createShaderModule({ code: o, label: e.name });
            or("verbose", () => `[WebGPU] ${e.name} shader code: ${o}`);
            let p = r.createComputePipeline({ compute: { module: d, entryPoint: "main" }, layout: "auto", label: e.name });
            return Ne(e.name), { programInfo: e, computePipeline: p, uniformVariablesInfo: i.variablesInfo };
          }
          normalizeDispatchGroupSize(e) {
            let t = typeof e == "number" ? e : e.x, r = typeof e == "number" ? 1 : e.y || 1, n = typeof e == "number" ? 1 : e.z || 1, i = this.backend.device.limits.maxComputeWorkgroupsPerDimension;
            if (t <= i && r <= i && n <= i) return [t, r, n];
            let a = t * r * n, o = Math.ceil(Math.sqrt(a));
            if (o > i) {
              if (o = Math.ceil(Math.cbrt(a)), o > i) throw new Error("Total dispatch size exceeds WebGPU maximum.");
              return [o, o, o];
            } else return [o, o, 1];
          }
        };
      }), aa, ns, Ps, la, Ti, ua = w(() => {
        Qe(), zt(), Yr(), Fn(), yt(), Ur(), jn(), aa = (e, t) => {
          if (t.length !== e.length) throw new Error(`inputDependencies length ${t.length} is not equal to inputTensors length ${e.length}.`);
          let r = [];
          for (let n = 0; n < e.length; ++n) {
            let i = e[n].dataType;
            switch (t[n]) {
              case "none": {
                r.push("");
                break;
              }
              case "type": {
                r.push(`${i}`);
                break;
              }
              case "rank": {
                let a = e[n].dims.length;
                r.push(`${i};${a}`);
                break;
              }
              case "dims": {
                let a = e[n].dims.join(",");
                r.push(`${i};${a}`);
                break;
              }
              default:
                throw new Error(`unsupported input dependency: ${t[n]}`);
            }
          }
          return r.join("|");
        }, ns = (e, t, r) => {
          var i, a;
          let n = e.name;
          return (i = e.shaderCache) != null && i.hint && (n += "[" + e.shaderCache.hint + "]"), n += ":" + r + `:${aa(t, ((a = e.shaderCache) == null ? void 0 : a.inputDependencies) ?? new Array(t.length).fill("dims"))}`, n;
        }, Ps = class {
          constructor(e) {
            e && (this.architecture = e.architecture, this.vendor = e.vendor);
          }
          isArchitecture(e) {
            return this.architecture === e;
          }
          isVendor(e) {
            return this.vendor === e;
          }
        }, la = class {
          constructor(e) {
            this.subgroupsSupported = e.features.has("subgroups"), this.subgroupsF16Supported = e.features.has("subgroups");
            let t = e.limits;
            !this.subgroupsSupported || !t.minSubgroupSize || !t.maxSubgroupSize ? this.subgroupSizeRange = void 0 : this.subgroupSizeRange = [t.minSubgroupSize, t.maxSubgroupSize];
          }
        }, Ti = class {
          constructor() {
            this.currentSessionId = null, this.currentKernelId = null, this.commandEncoder = null, this.computePassEncoder = null, this.maxDispatchNumber = 16, this.pendingDispatchNumber = 0, this.pendingKernels = [], this.pendingQueries = /* @__PURE__ */ new Map(), this.sessionStatus = "default", this.capturedCommandList = /* @__PURE__ */ new Map(), this.capturedPendingKernels = /* @__PURE__ */ new Map(), this.sessionExternalDataMapping = /* @__PURE__ */ new Map();
          }
          get currentKernelCustomData() {
            if (this.currentKernelId === null) throw new Error("currentKernelCustomData(): currentKernelId is null. (should not happen)");
            let e = this.kernelCustomData.get(this.currentKernelId);
            return e || (e = {}, this.kernelCustomData.set(this.currentKernelId, e)), e;
          }
          async initialize(e, t) {
            this.env = e;
            let r = [], n = { requiredLimits: { maxComputeWorkgroupStorageSize: t.limits.maxComputeWorkgroupStorageSize, maxComputeWorkgroupsPerDimension: t.limits.maxComputeWorkgroupsPerDimension, maxStorageBufferBindingSize: t.limits.maxStorageBufferBindingSize, maxBufferSize: t.limits.maxBufferSize, maxComputeInvocationsPerWorkgroup: t.limits.maxComputeInvocationsPerWorkgroup, maxComputeWorkgroupSizeX: t.limits.maxComputeWorkgroupSizeX, maxComputeWorkgroupSizeY: t.limits.maxComputeWorkgroupSizeY, maxComputeWorkgroupSizeZ: t.limits.maxComputeWorkgroupSizeZ }, requiredFeatures: r }, i = (a) => t.features.has(a) && r.push(a) && !0;
            i("chromium-experimental-timestamp-query-inside-passes") || i("timestamp-query"), i("shader-f16"), i("subgroups") && i("subgroups-f16"), this.device = await t.requestDevice(n), this.deviceInfo = new la(this.device), this.adapterInfo = new Ps(t.info || await t.requestAdapterInfo()), this.gpuDataManager = ct(this), this.programManager = new gs(this), this.kernels = /* @__PURE__ */ new Map(), this.kernelPersistentData = /* @__PURE__ */ new Map(), this.kernelCustomData = /* @__PURE__ */ new Map(), gn(e.logLevel, !!e.debug), this.device.onuncapturederror = (a) => {
              a.error instanceof GPUValidationError && console.error(`An uncaught WebGPU validation error was raised: ${a.error.message}`);
            }, Object.defineProperty(this.env.webgpu, "device", { value: this.device, writable: !1, enumerable: !0, configurable: !1 }), Object.defineProperty(this.env.webgpu, "adapter", { value: t, writable: !1, enumerable: !0, configurable: !1 }), this.setQueryType();
          }
          dispose() {
            typeof this.querySet < "u" && this.querySet.destroy(), this.gpuDataManager.dispose();
          }
          getCommandEncoder() {
            return this.commandEncoder || (this.commandEncoder = this.device.createCommandEncoder()), this.commandEncoder;
          }
          getComputePassEncoder() {
            if (!this.computePassEncoder) {
              let e = this.getCommandEncoder(), t = {};
              this.queryType === "at-passes" && (t.timestampWrites = { querySet: this.querySet, beginningOfPassWriteIndex: this.pendingDispatchNumber * 2, endOfPassWriteIndex: this.pendingDispatchNumber * 2 + 1 }), this.computePassEncoder = e.beginComputePass(t);
            }
            return this.computePassEncoder;
          }
          endComputePass() {
            this.computePassEncoder && (this.computePassEncoder.end(), this.computePassEncoder = null);
          }
          flush() {
            if (!this.commandEncoder) return;
            Ve(), this.endComputePass();
            let e;
            this.queryType !== "none" && (this.commandEncoder.resolveQuerySet(this.querySet, 0, this.pendingDispatchNumber * 2, this.queryResolveBuffer, 0), e = this.device.createBuffer({ size: this.pendingDispatchNumber * 2 * 8, usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST }), this.pendingQueries.set(e, this.pendingKernels), this.pendingKernels = [], this.commandEncoder.copyBufferToBuffer(this.queryResolveBuffer, 0, e, 0, this.pendingDispatchNumber * 2 * 8)), this.device.queue.submit([this.commandEncoder.finish()]), this.gpuDataManager.refreshPendingBuffers(), this.commandEncoder = null, this.pendingDispatchNumber = 0, this.queryType !== "none" && e.mapAsync(GPUMapMode.READ).then(() => {
              var n;
              let t = new BigUint64Array(e.getMappedRange()), r = this.pendingQueries.get(e);
              for (let i = 0; i < t.length / 2; i++) {
                let a = r[i], o = a.kernelId, d = this.kernels.get(o), p = d.kernelType, h = d.kernelName, k = a.programName, S = a.inputTensorViews, u = a.outputTensorViews, B = t[i * 2], R = t[i * 2 + 1];
                typeof this.queryTimeBase > "u" && (this.queryTimeBase = B);
                let N = Number(B - this.queryTimeBase), Z = Number(R - this.queryTimeBase);
                if (!Number.isSafeInteger(N) || !Number.isSafeInteger(Z)) throw new RangeError("incorrect timestamp range");
                if ((n = this.env.webgpu.profiling) != null && n.ondata) this.env.webgpu.profiling.ondata({ version: 1, inputsMetadata: S.map((te) => ({ dims: te.dims, dataType: xs(te.dataType) })), outputsMetadata: u.map((te) => ({ dims: te.dims, dataType: xs(te.dataType) })), kernelId: o, kernelType: p, kernelName: h, programName: k, startTime: N, endTime: Z });
                else {
                  let te = "";
                  S.forEach((_e, me) => {
                    te += `input[${me}]: [${_e.dims}] | ${xs(_e.dataType)}, `;
                  });
                  let Q = "";
                  u.forEach((_e, me) => {
                    Q += `output[${me}]: [${_e.dims}] | ${xs(_e.dataType)}, `;
                  }), console.log(`[profiling] kernel "${o}|${p}|${h}|${k}" ${te}${Q}execution time: ${Z - N} ns`);
                }
                Re("GPU", `${k}::${B}::${R}`);
              }
              e.unmap(), this.pendingQueries.delete(e);
            }), Ne();
          }
          run(e, t, r, n, i, a) {
            Ve(e.name);
            let o = [];
            for (let Q = 0; Q < t.length; ++Q) {
              let _e = t[Q].data;
              if (_e === 0) continue;
              let me = this.gpuDataManager.get(_e);
              if (!me) throw new Error(`no GPU data for input: ${_e}`);
              o.push(me);
            }
            let { outputs: d, dispatchGroup: p, programUniforms: h } = e.getRunData(t), k = r.length === 0 ? d.map((Q, _e) => _e) : r;
            if (k.length !== d.length) throw new Error(`Output size ${k.length} must be equal to ${d.length}.`);
            let S = [], u = [];
            for (let Q = 0; Q < d.length; ++Q) {
              if (!Number.isInteger(k[Q]) || k[Q] < -3 || k[Q] >= a) throw new Error(`Invalid output index: ${k[Q]}`);
              if (k[Q] === -3) continue;
              let _e = k[Q] === -1, me = k[Q] === -2, ye = _e || me ? i(d[Q].dataType, d[Q].dims) : n(k[Q], d[Q].dataType, d[Q].dims);
              if (S.push(ye), ye.data === 0) continue;
              let Ae = this.gpuDataManager.get(ye.data);
              if (!Ae) throw new Error(`no GPU data for output: ${ye.data}`);
              if (_e && this.temporaryData.push(Ae), me) {
                let Ie = this.kernelPersistentData.get(this.currentKernelId);
                Ie || (Ie = [], this.kernelPersistentData.set(this.currentKernelId, Ie)), Ie.push(Ae);
              }
              u.push(Ae);
            }
            if (o.length !== t.length || u.length !== S.length) {
              if (u.length === 0) return Ne(e.name), S;
              throw new Error(`Program ${e.name} has zero-sized tensor(s) in inputs or outputs. This is not supported now.`);
            }
            let B;
            if (h) {
              let Q = 0, _e = [];
              h.forEach((Ie) => {
                let Ge = typeof Ie.data == "number" ? [Ie.data] : Ie.data;
                if (Ge.length === 0) return;
                let lt = Ie.type === 10 ? 2 : 4, Tt, Kt;
                Ie.type === 10 ? (Kt = Ge.length > 4 ? 16 : Ge.length > 2 ? 8 : Ge.length * lt, Tt = Ge.length > 4 ? 16 : lt * Ge.length) : (Kt = Ge.length <= 2 ? Ge.length * lt : 16, Tt = 16), Q = Math.ceil(Q / Kt) * Kt, _e.push(Q);
                let Yt = Ie.type === 10 ? 8 : 4;
                Q += Ge.length > 4 ? Math.ceil(Ge.length / Yt) * Tt : Ge.length * lt;
              });
              let me = 16;
              Q = Math.ceil(Q / me) * me;
              let ye = new ArrayBuffer(Q);
              h.forEach((Ie, Ge) => {
                let lt = _e[Ge], Tt = typeof Ie.data == "number" ? [Ie.data] : Ie.data;
                if (Ie.type === 6) new Int32Array(ye, lt, Tt.length).set(Tt);
                else if (Ie.type === 12) new Uint32Array(ye, lt, Tt.length).set(Tt);
                else if (Ie.type === 10) new Uint16Array(ye, lt, Tt.length).set(Tt);
                else if (Ie.type === 1) new Float32Array(ye, lt, Tt.length).set(Tt);
                else throw new Error(`Unsupported uniform type: ${xs(Ie.type)}`);
              });
              let Ae = this.gpuDataManager.create(Q, GPUBufferUsage.COPY_DST | GPUBufferUsage.UNIFORM);
              this.device.queue.writeBuffer(Ae.buffer, 0, ye, 0, Q), this.gpuDataManager.release(Ae.id), B = { offset: 0, size: Q, buffer: Ae.buffer };
            }
            let R = this.programManager.normalizeDispatchGroupSize(p), N = R[1] === 1 && R[2] === 1, Z = ns(e, t, N), te = this.programManager.getArtifact(Z);
            if (te || (te = this.programManager.build(e, R), this.programManager.setArtifact(Z, te), or("info", () => `[artifact] key: ${Z}, programName: ${e.name}`)), h && te.uniformVariablesInfo) {
              if (h.length !== te.uniformVariablesInfo.length) throw new Error(`Uniform variables count mismatch: expect ${te.uniformVariablesInfo.length}, got ${h.length} in program "${te.programInfo.name}".`);
              for (let Q = 0; Q < h.length; Q++) {
                let _e = h[Q], me = _e.type, ye = typeof _e.data == "number" ? 1 : _e.data.length, [Ae, Ie] = te.uniformVariablesInfo[Q];
                if (me !== Ae || ye !== Ie) throw new Error(`Uniform variable ${Q} mismatch: expect type ${Ae} with size ${Ie}, got type ${me} with size ${ye} in program "${te.programInfo.name}".`);
              }
            }
            if (or("info", () => `[ProgramManager] run "${e.name}" (key=${Z}) with ${R[0]}x${R[1]}x${R[2]}`), this.queryType !== "none" || this.sessionStatus === "capturing") {
              let Q = { kernelId: this.currentKernelId, programName: te.programInfo.name, inputTensorViews: t, outputTensorViews: S };
              this.pendingKernels.push(Q), this.sessionStatus === "capturing" && this.capturedPendingKernels.get(this.currentSessionId).push(Q);
            }
            return this.programManager.run(te, o, u, R, B), Ne(e.name), S;
          }
          upload(e, t) {
            this.gpuDataManager.upload(e, t);
          }
          memcpy(e, t) {
            this.gpuDataManager.memcpy(e, t);
          }
          async download(e, t) {
            await this.gpuDataManager.download(e, t);
          }
          alloc(e) {
            return this.gpuDataManager.create(e).id;
          }
          free(e) {
            return this.gpuDataManager.release(e);
          }
          createKernel(e, t, r, n) {
            let i = Gr.get(e);
            if (!i) throw new Error(`kernel not implemented: ${e}`);
            let a = { kernelType: e, kernelName: n, kernelEntry: i[0], attributes: [i[1], r] };
            this.kernels.set(t, a);
          }
          releaseKernel(e) {
            let t = this.kernelPersistentData.get(e);
            if (t) {
              for (let r of t) this.gpuDataManager.release(r.id);
              this.kernelPersistentData.delete(e);
            }
            this.kernelCustomData.delete(e), this.kernels.delete(e);
          }
          computeKernel(e, t, r) {
            let n = this.kernels.get(e);
            if (!n) throw new Error(`kernel not created: ${e}`);
            let i = n.kernelType, a = n.kernelName, o = n.kernelEntry, d = n.attributes;
            if (this.currentKernelId !== null) throw new Error(`kernel "[${i}] ${a}" is not allowed to be called recursively`);
            this.currentKernelId = e, d[0] && (d[1] = d[0](d[1]), d[0] = void 0), or("info", () => `[WebGPU] Start to run kernel "[${i}] ${a}"...`);
            let p = this.env.debug;
            this.temporaryData = [];
            try {
              return p && this.device.pushErrorScope("validation"), o(t, d[1]), 0;
            } catch (h) {
              return r.push(Promise.resolve(`[WebGPU] Kernel "[${i}] ${a}" failed. ${h}`)), 1;
            } finally {
              p && r.push(this.device.popErrorScope().then((h) => h ? `GPU validation error for kernel "[${i}] ${a}": ${h.message}` : null));
              for (let h of this.temporaryData) this.gpuDataManager.release(h.id);
              this.temporaryData = [], this.currentKernelId = null;
            }
          }
          registerBuffer(e, t, r, n) {
            let i = this.sessionExternalDataMapping.get(e);
            i || (i = /* @__PURE__ */ new Map(), this.sessionExternalDataMapping.set(e, i));
            let a = i.get(t), o = this.gpuDataManager.registerExternalBuffer(r, n, a);
            return i.set(t, [o, r]), o;
          }
          unregisterBuffers(e) {
            let t = this.sessionExternalDataMapping.get(e);
            t && (t.forEach((r) => this.gpuDataManager.unregisterExternalBuffer(r[0])), this.sessionExternalDataMapping.delete(e));
          }
          getBuffer(e) {
            let t = this.gpuDataManager.get(e);
            if (!t) throw new Error(`no GPU data for buffer: ${e}`);
            return t.buffer;
          }
          createDownloader(e, t, r) {
            return async () => {
              let n = await Pe(this, e, t);
              return wn(n.buffer, r);
            };
          }
          writeTimestamp(e) {
            this.queryType === "inside-passes" && this.computePassEncoder.writeTimestamp(this.querySet, e);
          }
          setQueryType() {
            var e;
            this.queryType = "none", (((e = this.env.webgpu.profiling) == null ? void 0 : e.mode) === "default" || (typeof this.env.trace > "u" ? this.env.wasm.trace : this.env.trace)) && (this.device.features.has("chromium-experimental-timestamp-query-inside-passes") ? this.queryType = "inside-passes" : this.device.features.has("timestamp-query") && (this.queryType = "at-passes"), this.queryType !== "none" && typeof this.querySet > "u" && (this.querySet = this.device.createQuerySet({ type: "timestamp", count: this.maxDispatchNumber * 2 }), this.queryResolveBuffer = this.device.createBuffer({ size: this.maxDispatchNumber * 2 * 8, usage: GPUBufferUsage.COPY_SRC | GPUBufferUsage.QUERY_RESOLVE })));
          }
          captureBegin() {
            or("info", "captureBegin"), this.capturedCommandList.get(this.currentSessionId) || this.capturedCommandList.set(this.currentSessionId, []), this.capturedPendingKernels.get(this.currentSessionId) || this.capturedPendingKernels.set(this.currentSessionId, []), this.flush(), this.sessionStatus = "capturing";
          }
          captureEnd() {
            or("info", "captureEnd"), this.flush(), this.sessionStatus = "default";
          }
          replay() {
            or("info", "replay"), this.sessionStatus = "replaying";
            let e = this.capturedCommandList.get(this.currentSessionId), t = this.capturedPendingKernels.get(this.currentSessionId), r = e.length;
            this.pendingKernels = [];
            for (let n = 0; n < r; n++) {
              let i = this.getComputePassEncoder(), a = e[n];
              this.writeTimestamp(this.pendingDispatchNumber * 2), i.setPipeline(a.computePipeline), i.setBindGroup(0, a.bindGroup), i.dispatchWorkgroups(...a.dispatchGroup), this.writeTimestamp(this.pendingDispatchNumber * 2 + 1), this.pendingDispatchNumber++, this.queryType !== "none" && this.pendingKernels.push(t[n]), (this.pendingDispatchNumber >= this.maxDispatchNumber || this.queryType === "at-passes") && this.endComputePass(), this.pendingDispatchNumber >= this.maxDispatchNumber && this.flush();
            }
            this.flush(), this.sessionStatus = "default";
          }
          onCreateSession() {
            this.gpuDataManager.onCreateSession();
          }
          onReleaseSession(e) {
            this.unregisterBuffers(e), this.capturedCommandList.has(e) && this.capturedCommandList.delete(e), this.capturedPendingKernels.has(e) && this.capturedPendingKernels.delete(e), this.gpuDataManager.onReleaseSession(e);
          }
          onRunStart(e) {
            this.currentSessionId = e, this.setQueryType();
          }
        };
      }), Ei, Tr, Dr, Ds, Hs, ln, Pi, da, gc = w(() => {
        Yr(), Ei = 1, Tr = () => Ei++, Dr = /* @__PURE__ */ new Map([["float32", 32], ["float16", 16], ["int32", 32], ["uint32", 32], ["int64", 64], ["uint64", 64], ["int8", 8], ["uint8", 8], ["int4", 4], ["uint4", 4]]), Ds = (e, t) => {
          let r = Dr.get(e);
          if (!r) throw new Error("Unsupported data type.");
          return t.length > 0 ? Math.ceil(t.reduce((n, i) => n * i) * r / 8) : 0;
        }, Hs = class {
          constructor(e) {
            this.sessionId = e.sessionId, this.mlContext = e.context, this.mlTensor = e.tensor, this.dataType = e.dataType, this.tensorShape = e.shape;
          }
          get tensor() {
            return this.mlTensor;
          }
          get type() {
            return this.dataType;
          }
          get shape() {
            return this.tensorShape;
          }
          get byteLength() {
            return Ds(this.dataType, this.tensorShape);
          }
          destroy() {
            or("verbose", () => "[WebNN] TensorWrapper.destroy"), this.mlTensor.destroy();
          }
          write(e) {
            this.mlContext.writeTensor(this.mlTensor, e);
          }
          async read(e) {
            return e ? this.mlContext.readTensor(this.mlTensor, e) : this.mlContext.readTensor(this.mlTensor);
          }
          sameTypeAndShape(e, t) {
            return this.dataType === e && this.tensorShape.length === t.length && this.tensorShape.every((r, n) => r === t[n]);
          }
        }, ln = class {
          constructor(e, t) {
            this.tensorManager = e, this.wrapper = t;
          }
          get tensorWrapper() {
            return this.wrapper;
          }
          releaseTensor() {
            this.tensorWrapper && (this.tensorManager.releaseTensor(this.tensorWrapper), this.wrapper = void 0);
          }
          async ensureTensor(e, t, r) {
            if (this.wrapper) {
              if (this.wrapper.sameTypeAndShape(e, t)) return this.wrapper.tensor;
              if (r) {
                if (this.wrapper.byteLength !== Ds(e, t)) throw new Error("Unable to copy data to tensor with different size.");
                this.activeUpload = new Uint8Array(await this.wrapper.read());
              }
              this.tensorManager.releaseTensor(this.wrapper);
            }
            let n = typeof MLTensorUsage > "u" ? void 0 : MLTensorUsage.READ | MLTensorUsage.WRITE;
            return this.wrapper = await this.tensorManager.getCachedTensor(e, t, n, !0, !0), r && this.activeUpload && (this.wrapper.write(this.activeUpload), this.activeUpload = void 0), this.wrapper.tensor;
          }
          upload(e) {
            if (this.wrapper) if (e.byteLength === this.wrapper.byteLength) {
              this.wrapper.write(e);
              return;
            } else or("verbose", () => "Data size does not match tensor size. Releasing tensor."), this.releaseTensor();
            this.activeUpload ? this.activeUpload.set(e) : this.activeUpload = new Uint8Array(e);
          }
          async download(e) {
            if (this.activeUpload) if (e) {
              e instanceof ArrayBuffer ? new Uint8Array(e).set(this.activeUpload) : new Uint8Array(e.buffer, e.byteOffset, e.byteLength).set(this.activeUpload);
              return;
            } else return this.activeUpload.buffer;
            if (!this.wrapper) throw new Error("Tensor has not been created.");
            return e ? this.wrapper.read(e) : this.wrapper.read();
          }
        }, Pi = class {
          constructor(e) {
            this.backend = e, this.tensorTrackersById = /* @__PURE__ */ new Map(), this.freeTensors = [], this.externalTensors = /* @__PURE__ */ new Set();
          }
          reserveTensorId() {
            let e = Tr();
            return this.tensorTrackersById.set(e, new ln(this)), e;
          }
          releaseTensorId(e) {
            let t = this.tensorTrackersById.get(e);
            t && (this.tensorTrackersById.delete(e), t.tensorWrapper && this.releaseTensor(t.tensorWrapper));
          }
          async ensureTensor(e, t, r, n) {
            or("verbose", () => `[WebNN] TensorManager.ensureTensor {tensorId: ${e}, dataType: ${t}, shape: ${r}, copyOld: ${n}}`);
            let i = this.tensorTrackersById.get(e);
            if (!i) throw new Error("Tensor not found.");
            return i.ensureTensor(t, r, n);
          }
          upload(e, t) {
            let r = this.tensorTrackersById.get(e);
            if (!r) throw new Error("Tensor not found.");
            r.upload(t);
          }
          async download(e, t) {
            or("verbose", () => `[WebNN] TensorManager.download {tensorId: ${e}, dstBuffer: ${t == null ? void 0 : t.byteLength}}`);
            let r = this.tensorTrackersById.get(e);
            if (!r) throw new Error("Tensor not found.");
            return r.download(t);
          }
          releaseTensorsForSession(e) {
            for (let t of this.freeTensors) t.sessionId === e && t.destroy();
            this.freeTensors = this.freeTensors.filter((t) => t.sessionId !== e);
          }
          registerTensor(e, t, r, n) {
            let i = Tr(), a = new Hs({ sessionId: this.backend.currentSessionId, context: e, tensor: t, dataType: r, shape: n });
            return this.tensorTrackersById.set(i, new ln(this, a)), this.externalTensors.add(a), i;
          }
          async getCachedTensor(e, t, r, n, i) {
            let a = this.backend.currentSessionId;
            for (let [p, h] of this.freeTensors.entries()) if (h.sameTypeAndShape(e, t)) {
              or("verbose", () => `[WebNN] Reusing tensor {dataType: ${e}, shape: ${t}}`);
              let k = this.freeTensors.splice(p, 1)[0];
              return k.sessionId = a, k;
            }
            let o = this.backend.currentContext;
            or("verbose", () => `[WebNN] MLContext.createTensor {dataType: ${e}, shape: ${t}}`);
            let d = await o.createTensor({ dataType: e, shape: t, dimensions: t, usage: r, writable: n, readable: i });
            return new Hs({ sessionId: a, context: o, tensor: d, dataType: e, shape: t });
          }
          releaseTensor(e) {
            this.externalTensors.has(e) && this.externalTensors.delete(e), this.freeTensors.push(e);
          }
        }, da = (...e) => new Pi(...e);
      }), Ci, mp, ca, pa = w(() => {
        zt(), Ft(), Fn(), gc(), Yr(), Ci = /* @__PURE__ */ new Map([[1, "float32"], [10, "float16"], [6, "int32"], [12, "uint32"], [7, "int64"], [13, "uint64"], [22, "int4"], [21, "uint4"], [3, "int8"], [2, "uint8"], [9, "uint8"]]), mp = (e, t) => {
          if (e === t) return !0;
          if (e === void 0 || t === void 0) return !1;
          let r = Object.keys(e).sort(), n = Object.keys(t).sort();
          return r.length === n.length && r.every((i, a) => i === n[a] && e[i] === t[i]);
        }, ca = class {
          constructor(e) {
            this.tensorManager = da(this), this.mlContextBySessionId = /* @__PURE__ */ new Map(), this.sessionIdsByMLContext = /* @__PURE__ */ new Map(), this.mlContextCache = [], gn(e.logLevel, !!e.debug);
          }
          get currentSessionId() {
            if (this.activeSessionId === void 0) throw new Error("No active session");
            return this.activeSessionId;
          }
          onRunStart(e) {
            this.activeSessionId = e;
          }
          async createMLContext(e) {
            if (e instanceof GPUDevice) {
              let r = this.mlContextCache.findIndex((n) => n.gpuDevice === e);
              if (r !== -1) return this.mlContextCache[r].mlContext;
              {
                let n = await navigator.ml.createContext(e);
                return this.mlContextCache.push({ gpuDevice: e, mlContext: n }), n;
              }
            } else if (e === void 0) {
              let r = this.mlContextCache.findIndex((n) => n.options === void 0 && n.gpuDevice === void 0);
              if (r !== -1) return this.mlContextCache[r].mlContext;
              {
                let n = await navigator.ml.createContext();
                return this.mlContextCache.push({ mlContext: n }), n;
              }
            }
            let t = this.mlContextCache.findIndex((r) => mp(r.options, e));
            if (t !== -1) return this.mlContextCache[t].mlContext;
            {
              let r = await navigator.ml.createContext(e);
              return this.mlContextCache.push({ options: e, mlContext: r }), r;
            }
          }
          get currentContext() {
            let e = this.getMLContext(this.currentSessionId);
            if (!e) throw new Error(`No MLContext found for session ${this.currentSessionId}`);
            return e;
          }
          registerMLContext(e, t) {
            this.mlContextBySessionId.set(e, t);
            let r = this.sessionIdsByMLContext.get(t);
            r || (r = /* @__PURE__ */ new Set(), this.sessionIdsByMLContext.set(t, r)), r.add(e);
          }
          onReleaseSession(e) {
            let t = this.mlContextBySessionId.get(e);
            if (!t) return;
            this.tensorManager.releaseTensorsForSession(e), this.mlContextBySessionId.delete(e);
            let r = this.sessionIdsByMLContext.get(t);
            if (r.delete(e), r.size === 0) {
              this.sessionIdsByMLContext.delete(t);
              let n = this.mlContextCache.findIndex((i) => i.mlContext === t);
              n !== -1 && this.mlContextCache.splice(n, 1);
            }
          }
          getMLContext(e) {
            return this.mlContextBySessionId.get(e);
          }
          reserveTensorId() {
            return this.tensorManager.reserveTensorId();
          }
          releaseTensorId(e) {
            or("verbose", () => `[WebNN] releaseTensorId {tensorId: ${e}}`), this.tensorManager.releaseTensorId(e);
          }
          async ensureTensor(e, t, r, n) {
            let i = Ci.get(t);
            if (!i) throw new Error(`Unsupported ONNX data type: ${t}`);
            return this.tensorManager.ensureTensor(e, i, r, n);
          }
          uploadTensor(e, t) {
            if (!_t().shouldTransferToMLTensor) throw new Error("Trying to upload to a MLTensor while shouldTransferToMLTensor is false");
            or("verbose", () => `[WebNN] uploadTensor {tensorId: ${e}, data: ${t.byteLength}}`), this.tensorManager.upload(e, t);
          }
          async downloadTensor(e, t) {
            return this.tensorManager.download(e, t);
          }
          createMLTensorDownloader(e, t) {
            return async () => {
              let r = await this.tensorManager.download(e);
              return wn(r, t);
            };
          }
          registerMLTensor(e, t, r) {
            let n = Ci.get(t);
            if (!n) throw new Error(`Unsupported ONNX data type: ${t}`);
            let i = this.tensorManager.registerTensor(this.currentContext, e, n, r);
            return or("verbose", () => `[WebNN] registerMLTensor {tensor: ${e}, dataType: ${n}, dimensions: ${r}} -> {tensorId: ${i}}`), i;
          }
          registerMLConstant(e, t, r, n, i, a) {
            if (!a) throw new Error("External mounted files are not available.");
            let o = e;
            e.startsWith("./") && (o = e.substring(2));
            let d = a.get(o);
            if (!d) throw new Error(`File with name ${o} not found in preloaded files.`);
            if (t + r > d.byteLength) throw new Error("Out of bounds: data offset and length exceed the external file data size.");
            let p = d.slice(t, t + r).buffer, h;
            switch (i.dataType) {
              case "float32":
                h = new Float32Array(p);
                break;
              case "float16":
                h = new Uint16Array(p);
                break;
              case "int32":
                h = new Int32Array(p);
                break;
              case "uint32":
                h = new Uint32Array(p);
                break;
              case "int64":
                h = new BigInt64Array(p);
                break;
              case "uint64":
                h = new BigUint64Array(p);
                break;
              case "int8":
                h = new Int8Array(p);
                break;
              case "int4":
              case "uint4":
              case "uint8":
                h = new Uint8Array(p);
                break;
              default:
                throw new Error(`Unsupported data type: ${i.dataType} in creating WebNN Constant from external data.`);
            }
            return or("verbose", () => `[WebNN] registerMLConstant {dataType: ${i.dataType}, shape: ${i.shape}}}`), n.constant(i, h);
          }
          flush() {
          }
        };
      }), ha = {};
      x(ha, { init: () => Si });
      var ki, ma, Si, Bp = w(() => {
        zt(), ua(), Yr(), Bt(), pa(), ki = class s_ {
          constructor(t, r, n, i) {
            this.module = t, this.dataType = r, this.data = n, this.dims = i;
          }
          getFloat32Array() {
            if (this.dataType !== 1) throw new Error("Invalid data type");
            let t = $e.size(this.dims);
            return t === 0 ? new Float32Array() : new Float32Array(this.module.HEAP8.buffer, this.data, t);
          }
          getBigInt64Array() {
            if (this.dataType !== 7) throw new Error("Invalid data type");
            let t = $e.size(this.dims);
            return t === 0 ? new BigInt64Array() : new BigInt64Array(this.module.HEAP8.buffer, this.data, t);
          }
          getInt32Array() {
            if (this.dataType !== 6) throw new Error("Invalid data type");
            let t = $e.size(this.dims);
            return t === 0 ? new Int32Array() : new Int32Array(this.module.HEAP8.buffer, this.data, t);
          }
          getUint16Array() {
            if (this.dataType !== 10 && this.dataType !== 4) throw new Error("Invalid data type");
            let t = $e.size(this.dims);
            return t === 0 ? new Uint16Array() : new Uint16Array(this.module.HEAP8.buffer, this.data, t);
          }
          reshape(t) {
            if ($e.size(t) !== $e.size(this.dims)) throw new Error("Invalid new shape");
            return new s_(this.module, this.dataType, this.data, t);
          }
        }, ma = class {
          constructor(e, t, r) {
            this.module = e, this.backend = t, this.customDataOffset = 0, this.customDataSize = 0, this.adapterInfo = t.adapterInfo, this.deviceInfo = t.deviceInfo;
            let n = e.PTR_SIZE, i = r / e.PTR_SIZE, a = n === 4 ? "i32" : "i64";
            this.opKernelContext = Number(e.getValue(n * i++, a));
            let o = Number(e.getValue(n * i++, a));
            this.outputCount = Number(e.getValue(n * i++, a)), this.customDataOffset = Number(e.getValue(n * i++, "*")), this.customDataSize = Number(e.getValue(n * i++, a));
            let d = [];
            for (let p = 0; p < o; p++) {
              let h = Number(e.getValue(n * i++, a)), k = Number(e.getValue(n * i++, "*")), S = Number(e.getValue(n * i++, a)), u = [];
              for (let B = 0; B < S; B++) u.push(Number(e.getValue(n * i++, a)));
              d.push(new ki(e, h, k, u));
            }
            this.inputs = d;
          }
          get kernelCustomData() {
            return this.backend.currentKernelCustomData;
          }
          get customDataBuffer() {
            return this.module.HEAPU8.subarray(this.customDataOffset, this.customDataOffset + this.customDataSize);
          }
          compute(e, t) {
            var o;
            let r = ((o = t == null ? void 0 : t.inputs) == null ? void 0 : o.map((d) => typeof d == "number" ? this.inputs[d] : d)) ?? this.inputs, n = (t == null ? void 0 : t.outputs) ?? [], i = (d, p, h) => new ki(this.module, p, this.output(d, h), h), a = (d, p) => {
              let h = ls(d, p);
              if (!h) throw new Error(`Unsupported data type: ${d}`);
              let k = h > 0 ? this.backend.gpuDataManager.create(h).id : 0;
              return new ki(this.module, d, k, p);
            };
            return this.backend.run(e, r, n, i, a, this.outputCount);
          }
          output(e, t) {
            let r = this.module.stackSave();
            try {
              let n = this.module.PTR_SIZE, i = n === 4 ? "i32" : "i64", a = this.module.stackAlloc((1 + t.length) * n);
              this.module.setValue(a, t.length, i);
              for (let o = 0; o < t.length; o++) this.module.setValue(a + n * (o + 1), t[o], i);
              return this.module._JsepOutput(this.opKernelContext, e, a);
            } catch (n) {
              throw new Error(`Failed to generate kernel's output[${e}] with dims [${t}]. If you are running with pre-allocated output, please make sure the output type/dims are correct. Error: ${n}`);
            } finally {
              this.module.stackRestore(r);
            }
          }
        }, Si = async (e, t, r, n) => {
          let i = t.jsepInit;
          if (!i) throw new Error("Failed to initialize JSEP. The WebAssembly module is not built with JSEP support.");
          if (e === "webgpu") {
            let a = new Ti();
            await a.initialize(r, n), i("webgpu", [a, (o) => a.alloc(Number(o)), (o) => a.free(o), (o, d, p, h = !1) => {
              if (h) or("verbose", () => `[WebGPU] jsepCopyGpuToGpu: src=${Number(o)}, dst=${Number(d)}, size=${Number(p)}`), a.memcpy(Number(o), Number(d));
              else {
                or("verbose", () => `[WebGPU] jsepCopyCpuToGpu: dataOffset=${Number(o)}, gpuDataId=${Number(d)}, size=${Number(p)}`);
                let k = t.HEAPU8.subarray(Number(o >>> 0), Number(o >>> 0) + Number(p));
                a.upload(Number(d), k);
              }
            }, async (o, d, p) => {
              or("verbose", () => `[WebGPU] jsepCopyGpuToCpu: gpuDataId=${o}, dataOffset=${d}, size=${p}`), await a.download(Number(o), () => t.HEAPU8.subarray(Number(d) >>> 0, Number(d + p) >>> 0));
            }, (o, d, p) => a.createKernel(o, Number(d), p, t.UTF8ToString(t._JsepGetNodeName(Number(d)))), (o) => a.releaseKernel(o), (o, d, p, h) => {
              or("verbose", () => `[WebGPU] jsepRun: sessionHandle=${p}, kernel=${o}, contextDataOffset=${d}`);
              let k = new ma(t, a, Number(d));
              return a.computeKernel(Number(o), k, h);
            }, () => a.captureBegin(), () => a.captureEnd(), () => a.replay()]);
          } else {
            let a = new ca(r);
            i("webnn", [a, () => a.reserveTensorId(), (o) => a.releaseTensorId(o), async (o, d, p, h) => a.ensureTensor(o, d, p, h), (o, d) => {
              a.uploadTensor(o, d);
            }, async (o, d) => a.downloadTensor(o, d)]);
          }
        };
      }), _p, wc, qs, Kr, wh, fp, Rp, Np, jp, Up, Wp, Vp, yh = w(() => {
        Js(), Xn(), zt(), Ft(), ts(), kn(), _p = (e, t) => {
          _t()._OrtInit(e, t) !== 0 && tr("Can't initialize onnxruntime.");
        }, wc = async (e) => {
          _p(e.wasm.numThreads, Zs(e.logLevel));
        }, qs = async (e, t) => {
          {
            let r = (Bp(), M(ha)).init;
            if (t === "webgpu") {
              if (typeof navigator > "u" || !navigator.gpu) throw new Error("WebGPU is not supported in current environment");
              let n = e.webgpu.adapter;
              if (n) {
                if (typeof n.limits != "object" || typeof n.features != "object" || typeof n.requestDevice != "function") throw new Error("Invalid GPU adapter set in `env.webgpu.adapter`. It must be a GPUAdapter object.");
              } else {
                let i = e.webgpu.powerPreference;
                if (i !== void 0 && i !== "low-power" && i !== "high-performance") throw new Error(`Invalid powerPreference setting: "${i}"`);
                let a = e.webgpu.forceFallbackAdapter;
                if (a !== void 0 && typeof a != "boolean") throw new Error(`Invalid forceFallbackAdapter setting: "${a}"`);
                if (n = await navigator.gpu.requestAdapter({ powerPreference: i, forceFallbackAdapter: a }), !n) throw new Error('Failed to get GPU adapter. You may need to enable flag "--enable-unsafe-webgpu" if you are using Chrome.');
              }
              await r("webgpu", _t(), e, n);
            }
            if (t === "webnn") {
              if (typeof navigator > "u" || !navigator.ml) throw new Error("WebNN is not supported in current environment");
              await r("webnn", _t(), e);
            }
          }
        }, Kr = /* @__PURE__ */ new Map(), wh = (e) => {
          let t = _t(), r = t.stackSave();
          try {
            let n = t.PTR_SIZE, i = t.stackAlloc(2 * n);
            t._OrtGetInputOutputCount(e, i, i + n) !== 0 && tr("Can't get session input/output count.");
            let a = n === 4 ? "i32" : "i64";
            return [Number(t.getValue(i, a)), Number(t.getValue(i + n, a))];
          } finally {
            t.stackRestore(r);
          }
        }, fp = (e) => {
          let t = _t(), r = t._malloc(e.byteLength);
          if (r === 0) throw new Error(`Can't create a session. failed to allocate a buffer of size ${e.byteLength}.`);
          return t.HEAPU8.set(e, r), [r, e.byteLength];
        }, Rp = async (e, t) => {
          var S, u, B;
          let r, n, i = _t();
          Array.isArray(e) ? [r, n] = e : e.buffer === i.HEAPU8.buffer ? [r, n] = [e.byteOffset, e.byteLength] : [r, n] = fp(e);
          let a = 0, o = 0, d = 0, p = [], h = [], k = [];
          try {
            if ([o, p] = Cn(t), (t == null ? void 0 : t.externalData) && i.mountExternalData) {
              let ye = [];
              for (let Ae of t.externalData) {
                let Ie = typeof Ae == "string" ? Ae : Ae.path;
                ye.push(fn(typeof Ae == "string" ? Ae : Ae.data).then((Ge) => {
                  i.mountExternalData(Ie, Ge);
                }));
              }
              await Promise.all(ye);
            }
            for (let ye of (t == null ? void 0 : t.executionProviders) ?? []) if ((typeof ye == "string" ? ye : ye.name) === "webnn") {
              if (i.shouldTransferToMLTensor = !1, typeof ye != "string") {
                let Ae = ye, Ie = Ae == null ? void 0 : Ae.context, Ge = Ae == null ? void 0 : Ae.gpuDevice, lt = Ae == null ? void 0 : Ae.deviceType, Tt = Ae == null ? void 0 : Ae.powerPreference;
                Ie ? i.currentContext = Ie : Ge ? i.currentContext = await i.jsepCreateMLContext(Ge) : i.currentContext = await i.jsepCreateMLContext({ deviceType: lt, powerPreference: Tt });
              } else i.currentContext = await i.jsepCreateMLContext();
              break;
            }
            a = await i._OrtCreateSession(r, n, o), a === 0 && tr("Can't create a session."), (S = i.jsepOnCreateSession) == null || S.call(i), i.currentContext && (i.jsepRegisterMLContext(a, i.currentContext), i.currentContext = void 0, i.shouldTransferToMLTensor = !0);
            let [R, N] = wh(a), Z = !!(t != null && t.enableGraphCapture), te = [], Q = [], _e = [];
            for (let ye = 0; ye < R; ye++) {
              let Ae = i._OrtGetInputName(a, ye);
              Ae === 0 && tr("Can't get an input name."), h.push(Ae), te.push(i.UTF8ToString(Ae));
            }
            for (let ye = 0; ye < N; ye++) {
              let Ae = i._OrtGetOutputName(a, ye);
              Ae === 0 && tr("Can't get an output name."), k.push(Ae);
              let Ie = i.UTF8ToString(Ae);
              Q.push(Ie);
              {
                if (Z && (t == null ? void 0 : t.preferredOutputLocation) === void 0) {
                  _e.push("gpu-buffer");
                  continue;
                }
                let Ge = typeof (t == null ? void 0 : t.preferredOutputLocation) == "string" ? t.preferredOutputLocation : ((u = t == null ? void 0 : t.preferredOutputLocation) == null ? void 0 : u[Ie]) ?? "cpu";
                if (Ge !== "cpu" && Ge !== "cpu-pinned" && Ge !== "gpu-buffer" && Ge !== "ml-tensor") throw new Error(`Not supported preferred output location: ${Ge}.`);
                if (Z && Ge !== "gpu-buffer") throw new Error(`Not supported preferred output location: ${Ge}. Only 'gpu-buffer' location is supported when enableGraphCapture is true.`);
                _e.push(Ge);
              }
            }
            let me = null;
            return _e.some((ye) => ye === "gpu-buffer" || ye === "ml-tensor") && (d = i._OrtCreateBinding(a), d === 0 && tr("Can't create IO binding."), me = { handle: d, outputPreferredLocations: _e, outputPreferredLocationsEncoded: _e.map((ye) => Ts(ye)) }), Kr.set(a, [a, h, k, me, Z, !1]), [a, te, Q];
          } catch (R) {
            throw h.forEach((N) => i._OrtFree(N)), k.forEach((N) => i._OrtFree(N)), d !== 0 && i._OrtReleaseBinding(d) !== 0 && tr("Can't release IO binding."), a !== 0 && i._OrtReleaseSession(a) !== 0 && tr("Can't release session."), R;
          } finally {
            i._free(r), o !== 0 && i._OrtReleaseSessionOptions(o) !== 0 && tr("Can't release session options."), p.forEach((R) => i._free(R)), (B = i.unmountExternalData) == null || B.call(i);
          }
        }, Np = (e) => {
          var p;
          let t = _t(), r = Kr.get(e);
          if (!r) throw new Error(`cannot release session. invalid session id: ${e}`);
          let [n, i, a, o, d] = r;
          o && (d && t._OrtClearBoundOutputs(o.handle) !== 0 && tr("Can't clear bound outputs."), t._OrtReleaseBinding(o.handle) !== 0 && tr("Can't release IO binding.")), (p = t.jsepOnReleaseSession) == null || p.call(t, e), i.forEach((h) => t._OrtFree(h)), a.forEach((h) => t._OrtFree(h)), t._OrtReleaseSession(n) !== 0 && tr("Can't release session."), Kr.delete(e);
        }, jp = (e, t, r, n, i, a = !1) => {
          if (!e) {
            t.push(0);
            return;
          }
          let o = _t(), d = o.PTR_SIZE, p = e[0], h = e[1], k = e[3], S, u;
          if (p === "string" && (k === "gpu-buffer" || k === "ml-tensor")) throw new Error("String tensor is not supported on GPU.");
          if (a && k !== "gpu-buffer") throw new Error(`External buffer must be provided for input/output index ${i} when enableGraphCapture is true.`);
          if (k === "gpu-buffer") {
            let N = e[2].gpuBuffer;
            u = ls(Us(p), h);
            let Z = o.jsepRegisterBuffer;
            if (!Z) throw new Error('Tensor location "gpu-buffer" is not supported without using WebGPU.');
            S = Z(n, i, N, u);
          } else if (k === "ml-tensor") {
            let N = e[2].mlTensor;
            u = ls(Us(p), h);
            let Z = o.jsepRegisterMLTensor;
            if (!Z) throw new Error('Tensor location "ml-tensor" is not supported without using WebNN.');
            S = Z(N, Us(p), h);
          } else {
            let N = e[2];
            if (Array.isArray(N)) {
              u = d * N.length, S = o._malloc(u), r.push(S);
              for (let Z = 0; Z < N.length; Z++) {
                if (typeof N[Z] != "string") throw new TypeError(`tensor data at index ${Z} is not a string`);
                o.setValue(S + Z * d, lr(N[Z], r), "*");
              }
            } else u = N.byteLength, S = o._malloc(u), r.push(S), o.HEAPU8.set(new Uint8Array(N.buffer, N.byteOffset, u), S);
          }
          let B = o.stackSave(), R = o.stackAlloc(4 * h.length);
          try {
            h.forEach((Z, te) => o.setValue(R + te * d, Z, d === 4 ? "i32" : "i64"));
            let N = o._OrtCreateTensor(Us(p), S, u, R, h.length, Ts(k));
            N === 0 && tr(`Can't create tensor for input/output. session=${n}, index=${i}.`), t.push(N);
          } finally {
            o.stackRestore(B);
          }
        }, Up = async (e, t, r, n, i, a) => {
          var Kt, Yt;
          let o = _t(), d = o.PTR_SIZE, p = Kr.get(e);
          if (!p) throw new Error(`cannot run inference. invalid session id: ${e}`);
          let h = p[0], k = p[1], S = p[2], u = p[3], B = p[4], R = p[5], N = t.length, Z = n.length, te = 0, Q = [], _e = [], me = [], ye = [], Ae = o.stackSave(), Ie = o.stackAlloc(N * d), Ge = o.stackAlloc(N * d), lt = o.stackAlloc(Z * d), Tt = o.stackAlloc(Z * d);
          try {
            (Kt = o.jsepOnRunStart) == null || Kt.call(o, h), [te, Q] = Rs(a);
            for (let $t = 0; $t < N; $t++) jp(r[$t], _e, ye, e, t[$t], B);
            for (let $t = 0; $t < Z; $t++) jp(i[$t], me, ye, e, N + n[$t], B);
            for (let $t = 0; $t < N; $t++) o.setValue(Ie + $t * d, _e[$t], "*"), o.setValue(Ge + $t * d, k[t[$t]], "*");
            for (let $t = 0; $t < Z; $t++) o.setValue(lt + $t * d, me[$t], "*"), o.setValue(Tt + $t * d, S[n[$t]], "*");
            if (u && !R) {
              let { handle: $t, outputPreferredLocations: jt, outputPreferredLocationsEncoded: vr } = u;
              if (k.length !== N) throw new Error(`input count from feeds (${N}) is expected to be always equal to model's input count (${k.length}).`);
              for (let Ht = 0; Ht < N; Ht++) {
                let Gt = t[Ht];
                await o._OrtBindInput($t, k[Gt], _e[Ht]) !== 0 && tr(`Can't bind input[${Ht}] for session=${e}.`);
              }
              for (let Ht = 0; Ht < Z; Ht++) {
                let Gt = n[Ht];
                (Yt = i[Ht]) != null && Yt[3] ? o._OrtBindOutput($t, S[Gt], me[Ht], 0) !== 0 && tr(`Can't bind pre-allocated output[${Ht}] for session=${e}.`) : o._OrtBindOutput($t, S[Gt], 0, vr[Gt]) !== 0 && tr(`Can't bind output[${Ht}] to ${jt[Ht]} for session=${e}.`);
              }
              Kr.set(e, [h, k, S, u, B, !0]);
            }
            let Ct;
            u ? Ct = await o._OrtRunWithBinding(h, u.handle, Z, lt, te) : Ct = await o._OrtRun(h, Ge, Ie, N, Tt, Z, lt, te), Ct !== 0 && tr("failed to call OrtRun().");
            let Jt = [];
            for (let $t = 0; $t < Z; $t++) {
              let jt = Number(o.getValue(lt + $t * d, "*"));
              if (jt === me[$t]) {
                Jt.push(i[$t]);
                continue;
              }
              let vr = o.stackSave(), Ht = o.stackAlloc(4 * d), Gt = !1, Cr, it = 0;
              try {
                o._OrtGetTensorData(jt, Ht, Ht + d, Ht + 2 * d, Ht + 3 * d) !== 0 && tr(`Can't access output tensor data on index ${$t}.`);
                let Et = d === 4 ? "i32" : "i64", cr = Number(o.getValue(Ht, Et));
                it = o.getValue(Ht + d, "*");
                let Lr = o.getValue(Ht + d * 2, "*"), ys = Number(o.getValue(Ht + d * 3, Et)), Hr = [];
                for (let Ir = 0; Ir < ys; Ir++) Hr.push(Number(o.getValue(Lr + Ir * d, Et)));
                o._OrtFree(Lr) !== 0 && tr("Can't free memory for tensor dims.");
                let un = Hr.reduce((Ir, Er) => Ir * Er, 1);
                Cr = xs(cr);
                let Ii = u == null ? void 0 : u.outputPreferredLocations[n[$t]];
                if (Cr === "string") {
                  if (Ii === "gpu-buffer" || Ii === "ml-tensor") throw new Error("String tensor is not supported on GPU.");
                  let Ir = [];
                  for (let Er = 0; Er < un; Er++) {
                    let Wn = o.getValue(it + Er * d, "*"), bc = o.getValue(it + (Er + 1) * d, "*"), Xp = Er === un - 1 ? void 0 : bc - Wn;
                    Ir.push(o.UTF8ToString(Wn, Xp));
                  }
                  Jt.push([Cr, Hr, Ir, "cpu"]);
                } else if (Ii === "gpu-buffer" && un > 0) {
                  let Ir = o.jsepGetBuffer;
                  if (!Ir) throw new Error('preferredLocation "gpu-buffer" is not supported without using WebGPU.');
                  let Er = Ir(it), Wn = ls(cr, un);
                  if (Wn === void 0 || !_n(Cr)) throw new Error(`Unsupported data type: ${Cr}`);
                  Gt = !0, Jt.push([Cr, Hr, { gpuBuffer: Er, download: o.jsepCreateDownloader(Er, Wn, Cr), dispose: () => {
                    o._OrtReleaseTensor(jt) !== 0 && tr("Can't release tensor.");
                  } }, "gpu-buffer"]);
                } else if (Ii === "ml-tensor" && un > 0) {
                  let Ir = o.jsepEnsureTensor;
                  if (!Ir) throw new Error('preferredLocation "ml-tensor" is not supported without using WebNN.');
                  if (ls(cr, un) === void 0 || !en(Cr)) throw new Error(`Unsupported data type: ${Cr}`);
                  let Er = await Ir(it, cr, Hr, !1);
                  Gt = !0, Jt.push([Cr, Hr, { mlTensor: Er, download: o.jsepCreateMLTensorDownloader(it, Cr), dispose: () => {
                    o.jsepReleaseTensorId(it), o._OrtReleaseTensor(jt);
                  } }, "ml-tensor"]);
                } else {
                  let Ir = mn(Cr), Er = new Ir(un);
                  new Uint8Array(Er.buffer, Er.byteOffset, Er.byteLength).set(o.HEAPU8.subarray(it, it + Er.byteLength)), Jt.push([Cr, Hr, Er, "cpu"]);
                }
              } finally {
                o.stackRestore(vr), Cr === "string" && it && o._free(it), Gt || o._OrtReleaseTensor(jt);
              }
            }
            return u && !B && (o._OrtClearBoundOutputs(u.handle) !== 0 && tr("Can't clear bound outputs."), Kr.set(e, [h, k, S, u, B, !1])), Jt;
          } finally {
            o.stackRestore(Ae), _e.forEach((Ct) => o._OrtReleaseTensor(Ct)), me.forEach((Ct) => o._OrtReleaseTensor(Ct)), ye.forEach((Ct) => o._free(Ct)), te !== 0 && o._OrtReleaseRunOptions(te), Q.forEach((Ct) => o._free(Ct));
          }
        }, Wp = (e) => {
          let t = _t(), r = Kr.get(e);
          if (!r) throw new Error("invalid session id");
          let n = r[0], i = t._OrtEndProfiling(n);
          i === 0 && tr("Can't get an profile file name."), t._OrtFree(i);
        }, Vp = (e) => {
          let t = [];
          for (let r of e) {
            let n = r[2];
            !Array.isArray(n) && "buffer" in n && t.push(n.buffer);
          }
          return t;
        };
      }), Un, ws, _a, yc, Mc, gp, Gp, wp, $i, Ai, Mh, bh, vh, xh, Th, Eh, Ph, Ch, kh = w(() => {
        Qe(), yh(), Ft(), Ss(), Un = () => !!v.wasm.proxy && typeof document < "u", _a = !1, yc = !1, Mc = !1, wp = /* @__PURE__ */ new Map(), $i = (e, t) => {
          let r = wp.get(e);
          r ? r.push(t) : wp.set(e, [t]);
        }, Ai = () => {
          if (_a || !yc || Mc || !ws) throw new Error("worker not ready");
        }, Mh = (e) => {
          switch (e.data.type) {
            case "init-wasm":
              _a = !1, e.data.err ? (Mc = !0, Gp[1](e.data.err)) : (yc = !0, Gp[0]()), gp && (URL.revokeObjectURL(gp), gp = void 0);
              break;
            case "init-ep":
            case "copy-from":
            case "create":
            case "release":
            case "run":
            case "end-profiling": {
              let t = wp.get(e.data.type);
              e.data.err ? t.shift()[1](e.data.err) : t.shift()[0](e.data.out);
              break;
            }
          }
        }, bh = async () => {
          if (!yc) {
            if (_a) throw new Error("multiple calls to 'initWasm()' detected.");
            if (Mc) throw new Error("previous call to 'initWasm()' failed.");
            if (_a = !0, Un()) return new Promise((e, t) => {
              ws == null || ws.terminate(), Qs().then(([r, n]) => {
                try {
                  ws = n, ws.onerror = (a) => t(a), ws.onmessage = Mh, Gp = [e, t];
                  let i = { type: "init-wasm", in: v };
                  ws.postMessage(i), gp = r;
                } catch (i) {
                  t(i);
                }
              }, t);
            });
            try {
              await nt(v.wasm), await wc(v), yc = !0;
            } catch (e) {
              throw Mc = !0, e;
            } finally {
              _a = !1;
            }
          }
        }, vh = async (e) => {
          if (Un()) return Ai(), new Promise((t, r) => {
            $i("init-ep", [t, r]);
            let n = { type: "init-ep", in: { epName: e, env: v } };
            ws.postMessage(n);
          });
          await qs(v, e);
        }, xh = async (e) => Un() ? (Ai(), new Promise((t, r) => {
          $i("copy-from", [t, r]);
          let n = { type: "copy-from", in: { buffer: e } };
          ws.postMessage(n, [e.buffer]);
        })) : fp(e), Th = async (e, t) => {
          if (Un()) {
            if (t != null && t.preferredOutputLocation) throw new Error('session option "preferredOutputLocation" is not supported for proxy.');
            return Ai(), new Promise((r, n) => {
              $i("create", [r, n]);
              let i = { type: "create", in: { model: e, options: { ...t } } }, a = [];
              e instanceof Uint8Array && a.push(e.buffer), ws.postMessage(i, a);
            });
          } else return Rp(e, t);
        }, Eh = async (e) => {
          if (Un()) return Ai(), new Promise((t, r) => {
            $i("release", [t, r]);
            let n = { type: "release", in: e };
            ws.postMessage(n);
          });
          Np(e);
        }, Ph = async (e, t, r, n, i, a) => {
          if (Un()) {
            if (r.some((o) => o[3] !== "cpu")) throw new Error("input tensor on GPU is not supported for proxy.");
            if (i.some((o) => o)) throw new Error("pre-allocated output tensor is not supported for proxy.");
            return Ai(), new Promise((o, d) => {
              $i("run", [o, d]);
              let p = r, h = { type: "run", in: { sessionId: e, inputIndices: t, inputs: p, outputIndices: n, options: a } };
              ws.postMessage(h, Vp(p));
            });
          } else return Up(e, t, r, n, i, a);
        }, Ch = async (e) => {
          if (Un()) return Ai(), new Promise((t, r) => {
            $i("end-profiling", [t, r]);
            let n = { type: "end-profiling", in: e };
            ws.postMessage(n);
          });
          Wp(e);
        };
      }), Kp, Sh, $h, n_ = w(() => {
        Qe(), kh(), zt(), st(), kn(), Kp = (e, t) => {
          switch (e.location) {
            case "cpu":
              return [e.type, e.dims, e.data, "cpu"];
            case "gpu-buffer":
              return [e.type, e.dims, { gpuBuffer: e.gpuBuffer }, "gpu-buffer"];
            case "ml-tensor":
              return [e.type, e.dims, { mlTensor: e.mlTensor }, "ml-tensor"];
            default:
              throw new Error(`invalid data location: ${e.location} for ${t()}`);
          }
        }, Sh = (e) => {
          switch (e[3]) {
            case "cpu":
              return new ce(e[0], e[2], e[1]);
            case "gpu-buffer": {
              let t = e[0];
              if (!_n(t)) throw new Error(`not supported data type: ${t} for deserializing GPU tensor`);
              let { gpuBuffer: r, download: n, dispose: i } = e[2];
              return ce.fromGpuBuffer(r, { dataType: t, dims: e[1], download: n, dispose: i });
            }
            case "ml-tensor": {
              let t = e[0];
              if (!en(t)) throw new Error(`not supported data type: ${t} for deserializing MLTensor tensor`);
              let { mlTensor: r, download: n, dispose: i } = e[2];
              return ce.fromMLTensor(r, { dataType: t, dims: e[1], download: n, dispose: i });
            }
            default:
              throw new Error(`invalid data location: ${e[3]}`);
          }
        }, $h = class {
          async fetchModelAndCopyToWasmMemory(e) {
            return xh(await fn(e));
          }
          async loadModel(e, t) {
            Ve();
            let r;
            typeof e == "string" ? r = await this.fetchModelAndCopyToWasmMemory(e) : r = e, [this.sessionId, this.inputNames, this.outputNames] = await Th(r, t), Ne();
          }
          async dispose() {
            return Eh(this.sessionId);
          }
          async run(e, t, r) {
            Ve();
            let n = [], i = [];
            Object.entries(e).forEach((S) => {
              let u = S[0], B = S[1], R = this.inputNames.indexOf(u);
              if (R === -1) throw new Error(`invalid input '${u}'`);
              n.push(B), i.push(R);
            });
            let a = [], o = [];
            Object.entries(t).forEach((S) => {
              let u = S[0], B = S[1], R = this.outputNames.indexOf(u);
              if (R === -1) throw new Error(`invalid output '${u}'`);
              a.push(B), o.push(R);
            });
            let d = n.map((S, u) => Kp(S, () => `input "${this.inputNames[i[u]]}"`)), p = a.map((S, u) => S ? Kp(S, () => `output "${this.outputNames[o[u]]}"`) : null), h = await Ph(this.sessionId, i, d, o, p, r), k = {};
            for (let S = 0; S < h.length; S++) k[this.outputNames[o[S]]] = a[S] ?? Sh(h[S]);
            return Ne(), k;
          }
          startProfiling() {
          }
          endProfiling() {
            Ch(this.sessionId);
          }
        };
      }), Ah = {};
      x(Ah, { OnnxruntimeWebAssemblyBackend: () => qp, initializeFlags: () => Hp, wasmBackend: () => Ih });
      var Hp, qp, Ih, i_ = w(() => {
        Qe(), kh(), n_(), Ss(), Hp = () => {
          if ((typeof v.wasm.initTimeout != "number" || v.wasm.initTimeout < 0) && (v.wasm.initTimeout = 0), v.wasm.simd === !1 && console.warn('Deprecated property "env.wasm.simd" is set to false. non-SIMD build is no longer provided, and this setting will be ignored.'), typeof v.wasm.proxy != "boolean" && (v.wasm.proxy = !1), typeof v.wasm.trace != "boolean" && (v.wasm.trace = !1), typeof v.wasm.numThreads != "number" || !Number.isInteger(v.wasm.numThreads) || v.wasm.numThreads <= 0) if (typeof self < "u" && !self.crossOriginIsolated) v.wasm.numThreads = 1;
          else {
            let e = typeof navigator > "u" ? W("node:os").cpus().length : navigator.hardwareConcurrency;
            v.wasm.numThreads = Math.min(4, Math.ceil((e || 1) / 2));
          }
        }, qp = class {
          async init(e) {
            Hp(), await bh(), await vh(e);
          }
          async createInferenceSessionHandler(e, t) {
            let r = new $h();
            return await r.loadModel(e, t), Promise.resolve(r);
          }
        }, Ih = new qp();
      });
      Qe(), Qe(), Qe();
      var o_ = "1.21.0-dev.20241205-d27fecd3d3", a_ = Oe;
      {
        let e = (i_(), M(Ah)).wasmBackend;
        q("webgpu", e, 5), q("webnn", e, 5), q("cpu", e, 10), q("wasm", e, 10);
      }
      Object.defineProperty(v.versions, "web", { value: o_, enumerable: !0 });
      /**
       * @license
       * Copyright 2021 Google LLC. All Rights Reserved.
       * Licensed under the Apache License, Version 2.0 (the "License");
       * you may not use this file except in compliance with the License.
       * You may obtain a copy of the License at
       *
       * http://www.apache.org/licenses/LICENSE-2.0
       *
       * Unless required by applicable law or agreed to in writing, software
       * distributed under the License is distributed on an "AS IS" BASIS,
       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       * See the License for the specific language governing permissions and
       * limitations under the License.
       * =============================================================================
       */
      /**
       * @license
       * Copyright 2020 Google LLC. All Rights Reserved.
       * Licensed under the Apache License, Version 2.0 (the "License");
       * you may not use this file except in compliance with the License.
       * You may obtain a copy of the License at
       *
       * http://www.apache.org/licenses/LICENSE-2.0
       *
       * Unless required by applicable law or agreed to in writing, software
       * distributed under the License is distributed on an "AS IS" BASIS,
       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       * See the License for the specific language governing permissions and
       * limitations under the License.
       * =============================================================================
       */
      /**
       * @license
       * Copyright 2019 Google LLC. All Rights Reserved.
       * Licensed under the Apache License, Version 2.0 (the "License");
       * you may not use this file except in compliance with the License.
       * You may obtain a copy of the License at
       *
       * http://www.apache.org/licenses/LICENSE-2.0
       *
       * Unless required by applicable law or agreed to in writing, software
       * distributed under the License is distributed on an "AS IS" BASIS,
       * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       * See the License for the specific language governing permissions and
       * limitations under the License.
       * =============================================================================
       */
    }
  ),
  /***/
  "./src/backends/onnx.js": (
    /*!******************************!*\
      !*** ./src/backends/onnx.js ***!
      \******************************/
    /***/
    (ke, A, s) => {
      var f;
      s.r(A), s.d(A, {
        /* harmony export */
        Tensor: () => (
          /* reexport safe */
          W.Tensor
        ),
        /* harmony export */
        createInferenceSession: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        deviceToExecutionProviders: () => (
          /* binding */
          q
        ),
        /* harmony export */
        isONNXProxy: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        isONNXTensor: () => (
          /* binding */
          z
        )
        /* harmony export */
      });
      var L = s(
        /*! ../env.js */
        "./src/env.js"
      ), j = s(
        /*! onnxruntime-node */
        "?2ce3"
      ), J = s(
        /*! onnxruntime-web */
        "./node_modules/onnxruntime-web/dist/ort.bundle.min.mjs"
      ), W = s(
        /*! onnxruntime-common */
        "./node_modules/onnxruntime-common/dist/esm/index.js"
      );
      const w = Object.freeze({
        auto: null,
        // Auto-detect based on device and environment
        gpu: null,
        // Auto-detect GPU
        cpu: "cpu",
        // CPU
        wasm: "wasm",
        // WebAssembly
        webgpu: "webgpu",
        // WebGPU
        cuda: "cuda",
        // CUDA
        dml: "dml",
        // DirectML
        webnn: { name: "webnn", deviceType: "cpu" },
        // WebNN (default)
        "webnn-npu": { name: "webnn", deviceType: "npu" },
        // WebNN NPU
        "webnn-gpu": { name: "webnn", deviceType: "gpu" },
        // WebNN GPU
        "webnn-cpu": { name: "webnn", deviceType: "cpu" }
        // WebNN CPU
      }), x = [];
      let y, M;
      const b = Symbol.for("onnxruntime");
      if (b in globalThis)
        M = globalThis[b];
      else if (L.apis.IS_NODE_ENV) {
        switch (M = j ?? (f || (f = s.t(j, 2))), process.platform) {
          case "win32":
            x.push("dml");
            break;
          case "linux":
            process.arch === "x64" && x.push("cuda");
            break;
        }
        x.push("cpu"), y = ["cpu"];
      } else
        M = J, L.apis.IS_WEBNN_AVAILABLE && x.push("webnn-npu", "webnn-gpu", "webnn-cpu", "webnn"), L.apis.IS_WEBGPU_AVAILABLE && x.push("webgpu"), x.push("wasm"), y = ["wasm"];
      const D = M.InferenceSession;
      function q(O = null) {
        if (!O) return y;
        switch (O) {
          case "auto":
            return x;
          case "gpu":
            return x.filter(
              ($) => ["webgpu", "cuda", "dml", "webnn-gpu"].includes($)
            );
        }
        if (x.includes(O))
          return [w[O] ?? O];
        throw new Error(`Unsupported device: "${O}". Should be one of: ${x.join(", ")}.`);
      }
      let se = null;
      async function oe(O, $, g) {
        se && await se;
        const C = D.create(O, $);
        se ?? (se = C);
        const v = await C;
        return v.config = g, v;
      }
      function z(O) {
        return O instanceof M.Tensor;
      }
      const V = M == null ? void 0 : M.env;
      V != null && V.wasm && (V.wasm.wasmPaths = `https://cdn.jsdelivr.net/npm/@huggingface/transformers@${L.env.version}/dist/`, V.wasm.proxy = !1, (typeof crossOriginIsolated > "u" || !crossOriginIsolated) && (V.wasm.numThreads = 1)), V != null && V.webgpu && (V.webgpu.powerPreference = "high-performance");
      function Y() {
        var O;
        return (O = V == null ? void 0 : V.wasm) == null ? void 0 : O.proxy;
      }
      L.env.backends.onnx = V;
    }
  ),
  /***/
  "./src/base/feature_extraction_utils.js": (
    /*!**********************************************!*\
      !*** ./src/base/feature_extraction_utils.js ***!
      \**********************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        FeatureExtractor: () => (
          /* binding */
          J
        ),
        /* harmony export */
        validate_audio_inputs: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/constants.js */
        "./src/utils/constants.js"
      ), L = s(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), j = s(
        /*! ../utils/hub.js */
        "./src/utils/hub.js"
      );
      class J extends L.Callable {
        /**
         * Constructs a new FeatureExtractor instance.
         *
         * @param {Object} config The configuration for the feature extractor.
         */
        constructor(x) {
          super(), this.config = x;
        }
        /**
         * Instantiate one of the processor classes of the library from a pretrained model.
         * 
         * The processor class to instantiate is selected based on the `image_processor_type` (or `feature_extractor_type`; legacy)
         * property of the config object (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.
         * @param {import('../utils/hub.js').PretrainedOptions} options Additional options for loading the processor.
         * 
         * @returns {Promise<FeatureExtractor>} A new instance of the Processor class.
         */
        static async from_pretrained(x, y) {
          const M = await (0, j.getModelJSON)(x, f.FEATURE_EXTRACTOR_NAME, !0, y);
          return new this(M);
        }
      }
      function W(w, x) {
        var y;
        if (!(w instanceof Float32Array || w instanceof Float64Array))
          throw new Error(
            `${x} expects input to be a Float32Array or a Float64Array, but got ${((y = w == null ? void 0 : w.constructor) == null ? void 0 : y.name) ?? typeof w} instead. If using the feature extractor directly, remember to use \`read_audio(url, sampling_rate)\` to obtain the raw audio data of the file/url.`
          );
      }
    }
  ),
  /***/
  "./src/base/image_processors_utils.js": (
    /*!********************************************!*\
      !*** ./src/base/image_processors_utils.js ***!
      \********************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ImageProcessor: () => (
          /* binding */
          O
        ),
        /* harmony export */
        post_process_instance_segmentation: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        post_process_object_detection: () => (
          /* binding */
          b
        ),
        /* harmony export */
        post_process_panoptic_segmentation: () => (
          /* binding */
          V
        ),
        /* harmony export */
        post_process_semantic_segmentation: () => (
          /* binding */
          D
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), L = s(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      ), j = s(
        /*! ../utils/maths.js */
        "./src/utils/maths.js"
      );
      s(
        /*! ../utils/image.js */
        "./src/utils/image.js"
      );
      var J = s(
        /*! ../utils/core.js */
        "./src/utils/core.js"
      ), W = s(
        /*! ../utils/hub.js */
        "./src/utils/hub.js"
      ), w = s(
        /*! ../utils/constants.js */
        "./src/utils/constants.js"
      );
      function x($, g, C = 0, v = null) {
        const ee = $ / g;
        let X = (0, j.bankers_round)(ee) * g;
        return v !== null && X > v && (X = Math.floor(ee) * g), X < C && (X = Math.ceil(ee) * g), X;
      }
      function y([$, g], C) {
        return [
          Math.max(Math.floor($ / C), 1) * C,
          Math.max(Math.floor(g / C), 1) * C
        ];
      }
      function M([$, g, C, v]) {
        return [
          $ - C / 2,
          g - v / 2,
          $ + C / 2,
          g + v / 2
        ];
      }
      function b($, g = 0.5, C = null, v = !1) {
        const ee = $.logits, X = $.pred_boxes, [le, ue, fe] = ee.dims;
        if (C !== null && C.length !== le)
          throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");
        let Ce = [];
        for (let xe = 0; xe < le; ++xe) {
          let Le = C !== null ? C[xe] : null, qe = {
            boxes: [],
            classes: [],
            scores: []
          }, Ue = ee[xe], ut = X[xe];
          for (let de = 0; de < ue; ++de) {
            let re = Ue[de], he = [], Ee;
            if (v) {
              Ee = re.sigmoid().data;
              for (let Be = 0; Be < Ee.length; ++Be)
                Ee[Be] > g && he.push(Be);
            } else {
              let Be = (0, j.max)(re.data)[1];
              if (Be === fe - 1 || (Ee = (0, j.softmax)(re.data), Ee[Be] < g))
                continue;
              he.push(Be);
            }
            for (const Be of he) {
              let et = ut[de].data;
              et = M(et), Le !== null && (et = et.map((Xe, ie) => Xe * Le[(ie + 1) % 2])), qe.boxes.push(et), qe.classes.push(Be), qe.scores.push(Ee[Be]);
            }
          }
          Ce.push(qe);
        }
        return Ce;
      }
      function D($, g = null) {
        const C = $.logits, v = C.dims[0];
        if (g !== null && g.length !== v)
          throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");
        const ee = [];
        for (let X = 0; X < v; ++X) {
          const le = g !== null ? g[X] : null;
          let ue = C[X];
          le !== null && (ue = (0, L.interpolate)(ue, le, "bilinear", !1));
          const [fe, Ce] = le ?? ue.dims.slice(-2), xe = new L.Tensor(
            "int32",
            new Int32Array(fe * Ce),
            [fe, Ce]
          ), Le = ue[0].data, qe = xe.data;
          for (let de = 1; de < ue.dims[0]; ++de) {
            const re = ue[de].data;
            for (let he = 0; he < re.length; ++he)
              re[he] > Le[he] && (Le[he] = re[he], qe[he] = de);
          }
          const Ue = new Array(ue.dims[0]);
          for (let de = 0; de < qe.length; ++de) {
            const re = qe[de];
            Ue[re] = re;
          }
          const ut = Ue.filter((de) => de !== void 0);
          ee.push({ segmentation: xe, labels: ut });
        }
        return ee;
      }
      function q($, g, C, v) {
        const ee = [], X = [], le = [];
        for (let ue = 0; ue < $.dims[0]; ++ue) {
          const fe = $[ue], Ce = g[ue], xe = (0, j.max)(fe.data)[1];
          if (xe === v)
            continue;
          const qe = (0, j.softmax)(fe.data)[xe];
          qe > C && (ee.push(Ce), X.push(qe), le.push(xe));
        }
        return [ee, X, le];
      }
      function se($, g, C, v = 0.5, ee = 0.8) {
        const X = [];
        let le = 0, ue = 0;
        const fe = g[C].data;
        for (let xe = 0; xe < $.length; ++xe)
          $[xe] === C && (X.push(xe), ++le), fe[xe] >= v && ++ue;
        let Ce = le > 0 && ue > 0;
        return Ce && (Ce = le / ue > ee), [Ce, X];
      }
      function oe($, g, C, v, ee, X = null, le = null) {
        const [ue, fe] = le ?? $[0].dims, Ce = new L.Tensor(
          "int32",
          new Int32Array(ue * fe),
          [ue, fe]
        ), xe = [];
        if (le !== null)
          for (let de = 0; de < $.length; ++de)
            $[de] = (0, L.interpolate)($[de], le, "bilinear", !1);
        const Le = new Int32Array($[0].data.length), qe = new Float32Array($[0].data.length);
        for (let de = 0; de < $.length; ++de) {
          let re = g[de];
          const he = $[de].data;
          for (let Ee = 0; Ee < he.length; ++Ee)
            he[Ee] *= re, he[Ee] > qe[Ee] && (Le[Ee] = de, qe[Ee] = he[Ee]);
        }
        let Ue = 0;
        const ut = Ce.data;
        for (let de = 0; de < C.length; ++de) {
          const re = C[de], [he, Ee] = se(
            Le,
            $,
            de,
            v,
            ee
          );
          if (he) {
            ++Ue;
            for (const Be of Ee)
              ut[Be] = Ue;
            xe.push({
              id: Ue,
              label_id: re,
              // was_fused: should_fuse, TODO
              score: g[de]
            });
          }
        }
        return [Ce, xe];
      }
      function z($, g, C = 28, v = 3136, ee = 1003520) {
        if ($ < C || g < C)
          throw new Error(`height:${$} or width:${g} must be larger than factor:${C}`);
        if (Math.max($, g) / Math.min($, g) > 200)
          throw new Error(
            `absolute aspect ratio must be smaller than 200, got ${Math.max($, g) / Math.min($, g)}`
          );
        let X = Math.round($ / C) * C, le = Math.round(g / C) * C;
        if (X * le > ee) {
          const ue = Math.sqrt($ * g / ee);
          X = Math.floor($ / ue / C) * C, le = Math.floor(g / ue / C) * C;
        } else if (X * le < v) {
          const ue = Math.sqrt(v / ($ * g));
          X = Math.ceil($ * ue / C) * C, le = Math.ceil(g * ue / C) * C;
        }
        return [X, le];
      }
      function V($, g = 0.5, C = 0.5, v = 0.8, ee = null, X = null) {
        ee === null && (console.warn("`label_ids_to_fuse` unset. No instance will be fused."), ee = /* @__PURE__ */ new Set());
        const le = $.class_queries_logits ?? $.logits, fe = ($.masks_queries_logits ?? $.pred_masks).sigmoid();
        let [Ce, xe, Le] = le.dims;
        if (Le -= 1, X !== null && X.length !== Ce)
          throw Error("Make sure that you pass in as many target sizes as the batch dimension of the logits");
        let qe = [];
        for (let Ue = 0; Ue < Ce; ++Ue) {
          let ut = X !== null ? X[Ue] : null, de = le[Ue], re = fe[Ue], [he, Ee, Be] = q(de, re, g, Le);
          if (Be.length === 0) {
            let [ie, Je] = ut ?? re.dims.slice(-2), De = new L.Tensor(
              "int32",
              new Int32Array(ie * Je).fill(-1),
              [ie, Je]
            );
            qe.push({
              segmentation: De,
              segments_info: []
            });
            continue;
          }
          let [et, Xe] = oe(
            he,
            Ee,
            Be,
            C,
            v,
            ee,
            ut
          );
          qe.push({
            segmentation: et,
            segments_info: Xe
          });
        }
        return qe;
      }
      function Y($, g = 0.5, C = null) {
        throw new Error("`post_process_instance_segmentation` is not yet implemented.");
      }
      class O extends f.Callable {
        /**
         * Constructs a new `ImageProcessor`.
         * @param {ImageProcessorConfig} config The configuration object.
         */
        constructor(g) {
          super(), this.image_mean = g.image_mean ?? g.mean, this.image_std = g.image_std ?? g.std, this.resample = g.resample ?? 2, this.do_rescale = g.do_rescale ?? !0, this.rescale_factor = g.rescale_factor ?? 0.00392156862745098, this.do_normalize = g.do_normalize, this.do_thumbnail = g.do_thumbnail, this.size = g.size ?? g.image_size, this.do_resize = g.do_resize ?? this.size !== void 0, this.size_divisibility = g.size_divisibility ?? g.size_divisor, this.do_center_crop = g.do_center_crop, this.crop_size = g.crop_size, this.do_convert_rgb = g.do_convert_rgb ?? !0, this.do_crop_margin = g.do_crop_margin, this.pad_size = g.pad_size, this.do_pad = g.do_pad, this.do_pad && !this.pad_size && this.size && this.size.width !== void 0 && this.size.height !== void 0 && (this.pad_size = this.size), this.do_flip_channel_order = g.do_flip_channel_order ?? !1, this.config = g;
        }
        /**
         * Resize the image to make a thumbnail. The image is resized so that no dimension is larger than any
         * corresponding dimension of the specified size.
         * @param {RawImage} image The image to be resized.
         * @param {{height:number, width:number}} size The size `{"height": h, "width": w}` to resize the image to.
         * @param {string | 0 | 1 | 2 | 3 | 4 | 5} [resample=2] The resampling filter to use.
         * @returns {Promise<RawImage>} The resized image.
         */
        async thumbnail(g, C, v = 2) {
          const ee = g.height, X = g.width, le = C.height, ue = C.width;
          let fe = Math.min(ee, le), Ce = Math.min(X, ue);
          return fe === ee && Ce === X ? g : (ee > X ? Ce = Math.floor(X * fe / ee) : X > ee && (fe = Math.floor(ee * Ce / X)), await g.resize(Ce, fe, { resample: v }));
        }
        /**
         * Crops the margin of the image. Gray pixels are considered margin (i.e., pixels with a value below the threshold).
         * @param {RawImage} image The image to be cropped.
         * @param {number} gray_threshold Value below which pixels are considered to be gray.
         * @returns {Promise<RawImage>} The cropped image.
         */
        async crop_margin(g, C = 200) {
          const v = g.clone().grayscale(), ee = (0, j.min)(v.data)[0], le = (0, j.max)(v.data)[0] - ee;
          if (le === 0)
            return g;
          const ue = C / 255;
          let fe = v.width, Ce = v.height, xe = 0, Le = 0;
          const qe = v.data;
          for (let Ue = 0; Ue < v.height; ++Ue) {
            const ut = Ue * v.width;
            for (let de = 0; de < v.width; ++de)
              (qe[ut + de] - ee) / le < ue && (fe = Math.min(fe, de), Ce = Math.min(Ce, Ue), xe = Math.max(xe, de), Le = Math.max(Le, Ue));
          }
          return g = await g.crop([fe, Ce, xe, Le]), g;
        }
        /**
         * Pad the image by a certain amount.
         * @param {Float32Array} pixelData The pixel data to pad.
         * @param {number[]} imgDims The dimensions of the image (height, width, channels).
         * @param {{width:number; height:number}|number|'square'} padSize The dimensions of the padded image.
         * @param {Object} options The options for padding.
         * @param {'constant'|'symmetric'} [options.mode='constant'] The type of padding to add.
         * @param {boolean} [options.center=false] Whether to center the image.
         * @param {number|number[]} [options.constant_values=0] The constant value to use for padding.
         * @returns {[Float32Array, number[]]} The padded pixel data and image dimensions.
         */
        pad_image(g, C, v, {
          mode: ee = "constant",
          center: X = !1,
          constant_values: le = 0
        } = {}) {
          const [ue, fe, Ce] = C;
          let xe, Le;
          if (typeof v == "number" ? (xe = v, Le = v) : v === "square" ? xe = Le = Math.max(ue, fe) : (xe = v.width, Le = v.height), xe !== fe || Le !== ue) {
            const qe = new Float32Array(xe * Le * Ce);
            if (Array.isArray(le))
              for (let de = 0; de < qe.length; ++de)
                qe[de] = le[de % Ce];
            else le !== 0 && qe.fill(le);
            const [Ue, ut] = X ? [Math.floor((xe - fe) / 2), Math.floor((Le - ue) / 2)] : [0, 0];
            for (let de = 0; de < ue; ++de) {
              const re = (de + ut) * xe, he = de * fe;
              for (let Ee = 0; Ee < fe; ++Ee) {
                const Be = (re + Ee + Ue) * Ce, et = (he + Ee) * Ce;
                for (let Xe = 0; Xe < Ce; ++Xe)
                  qe[Be + Xe] = g[et + Xe];
              }
            }
            if (ee === "symmetric") {
              if (X)
                throw new Error("`center` padding is not supported when `mode` is set to `symmetric`.");
              const de = ue - 1, re = fe - 1;
              for (let he = 0; he < Le; ++he) {
                const Ee = he * xe, Be = (0, J.calculateReflectOffset)(he, de) * fe;
                for (let et = 0; et < xe; ++et) {
                  if (he < ue && et < fe) continue;
                  const Xe = (Ee + et) * Ce, ie = (Be + (0, J.calculateReflectOffset)(et, re)) * Ce;
                  for (let Je = 0; Je < Ce; ++Je)
                    qe[Xe + Je] = g[ie + Je];
                }
              }
            }
            g = qe, C = [Le, xe, Ce];
          }
          return [g, C];
        }
        /**
         * Rescale the image' pixel values by `this.rescale_factor`.
         * @param {Float32Array} pixelData The pixel data to rescale.
         * @returns {void}
         */
        rescale(g) {
          for (let C = 0; C < g.length; ++C)
            g[C] = this.rescale_factor * g[C];
        }
        /**
         * Find the target (width, height) dimension of the output image after
         * resizing given the input image and the desired size.
         * @param {RawImage} image The image to resize.
         * @param {any} size The size to use for resizing the image. 
         * @returns {[number, number]} The target (width, height) dimension of the output image after resizing.
         */
        get_resize_output_image_size(g, C) {
          const [v, ee] = g.size;
          let X, le;
          if (this.do_thumbnail) {
            const { height: ue, width: fe } = C;
            X = Math.min(ue, fe);
          } else Number.isInteger(C) ? (X = C, le = this.config.max_size ?? X) : C !== void 0 && (X = C.shortest_edge, le = C.longest_edge);
          if (X !== void 0 || le !== void 0) {
            const ue = X === void 0 ? 1 : Math.max(X / v, X / ee), fe = v * ue, Ce = ee * ue, xe = le === void 0 ? 1 : Math.min(le / fe, le / Ce);
            let Le = Math.floor(Number((fe * xe).toFixed(2))), qe = Math.floor(Number((Ce * xe).toFixed(2)));
            return this.size_divisibility !== void 0 && ([Le, qe] = y([Le, qe], this.size_divisibility)), [Le, qe];
          } else if (C !== void 0 && C.width !== void 0 && C.height !== void 0) {
            let ue = C.width, fe = C.height;
            if (this.config.keep_aspect_ratio && this.config.ensure_multiple_of) {
              let Ce = fe / ee, xe = ue / v;
              Math.abs(1 - xe) < Math.abs(1 - Ce) ? Ce = xe : xe = Ce, fe = x(Ce * ee, this.config.ensure_multiple_of), ue = x(xe * v, this.config.ensure_multiple_of);
            }
            return [ue, fe];
          } else {
            if (this.size_divisibility !== void 0)
              return y([v, ee], this.size_divisibility);
            if (C.min_pixels !== void 0 && C.max_pixels !== void 0) {
              const { min_pixels: ue, max_pixels: fe } = C, Ce = this.config.patch_size * this.config.merge_size;
              return z(ee, v, Ce, ue, fe);
            } else
              throw new Error(`Could not resize image due to unsupported \`this.size\` option in config: ${JSON.stringify(C)}`);
          }
        }
        /**
         * Resizes the image.
         * @param {RawImage} image The image to resize.
         * @returns {Promise<RawImage>} The resized image.
         */
        async resize(g) {
          const [C, v] = this.get_resize_output_image_size(g, this.size);
          return await g.resize(C, v, {
            resample: this.resample
          });
        }
        /**
         * @typedef {object} PreprocessedImage
         * @property {HeightWidth} original_size The original size of the image.
         * @property {HeightWidth} reshaped_input_size The reshaped input size of the image.
         * @property {Tensor} pixel_values The pixel values of the preprocessed image.
         */
        /**
         * Preprocesses the given image.
         *
         * @param {RawImage} image The image to preprocess.
         * @param {Object} overrides The overrides for the preprocessing options.
         * @returns {Promise<PreprocessedImage>} The preprocessed image.
         */
        async preprocess(g, {
          do_normalize: C = null,
          do_pad: v = null,
          do_convert_rgb: ee = null,
          do_convert_grayscale: X = null,
          do_flip_channel_order: le = null
        } = {}) {
          this.do_crop_margin && (g = await this.crop_margin(g));
          const [ue, fe] = g.size;
          if (ee ?? this.do_convert_rgb ? g = g.rgb() : X && (g = g.grayscale()), this.do_resize && (g = await this.resize(g)), this.do_thumbnail && (g = await this.thumbnail(g, this.size, this.resample)), this.do_center_crop) {
            let Ue, ut;
            Number.isInteger(this.crop_size) ? (Ue = this.crop_size, ut = this.crop_size) : (Ue = this.crop_size.width, ut = this.crop_size.height), g = await g.center_crop(Ue, ut);
          }
          const Ce = [g.height, g.width];
          let xe = Float32Array.from(g.data), Le = [g.height, g.width, g.channels];
          if (this.do_rescale && this.rescale(xe), C ?? this.do_normalize) {
            let Ue = this.image_mean;
            Array.isArray(this.image_mean) || (Ue = new Array(g.channels).fill(Ue));
            let ut = this.image_std;
            if (Array.isArray(this.image_std) || (ut = new Array(g.channels).fill(Ue)), Ue.length !== g.channels || ut.length !== g.channels)
              throw new Error(`When set to arrays, the length of \`image_mean\` (${Ue.length}) and \`image_std\` (${ut.length}) must match the number of channels in the image (${g.channels}).`);
            for (let de = 0; de < xe.length; de += g.channels)
              for (let re = 0; re < g.channels; ++re)
                xe[de + re] = (xe[de + re] - Ue[re]) / ut[re];
          }
          if (v ?? this.do_pad) {
            if (this.pad_size)
              [xe, Le] = this.pad_image(xe, [g.height, g.width, g.channels], this.pad_size);
            else if (this.size_divisibility) {
              const [Ue, ut] = y([Le[1], Le[0]], this.size_divisibility);
              [xe, Le] = this.pad_image(xe, Le, { width: Ue, height: ut });
            }
          }
          if (le ?? this.do_flip_channel_order) {
            if (Le[2] !== 3)
              throw new Error("Flipping channel order is only supported for RGB images.");
            for (let Ue = 0; Ue < xe.length; Ue += 3) {
              const ut = xe[Ue];
              xe[Ue] = xe[Ue + 2], xe[Ue + 2] = ut;
            }
          }
          const qe = new L.Tensor("float32", xe, Le).permute(2, 0, 1);
          return {
            original_size: [fe, ue],
            reshaped_input_size: Ce,
            pixel_values: qe
          };
        }
        /**
         * Calls the feature extraction process on an array of images,
         * preprocesses each image, and concatenates the resulting
         * features into a single Tensor.
         * @param {RawImage[]} images The image(s) to extract features from.
         * @param {...any} args Additional arguments.
         * @returns {Promise<ImageProcessorResult>} An object containing the concatenated pixel values (and other metadata) of the preprocessed images.
         */
        async _call(g, ...C) {
          Array.isArray(g) || (g = [g]);
          const v = await Promise.all(g.map((X) => this.preprocess(X)));
          return {
            pixel_values: (0, L.stack)(v.map((X) => X.pixel_values), 0),
            // Original sizes of images
            original_sizes: v.map((X) => X.original_size),
            // Reshaped sizes of images, before padding or cropping
            reshaped_input_sizes: v.map((X) => X.reshaped_input_size)
          };
        }
        /**
         * Instantiate one of the processor classes of the library from a pretrained model.
         * 
         * The processor class to instantiate is selected based on the `image_processor_type` (or `feature_extractor_type`; legacy)
         * property of the config object (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.
         * @param {import('../utils/hub.js').PretrainedOptions} options Additional options for loading the processor.
         * 
         * @returns {Promise<ImageProcessor>} A new instance of the Processor class.
         */
        static async from_pretrained(g, C) {
          const v = await (0, W.getModelJSON)(g, w.IMAGE_PROCESSOR_NAME, !0, C);
          return new this(v);
        }
      }
    }
  ),
  /***/
  "./src/base/processing_utils.js": (
    /*!**************************************!*\
      !*** ./src/base/processing_utils.js ***!
      \**************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Processor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/constants.js */
        "./src/utils/constants.js"
      ), L = s(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), j = s(
        /*! ../utils/hub.js */
        "./src/utils/hub.js"
      );
      class J extends L.Callable {
        /**
         * Creates a new Processor with the given components
         * @param {Object} config 
         * @param {Record<string, Object>} components 
         */
        constructor(w, x) {
          super(), this.config = w, this.components = x;
        }
        /**
         * @returns {import('./image_processors_utils.js').ImageProcessor|undefined} The image processor of the processor, if it exists.
         */
        get image_processor() {
          return this.components.image_processor;
        }
        /**
         * @returns {import('../tokenizers.js').PreTrainedTokenizer|undefined} The tokenizer of the processor, if it exists.
         */
        get tokenizer() {
          return this.components.tokenizer;
        }
        /**
         * @returns {import('./feature_extraction_utils.js').FeatureExtractor|undefined} The feature extractor of the processor, if it exists.
         */
        get feature_extractor() {
          return this.components.feature_extractor;
        }
        apply_chat_template(w, x = {}) {
          if (!this.tokenizer)
            throw new Error("Unable to apply chat template without a tokenizer.");
          return this.tokenizer.apply_chat_template(w, {
            tokenize: !1,
            // default to false
            ...x
          });
        }
        batch_decode(...w) {
          if (!this.tokenizer)
            throw new Error("Unable to decode without a tokenizer.");
          return this.tokenizer.batch_decode(...w);
        }
        /**
         * Calls the feature_extractor function with the given input.
         * @param {any} input The input to extract features from.
         * @param {...any} args Additional arguments.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(w, ...x) {
          for (const y of [this.image_processor, this.feature_extractor, this.tokenizer])
            if (y)
              return y(w, ...x);
          throw new Error("No image processor, feature extractor, or tokenizer found.");
        }
        /**
         * Instantiate one of the processor classes of the library from a pretrained model.
         * 
         * The processor class to instantiate is selected based on the `feature_extractor_type` property of the config object
         * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.
         * @param {PretrainedProcessorOptions} options Additional options for loading the processor.
         * 
         * @returns {Promise<Processor>} A new instance of the Processor class.
         */
        static async from_pretrained(w, x) {
          const [y, M] = await Promise.all([
            // TODO:
            this.uses_processor_config ? (0, j.getModelJSON)(w, f.PROCESSOR_NAME, !0, x) : {},
            Promise.all(
              this.classes.filter((b) => b in this).map(async (b) => {
                const D = await this[b].from_pretrained(w, x);
                return [b.replace(/_class$/, ""), D];
              })
            ).then(Object.fromEntries)
          ]);
          return new this(y, M);
        }
      }
      ge(J, "classes", [
        "image_processor_class",
        "tokenizer_class",
        "feature_extractor_class"
      ]), ge(J, "uses_processor_config", !1);
    }
  ),
  /***/
  "./src/configs.js": (
    /*!************************!*\
      !*** ./src/configs.js ***!
      \************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        AutoConfig: () => (
          /* binding */
          x
        ),
        /* harmony export */
        PretrainedConfig: () => (
          /* binding */
          w
        ),
        /* harmony export */
        getKeyValueShapes: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      var f = s(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), L = s(
        /*! ./utils/hub.js */
        "./src/utils/hub.js"
      );
      async function j(y, M) {
        return await (0, L.getModelJSON)(y, "config.json", !0, M);
      }
      function J(y) {
        const M = {};
        let b = {};
        switch (y.model_type) {
          // Sub-configs
          case "llava":
          case "paligemma":
          case "florence2":
          case "llava_onevision":
          case "idefics3":
            b = J(y.text_config);
            break;
          case "moondream1":
            b = J(y.phi_config);
            break;
          case "musicgen":
            b = J(y.decoder);
            break;
          case "multi_modality":
            b = J(y.language_config);
            break;
          // Decoder-only models
          case "gpt2":
          case "gptj":
          case "jais":
          case "codegen":
          case "gpt_bigcode":
            M.num_heads = "n_head", M.num_layers = "n_layer", M.hidden_size = "n_embd";
            break;
          case "gpt_neox":
          case "stablelm":
          case "opt":
          case "falcon":
            M.num_heads = "num_attention_heads", M.num_layers = "num_hidden_layers", M.hidden_size = "hidden_size";
            break;
          case "llama":
          case "olmo":
          case "olmo2":
          case "mobilellm":
          case "granite":
          case "cohere":
          case "mistral":
          case "starcoder2":
          case "qwen2":
          case "qwen2_vl":
          case "phi":
          case "phi3":
          case "phi3_v":
            M.num_heads = "num_key_value_heads", M.num_layers = "num_hidden_layers", M.hidden_size = "hidden_size", M.num_attention_heads = "num_attention_heads";
            break;
          case "gemma":
          case "gemma2":
            M.num_heads = "num_key_value_heads", M.num_layers = "num_hidden_layers", M.dim_kv = "head_dim";
            break;
          case "openelm":
            M.num_heads = "num_kv_heads", M.num_layers = "num_transformer_layers", M.dim_kv = "head_dim";
            break;
          case "gpt_neo":
          case "donut-swin":
            M.num_heads = "num_heads", M.num_layers = "num_layers", M.hidden_size = "hidden_size";
            break;
          case "bloom":
            M.num_heads = "n_head", M.num_layers = "n_layer", M.hidden_size = "hidden_size";
            break;
          case "mpt":
            M.num_heads = "n_heads", M.num_layers = "n_layers", M.hidden_size = "d_model";
            break;
          case "exaone":
            M.num_heads = "num_key_value_heads", M.num_layers = "num_layers", M.dim_kv = "head_dim", M.num_attention_heads = "num_attention_heads";
            break;
          // Encoder-decoder models
          case "t5":
          case "mt5":
          case "longt5":
            M.num_decoder_layers = "num_decoder_layers", M.num_decoder_heads = "num_heads", M.decoder_dim_kv = "d_kv", M.num_encoder_layers = "num_layers", M.num_encoder_heads = "num_heads", M.encoder_dim_kv = "d_kv";
            break;
          case "bart":
          case "mbart":
          case "marian":
          case "whisper":
          case "m2m_100":
          case "blenderbot":
          case "blenderbot-small":
          case "florence2_language":
            M.num_decoder_layers = "decoder_layers", M.num_decoder_heads = "decoder_attention_heads", M.decoder_hidden_size = "d_model", M.num_encoder_layers = "encoder_layers", M.num_encoder_heads = "encoder_attention_heads", M.encoder_hidden_size = "d_model";
            break;
          case "speecht5":
            M.num_decoder_layers = "decoder_layers", M.num_decoder_heads = "decoder_attention_heads", M.decoder_hidden_size = "hidden_size", M.num_encoder_layers = "encoder_layers", M.num_encoder_heads = "encoder_attention_heads", M.encoder_hidden_size = "hidden_size";
            break;
          case "trocr":
            M.num_encoder_layers = M.num_decoder_layers = "decoder_layers", M.num_encoder_heads = M.num_decoder_heads = "decoder_attention_heads", M.encoder_hidden_size = M.decoder_hidden_size = "d_model";
            break;
          case "musicgen_decoder":
          case "moonshine":
            M.num_encoder_layers = M.num_decoder_layers = "num_hidden_layers", M.num_encoder_heads = M.num_decoder_heads = "num_attention_heads", M.encoder_hidden_size = M.decoder_hidden_size = "hidden_size";
            break;
          case "vision-encoder-decoder":
            const q = J(y.decoder), se = "num_decoder_layers" in q, oe = (0, f.pick)(y, ["model_type", "is_encoder_decoder"]);
            return se ? (oe.num_decoder_layers = q.num_decoder_layers, oe.num_decoder_heads = q.num_decoder_heads, oe.decoder_hidden_size = q.decoder_hidden_size, oe.num_encoder_layers = q.num_encoder_layers, oe.num_encoder_heads = q.num_encoder_heads, oe.encoder_hidden_size = q.encoder_hidden_size) : (oe.num_layers = q.num_layers, oe.num_heads = q.num_heads, oe.hidden_size = q.hidden_size), oe;
        }
        const D = {
          ...b,
          ...(0, f.pick)(y, ["model_type", "multi_query", "is_encoder_decoder"])
        };
        for (const q in M)
          D[q] = y[M[q]];
        return D;
      }
      function W(y, {
        prefix: M = "past_key_values",
        batch_size: b = 1
      } = {}) {
        const D = {}, q = y.normalized_config;
        if (q.is_encoder_decoder && "num_encoder_heads" in q && "num_decoder_heads" in q) {
          const se = q.encoder_dim_kv ?? q.encoder_hidden_size / q.num_encoder_heads, oe = q.decoder_dim_kv ?? q.decoder_hidden_size / q.num_decoder_heads, z = [b, q.num_encoder_heads, 0, se], V = [b, q.num_decoder_heads, 0, oe];
          for (let Y = 0; Y < q.num_decoder_layers; ++Y)
            D[`${M}.${Y}.encoder.key`] = z, D[`${M}.${Y}.encoder.value`] = z, D[`${M}.${Y}.decoder.key`] = V, D[`${M}.${Y}.decoder.value`] = V;
        } else {
          const se = q.num_heads, oe = q.num_layers, z = q.dim_kv ?? q.hidden_size / (q.num_attention_heads ?? se);
          if (q.model_type === "falcon") {
            const V = [b * se, 0, z];
            for (let Y = 0; Y < oe; ++Y)
              D[`${M}.${Y}.key`] = V, D[`${M}.${Y}.value`] = V;
          } else if (q.multi_query) {
            const V = [b * se, 0, 2 * z];
            for (let Y = 0; Y < oe; ++Y)
              D[`${M}.${Y}.key_value`] = V;
          } else if (q.model_type === "bloom") {
            const V = [b * se, z, 0], Y = [b * se, 0, z];
            for (let O = 0; O < oe; ++O)
              D[`${M}.${O}.key`] = V, D[`${M}.${O}.value`] = Y;
          } else if (q.model_type === "openelm")
            for (let V = 0; V < oe; ++V) {
              const Y = [b, se[V], 0, z];
              D[`${M}.${V}.key`] = Y, D[`${M}.${V}.value`] = Y;
            }
          else {
            const V = [b, se, 0, z];
            for (let Y = 0; Y < oe; ++Y)
              D[`${M}.${Y}.key`] = V, D[`${M}.${Y}.value`] = V;
          }
        }
        return D;
      }
      class w {
        /**
         * Create a new PreTrainedTokenizer instance.
         * @param {Object} configJSON The JSON of the config.
         */
        constructor(M) {
          // NOTE: Typo in original
          /** @type {string|null} */
          ge(this, "model_type", null);
          /** @type {boolean} */
          ge(this, "is_encoder_decoder", !1);
          /** @type {number} */
          ge(this, "max_position_embeddings");
          /** @type {TransformersJSConfig} */
          ge(this, "transformers.js_config");
          Object.assign(this, M), this.normalized_config = J(this);
        }
        /**
         * Loads a pre-trained config from the given `pretrained_model_name_or_path`. 
         * 
         * @param {string} pretrained_model_name_or_path The path to the pre-trained config.
         * @param {PretrainedOptions} options Additional options for loading the config.
         * @throws {Error} Throws an error if the config.json is not found in the `pretrained_model_name_or_path`.
         * 
         * @returns {Promise<PretrainedConfig>} A new instance of the `PretrainedConfig` class.
         */
        static async from_pretrained(M, {
          progress_callback: b = null,
          config: D = null,
          cache_dir: q = null,
          local_files_only: se = !1,
          revision: oe = "main"
        } = {}) {
          D && !(D instanceof w) && (D = new w(D));
          const z = D ?? await j(M, {
            progress_callback: b,
            config: D,
            cache_dir: q,
            local_files_only: se,
            revision: oe
          });
          return new this(z);
        }
      }
      class x {
        /** @type {typeof PretrainedConfig.from_pretrained} */
        static async from_pretrained(...M) {
          return w.from_pretrained(...M);
        }
      }
    }
  ),
  /***/
  "./src/env.js": (
    /*!********************!*\
      !*** ./src/env.js ***!
      \********************/
    /***/
    (ke, A, s) => {
      var v, ee;
      s.r(A), s.d(A, {
        /* harmony export */
        apis: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        env: () => (
          /* binding */
          g
        )
        /* harmony export */
      });
      var f = s(
        /*! fs */
        "?569f"
      ), L = s(
        /*! path */
        "?3f59"
      ), j = s(
        /*! url */
        "?154a"
      );
      const J = "3.2.2", W = typeof window < "u" && typeof window.document < "u", w = typeof self < "u" && ((v = self.constructor) == null ? void 0 : v.name) === "DedicatedWorkerGlobalScope", x = typeof self < "u" && "caches" in self, y = typeof navigator < "u" && "gpu" in navigator, M = typeof navigator < "u" && "ml" in navigator, b = typeof process < "u", D = b && ((ee = process == null ? void 0 : process.release) == null ? void 0 : ee.name) === "node", q = !C(f), se = !C(L), oe = Object.freeze({
        /** Whether we are running in a browser environment (and not a web worker) */
        IS_BROWSER_ENV: W,
        /** Whether we are running in a web worker environment */
        IS_WEBWORKER_ENV: w,
        /** Whether the Cache API is available */
        IS_WEB_CACHE_AVAILABLE: x,
        /** Whether the WebGPU API is available */
        IS_WEBGPU_AVAILABLE: y,
        /** Whether the WebNN API is available */
        IS_WEBNN_AVAILABLE: M,
        /** Whether the Node.js process API is available */
        IS_PROCESS_AVAILABLE: b,
        /** Whether we are running in a Node.js environment */
        IS_NODE_ENV: D,
        /** Whether the filesystem API is available */
        IS_FS_AVAILABLE: q,
        /** Whether the path API is available */
        IS_PATH_AVAILABLE: se
      }), z = q && se;
      let V = "./";
      if (z) {
        const X = Object(import.meta).url;
        X ? V = L.dirname(L.dirname(j.fileURLToPath(X))) : typeof __dirname < "u" && (V = L.dirname(__dirname));
      }
      const Y = z ? L.join(V, "/.cache/") : null, O = "/models/", $ = z ? L.join(V, O) : O, g = {
        version: J,
        /////////////////// Backends settings ///////////////////
        // NOTE: These will be populated later by the backends themselves.
        backends: {
          // onnxruntime-web/onnxruntime-node
          onnx: {}
        },
        /////////////////// Model settings ///////////////////
        allowRemoteModels: !0,
        remoteHost: "https://huggingface.co/",
        remotePathTemplate: "{model}/resolve/{revision}/",
        allowLocalModels: !(W || w),
        localModelPath: $,
        useFS: q,
        /////////////////// Cache settings ///////////////////
        useBrowserCache: x,
        useFSCache: q,
        cacheDir: Y,
        useCustomCache: !1,
        customCache: null
        //////////////////////////////////////////////////////
      };
      function C(X) {
        return Object.keys(X).length === 0;
      }
    }
  ),
  /***/
  "./src/generation/configuration_utils.js": (
    /*!***********************************************!*\
      !*** ./src/generation/configuration_utils.js ***!
      \***********************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        GenerationConfig: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/core.js */
        "./src/utils/core.js"
      );
      class L {
        /**
         * 
         * @param {GenerationConfig|import('../configs.js').PretrainedConfig} config 
         */
        constructor(J) {
          // Parameters that control the length of the output
          /**
           * The maximum length the generated tokens can have.
           * Corresponds to the length of the input prompt + `max_new_tokens`.
           * Its effect is overridden by `max_new_tokens`, if also set.
           * @type {number}
           * @default 20
           */
          ge(this, "max_length", 20);
          /**
           * The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
           * @type {number}
           * @default null
           */
          ge(this, "max_new_tokens", null);
          /**
           * The minimum length of the sequence to be generated.
           * Corresponds to the length of the input prompt + `min_new_tokens`.
           * Its effect is overridden by `min_new_tokens`, if also set.
           * @type {number}
           * @default 0
           */
          ge(this, "min_length", 0);
          /**
           * The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt.
           * @type {number}
           * @default null
           */
          ge(this, "min_new_tokens", null);
          /**
           * Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:
           * - `true`, where the generation stops as soon as there are `num_beams` complete candidates;
           * - `false`, where an heuristic is applied and the generation stops when is it very unlikely to find better candidates;
           * - `"never"`, where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm).
           * @type {boolean|"never"}
           * @default false
           */
          ge(this, "early_stopping", !1);
          /**
           * The maximum amount of time you allow the computation to run for in seconds.
           * Generation will still finish the current pass after allocated time has been passed.
           * @type {number}
           * @default null
           */
          ge(this, "max_time", null);
          // Parameters that control the generation strategy used
          /**
           * Whether or not to use sampling; use greedy decoding otherwise.
           * @type {boolean}
           * @default false
           */
          ge(this, "do_sample", !1);
          /**
           * Number of beams for beam search. 1 means no beam search.
           * @type {number}
           * @default 1
           */
          ge(this, "num_beams", 1);
          /**
           * Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.
           * See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.
           * @type {number}
           * @default 1
           */
          ge(this, "num_beam_groups", 1);
          /**
           * The values balance the model confidence and the degeneration penalty in contrastive search decoding.
           * @type {number}
           * @default null
           */
          ge(this, "penalty_alpha", null);
          /**
           * Whether or not the model should use the past last key/values attentions (if applicable to the model) to speed up decoding.
           * @type {boolean}
           * @default true
           */
          ge(this, "use_cache", !0);
          // Parameters for manipulation of the model output logits
          /**
           * The value used to modulate the next token probabilities.
           * @type {number}
           * @default 1.0
           */
          ge(this, "temperature", 1);
          /**
           * The number of highest probability vocabulary tokens to keep for top-k-filtering.
           * @type {number}
           * @default 50
           */
          ge(this, "top_k", 50);
          /**
           * If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.
           * @type {number}
           * @default 1.0
           */
          ge(this, "top_p", 1);
          /**
           * Local typicality measures how similar the conditional probability of predicting a target token next is to the expected conditional probability of predicting a random token next, given the partial text already generated.
           * If set to float < 1, the smallest set of the most locally typical tokens with probabilities that add up to `typical_p` or higher are kept for generation.
           * See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.
           * @type {number}
           * @default 1.0
           */
          ge(this, "typical_p", 1);
          /**
           * If set to float strictly between 0 and 1, only tokens with a conditional probability greater than `epsilon_cutoff` will be sampled.
           * In the paper, suggested values range from 3e-4 to 9e-4, depending on the size of the model.
           * See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191) for more details.
           * @type {number}
           * @default 0.0
           */
          ge(this, "epsilon_cutoff", 0);
          /**
           * Eta sampling is a hybrid of locally typical sampling and epsilon sampling.
           * If set to float strictly between 0 and 1, a token is only considered if it is greater than either `eta_cutoff` or `sqrt(eta_cutoff) * exp(-entropy(softmax(next_token_logits)))`.
           * The latter term is intuitively the expected next token probability, scaled by `sqrt(eta_cutoff)`. In the paper, suggested values range from 3e-4 to 2e-3, depending on the size of the model.
           * See [Truncation Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191) for more details.
           * @type {number}
           * @default 0.0
           */
          ge(this, "eta_cutoff", 0);
          /**
           * This value is subtracted from a beam's score if it generates a token same as any beam from other group at a particular time.
           * Note that `diversity_penalty` is only effective if `group beam search` is enabled.
           * @type {number}
           * @default 0.0
           */
          ge(this, "diversity_penalty", 0);
          /**
           * The parameter for repetition penalty. 1.0 means no penalty.
           * See [this paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
           * @type {number}
           * @default 1.0
           */
          ge(this, "repetition_penalty", 1);
          /**
           * The paramater for encoder_repetition_penalty.
           * An exponential penalty on sequences that are not in the original input.
           * 1.0 means no penalty.
           * @type {number}
           * @default 1.0
           */
          ge(this, "encoder_repetition_penalty", 1);
          /**
           * Exponential penalty to the length that is used with beam-based generation.
           * It is applied as an exponent to the sequence length, which in turn is used to divide the score of the sequence.
           * Since the score is the log likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while `length_penalty` < 0.0 encourages shorter sequences.
           * @type {number}
           * @default 1.0
           */
          ge(this, "length_penalty", 1);
          /**
           * If set to int > 0, all ngrams of that size can only occur once.
           * @type {number}
           * @default 0
           */
          ge(this, "no_repeat_ngram_size", 0);
          /**
           * List of token ids that are not allowed to be generated.
           * In order to get the token ids of the words that should not appear in the generated text, use
           * `tokenizer(bad_words, { add_prefix_space: true, add_special_tokens: false }).input_ids`.
           * @type {number[][]}
           * @default null
           */
          ge(this, "bad_words_ids", null);
          /**
           * List of token ids that must be generated.
           * If given a `number[][]`, this is treated as a simple list of words that must be included, the opposite to `bad_words_ids`.
           * If given `number[][][]`, this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081), where one can allow different forms of each word.
           * @type {number[][]|number[][][]}
           * @default null
           */
          ge(this, "force_words_ids", null);
          /**
           * Whether to renormalize the logits after applying all the logits processors or warpers (including the custom ones).
           * It's highly recommended to set this flag to `true` as the search algorithms suppose the score logits are normalized but some logit processors or warpers break the normalization.
           * @type {boolean}
           * @default false
           */
          ge(this, "renormalize_logits", !1);
          /**
           * Custom constraints that can be added to the generation to ensure that the output will contain the use of certain tokens as defined by `Constraint` objects, in the most sensible way possible.
           * @type {Object[]}
           * @default null
           */
          ge(this, "constraints", null);
          /**
           * The id of the token to force as the first generated token after the `decoder_start_token_id`.
           * Useful for multilingual models like mBART where the first generated token needs to be the target language token.
           * @type {number}
           * @default null
           */
          ge(this, "forced_bos_token_id", null);
          /**
           * The id of the token to force as the last generated token when `max_length` is reached.
           * Optionally, use a list to set multiple *end-of-sequence* tokens.
           * @type {number|number[]}
           * @default null
           */
          ge(this, "forced_eos_token_id", null);
          /**
           * Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to crash. Note that using `remove_invalid_values` can slow down generation.
           * @type {boolean}
           */
          ge(this, "remove_invalid_values", !1);
          /**
           * This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been generated.
           * The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where penalty starts and `decay_factor` represents the factor of exponential decay.
           * @type {[number, number]}
           * @default null
           */
          ge(this, "exponential_decay_length_penalty", null);
          /**
           * A list of tokens that will be suppressed at generation.
           * The `SuppressTokens` logit processor will set their log probs to `-inf` so that they are not sampled.
           * @type {number[]}
           * @default null
           */
          ge(this, "suppress_tokens", null);
          /**
           * A streamer that will be used to stream the generation.
           * @type {import('./streamers.js').TextStreamer}
           * @default null
           */
          ge(this, "streamer", null);
          /**
           * A list of tokens that will be suppressed at the beginning of the generation.
           * The `SuppressBeginTokens` logit processor will set their log probs to `-inf` so that they are not sampled.
           * @type {number[]}
           * @default null
           */
          ge(this, "begin_suppress_tokens", null);
          /**
           * A list of pairs of integers which indicates a mapping from generation indices to token indices that will be forced before sampling.
           * For example, `[[1, 123]]` means the second generated token will always be a token of index 123.
           * @type {[number, number][]}
           * @default null
           */
          ge(this, "forced_decoder_ids", null);
          /**
           * The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.
           * Higher guidance scale encourages the model to generate samples that are more closely linked to the input
           * prompt, usually at the expense of poorer quality.
           * @type {number}
           * @default null
           */
          ge(this, "guidance_scale", null);
          // Parameters that define the output variables of `generate`
          /**
           * The number of independently computed returned sequences for each element in the batch.
           * @type {number}
           * @default 1
           */
          ge(this, "num_return_sequences", 1);
          /**
           * Whether or not to return the attentions tensors of all attention layers.
           * See `attentions` under returned tensors for more details.
           * @type {boolean}
           * @default false
           */
          ge(this, "output_attentions", !1);
          /**
           * Whether or not to return the hidden states of all layers.
           * See `hidden_states` under returned tensors for more details.
           * @type {boolean}
           * @default false
           */
          ge(this, "output_hidden_states", !1);
          /**
           * Whether or not to return the prediction scores.
           * See `scores` under returned tensors for more details.
           * @type {boolean}
           * @default false
           */
          ge(this, "output_scores", !1);
          /**
           * Whether or not to return a `ModelOutput` instead of a plain tuple.
           * @type {boolean}
           * @default false
           */
          ge(this, "return_dict_in_generate", !1);
          // Special tokens that can be used at generation time
          /**
           * The id of the *padding* token.
           * @type {number}
           * @default null
           */
          ge(this, "pad_token_id", null);
          /**
           * The id of the *beginning-of-sequence* token.
           * @type {number}
           * @default null
           */
          ge(this, "bos_token_id", null);
          /**
           * The id of the *end-of-sequence* token.
           * Optionally, use a list to set multiple *end-of-sequence* tokens.
           * @type {number|number[]}
           * @default null
           */
          ge(this, "eos_token_id", null);
          // Generation parameters exclusive to encoder-decoder models
          /**
           * If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the `decoder_input_ids`.
           * @type {number}
           * @default 0
           */
          ge(this, "encoder_no_repeat_ngram_size", 0);
          /**
           * If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.
           * @type {number}
           * @default null
           */
          ge(this, "decoder_start_token_id", null);
          // Wild card
          /**
           * Additional generation kwargs will be forwarded to the `generate` function of the model.
           * Kwargs that are not present in `generate`'s signature will be used in the model forward pass.
           * @type {Object}
           * @default {}
           */
          ge(this, "generation_kwargs", {});
          Object.assign(this, (0, f.pick)(J, Object.getOwnPropertyNames(this)));
        }
      }
    }
  ),
  /***/
  "./src/generation/logits_process.js": (
    /*!******************************************!*\
      !*** ./src/generation/logits_process.js ***!
      \******************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ClassifierFreeGuidanceLogitsProcessor: () => (
          /* binding */
          z
        ),
        /* harmony export */
        ForcedBOSTokenLogitsProcessor: () => (
          /* binding */
          w
        ),
        /* harmony export */
        ForcedEOSTokenLogitsProcessor: () => (
          /* binding */
          x
        ),
        /* harmony export */
        LogitsProcessor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        LogitsProcessorList: () => (
          /* binding */
          W
        ),
        /* harmony export */
        LogitsWarper: () => (
          /* binding */
          J
        ),
        /* harmony export */
        MinLengthLogitsProcessor: () => (
          /* binding */
          q
        ),
        /* harmony export */
        MinNewTokensLengthLogitsProcessor: () => (
          /* binding */
          se
        ),
        /* harmony export */
        NoBadWordsLogitsProcessor: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        NoRepeatNGramLogitsProcessor: () => (
          /* binding */
          b
        ),
        /* harmony export */
        RepetitionPenaltyLogitsProcessor: () => (
          /* binding */
          D
        ),
        /* harmony export */
        SuppressTokensAtBeginLogitsProcessor: () => (
          /* binding */
          y
        ),
        /* harmony export */
        TemperatureLogitsWarper: () => (
          /* binding */
          V
        ),
        /* harmony export */
        TopKLogitsWarper: () => (
          /* binding */
          O
        ),
        /* harmony export */
        TopPLogitsWarper: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        WhisperTimeStampLogitsProcessor: () => (
          /* binding */
          M
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      );
      s(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var L = s(
        /*! ../utils/maths.js */
        "./src/utils/maths.js"
      );
      class j extends f.Callable {
        /**
         * Apply the processor to the input logits.
         *
         * @abstract
         * @param {bigint[][]} input_ids The input ids.
         * @param {Tensor} logits The logits to process.
         * @throws {Error} Throws an error if `_call` is not implemented in the subclass.
         */
        _call(g, C) {
          throw Error("`_call` should be implemented in a subclass");
        }
      }
      class J extends f.Callable {
        /**
         * Apply the processor to the input logits.
         *
         * @abstract
         * @param {bigint[][]} input_ids The input ids.
         * @param {Tensor} logits The logits to process.
         * @throws {Error} Throws an error if `_call` is not implemented in the subclass.
         */
        _call(g, C) {
          throw Error("`_call` should be implemented in a subclass");
        }
      }
      class W extends f.Callable {
        /**
         * Constructs a new instance of `LogitsProcessorList`.
         */
        constructor() {
          super(), this.processors = [];
        }
        /**
         * Adds a new logits processor to the list.
         *
         * @param {LogitsProcessor} item The logits processor function to add.
         */
        push(g) {
          this.processors.push(g);
        }
        /**
         * Adds multiple logits processors to the list.
         *
         * @param {LogitsProcessor[]} items The logits processor functions to add.
         */
        extend(g) {
          this.processors.push(...g);
        }
        /**
         * Applies all logits processors in the list to a batch of logits, modifying them in-place.
         *
         * @param {bigint[][]} input_ids The input IDs for the language model.
         * @param {Tensor} logits
         */
        _call(g, C) {
          let v = C;
          for (const ee of this.processors)
            v = ee(g, v);
          return v;
        }
        [Symbol.iterator]() {
          return this.processors.values();
        }
      }
      class w extends j {
        /**
         * Create a ForcedBOSTokenLogitsProcessor.
         * @param {number} bos_token_id The ID of the beginning-of-sequence token to be forced.
         */
        constructor(g) {
          super(), this.bos_token_id = g;
        }
        /**
         * Apply the BOS token forcing to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with BOS token forcing.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v)
            if (g[v].length === 1) {
              const ee = (
                /** @type {Float32Array} */
                C[v].data
              );
              ee.fill(-1 / 0), ee[this.bos_token_id] = 0;
            }
          return C;
        }
      }
      class x extends j {
        /**
         * Create a ForcedEOSTokenLogitsProcessor.
         * @param {number} max_length The maximum length of the sequence to be generated.
         * @param {number|number[]} eos_token_id The id(s) of the *end-of-sequence* token.
         */
        constructor(g, C) {
          super(), this.max_length = g, this.eos_token_id = Array.isArray(C) ? C : [C];
        }
        /**
         * Apply the processor to input_ids and logits.
         * 
         * @param {bigint[][]} input_ids The input ids.
         * @param {Tensor} logits The logits tensor.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v)
            if (g[v].length === this.max_length - 1) {
              const ee = (
                /** @type {Float32Array} */
                C[v].data
              );
              ee.fill(-1 / 0);
              for (const X of this.eos_token_id)
                ee[X] = 0;
            }
          return C;
        }
      }
      class y extends j {
        /**
         * Create a SuppressTokensAtBeginLogitsProcessor.
         * @param {number[]} begin_suppress_tokens The IDs of the tokens to suppress.
         * @param {number} begin_index The number of tokens to generate before suppressing tokens.
         */
        constructor(g, C) {
          super(), this.begin_suppress_tokens = g, this.begin_index = C;
        }
        /**
         * Apply the BOS token forcing to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with BOS token forcing.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v)
            if (g[v].length === this.begin_index) {
              const ee = (
                /** @type {Float32Array} */
                C[v].data
              );
              for (const X of this.begin_suppress_tokens)
                ee[X] = -1 / 0;
            }
          return C;
        }
      }
      class M extends j {
        /**
         * Constructs a new WhisperTimeStampLogitsProcessor.
         * @param {import('../models/whisper/generation_whisper.js').WhisperGenerationConfig} generate_config The config object passed to the `generate()` method of a transformer model.
         * @param {number[]} init_tokens The initial tokens of the input sequence.
         */
        constructor(g, C) {
          super(), this.eos_token_id = Array.isArray(g.eos_token_id) ? g.eos_token_id[0] : g.eos_token_id, this.no_timestamps_token_id = g.no_timestamps_token_id, this.timestamp_begin = this.no_timestamps_token_id + 1, this.begin_index = C.length, C.at(-1) === this.no_timestamps_token_id && (this.begin_index -= 1), this.max_initial_timestamp_index = g.max_initial_timestamp_index;
        }
        /**
         * Modify the logits to handle timestamp tokens.
         * @param {bigint[][]} input_ids The input sequence of tokens.
         * @param {Tensor} logits The logits output by the model.
         * @returns {Tensor} The modified logits.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v) {
            const ee = (
              /** @type {Float32Array} */
              C[v].data
            );
            if (ee[this.no_timestamps_token_id] = -1 / 0, g[v].length === this.begin_index - 1) {
              ee.fill(-1 / 0), ee[this.timestamp_begin] = 0;
              continue;
            }
            const X = g[v].slice(this.begin_index), le = X.length >= 1 && X[X.length - 1] >= this.timestamp_begin, ue = X.length < 2 || X[X.length - 2] >= this.timestamp_begin;
            if (le && (ue ? ee.subarray(this.timestamp_begin).fill(-1 / 0) : ee.subarray(0, this.eos_token_id).fill(-1 / 0)), g[v].length === this.begin_index && this.max_initial_timestamp_index !== null) {
              const Le = this.timestamp_begin + this.max_initial_timestamp_index;
              ee.subarray(Le + 1).fill(-1 / 0);
            }
            const fe = (0, L.log_softmax)(ee), Ce = Math.log(fe.subarray(this.timestamp_begin).map(Math.exp).reduce((Le, qe) => Le + qe)), xe = (0, L.max)(fe.subarray(0, this.timestamp_begin))[0];
            Ce > xe && ee.subarray(0, this.timestamp_begin).fill(-1 / 0);
          }
          return C;
        }
      }
      class b extends j {
        /**
         * Create a NoRepeatNGramLogitsProcessor.
         * @param {number} no_repeat_ngram_size The no-repeat-ngram size. All ngrams of this size can only occur once.
         */
        constructor(g) {
          super(), this.no_repeat_ngram_size = g;
        }
        /**
         * Generate n-grams from a sequence of token ids.
         * @param {bigint[]} prevInputIds List of previous input ids
         * @returns {Map<string, number[]>} Map of generated n-grams
         */
        getNgrams(g) {
          const C = g.length, v = [];
          for (let X = 0; X < C + 1 - this.no_repeat_ngram_size; ++X) {
            const le = [];
            for (let ue = 0; ue < this.no_repeat_ngram_size; ++ue)
              le.push(g[X + ue]);
            v.push(le.map(Number));
          }
          const ee = /* @__PURE__ */ new Map();
          for (const X of v) {
            const le = X.slice(0, X.length - 1), ue = JSON.stringify(le), fe = ee.get(ue) ?? [];
            fe.push(X[X.length - 1]), ee.set(ue, fe);
          }
          return ee;
        }
        /**
         * Generate n-grams from a sequence of token ids.
         * @param {Map<string, number[]>} bannedNgrams Map of banned n-grams
         * @param {bigint[]} prevInputIds List of previous input ids
         * @returns {number[]} Map of generated n-grams
         */
        getGeneratedNgrams(g, C) {
          const v = C.slice(C.length + 1 - this.no_repeat_ngram_size, C.length);
          return g.get(JSON.stringify(v.map(Number))) ?? [];
        }
        /**
         * Calculate banned n-gram tokens
         * @param {bigint[]} prevInputIds List of previous input ids
         * @returns {number[]} Map of generated n-grams
         */
        calcBannedNgramTokens(g) {
          const C = [];
          if (g.length + 1 < this.no_repeat_ngram_size)
            return C;
          {
            const v = this.getNgrams(g);
            return this.getGeneratedNgrams(v, g);
          }
        }
        /**
         * Apply the no-repeat-ngram processor to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with no-repeat-ngram processing.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v) {
            const ee = (
              /** @type {Float32Array} */
              C[v].data
            ), X = this.calcBannedNgramTokens(g[v]);
            for (const le of X)
              ee[le] = -1 / 0;
          }
          return C;
        }
      }
      class D extends j {
        /**
         * Create a RepetitionPenaltyLogitsProcessor.
         * @param {number} penalty The parameter for repetition penalty.
         * - 1.0 means no penalty. Above 1.0 penalizes previously generated tokens.
         * - Between 0.0 and 1.0 rewards previously generated tokens.
         */
        constructor(g) {
          super(), this.penalty = g;
        }
        /**
         * Apply the repetition penalty to the logits.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The logits with repetition penalty processing.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v) {
            const ee = (
              /** @type {Float32Array} */
              C[v].data
            );
            for (const X of new Set(g[v])) {
              const le = Number(X);
              ee[le] < 0 ? ee[le] *= this.penalty : ee[le] /= this.penalty;
            }
          }
          return C;
        }
      }
      class q extends j {
        /**
         * Create a MinLengthLogitsProcessor.
         * @param {number} min_length The minimum length below which the score of `eos_token_id` is set to negative infinity.
         * @param {number|number[]} eos_token_id The ID/IDs of the end-of-sequence token.
         */
        constructor(g, C) {
          super(), this.min_length = g, this.eos_token_id = Array.isArray(C) ? C : [C];
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v)
            if (g[v].length < this.min_length) {
              const ee = (
                /** @type {Float32Array} */
                C[v].data
              );
              for (const X of this.eos_token_id)
                ee[X] = -1 / 0;
            }
          return C;
        }
      }
      class se extends j {
        /**
         * Create a MinNewTokensLengthLogitsProcessor.
         * @param {number} prompt_length_to_skip The input tokens length.
         * @param {number} min_new_tokens The minimum *new* tokens length below which the score of `eos_token_id` is set to negative infinity.
         * @param {number|number[]} eos_token_id The ID/IDs of the end-of-sequence token.
         */
        constructor(g, C, v) {
          super(), this.prompt_length_to_skip = g, this.min_new_tokens = C, this.eos_token_id = Array.isArray(v) ? v : [v];
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v)
            if (g[v].length - this.prompt_length_to_skip < this.min_new_tokens) {
              const X = (
                /** @type {Float32Array} */
                C[v].data
              );
              for (const le of this.eos_token_id)
                X[le] = -1 / 0;
            }
          return C;
        }
      }
      class oe extends j {
        /**
         * Create a `NoBadWordsLogitsProcessor`.
         * @param {number[][]} bad_words_ids List of list of token ids that are not allowed to be generated.
         * @param {number|number[]} eos_token_id The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.
         */
        constructor(g, C) {
          super(), this.bad_words_ids = g, this.eos_token_id = Array.isArray(C) ? C : [C];
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(g, C) {
          for (let v = 0; v < g.length; ++v) {
            const ee = (
              /** @type {Float32Array} */
              C[v].data
            ), X = g[v];
            for (const le of this.bad_words_ids) {
              let ue = !0;
              for (let fe = 1; fe <= le.length - 1 && le.length < X.length; ++fe)
                if (le.at(-fe - 1) != X.at(-fe)) {
                  ue = !1;
                  break;
                }
              ue && (ee[le.at(-1)] = -1 / 0);
            }
          }
          return C;
        }
      }
      class z extends j {
        /**
         * Create a `ClassifierFreeGuidanceLogitsProcessor`.
         * @param {number} guidance_scale The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.
         * Higher guidance scale encourages the model to generate samples that are more closely linked to the input
         * prompt, usually at the expense of poorer quality.
         */
        constructor(g) {
          if (super(), g <= 1)
            throw new Error(
              `Require guidance scale >1 to use the classifier free guidance processor, got guidance scale ${g}.`
            );
          this.guidance_scale = g;
        }
        /**
         * Apply logit processor.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(g, C) {
          if (C.dims[0] !== 2 * g.length)
            throw new Error(
              `Logits should have twice the batch size of the input ids, the first half of batches corresponding to the conditional inputs, and the second half of batches corresponding to the unconditional inputs. Got batch size ${C.dims[0]} for the logits and ${g.length} for the input ids.`
            );
          const v = g.length, ee = C.slice([0, v], null), X = C.slice([v, C.dims[0]], null);
          for (let le = 0; le < X.data.length; ++le)
            X.data[le] += (ee.data[le] - X.data[le]) * this.guidance_scale;
          return X;
        }
      }
      class V extends J {
        /**
         * Create a `TemperatureLogitsWarper`.
         * @param {number} temperature Strictly positive float value used to modulate the logits distribution.
         * A value smaller than `1` decreases randomness (and vice versa), with `0` being equivalent to shifting
         * all probability mass to the most likely token.
         */
        constructor(g) {
          super(), this.temperature = g;
        }
        /**
         * Apply logit warper.
         * @param {bigint[][]} input_ids The input IDs.
         * @param {Tensor} logits The logits.
         * @returns {Tensor} The processed logits.
         */
        _call(g, C) {
          const v = (
            /** @type {Float32Array} */
            C.data
          );
          for (let ee = 0; ee < v.length; ++ee)
            v[ee] /= this.temperature;
          return C;
        }
      }
      class Y extends J {
        /**
         * Create a `TopPLogitsWarper`.
         * @param {number} top_p If set to < 1, only the smallest set of most probable tokens with
         * probabilities that add up to `top_p` or higher are kept for generation.
         * @param {Object} options Additional options for the top-p sampling.
         * @param {number} [options.filter_value=-Infinity] All filtered values will be set to this float value.
         * @param {number} [options.min_tokens_to_keep=1] Minimum number of tokens that cannot be filtered.
         */
        constructor(g, {
          filter_value: C = -1 / 0,
          min_tokens_to_keep: v = 1
        } = {}) {
          if (super(), g < 0 || g > 1)
            throw new Error(`\`top_p\` must be a float > 0 and < 1, but is ${g}`);
          if (!Number.isInteger(v) || v < 1)
            throw new Error(`\`min_tokens_to_keep\` must be a positive integer, but is ${v}`);
          this.top_p = g, this.filter_value = C, this.min_tokens_to_keep = v;
        }
      }
      class O extends J {
        /**
         * Create a `TopKLogitsWarper`.
         * @param {number} top_k If set to > 0, only the top `top_k` tokens are kept for generation.
         * @param {Object} options Additional options for the top-k sampling.
         * @param {number} [options.filter_value=-Infinity] All filtered values will be set to this float value.
         * @param {number} [options.min_tokens_to_keep=1] Minimum number of tokens that cannot be filtered.
         */
        constructor(g, {
          filter_value: C = -1 / 0,
          min_tokens_to_keep: v = 1
        } = {}) {
          if (super(), !Number.isInteger(g) || g < 0)
            throw new Error(`\`top_k\` must be a positive integer, but is ${g}`);
          this.top_k = Math.max(g, v), this.filter_value = C;
        }
      }
    }
  ),
  /***/
  "./src/generation/logits_sampler.js": (
    /*!******************************************!*\
      !*** ./src/generation/logits_sampler.js ***!
      \******************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        LogitsSampler: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      ), L = s(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      ), j = s(
        /*! ../utils/maths.js */
        "./src/utils/maths.js"
      );
      s(
        /*! ../generation/configuration_utils.js */
        "./src/generation/configuration_utils.js"
      );
      class J extends f.Callable {
        /**
         * Creates a new Sampler object with the specified generation config.
         * @param {GenerationConfig} generation_config The generation config.
         */
        constructor(M) {
          super(), this.generation_config = M;
        }
        /**
         * Executes the sampler, using the specified logits.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>}
         */
        async _call(M) {
          return this.sample(M);
        }
        /**
         * Abstract method for sampling the logits.
         * @param {Tensor} logits
         * @throws {Error} If not implemented in subclass.
         * @returns {Promise<[bigint, number][]>}
         */
        async sample(M) {
          throw Error("sample should be implemented in subclasses.");
        }
        /**
         * Returns the specified logits as an array, with temperature applied.
         * @param {Tensor} logits
         * @param {number} index
         * @returns {Float32Array}
         */
        getLogits(M, b) {
          let D = M.dims.at(-1), q = (
            /** @type {Float32Array} */
            M.data
          );
          if (b === -1)
            q = q.slice(-D);
          else {
            let se = b * D;
            q = q.slice(se, se + D);
          }
          return q;
        }
        /**
         * Selects an item randomly based on the specified probabilities.
         * @param {import("../transformers.js").DataArray} probabilities An array of probabilities to use for selection.
         * @returns {number} The index of the selected item.
         */
        randomSelect(M) {
          let b = 0;
          for (let q = 0; q < M.length; ++q)
            b += M[q];
          let D = Math.random() * b;
          for (let q = 0; q < M.length; ++q)
            if (D -= M[q], D <= 0)
              return q;
          return 0;
        }
        /**
         * Returns a Sampler object based on the specified options.
         * @param {GenerationConfig} generation_config An object containing options for the sampler.
         * @returns {LogitsSampler} A Sampler object.
         */
        static getSampler(M) {
          if (M.do_sample)
            return new w(M);
          if (M.num_beams > 1)
            return new x(M);
          if (M.num_return_sequences > 1)
            throw Error(`num_return_sequences has to be 1 when doing greedy search, but is ${M.num_return_sequences}.`);
          return new W(M);
        }
      }
      class W extends J {
        /**
         * Sample the maximum probability of a given logits tensor.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>} An array with a single tuple, containing the index of the maximum value and a meaningless score (since this is a greedy search).
         */
        async sample(M) {
          const b = (0, j.max)(M.data)[1];
          return [
            [BigInt(b), 0]
          ];
        }
      }
      class w extends J {
        /**
         * Sample from the logits.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>}
         */
        async sample(M) {
          let b = M.dims.at(-1);
          this.generation_config.top_k > 0 && (b = Math.min(this.generation_config.top_k, b));
          const [D, q] = await (0, L.topk)(M, b), se = (0, j.softmax)(
            /** @type {Float32Array} */
            D.data
          );
          return Array.from({ length: this.generation_config.num_beams }, () => {
            const oe = this.randomSelect(se);
            return [
              q.data[oe],
              // token id
              Math.log(se[oe])
              // score
            ];
          });
        }
      }
      class x extends J {
        /**
         * Sample from the logits.
         * @param {Tensor} logits
         * @returns {Promise<[bigint, number][]>}
         */
        async sample(M) {
          let b = M.dims.at(-1);
          this.generation_config.top_k > 0 && (b = Math.min(this.generation_config.top_k, b));
          const [D, q] = await (0, L.topk)(M, b), se = (0, j.softmax)(
            /** @type {Float32Array} */
            D.data
          );
          return Array.from({ length: this.generation_config.num_beams }, (oe, z) => [
            q.data[z],
            // token id
            Math.log(se[z])
            // score
          ]);
        }
      }
    }
  ),
  /***/
  "./src/generation/stopping_criteria.js": (
    /*!*********************************************!*\
      !*** ./src/generation/stopping_criteria.js ***!
      \*********************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        EosTokenCriteria: () => (
          /* binding */
          W
        ),
        /* harmony export */
        InterruptableStoppingCriteria: () => (
          /* binding */
          w
        ),
        /* harmony export */
        MaxLengthCriteria: () => (
          /* binding */
          J
        ),
        /* harmony export */
        StoppingCriteria: () => (
          /* binding */
          L
        ),
        /* harmony export */
        StoppingCriteriaList: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/generic.js */
        "./src/utils/generic.js"
      );
      class L extends f.Callable {
        /**
         * 
         * @param {number[][]} input_ids (`number[][]` of shape `(batch_size, sequence_length)`):
         * Indices of input sequence tokens in the vocabulary.
         * @param {number[][]} scores scores (`number[][]` of shape `(batch_size, config.vocab_size)`):
         * Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax
         * or scores for each vocabulary token after SoftMax.
         * @returns {boolean[]} A list of booleans indicating whether each sequence should be stopped.
         */
        _call(y, M) {
          throw Error("StoppingCriteria needs to be subclassed");
        }
      }
      class j extends f.Callable {
        /**
         * Constructs a new instance of `StoppingCriteriaList`.
         */
        constructor() {
          super(), this.criteria = [];
        }
        /**
         * Adds a new stopping criterion to the list.
         *
         * @param {StoppingCriteria} item The stopping criterion to add.
         */
        push(y) {
          this.criteria.push(y);
        }
        /**
         * Adds multiple stopping criteria to the list.
         *
         * @param {StoppingCriteria|StoppingCriteriaList|StoppingCriteria[]} items The stopping criteria to add.
         */
        extend(y) {
          y instanceof j ? y = y.criteria : y instanceof L && (y = [y]), this.criteria.push(...y);
        }
        _call(y, M) {
          const b = new Array(y.length).fill(!1);
          for (const D of this.criteria) {
            const q = D(y, M);
            for (let se = 0; se < b.length; ++se)
              b[se] || (b[se] = q[se]);
          }
          return b;
        }
        [Symbol.iterator]() {
          return this.criteria.values();
        }
      }
      class J extends L {
        /**
         * 
         * @param {number} max_length The maximum length that the output sequence can have in number of tokens.
         * @param {number} [max_position_embeddings=null] The maximum model length, as defined by the model's `config.max_position_embeddings` attribute.
         */
        constructor(y, M = null) {
          super(), this.max_length = y, this.max_position_embeddings = M;
        }
        _call(y) {
          return y.map((M) => M.length >= this.max_length);
        }
      }
      class W extends L {
        /**
         * 
         * @param {number|number[]} eos_token_id The id of the *end-of-sequence* token.
         * Optionally, use a list to set multiple *end-of-sequence* tokens.
         */
        constructor(y) {
          super(), Array.isArray(y) || (y = [y]), this.eos_token_id = y;
        }
        /**
         * 
         * @param {number[][]} input_ids 
         * @param {number[][]} scores 
         * @returns {boolean[]}
         */
        _call(y, M) {
          return y.map((b) => {
            const D = b.at(-1);
            return this.eos_token_id.some((q) => D == q);
          });
        }
      }
      class w extends L {
        constructor() {
          super(), this.interrupted = !1;
        }
        interrupt() {
          this.interrupted = !0;
        }
        reset() {
          this.interrupted = !1;
        }
        _call(y, M) {
          return new Array(y.length).fill(this.interrupted);
        }
      }
    }
  ),
  /***/
  "./src/generation/streamers.js": (
    /*!*************************************!*\
      !*** ./src/generation/streamers.js ***!
      \*************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        BaseStreamer: () => (
          /* binding */
          J
        ),
        /* harmony export */
        TextStreamer: () => (
          /* binding */
          w
        ),
        /* harmony export */
        WhisperTextStreamer: () => (
          /* binding */
          x
        )
        /* harmony export */
      });
      var f = s(
        /*! ../utils/core.js */
        "./src/utils/core.js"
      ), L = s(
        /*! ../tokenizers.js */
        "./src/tokenizers.js"
      ), j = s(
        /*! ../env.js */
        "./src/env.js"
      );
      class J {
        /**
         * Function that is called by `.generate()` to push new tokens
         * @param {bigint[][]} value 
         */
        put(M) {
          throw Error("Not implemented");
        }
        /**
         * Function that is called by `.generate()` to signal the end of generation
         */
        end() {
          throw Error("Not implemented");
        }
      }
      const W = j.apis.IS_PROCESS_AVAILABLE ? (y) => process.stdout.write(y) : (y) => console.log(y);
      class w extends J {
        /**
         * 
         * @param {import('../tokenizers.js').PreTrainedTokenizer} tokenizer
         * @param {Object} options
         * @param {boolean} [options.skip_prompt=false] Whether to skip the prompt tokens
         * @param {function(string): void} [options.callback_function=null] Function to call when a piece of text is ready to display
         * @param {function(bigint[]): void} [options.token_callback_function=null] Function to call when a new token is generated
         * @param {Object} [options.decode_kwargs={}] Additional keyword arguments to pass to the tokenizer's decode method
         */
        constructor(M, {
          skip_prompt: b = !1,
          callback_function: D = null,
          token_callback_function: q = null,
          decode_kwargs: se = {},
          ...oe
        } = {}) {
          super(), this.tokenizer = M, this.skip_prompt = b, this.callback_function = D ?? W, this.token_callback_function = q, this.decode_kwargs = { ...se, ...oe }, this.token_cache = [], this.print_len = 0, this.next_tokens_are_prompt = !0;
        }
        /**
         * Receives tokens, decodes them, and prints them to stdout as soon as they form entire words.
         * @param {bigint[][]} value 
         */
        put(M) {
          var se;
          if (M.length > 1)
            throw Error("TextStreamer only supports batch size of 1");
          if (this.skip_prompt && this.next_tokens_are_prompt) {
            this.next_tokens_are_prompt = !1;
            return;
          }
          const b = M[0];
          (se = this.token_callback_function) == null || se.call(this, b), this.token_cache = (0, f.mergeArrays)(this.token_cache, b);
          const D = this.tokenizer.decode(this.token_cache, this.decode_kwargs);
          let q;
          D.endsWith(`
`) ? (q = D.slice(this.print_len), this.token_cache = [], this.print_len = 0) : D.length > 0 && (0, L.is_chinese_char)(D.charCodeAt(D.length - 1)) ? (q = D.slice(this.print_len), this.print_len += q.length) : (q = D.slice(this.print_len, D.lastIndexOf(" ") + 1), this.print_len += q.length), this.on_finalized_text(q, !1);
        }
        /**
         * Flushes any remaining cache and prints a newline to stdout.
         */
        end() {
          let M;
          this.token_cache.length > 0 ? (M = this.tokenizer.decode(this.token_cache, this.decode_kwargs).slice(this.print_len), this.token_cache = [], this.print_len = 0) : M = "", this.next_tokens_are_prompt = !0, this.on_finalized_text(M, !0);
        }
        /**
         * Prints the new text to stdout. If the stream is ending, also prints a newline.
         * @param {string} text 
         * @param {boolean} stream_end 
         */
        on_finalized_text(M, b) {
          var D, q;
          M.length > 0 && ((D = this.callback_function) == null || D.call(this, M)), b && this.callback_function === W && j.apis.IS_PROCESS_AVAILABLE && ((q = this.callback_function) == null || q.call(this, `
`));
        }
      }
      class x extends w {
        /**
         * @param {import('../tokenizers.js').WhisperTokenizer} tokenizer
         * @param {Object} options
         * @param {boolean} [options.skip_prompt=false] Whether to skip the prompt tokens
         * @param {function(string): void} [options.callback_function=null] Function to call when a piece of text is ready to display
         * @param {function(bigint[]): void} [options.token_callback_function=null] Function to call when a new token is generated
         * @param {function(number): void} [options.on_chunk_start=null] Function to call when a new chunk starts
         * @param {function(number): void} [options.on_chunk_end=null] Function to call when a chunk ends
         * @param {function(): void} [options.on_finalize=null] Function to call when the stream is finalized
         * @param {number} [options.time_precision=0.02] Precision of the timestamps
         * @param {boolean} [options.skip_special_tokens=true] Whether to skip special tokens when decoding
         * @param {Object} [options.decode_kwargs={}] Additional keyword arguments to pass to the tokenizer's decode method
         */
        constructor(M, {
          skip_prompt: b = !1,
          callback_function: D = null,
          token_callback_function: q = null,
          on_chunk_start: se = null,
          on_chunk_end: oe = null,
          on_finalize: z = null,
          time_precision: V = 0.02,
          skip_special_tokens: Y = !0,
          decode_kwargs: O = {}
        } = {}) {
          super(M, {
            skip_prompt: b,
            callback_function: D,
            token_callback_function: q,
            decode_kwargs: { skip_special_tokens: Y, ...O }
          }), this.timestamp_begin = M.timestamp_begin, this.on_chunk_start = se, this.on_chunk_end = oe, this.on_finalize = z, this.time_precision = V, this.waiting_for_timestamp = !1;
        }
        /**
         * @param {bigint[][]} value 
         */
        put(M) {
          var D, q;
          if (M.length > 1)
            throw Error("WhisperTextStreamer only supports batch size of 1");
          const b = M[0];
          if (b.length === 1) {
            const se = Number(b[0]) - this.timestamp_begin;
            if (se >= 0) {
              const oe = se * this.time_precision;
              this.waiting_for_timestamp ? (D = this.on_chunk_end) == null || D.call(this, oe) : (q = this.on_chunk_start) == null || q.call(this, oe), this.waiting_for_timestamp = !this.waiting_for_timestamp, M = [[]];
            }
          }
          return super.put(M);
        }
        end() {
          var M;
          super.end(), (M = this.on_finalize) == null || M.call(this);
        }
      }
    }
  ),
  /***/
  "./src/models.js": (
    /*!***********************!*\
      !*** ./src/models.js ***!
      \***********************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ASTForAudioClassification: () => (
          /* binding */
          ss
        ),
        /* harmony export */
        ASTModel: () => (
          /* binding */
          va
        ),
        /* harmony export */
        ASTPreTrainedModel: () => (
          /* binding */
          Bi
        ),
        /* harmony export */
        AlbertForMaskedLM: () => (
          /* binding */
          Mn
        ),
        /* harmony export */
        AlbertForQuestionAnswering: () => (
          /* binding */
          yn
        ),
        /* harmony export */
        AlbertForSequenceClassification: () => (
          /* binding */
          Fn
        ),
        /* harmony export */
        AlbertModel: () => (
          /* binding */
          wn
        ),
        /* harmony export */
        AlbertPreTrainedModel: () => (
          /* binding */
          Yr
        ),
        /* harmony export */
        AutoModel: () => (
          /* binding */
          Wd
        ),
        /* harmony export */
        AutoModelForAudioClassification: () => (
          /* binding */
          oc
        ),
        /* harmony export */
        AutoModelForAudioFrameClassification: () => (
          /* binding */
          oa
        ),
        /* harmony export */
        AutoModelForCTC: () => (
          /* binding */
          ic
        ),
        /* harmony export */
        AutoModelForCausalLM: () => (
          /* binding */
          Xd
        ),
        /* harmony export */
        AutoModelForDepthEstimation: () => (
          /* binding */
          cc
        ),
        /* harmony export */
        AutoModelForDocumentQuestionAnswering: () => (
          /* binding */
          lc
        ),
        /* harmony export */
        AutoModelForImageClassification: () => (
          /* binding */
          Jd
        ),
        /* harmony export */
        AutoModelForImageFeatureExtraction: () => (
          /* binding */
          mc
        ),
        /* harmony export */
        AutoModelForImageMatting: () => (
          /* binding */
          uc
        ),
        /* harmony export */
        AutoModelForImageSegmentation: () => (
          /* binding */
          Zd
        ),
        /* harmony export */
        AutoModelForImageToImage: () => (
          /* binding */
          dc
        ),
        /* harmony export */
        AutoModelForMaskGeneration: () => (
          /* binding */
          nc
        ),
        /* harmony export */
        AutoModelForMaskedLM: () => (
          /* binding */
          Qd
        ),
        /* harmony export */
        AutoModelForNormalEstimation: () => (
          /* binding */
          pc
        ),
        /* harmony export */
        AutoModelForObjectDetection: () => (
          /* binding */
          rc
        ),
        /* harmony export */
        AutoModelForPoseEstimation: () => (
          /* binding */
          hc
        ),
        /* harmony export */
        AutoModelForQuestionAnswering: () => (
          /* binding */
          cp
        ),
        /* harmony export */
        AutoModelForSemanticSegmentation: () => (
          /* binding */
          ec
        ),
        /* harmony export */
        AutoModelForSeq2SeqLM: () => (
          /* binding */
          Kd
        ),
        /* harmony export */
        AutoModelForSequenceClassification: () => (
          /* binding */
          Vd
        ),
        /* harmony export */
        AutoModelForSpeechSeq2Seq: () => (
          /* binding */
          dp
        ),
        /* harmony export */
        AutoModelForTextToSpectrogram: () => (
          /* binding */
          Hd
        ),
        /* harmony export */
        AutoModelForTextToWaveform: () => (
          /* binding */
          qd
        ),
        /* harmony export */
        AutoModelForTokenClassification: () => (
          /* binding */
          Gd
        ),
        /* harmony export */
        AutoModelForUniversalSegmentation: () => (
          /* binding */
          tc
        ),
        /* harmony export */
        AutoModelForVision2Seq: () => (
          /* binding */
          Yd
        ),
        /* harmony export */
        AutoModelForXVector: () => (
          /* binding */
          ac
        ),
        /* harmony export */
        AutoModelForZeroShotObjectDetection: () => (
          /* binding */
          sc
        ),
        /* harmony export */
        BartForConditionalGeneration: () => (
          /* binding */
          Pt
        ),
        /* harmony export */
        BartForSequenceClassification: () => (
          /* binding */
          hr
        ),
        /* harmony export */
        BartModel: () => (
          /* binding */
          ot
        ),
        /* harmony export */
        BartPretrainedModel: () => (
          /* binding */
          ht
        ),
        /* harmony export */
        BaseModelOutput: () => (
          /* binding */
          De
        ),
        /* harmony export */
        BeitForImageClassification: () => (
          /* binding */
          sn
        ),
        /* harmony export */
        BeitModel: () => (
          /* binding */
          Lc
        ),
        /* harmony export */
        BeitPreTrainedModel: () => (
          /* binding */
          To
        ),
        /* harmony export */
        BertForMaskedLM: () => (
          /* binding */
          Re
        ),
        /* harmony export */
        BertForQuestionAnswering: () => (
          /* binding */
          Ne
        ),
        /* harmony export */
        BertForSequenceClassification: () => (
          /* binding */
          je
        ),
        /* harmony export */
        BertForTokenClassification: () => (
          /* binding */
          Ve
        ),
        /* harmony export */
        BertModel: () => (
          /* binding */
          ve
        ),
        /* harmony export */
        BertPreTrainedModel: () => (
          /* binding */
          ce
        ),
        /* harmony export */
        BlenderbotForConditionalGeneration: () => (
          /* binding */
          Nr
        ),
        /* harmony export */
        BlenderbotModel: () => (
          /* binding */
          Bt
        ),
        /* harmony export */
        BlenderbotPreTrainedModel: () => (
          /* binding */
          Zr
        ),
        /* harmony export */
        BlenderbotSmallForConditionalGeneration: () => (
          /* binding */
          mr
        ),
        /* harmony export */
        BlenderbotSmallModel: () => (
          /* binding */
          er
        ),
        /* harmony export */
        BlenderbotSmallPreTrainedModel: () => (
          /* binding */
          ps
        ),
        /* harmony export */
        BloomForCausalLM: () => (
          /* binding */
          Tl
        ),
        /* harmony export */
        BloomModel: () => (
          /* binding */
          xl
        ),
        /* harmony export */
        BloomPreTrainedModel: () => (
          /* binding */
          po
        ),
        /* harmony export */
        CLIPModel: () => (
          /* binding */
          Fa
        ),
        /* harmony export */
        CLIPPreTrainedModel: () => (
          /* binding */
          rn
        ),
        /* harmony export */
        CLIPSegForImageSegmentation: () => (
          /* binding */
          ja
        ),
        /* harmony export */
        CLIPSegModel: () => (
          /* binding */
          Na
        ),
        /* harmony export */
        CLIPSegPreTrainedModel: () => (
          /* binding */
          Gi
        ),
        /* harmony export */
        CLIPTextModel: () => (
          /* binding */
          Ac
        ),
        /* harmony export */
        CLIPTextModelWithProjection: () => (
          /* binding */
          Oa
        ),
        /* harmony export */
        CLIPVisionModel: () => (
          /* binding */
          Ic
        ),
        /* harmony export */
        CLIPVisionModelWithProjection: () => (
          /* binding */
          Fc
        ),
        /* harmony export */
        CamembertForMaskedLM: () => (
          /* binding */
          Qs
        ),
        /* harmony export */
        CamembertForQuestionAnswering: () => (
          /* binding */
          Ss
        ),
        /* harmony export */
        CamembertForSequenceClassification: () => (
          /* binding */
          ks
        ),
        /* harmony export */
        CamembertForTokenClassification: () => (
          /* binding */
          Bs
        ),
        /* harmony export */
        CamembertModel: () => (
          /* binding */
          Nt
        ),
        /* harmony export */
        CamembertPreTrainedModel: () => (
          /* binding */
          Ms
        ),
        /* harmony export */
        CausalLMOutput: () => (
          /* binding */
          an
        ),
        /* harmony export */
        CausalLMOutputWithPast: () => (
          /* binding */
          pp
        ),
        /* harmony export */
        ChineseCLIPModel: () => (
          /* binding */
          La
        ),
        /* harmony export */
        ChineseCLIPPreTrainedModel: () => (
          /* binding */
          ms
        ),
        /* harmony export */
        ClapAudioModelWithProjection: () => (
          /* binding */
          ad
        ),
        /* harmony export */
        ClapModel: () => (
          /* binding */
          id
        ),
        /* harmony export */
        ClapPreTrainedModel: () => (
          /* binding */
          bi
        ),
        /* harmony export */
        ClapTextModelWithProjection: () => (
          /* binding */
          od
        ),
        /* harmony export */
        CodeGenForCausalLM: () => (
          /* binding */
          ei
        ),
        /* harmony export */
        CodeGenModel: () => (
          /* binding */
          Ya
        ),
        /* harmony export */
        CodeGenPreTrainedModel: () => (
          /* binding */
          Qa
        ),
        /* harmony export */
        CohereForCausalLM: () => (
          /* binding */
          dl
        ),
        /* harmony export */
        CohereModel: () => (
          /* binding */
          ul
        ),
        /* harmony export */
        CoherePreTrainedModel: () => (
          /* binding */
          no
        ),
        /* harmony export */
        ConvBertForMaskedLM: () => (
          /* binding */
          Ot
        ),
        /* harmony export */
        ConvBertForQuestionAnswering: () => (
          /* binding */
          gr
        ),
        /* harmony export */
        ConvBertForSequenceClassification: () => (
          /* binding */
          At
        ),
        /* harmony export */
        ConvBertForTokenClassification: () => (
          /* binding */
          nr
        ),
        /* harmony export */
        ConvBertModel: () => (
          /* binding */
          St
        ),
        /* harmony export */
        ConvBertPreTrainedModel: () => (
          /* binding */
          It
        ),
        /* harmony export */
        ConvNextForImageClassification: () => (
          /* binding */
          gu
        ),
        /* harmony export */
        ConvNextModel: () => (
          /* binding */
          fu
        ),
        /* harmony export */
        ConvNextPreTrainedModel: () => (
          /* binding */
          Oo
        ),
        /* harmony export */
        ConvNextV2ForImageClassification: () => (
          /* binding */
          yu
        ),
        /* harmony export */
        ConvNextV2Model: () => (
          /* binding */
          wu
        ),
        /* harmony export */
        ConvNextV2PreTrainedModel: () => (
          /* binding */
          mi
        ),
        /* harmony export */
        DPTForDepthEstimation: () => (
          /* binding */
          du
        ),
        /* harmony export */
        DPTModel: () => (
          /* binding */
          uu
        ),
        /* harmony export */
        DPTPreTrainedModel: () => (
          /* binding */
          $o
        ),
        /* harmony export */
        DebertaForMaskedLM: () => (
          /* binding */
          cs
        ),
        /* harmony export */
        DebertaForQuestionAnswering: () => (
          /* binding */
          as
        ),
        /* harmony export */
        DebertaForSequenceClassification: () => (
          /* binding */
          As
        ),
        /* harmony export */
        DebertaForTokenClassification: () => (
          /* binding */
          Ys
        ),
        /* harmony export */
        DebertaModel: () => (
          /* binding */
          $s
        ),
        /* harmony export */
        DebertaPreTrainedModel: () => (
          /* binding */
          os
        ),
        /* harmony export */
        DebertaV2ForMaskedLM: () => (
          /* binding */
          Ft
        ),
        /* harmony export */
        DebertaV2ForQuestionAnswering: () => (
          /* binding */
          tr
        ),
        /* harmony export */
        DebertaV2ForSequenceClassification: () => (
          /* binding */
          lr
        ),
        /* harmony export */
        DebertaV2ForTokenClassification: () => (
          /* binding */
          bs
        ),
        /* harmony export */
        DebertaV2Model: () => (
          /* binding */
          _t
        ),
        /* harmony export */
        DebertaV2PreTrainedModel: () => (
          /* binding */
          nt
        ),
        /* harmony export */
        DecisionTransformerModel: () => (
          /* binding */
          wd
        ),
        /* harmony export */
        DecisionTransformerPreTrainedModel: () => (
          /* binding */
          gd
        ),
        /* harmony export */
        DeiTForImageClassification: () => (
          /* binding */
          ru
        ),
        /* harmony export */
        DeiTModel: () => (
          /* binding */
          Co
        ),
        /* harmony export */
        DeiTPreTrainedModel: () => (
          /* binding */
          oi
        ),
        /* harmony export */
        DepthAnythingForDepthEstimation: () => (
          /* binding */
          cu
        ),
        /* harmony export */
        DepthAnythingPreTrainedModel: () => (
          /* binding */
          Bc
        ),
        /* harmony export */
        DepthProForDepthEstimation: () => (
          /* binding */
          pi
        ),
        /* harmony export */
        DepthProPreTrainedModel: () => (
          /* binding */
          hu
        ),
        /* harmony export */
        DetrForObjectDetection: () => (
          /* binding */
          Eo
        ),
        /* harmony export */
        DetrForSegmentation: () => (
          /* binding */
          Gs
        ),
        /* harmony export */
        DetrModel: () => (
          /* binding */
          nn
        ),
        /* harmony export */
        DetrObjectDetectionOutput: () => (
          /* binding */
          Wr
        ),
        /* harmony export */
        DetrPreTrainedModel: () => (
          /* binding */
          Is
        ),
        /* harmony export */
        DetrSegmentationOutput: () => (
          /* binding */
          Jl
        ),
        /* harmony export */
        Dinov2ForImageClassification: () => (
          /* binding */
          bu
        ),
        /* harmony export */
        Dinov2Model: () => (
          /* binding */
          Mu
        ),
        /* harmony export */
        Dinov2PreTrainedModel: () => (
          /* binding */
          _i
        ),
        /* harmony export */
        DistilBertForMaskedLM: () => (
          /* binding */
          Pn
        ),
        /* harmony export */
        DistilBertForQuestionAnswering: () => (
          /* binding */
          vs
        ),
        /* harmony export */
        DistilBertForSequenceClassification: () => (
          /* binding */
          Js
        ),
        /* harmony export */
        DistilBertForTokenClassification: () => (
          /* binding */
          Ns
        ),
        /* harmony export */
        DistilBertModel: () => (
          /* binding */
          Rs
        ),
        /* harmony export */
        DistilBertPreTrainedModel: () => (
          /* binding */
          ts
        ),
        /* harmony export */
        DonutSwinModel: () => (
          /* binding */
          jc
        ),
        /* harmony export */
        DonutSwinPreTrainedModel: () => (
          /* binding */
          _u
        ),
        /* harmony export */
        EfficientNetForImageClassification: () => (
          /* binding */
          dd
        ),
        /* harmony export */
        EfficientNetModel: () => (
          /* binding */
          Rn
        ),
        /* harmony export */
        EfficientNetPreTrainedModel: () => (
          /* binding */
          jo
        ),
        /* harmony export */
        ElectraForMaskedLM: () => (
          /* binding */
          Qr
        ),
        /* harmony export */
        ElectraForQuestionAnswering: () => (
          /* binding */
          zs
        ),
        /* harmony export */
        ElectraForSequenceClassification: () => (
          /* binding */
          is
        ),
        /* harmony export */
        ElectraForTokenClassification: () => (
          /* binding */
          Xs
        ),
        /* harmony export */
        ElectraModel: () => (
          /* binding */
          Ar
        ),
        /* harmony export */
        ElectraPreTrainedModel: () => (
          /* binding */
          kr
        ),
        /* harmony export */
        EsmForMaskedLM: () => (
          /* binding */
          Xn
        ),
        /* harmony export */
        EsmForSequenceClassification: () => (
          /* binding */
          Us
        ),
        /* harmony export */
        EsmForTokenClassification: () => (
          /* binding */
          xs
        ),
        /* harmony export */
        EsmModel: () => (
          /* binding */
          Cn
        ),
        /* harmony export */
        EsmPreTrainedModel: () => (
          /* binding */
          js
        ),
        /* harmony export */
        ExaoneForCausalLM: () => (
          /* binding */
          tl
        ),
        /* harmony export */
        ExaoneModel: () => (
          /* binding */
          el
        ),
        /* harmony export */
        ExaonePreTrainedModel: () => (
          /* binding */
          vn
        ),
        /* harmony export */
        FalconForCausalLM: () => (
          /* binding */
          nd
        ),
        /* harmony export */
        FalconModel: () => (
          /* binding */
          sd
        ),
        /* harmony export */
        FalconPreTrainedModel: () => (
          /* binding */
          Ks
        ),
        /* harmony export */
        FastViTForImageClassification: () => (
          /* binding */
          Nl
        ),
        /* harmony export */
        FastViTModel: () => (
          /* binding */
          Rl
        ),
        /* harmony export */
        FastViTPreTrainedModel: () => (
          /* binding */
          ds
        ),
        /* harmony export */
        Florence2ForConditionalGeneration: () => (
          /* binding */
          Sa
        ),
        /* harmony export */
        Florence2PreTrainedModel: () => (
          /* binding */
          ka
        ),
        /* harmony export */
        GLPNForDepthEstimation: () => (
          /* binding */
          Nc
        ),
        /* harmony export */
        GLPNModel: () => (
          /* binding */
          Fo
        ),
        /* harmony export */
        GLPNPreTrainedModel: () => (
          /* binding */
          Io
        ),
        /* harmony export */
        GPT2LMHeadModel: () => (
          /* binding */
          _s
        ),
        /* harmony export */
        GPT2Model: () => (
          /* binding */
          Ua
        ),
        /* harmony export */
        GPT2PreTrainedModel: () => (
          /* binding */
          Ki
        ),
        /* harmony export */
        GPTBigCodeForCausalLM: () => (
          /* binding */
          eo
        ),
        /* harmony export */
        GPTBigCodeModel: () => (
          /* binding */
          Xa
        ),
        /* harmony export */
        GPTBigCodePreTrainedModel: () => (
          /* binding */
          Zi
        ),
        /* harmony export */
        GPTJForCausalLM: () => (
          /* binding */
          Ji
        ),
        /* harmony export */
        GPTJModel: () => (
          /* binding */
          Yi
        ),
        /* harmony export */
        GPTJPreTrainedModel: () => (
          /* binding */
          Qi
        ),
        /* harmony export */
        GPTNeoForCausalLM: () => (
          /* binding */
          Ka
        ),
        /* harmony export */
        GPTNeoModel: () => (
          /* binding */
          Ga
        ),
        /* harmony export */
        GPTNeoPreTrainedModel: () => (
          /* binding */
          qi
        ),
        /* harmony export */
        GPTNeoXForCausalLM: () => (
          /* binding */
          qa
        ),
        /* harmony export */
        GPTNeoXModel: () => (
          /* binding */
          Ha
        ),
        /* harmony export */
        GPTNeoXPreTrainedModel: () => (
          /* binding */
          Xi
        ),
        /* harmony export */
        Gemma2ForCausalLM: () => (
          /* binding */
          ml
        ),
        /* harmony export */
        Gemma2Model: () => (
          /* binding */
          hl
        ),
        /* harmony export */
        Gemma2PreTrainedModel: () => (
          /* binding */
          oo
        ),
        /* harmony export */
        GemmaForCausalLM: () => (
          /* binding */
          pl
        ),
        /* harmony export */
        GemmaModel: () => (
          /* binding */
          cl
        ),
        /* harmony export */
        GemmaPreTrainedModel: () => (
          /* binding */
          io
        ),
        /* harmony export */
        GraniteForCausalLM: () => (
          /* binding */
          ur
        ),
        /* harmony export */
        GraniteModel: () => (
          /* binding */
          ll
        ),
        /* harmony export */
        GranitePreTrainedModel: () => (
          /* binding */
          al
        ),
        /* harmony export */
        GroupViTModel: () => (
          /* binding */
          Bl
        ),
        /* harmony export */
        GroupViTPreTrainedModel: () => (
          /* binding */
          zl
        ),
        /* harmony export */
        HieraForImageClassification: () => (
          /* binding */
          li
        ),
        /* harmony export */
        HieraModel: () => (
          /* binding */
          su
        ),
        /* harmony export */
        HieraPreTrainedModel: () => (
          /* binding */
          ai
        ),
        /* harmony export */
        HubertForCTC: () => (
          /* binding */
          Ku
        ),
        /* harmony export */
        HubertForSequenceClassification: () => (
          /* binding */
          Hu
        ),
        /* harmony export */
        HubertModel: () => (
          /* binding */
          Gu
        ),
        /* harmony export */
        HubertPreTrainedModel: () => (
          /* binding */
          Dp
        ),
        /* harmony export */
        IJepaForImageClassification: () => (
          /* binding */
          $l
        ),
        /* harmony export */
        IJepaModel: () => (
          /* binding */
          go
        ),
        /* harmony export */
        IJepaPreTrainedModel: () => (
          /* binding */
          si
        ),
        /* harmony export */
        Idefics3ForConditionalGeneration: () => (
          /* binding */
          Ui
        ),
        /* harmony export */
        Idefics3PreTrainedModel: () => (
          /* binding */
          Aa
        ),
        /* harmony export */
        ImageMattingOutput: () => (
          /* binding */
          hp
        ),
        /* harmony export */
        JAISLMHeadModel: () => (
          /* binding */
          Va
        ),
        /* harmony export */
        JAISModel: () => (
          /* binding */
          Wa
        ),
        /* harmony export */
        JAISPreTrainedModel: () => (
          /* binding */
          Hi
        ),
        /* harmony export */
        JinaCLIPModel: () => (
          /* binding */
          za
        ),
        /* harmony export */
        JinaCLIPPreTrainedModel: () => (
          /* binding */
          Zn
        ),
        /* harmony export */
        JinaCLIPTextModel: () => (
          /* binding */
          Ba
        ),
        /* harmony export */
        JinaCLIPVisionModel: () => (
          /* binding */
          Ra
        ),
        /* harmony export */
        LlamaForCausalLM: () => (
          /* binding */
          Za
        ),
        /* harmony export */
        LlamaModel: () => (
          /* binding */
          Ja
        ),
        /* harmony export */
        LlamaPreTrainedModel: () => (
          /* binding */
          to
        ),
        /* harmony export */
        LlavaForConditionalGeneration: () => (
          /* binding */
          Yn
        ),
        /* harmony export */
        LlavaOnevisionForConditionalGeneration: () => (
          /* binding */
          Pa
        ),
        /* harmony export */
        LlavaPreTrainedModel: () => (
          /* binding */
          Ea
        ),
        /* harmony export */
        LongT5ForConditionalGeneration: () => (
          /* binding */
          Pe
        ),
        /* harmony export */
        LongT5Model: () => (
          /* binding */
          Me
        ),
        /* harmony export */
        LongT5PreTrainedModel: () => (
          /* binding */
          ae
        ),
        /* harmony export */
        M2M100ForConditionalGeneration: () => (
          /* binding */
          Ln
        ),
        /* harmony export */
        M2M100Model: () => (
          /* binding */
          fi
        ),
        /* harmony export */
        M2M100PreTrainedModel: () => (
          /* binding */
          $u
        ),
        /* harmony export */
        MBartForCausalLM: () => (
          /* binding */
          Jr
        ),
        /* harmony export */
        MBartForConditionalGeneration: () => (
          /* binding */
          wr
        ),
        /* harmony export */
        MBartForSequenceClassification: () => (
          /* binding */
          Rr
        ),
        /* harmony export */
        MBartModel: () => (
          /* binding */
          $e
        ),
        /* harmony export */
        MBartPreTrainedModel: () => (
          /* binding */
          rr
        ),
        /* harmony export */
        MPNetForMaskedLM: () => (
          /* binding */
          fn
        ),
        /* harmony export */
        MPNetForQuestionAnswering: () => (
          /* binding */
          $n
        ),
        /* harmony export */
        MPNetForSequenceClassification: () => (
          /* binding */
          kn
        ),
        /* harmony export */
        MPNetForTokenClassification: () => (
          /* binding */
          Sn
        ),
        /* harmony export */
        MPNetModel: () => (
          /* binding */
          zt
        ),
        /* harmony export */
        MPNetPreTrainedModel: () => (
          /* binding */
          Ts
        ),
        /* harmony export */
        MT5ForConditionalGeneration: () => (
          /* binding */
          yt
        ),
        /* harmony export */
        MT5Model: () => (
          /* binding */
          ct
        ),
        /* harmony export */
        MT5PreTrainedModel: () => (
          /* binding */
          He
        ),
        /* harmony export */
        MarianMTModel: () => (
          /* binding */
          Su
        ),
        /* harmony export */
        MarianModel: () => (
          /* binding */
          ku
        ),
        /* harmony export */
        MarianPreTrainedModel: () => (
          /* binding */
          Do
        ),
        /* harmony export */
        MaskFormerForInstanceSegmentation: () => (
          /* binding */
          mu
        ),
        /* harmony export */
        MaskFormerModel: () => (
          /* binding */
          Ao
        ),
        /* harmony export */
        MaskFormerPreTrainedModel: () => (
          /* binding */
          hi
        ),
        /* harmony export */
        MaskedLMOutput: () => (
          /* binding */
          Vr
        ),
        /* harmony export */
        MgpstrForSceneTextRecognition: () => (
          /* binding */
          tp
        ),
        /* harmony export */
        MgpstrModelOutput: () => (
          /* binding */
          Md
        ),
        /* harmony export */
        MgpstrPreTrainedModel: () => (
          /* binding */
          bd
        ),
        /* harmony export */
        MistralForCausalLM: () => (
          /* binding */
          Xc
        ),
        /* harmony export */
        MistralModel: () => (
          /* binding */
          rd
        ),
        /* harmony export */
        MistralPreTrainedModel: () => (
          /* binding */
          zo
        ),
        /* harmony export */
        MobileBertForMaskedLM: () => (
          /* binding */
          Zs
        ),
        /* harmony export */
        MobileBertForQuestionAnswering: () => (
          /* binding */
          en
        ),
        /* harmony export */
        MobileBertForSequenceClassification: () => (
          /* binding */
          _n
        ),
        /* harmony export */
        MobileBertModel: () => (
          /* binding */
          mn
        ),
        /* harmony export */
        MobileBertPreTrainedModel: () => (
          /* binding */
          ls
        ),
        /* harmony export */
        MobileLLMForCausalLM: () => (
          /* binding */
          sl
        ),
        /* harmony export */
        MobileLLMModel: () => (
          /* binding */
          rl
        ),
        /* harmony export */
        MobileLLMPreTrainedModel: () => (
          /* binding */
          ti
        ),
        /* harmony export */
        MobileNetV1ForImageClassification: () => (
          /* binding */
          cd
        ),
        /* harmony export */
        MobileNetV1Model: () => (
          /* binding */
          Go
        ),
        /* harmony export */
        MobileNetV1PreTrainedModel: () => (
          /* binding */
          Vo
        ),
        /* harmony export */
        MobileNetV2ForImageClassification: () => (
          /* binding */
          hd
        ),
        /* harmony export */
        MobileNetV2Model: () => (
          /* binding */
          pd
        ),
        /* harmony export */
        MobileNetV2PreTrainedModel: () => (
          /* binding */
          Ko
        ),
        /* harmony export */
        MobileNetV3ForImageClassification: () => (
          /* binding */
          md
        ),
        /* harmony export */
        MobileNetV3Model: () => (
          /* binding */
          qo
        ),
        /* harmony export */
        MobileNetV3PreTrainedModel: () => (
          /* binding */
          Ho
        ),
        /* harmony export */
        MobileNetV4ForImageClassification: () => (
          /* binding */
          fd
        ),
        /* harmony export */
        MobileNetV4Model: () => (
          /* binding */
          Xo
        ),
        /* harmony export */
        MobileNetV4PreTrainedModel: () => (
          /* binding */
          _d
        ),
        /* harmony export */
        MobileViTForImageClassification: () => (
          /* binding */
          Vl
        ),
        /* harmony export */
        MobileViTModel: () => (
          /* binding */
          Wl
        ),
        /* harmony export */
        MobileViTPreTrainedModel: () => (
          /* binding */
          bo
        ),
        /* harmony export */
        MobileViTV2ForImageClassification: () => (
          /* binding */
          Kl
        ),
        /* harmony export */
        MobileViTV2Model: () => (
          /* binding */
          Gl
        ),
        /* harmony export */
        MobileViTV2PreTrainedModel: () => (
          /* binding */
          vo
        ),
        /* harmony export */
        ModelOutput: () => (
          /* binding */
          Je
        ),
        /* harmony export */
        ModernBertForMaskedLM: () => (
          /* binding */
          ft
        ),
        /* harmony export */
        ModernBertForSequenceClassification: () => (
          /* binding */
          dt
        ),
        /* harmony export */
        ModernBertForTokenClassification: () => (
          /* binding */
          gt
        ),
        /* harmony export */
        ModernBertModel: () => (
          /* binding */
          at
        ),
        /* harmony export */
        ModernBertPreTrainedModel: () => (
          /* binding */
          Ze
        ),
        /* harmony export */
        Moondream1ForConditionalGeneration: () => (
          /* binding */
          Ca
        ),
        /* harmony export */
        MoonshineForConditionalGeneration: () => (
          /* binding */
          Ta
        ),
        /* harmony export */
        MoonshineModel: () => (
          /* binding */
          $c
        ),
        /* harmony export */
        MoonshinePreTrainedModel: () => (
          /* binding */
          Ni
        ),
        /* harmony export */
        MptForCausalLM: () => (
          /* binding */
          Pl
        ),
        /* harmony export */
        MptModel: () => (
          /* binding */
          El
        ),
        /* harmony export */
        MptPreTrainedModel: () => (
          /* binding */
          ho
        ),
        /* harmony export */
        MultiModalityCausalLM: () => (
          /* binding */
          yd
        ),
        /* harmony export */
        MultiModalityPreTrainedModel: () => (
          /* binding */
          ep
        ),
        /* harmony export */
        MusicgenForCausalLM: () => (
          /* binding */
          Zc
        ),
        /* harmony export */
        MusicgenForConditionalGeneration: () => (
          /* binding */
          Wo
        ),
        /* harmony export */
        MusicgenModel: () => (
          /* binding */
          Jc
        ),
        /* harmony export */
        MusicgenPreTrainedModel: () => (
          /* binding */
          Uo
        ),
        /* harmony export */
        NomicBertModel: () => (
          /* binding */
          ne
        ),
        /* harmony export */
        NomicBertPreTrainedModel: () => (
          /* binding */
          F
        ),
        /* harmony export */
        OPTForCausalLM: () => (
          /* binding */
          kl
        ),
        /* harmony export */
        OPTModel: () => (
          /* binding */
          Cl
        ),
        /* harmony export */
        OPTPreTrainedModel: () => (
          /* binding */
          mo
        ),
        /* harmony export */
        Olmo2ForCausalLM: () => (
          /* binding */
          ol
        ),
        /* harmony export */
        Olmo2Model: () => (
          /* binding */
          il
        ),
        /* harmony export */
        Olmo2PreTrainedModel: () => (
          /* binding */
          so
        ),
        /* harmony export */
        OlmoForCausalLM: () => (
          /* binding */
          Oc
        ),
        /* harmony export */
        OlmoModel: () => (
          /* binding */
          nl
        ),
        /* harmony export */
        OlmoPreTrainedModel: () => (
          /* binding */
          ro
        ),
        /* harmony export */
        OpenELMForCausalLM: () => (
          /* binding */
          fl
        ),
        /* harmony export */
        OpenELMModel: () => (
          /* binding */
          _l
        ),
        /* harmony export */
        OpenELMPreTrainedModel: () => (
          /* binding */
          ao
        ),
        /* harmony export */
        OwlViTForObjectDetection: () => (
          /* binding */
          Xl
        ),
        /* harmony export */
        OwlViTModel: () => (
          /* binding */
          ql
        ),
        /* harmony export */
        OwlViTPreTrainedModel: () => (
          /* binding */
          Hl
        ),
        /* harmony export */
        Owlv2ForObjectDetection: () => (
          /* binding */
          Yl
        ),
        /* harmony export */
        Owlv2Model: () => (
          /* binding */
          Ql
        ),
        /* harmony export */
        Owlv2PreTrainedModel: () => (
          /* binding */
          xo
        ),
        /* harmony export */
        PaliGemmaForConditionalGeneration: () => (
          /* binding */
          $a
        ),
        /* harmony export */
        PaliGemmaPreTrainedModel: () => (
          /* binding */
          hs
        ),
        /* harmony export */
        PatchTSMixerForPrediction: () => (
          /* binding */
          Td
        ),
        /* harmony export */
        PatchTSMixerModel: () => (
          /* binding */
          xd
        ),
        /* harmony export */
        PatchTSMixerPreTrainedModel: () => (
          /* binding */
          Yo
        ),
        /* harmony export */
        PatchTSTForPrediction: () => (
          /* binding */
          rp
        ),
        /* harmony export */
        PatchTSTModel: () => (
          /* binding */
          vd
        ),
        /* harmony export */
        PatchTSTPreTrainedModel: () => (
          /* binding */
          Qo
        ),
        /* harmony export */
        Phi3ForCausalLM: () => (
          /* binding */
          vl
        ),
        /* harmony export */
        Phi3Model: () => (
          /* binding */
          bl
        ),
        /* harmony export */
        Phi3PreTrainedModel: () => (
          /* binding */
          co
        ),
        /* harmony export */
        Phi3VForCausalLM: () => (
          /* binding */
          Wi
        ),
        /* harmony export */
        Phi3VPreTrainedModel: () => (
          /* binding */
          Ia
        ),
        /* harmony export */
        PhiForCausalLM: () => (
          /* binding */
          Ml
        ),
        /* harmony export */
        PhiModel: () => (
          /* binding */
          yl
        ),
        /* harmony export */
        PhiPreTrainedModel: () => (
          /* binding */
          uo
        ),
        /* harmony export */
        PreTrainedModel: () => (
          /* binding */
          ie
        ),
        /* harmony export */
        PretrainedMixin: () => (
          /* binding */
          _r
        ),
        /* harmony export */
        PvtForImageClassification: () => (
          /* binding */
          Ol
        ),
        /* harmony export */
        PvtModel: () => (
          /* binding */
          Fl
        ),
        /* harmony export */
        PvtPreTrainedModel: () => (
          /* binding */
          wo
        ),
        /* harmony export */
        PyAnnoteForAudioFrameClassification: () => (
          /* binding */
          Lu
        ),
        /* harmony export */
        PyAnnoteModel: () => (
          /* binding */
          Du
        ),
        /* harmony export */
        PyAnnotePreTrainedModel: () => (
          /* binding */
          gi
        ),
        /* harmony export */
        QuestionAnsweringModelOutput: () => (
          /* binding */
          Xr
        ),
        /* harmony export */
        Qwen2ForCausalLM: () => (
          /* binding */
          On
        ),
        /* harmony export */
        Qwen2Model: () => (
          /* binding */
          gl
        ),
        /* harmony export */
        Qwen2PreTrainedModel: () => (
          /* binding */
          lo
        ),
        /* harmony export */
        Qwen2VLForConditionalGeneration: () => (
          /* binding */
          ri
        ),
        /* harmony export */
        Qwen2VLPreTrainedModel: () => (
          /* binding */
          wl
        ),
        /* harmony export */
        RTDetrForObjectDetection: () => (
          /* binding */
          zc
        ),
        /* harmony export */
        RTDetrModel: () => (
          /* binding */
          Zl
        ),
        /* harmony export */
        RTDetrObjectDetectionOutput: () => (
          /* binding */
          Dn
        ),
        /* harmony export */
        RTDetrPreTrainedModel: () => (
          /* binding */
          ni
        ),
        /* harmony export */
        ResNetForImageClassification: () => (
          /* binding */
          iu
        ),
        /* harmony export */
        ResNetModel: () => (
          /* binding */
          nu
        ),
        /* harmony export */
        ResNetPreTrainedModel: () => (
          /* binding */
          ui
        ),
        /* harmony export */
        RoFormerForMaskedLM: () => (
          /* binding */
          Oe
        ),
        /* harmony export */
        RoFormerForQuestionAnswering: () => (
          /* binding */
          pt
        ),
        /* harmony export */
        RoFormerForSequenceClassification: () => (
          /* binding */
          Qe
        ),
        /* harmony export */
        RoFormerForTokenClassification: () => (
          /* binding */
          st
        ),
        /* harmony export */
        RoFormerModel: () => (
          /* binding */
          pe
        ),
        /* harmony export */
        RoFormerPreTrainedModel: () => (
          /* binding */
          K
        ),
        /* harmony export */
        RobertaForMaskedLM: () => (
          /* binding */
          Es
        ),
        /* harmony export */
        RobertaForQuestionAnswering: () => (
          /* binding */
          Mt
        ),
        /* harmony export */
        RobertaForSequenceClassification: () => (
          /* binding */
          Or
        ),
        /* harmony export */
        RobertaForTokenClassification: () => (
          /* binding */
          qr
        ),
        /* harmony export */
        RobertaModel: () => (
          /* binding */
          yr
        ),
        /* harmony export */
        RobertaPreTrainedModel: () => (
          /* binding */
          vt
        ),
        /* harmony export */
        SamImageSegmentationOutput: () => (
          /* binding */
          Cu
        ),
        /* harmony export */
        SamModel: () => (
          /* binding */
          Pu
        ),
        /* harmony export */
        SamPreTrainedModel: () => (
          /* binding */
          Uc
        ),
        /* harmony export */
        SapiensForDepthEstimation: () => (
          /* binding */
          pu
        ),
        /* harmony export */
        SapiensForNormalEstimation: () => (
          /* binding */
          ci
        ),
        /* harmony export */
        SapiensForSemanticSegmentation: () => (
          /* binding */
          Rc
        ),
        /* harmony export */
        SapiensPreTrainedModel: () => (
          /* binding */
          di
        ),
        /* harmony export */
        SegformerForImageClassification: () => (
          /* binding */
          Yc
        ),
        /* harmony export */
        SegformerForSemanticSegmentation: () => (
          /* binding */
          es
        ),
        /* harmony export */
        SegformerModel: () => (
          /* binding */
          Qc
        ),
        /* harmony export */
        SegformerPreTrainedModel: () => (
          /* binding */
          vi
        ),
        /* harmony export */
        Seq2SeqLMOutput: () => (
          /* binding */
          zp
        ),
        /* harmony export */
        SequenceClassifierOutput: () => (
          /* binding */
          Xt
        ),
        /* harmony export */
        SiglipModel: () => (
          /* binding */
          Da
        ),
        /* harmony export */
        SiglipPreTrainedModel: () => (
          /* binding */
          us
        ),
        /* harmony export */
        SiglipTextModel: () => (
          /* binding */
          Jn
        ),
        /* harmony export */
        SiglipVisionModel: () => (
          /* binding */
          Vi
        ),
        /* harmony export */
        SpeechT5ForSpeechToText: () => (
          /* binding */
          Ju
        ),
        /* harmony export */
        SpeechT5ForTextToSpeech: () => (
          /* binding */
          Zu
        ),
        /* harmony export */
        SpeechT5HifiGan: () => (
          /* binding */
          qc
        ),
        /* harmony export */
        SpeechT5Model: () => (
          /* binding */
          Hc
        ),
        /* harmony export */
        SpeechT5PreTrainedModel: () => (
          /* binding */
          Mi
        ),
        /* harmony export */
        SqueezeBertForMaskedLM: () => (
          /* binding */
          gn
        ),
        /* harmony export */
        SqueezeBertForQuestionAnswering: () => (
          /* binding */
          or
        ),
        /* harmony export */
        SqueezeBertForSequenceClassification: () => (
          /* binding */
          In
        ),
        /* harmony export */
        SqueezeBertModel: () => (
          /* binding */
          An
        ),
        /* harmony export */
        SqueezeBertPreTrainedModel: () => (
          /* binding */
          Ws
        ),
        /* harmony export */
        StableLmForCausalLM: () => (
          /* binding */
          No
        ),
        /* harmony export */
        StableLmModel: () => (
          /* binding */
          ud
        ),
        /* harmony export */
        StableLmPreTrainedModel: () => (
          /* binding */
          Ro
        ),
        /* harmony export */
        Starcoder2ForCausalLM: () => (
          /* binding */
          on
        ),
        /* harmony export */
        Starcoder2Model: () => (
          /* binding */
          Os
        ),
        /* harmony export */
        Starcoder2PreTrainedModel: () => (
          /* binding */
          fs
        ),
        /* harmony export */
        Swin2SRForImageSuperResolution: () => (
          /* binding */
          So
        ),
        /* harmony export */
        Swin2SRModel: () => (
          /* binding */
          lu
        ),
        /* harmony export */
        Swin2SRPreTrainedModel: () => (
          /* binding */
          xn
        ),
        /* harmony export */
        SwinForImageClassification: () => (
          /* binding */
          ko
        ),
        /* harmony export */
        SwinModel: () => (
          /* binding */
          au
        ),
        /* harmony export */
        SwinPreTrainedModel: () => (
          /* binding */
          ou
        ),
        /* harmony export */
        T5ForConditionalGeneration: () => (
          /* binding */
          H
        ),
        /* harmony export */
        T5Model: () => (
          /* binding */
          P
        ),
        /* harmony export */
        T5PreTrainedModel: () => (
          /* binding */
          Te
        ),
        /* harmony export */
        TableTransformerForObjectDetection: () => (
          /* binding */
          eu
        ),
        /* harmony export */
        TableTransformerModel: () => (
          /* binding */
          Po
        ),
        /* harmony export */
        TableTransformerObjectDetectionOutput: () => (
          /* binding */
          tu
        ),
        /* harmony export */
        TableTransformerPreTrainedModel: () => (
          /* binding */
          ii
        ),
        /* harmony export */
        TokenClassifierOutput: () => (
          /* binding */
          jr
        ),
        /* harmony export */
        TrOCRForCausalLM: () => (
          /* binding */
          td
        ),
        /* harmony export */
        TrOCRPreTrainedModel: () => (
          /* binding */
          ed
        ),
        /* harmony export */
        UniSpeechForCTC: () => (
          /* binding */
          Bu
        ),
        /* harmony export */
        UniSpeechForSequenceClassification: () => (
          /* binding */
          Ru
        ),
        /* harmony export */
        UniSpeechModel: () => (
          /* binding */
          Lo
        ),
        /* harmony export */
        UniSpeechPreTrainedModel: () => (
          /* binding */
          wi
        ),
        /* harmony export */
        UniSpeechSatForAudioFrameClassification: () => (
          /* binding */
          Gc
        ),
        /* harmony export */
        UniSpeechSatForCTC: () => (
          /* binding */
          Nu
        ),
        /* harmony export */
        UniSpeechSatForSequenceClassification: () => (
          /* binding */
          ju
        ),
        /* harmony export */
        UniSpeechSatModel: () => (
          /* binding */
          Vc
        ),
        /* harmony export */
        UniSpeechSatPreTrainedModel: () => (
          /* binding */
          zn
        ),
        /* harmony export */
        ViTForImageClassification: () => (
          /* binding */
          Sl
        ),
        /* harmony export */
        ViTMAEModel: () => (
          /* binding */
          Dl
        ),
        /* harmony export */
        ViTMAEPreTrainedModel: () => (
          /* binding */
          yo
        ),
        /* harmony export */
        ViTMSNForImageClassification: () => (
          /* binding */
          Dc
        ),
        /* harmony export */
        ViTMSNModel: () => (
          /* binding */
          Ll
        ),
        /* harmony export */
        ViTMSNPreTrainedModel: () => (
          /* binding */
          Mo
        ),
        /* harmony export */
        ViTModel: () => (
          /* binding */
          fo
        ),
        /* harmony export */
        ViTPreTrainedModel: () => (
          /* binding */
          _o
        ),
        /* harmony export */
        VisionEncoderDecoderModel: () => (
          /* binding */
          ji
        ),
        /* harmony export */
        VitMatteForImageMatting: () => (
          /* binding */
          Ul
        ),
        /* harmony export */
        VitMattePreTrainedModel: () => (
          /* binding */
          jl
        ),
        /* harmony export */
        VitPoseForPoseEstimation: () => (
          /* binding */
          Il
        ),
        /* harmony export */
        VitPosePreTrainedModel: () => (
          /* binding */
          Al
        ),
        /* harmony export */
        VitsModel: () => (
          /* binding */
          Bo
        ),
        /* harmony export */
        VitsModelOutput: () => (
          /* binding */
          fc
        ),
        /* harmony export */
        VitsPreTrainedModel: () => (
          /* binding */
          ld
        ),
        /* harmony export */
        Wav2Vec2BertForCTC: () => (
          /* binding */
          Wu
        ),
        /* harmony export */
        Wav2Vec2BertForSequenceClassification: () => (
          /* binding */
          Vu
        ),
        /* harmony export */
        Wav2Vec2BertModel: () => (
          /* binding */
          Uu
        ),
        /* harmony export */
        Wav2Vec2BertPreTrainedModel: () => (
          /* binding */
          yi
        ),
        /* harmony export */
        Wav2Vec2ForAudioFrameClassification: () => (
          /* binding */
          Ou
        ),
        /* harmony export */
        Wav2Vec2ForCTC: () => (
          /* binding */
          Iu
        ),
        /* harmony export */
        Wav2Vec2ForSequenceClassification: () => (
          /* binding */
          Fu
        ),
        /* harmony export */
        Wav2Vec2Model: () => (
          /* binding */
          Au
        ),
        /* harmony export */
        Wav2Vec2PreTrainedModel: () => (
          /* binding */
          Fs
        ),
        /* harmony export */
        WavLMForAudioFrameClassification: () => (
          /* binding */
          Kc
        ),
        /* harmony export */
        WavLMForCTC: () => (
          /* binding */
          Xu
        ),
        /* harmony export */
        WavLMForSequenceClassification: () => (
          /* binding */
          Qu
        ),
        /* harmony export */
        WavLMForXVector: () => (
          /* binding */
          Yu
        ),
        /* harmony export */
        WavLMModel: () => (
          /* binding */
          qu
        ),
        /* harmony export */
        WavLMPreTrainedModel: () => (
          /* binding */
          Bn
        ),
        /* harmony export */
        WeSpeakerResNetModel: () => (
          /* binding */
          Wc
        ),
        /* harmony export */
        WeSpeakerResNetPreTrainedModel: () => (
          /* binding */
          zu
        ),
        /* harmony export */
        WhisperForConditionalGeneration: () => (
          /* binding */
          Vs
        ),
        /* harmony export */
        WhisperModel: () => (
          /* binding */
          xa
        ),
        /* harmony export */
        WhisperPreTrainedModel: () => (
          /* binding */
          Ri
        ),
        /* harmony export */
        XLMForQuestionAnswering: () => (
          /* binding */
          Qn
        ),
        /* harmony export */
        XLMForSequenceClassification: () => (
          /* binding */
          rs
        ),
        /* harmony export */
        XLMForTokenClassification: () => (
          /* binding */
          tn
        ),
        /* harmony export */
        XLMModel: () => (
          /* binding */
          ze
        ),
        /* harmony export */
        XLMPreTrainedModel: () => (
          /* binding */
          br
        ),
        /* harmony export */
        XLMRobertaForMaskedLM: () => (
          /* binding */
          ya
        ),
        /* harmony export */
        XLMRobertaForQuestionAnswering: () => (
          /* binding */
          ba
        ),
        /* harmony export */
        XLMRobertaForSequenceClassification: () => (
          /* binding */
          zi
        ),
        /* harmony export */
        XLMRobertaForTokenClassification: () => (
          /* binding */
          Ma
        ),
        /* harmony export */
        XLMRobertaModel: () => (
          /* binding */
          Qt
        ),
        /* harmony export */
        XLMRobertaPreTrainedModel: () => (
          /* binding */
          bn
        ),
        /* harmony export */
        XLMWithLMHeadModel: () => (
          /* binding */
          wt
        ),
        /* harmony export */
        XVectorOutput: () => (
          /* binding */
          _c
        ),
        /* harmony export */
        YolosForObjectDetection: () => (
          /* binding */
          Tu
        ),
        /* harmony export */
        YolosModel: () => (
          /* binding */
          xu
        ),
        /* harmony export */
        YolosObjectDetectionOutput: () => (
          /* binding */
          Eu
        ),
        /* harmony export */
        YolosPreTrainedModel: () => (
          /* binding */
          vu
        )
        /* harmony export */
      });
      var f = s(
        /*! ./configs.js */
        "./src/configs.js"
      ), L = s(
        /*! ./backends/onnx.js */
        "./src/backends/onnx.js"
      ), j = s(
        /*! ./utils/dtypes.js */
        "./src/utils/dtypes.js"
      ), J = s(
        /*! ./utils/generic.js */
        "./src/utils/generic.js"
      ), W = s(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), w = s(
        /*! ./utils/hub.js */
        "./src/utils/hub.js"
      ), x = s(
        /*! ./utils/constants.js */
        "./src/utils/constants.js"
      ), y = s(
        /*! ./generation/logits_process.js */
        "./src/generation/logits_process.js"
      ), M = s(
        /*! ./generation/configuration_utils.js */
        "./src/generation/configuration_utils.js"
      ), b = s(
        /*! ./utils/tensor.js */
        "./src/utils/tensor.js"
      ), D = s(
        /*! ./utils/image.js */
        "./src/utils/image.js"
      ), q = s(
        /*! ./utils/maths.js */
        "./src/utils/maths.js"
      ), se = s(
        /*! ./generation/stopping_criteria.js */
        "./src/generation/stopping_criteria.js"
      ), oe = s(
        /*! ./generation/logits_sampler.js */
        "./src/generation/logits_sampler.js"
      ), z = s(
        /*! ./env.js */
        "./src/env.js"
      ), V = s(
        /*! ./models/whisper/generation_whisper.js */
        "./src/models/whisper/generation_whisper.js"
      ), Y = s(
        /*! ./models/whisper/common_whisper.js */
        "./src/models/whisper/common_whisper.js"
      );
      const O = {
        EncoderOnly: 0,
        EncoderDecoder: 1,
        Seq2Seq: 2,
        Vision2Seq: 3,
        DecoderOnly: 4,
        MaskGeneration: 5,
        ImageTextToText: 6,
        Musicgen: 7,
        MultiModality: 8,
        Phi3V: 9
      }, $ = /* @__PURE__ */ new Map(), g = /* @__PURE__ */ new Map(), C = /* @__PURE__ */ new Map();
      async function v(_, T, U) {
        var xr;
        const be = ((xr = U.config) == null ? void 0 : xr["transformers.js_config"]) ?? {};
        let Fe = U.device ?? be.device;
        Fe && typeof Fe != "string" && (Fe.hasOwnProperty(T) ? Fe = Fe[T] : (console.warn(`device not specified for "${T}". Using the default device.`), Fe = null));
        const Se = (
          /** @type {import("./utils/devices.js").DeviceType} */
          Fe ?? (z.apis.IS_NODE_ENV ? "cpu" : "wasm")
        ), Ye = (0, L.deviceToExecutionProviders)(Se);
        let rt = U.dtype ?? be.dtype;
        if (typeof rt != "string" && (rt && rt.hasOwnProperty(T) ? rt = rt[T] : (rt = j.DEFAULT_DEVICE_DTYPE_MAPPING[Se] ?? j.DATA_TYPES.fp32, console.warn(`dtype not specified for "${T}". Using the default dtype (${rt}) for this device (${Se}).`))), rt === j.DATA_TYPES.auto) {
          let dr = be.dtype;
          typeof dr != "string" && (dr = dr[T]), dr && dr !== j.DATA_TYPES.auto && j.DATA_TYPES.hasOwnProperty(dr) ? rt = dr : rt = j.DEFAULT_DEVICE_DTYPE_MAPPING[Se] ?? j.DATA_TYPES.fp32;
        }
        const mt = (
          /** @type {import("./utils/dtypes.js").DataType} */
          rt
        );
        if (j.DEFAULT_DTYPE_SUFFIX_MAPPING.hasOwnProperty(mt)) {
          if (mt === j.DATA_TYPES.fp16 && Se === "webgpu" && !await (0, j.isWebGpuFp16Supported)())
            throw new Error(`The device (${Se}) does not support fp16.`);
        } else throw new Error(`Invalid dtype: ${mt}. Should be one of: ${Object.keys(j.DATA_TYPES).join(", ")}`);
        const xt = be.kv_cache_dtype ? typeof be.kv_cache_dtype == "string" ? be.kv_cache_dtype : be.kv_cache_dtype[mt] ?? "float32" : void 0;
        if (xt && !["float32", "float16"].includes(xt))
          throw new Error(`Invalid kv_cache_dtype: ${xt}. Should be one of: float32, float16`);
        const Lt = {
          dtype: mt,
          kv_cache_dtype: xt
        }, Ut = j.DEFAULT_DTYPE_SUFFIX_MAPPING[mt], Dt = `${U.subfolder ?? ""}/${T}${Ut}.onnx`, Wt = { ...U.session_options };
        Wt.executionProviders ?? (Wt.executionProviders = Ye);
        const Zt = be.free_dimension_overrides;
        Zt ? Wt.freeDimensionOverrides ?? (Wt.freeDimensionOverrides = Zt) : Se.startsWith("webnn") && !Wt.freeDimensionOverrides && console.warn(
          'WebNN does not currently support dynamic shapes and requires `free_dimension_overrides` to be set in config.json as a field within "transformers.js_config". When `free_dimension_overrides` is not set, you may experience significant performance degradation.'
        );
        const sr = (0, w.getModelFile)(_, Dt, !0, U), qt = U.use_external_data_format ?? be.use_external_data_format;
        let ir = [];
        if (qt && (qt === !0 || typeof qt == "object" && qt.hasOwnProperty(T) && qt[T] === !0)) {
          if (z.apis.IS_NODE_ENV)
            throw new Error("External data format is not yet supported in Node.js");
          const dr = `${T}${Ut}.onnx_data`, Pr = `${U.subfolder ?? ""}/${dr}`;
          ir.push(new Promise(async ($r, Gr) => {
            const Ur = await (0, w.getModelFile)(_, Pr, !0, U);
            $r({ path: dr, data: Ur });
          }));
        } else Wt.externalData !== void 0 && (ir = Wt.externalData.map(async (dr) => {
          if (typeof dr.data == "string") {
            const Pr = await (0, w.getModelFile)(_, dr.data, !0, U);
            return { ...dr, data: Pr };
          }
          return dr;
        }));
        if (ir.length > 0 && (Wt.externalData = await Promise.all(ir)), Se === "webgpu") {
          const dr = (0, f.getKeyValueShapes)(U.config, {
            prefix: "present"
          });
          if (Object.keys(dr).length > 0 && !(0, L.isONNXProxy)()) {
            const Pr = {};
            for (const $r in dr)
              Pr[$r] = "gpu-buffer";
            Wt.preferredOutputLocation = Pr;
          }
        }
        return { buffer: await sr, session_options: Wt, session_config: Lt };
      }
      async function ee(_, T, U) {
        return Object.fromEntries(await Promise.all(
          Object.keys(T).map(async (be) => {
            const { buffer: Fe, session_options: Se, session_config: Ye } = await v(_, T[be], U), rt = await (0, L.createInferenceSession)(Fe, Se, Ye);
            return [be, rt];
          })
        ));
      }
      async function X(_, T, U) {
        return Object.fromEntries(await Promise.all(
          Object.keys(T).map(async (be) => {
            const Fe = await (0, w.getModelJSON)(_, T[be], !1, U);
            return [be, Fe];
          })
        ));
      }
      function le(_, T) {
        const U = /* @__PURE__ */ Object.create(null), be = [];
        for (const Ye of _.inputNames) {
          const rt = T[Ye];
          if (!(rt instanceof b.Tensor)) {
            be.push(Ye);
            continue;
          }
          U[Ye] = (0, L.isONNXProxy)() ? rt.clone() : rt;
        }
        if (be.length > 0)
          throw new Error(
            `An error occurred during model execution: "Missing the following inputs: ${be.join(", ")}.`
          );
        const Fe = Object.keys(T).length, Se = _.inputNames.length;
        if (Fe > Se) {
          let Ye = Object.keys(T).filter((rt) => !_.inputNames.includes(rt));
          console.warn(`WARNING: Too many inputs were provided (${Fe} > ${Se}). The following inputs will be ignored: "${Ye.join(", ")}".`);
        }
        return U;
      }
      async function ue(_, T) {
        const U = le(_, T);
        try {
          const be = Object.fromEntries(Object.entries(U).map(([Se, Ye]) => [Se, Ye.ort_tensor]));
          let Fe = await _.run(be);
          return Fe = fe(Fe), Fe;
        } catch (be) {
          const Fe = Object.fromEntries(Object.entries(U).map(([Se, { type: Ye, dims: rt, data: mt }]) => [Se, {
            // Extract these properties from the underlying ORT tensor
            type: Ye,
            dims: rt,
            data: mt
          }]));
          throw console.error(`An error occurred during model execution: "${be}".`), console.error("Inputs given to model:", Fe), be;
        }
      }
      function fe(_) {
        for (let T in _)
          (0, L.isONNXTensor)(_[T]) ? _[T] = new b.Tensor(_[T]) : typeof _[T] == "object" && fe(_[T]);
        return _;
      }
      function Ce(_) {
        if (_ instanceof b.Tensor)
          return _;
        if (_.length === 0)
          throw Error("items must be non-empty");
        if (Array.isArray(_[0])) {
          if (_.some((T) => T.length !== _[0].length))
            throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.");
          return new b.Tensor(
            "int64",
            BigInt64Array.from(_.flat().map((T) => BigInt(T))),
            [_.length, _[0].length]
          );
        } else
          return new b.Tensor(
            "int64",
            BigInt64Array.from(_.map((T) => BigInt(T))),
            [1, _.length]
          );
      }
      function xe(_) {
        return new b.Tensor("bool", [_], [1]);
      }
      async function Le(_, T) {
        let { encoder_outputs: U, input_ids: be, decoder_input_ids: Fe, ...Se } = T;
        if (!U) {
          const rt = (0, W.pick)(T, _.sessions.model.inputNames);
          U = (await qe(_, rt)).last_hidden_state;
        }
        return Se.input_ids = Fe, Se.encoder_hidden_states = U, _.sessions.decoder_model_merged.inputNames.includes("encoder_attention_mask") && (Se.encoder_attention_mask = T.attention_mask), await Ue(_, Se, !0);
      }
      async function qe(_, T) {
        const U = _.sessions.model, be = (0, W.pick)(T, U.inputNames);
        if (U.inputNames.includes("inputs_embeds") && !be.inputs_embeds) {
          if (!T.input_ids)
            throw new Error("Both `input_ids` and `inputs_embeds` are missing in the model inputs.");
          be.inputs_embeds = await _.encode_text({ input_ids: T.input_ids });
        }
        return U.inputNames.includes("token_type_ids") && !be.token_type_ids && (be.token_type_ids = new b.Tensor(
          "int64",
          new BigInt64Array(be.input_ids.data.length),
          be.input_ids.dims
        )), await ue(U, be);
      }
      async function Ue(_, T, U = !1) {
        const be = _.sessions[U ? "decoder_model_merged" : "model"], { past_key_values: Fe, ...Se } = T;
        if (be.inputNames.includes("use_cache_branch") && (Se.use_cache_branch = xe(!!Fe)), be.inputNames.includes("position_ids") && Se.attention_mask && !Se.position_ids) {
          const rt = _.config.model_type === "paligemma" ? 1 : 0;
          Se.position_ids = he(Se, Fe, rt);
        }
        _.addPastKeyValues(Se, Fe);
        const Ye = (0, W.pick)(Se, be.inputNames);
        return await ue(be, Ye);
      }
      function ut({
        image_token_id: _,
        inputs_embeds: T,
        image_features: U,
        input_ids: be,
        attention_mask: Fe
      }) {
        const Se = be.tolist().map(
          (xt) => xt.reduce((Lt, Ut, Dt) => (Ut == _ && Lt.push(Dt), Lt), [])
        ), Ye = Se.reduce((xt, Lt) => xt + Lt.length, 0), rt = U.dims[0];
        if (Ye !== rt)
          throw new Error(`Image features and image tokens do not match: tokens: ${Ye}, features ${rt}`);
        let mt = 0;
        for (let xt = 0; xt < Se.length; ++xt) {
          const Lt = Se[xt], Ut = T[xt];
          for (let Dt = 0; Dt < Lt.length; ++Dt)
            Ut[Lt[Dt]].data.set(U[mt++].data);
        }
        return { inputs_embeds: T, attention_mask: Fe };
      }
      async function de(_, {
        // Produced by the tokenizer/processor:
        input_ids: T = null,
        attention_mask: U = null,
        pixel_values: be = null,
        // Used during generation:
        position_ids: Fe = null,
        inputs_embeds: Se = null,
        past_key_values: Ye = null,
        // Generic generation parameters
        generation_config: rt = null,
        logits_processor: mt = null,
        // TODO: needed?
        ...xt
      }) {
        if (!Se) {
          if (Se = await _.encode_text({ input_ids: T, ...xt }), be && T.dims[1] !== 1) {
            const Ut = await _.encode_image({ pixel_values: be, ...xt });
            ({ inputs_embeds: Se, attention_mask: U } = _._merge_input_ids_with_image_features({
              image_features: Ut,
              inputs_embeds: Se,
              input_ids: T,
              attention_mask: U
            }));
          } else if (Ye && be && T.dims[1] === 1) {
            const Ut = T.dims[1], Dt = Object.values(Ye)[0].dims.at(-2);
            U = (0, b.cat)([
              (0, b.ones)([T.dims[0], Dt]),
              U.slice(null, [U.dims[1] - Ut, U.dims[1]])
            ], 1);
          }
        }
        if (!Fe && _.config.model_type === "qwen2_vl") {
          const { image_grid_thw: Ut, video_grid_thw: Dt } = xt;
          [Fe] = _.get_rope_index(T, Ut, Dt, U);
        }
        return await Ue(_, {
          inputs_embeds: Se,
          past_key_values: Ye,
          attention_mask: U,
          position_ids: Fe,
          generation_config: rt,
          logits_processor: mt
        }, !0);
      }
      function re(_, T = 0) {
        const [U, be] = _.dims, Fe = _.data, Se = new BigInt64Array(Fe.length);
        for (let Ye = 0; Ye < U; ++Ye) {
          const rt = Ye * be;
          let mt = BigInt(T);
          for (let xt = 0; xt < be; ++xt) {
            const Lt = rt + xt;
            Fe[Lt] === 0n ? Se[Lt] = BigInt(1) : (Se[Lt] = mt, mt += Fe[Lt]);
          }
        }
        return { data: Se, dims: _.dims };
      }
      function he(_, T = null, U = 0) {
        const { input_ids: be, inputs_embeds: Fe, attention_mask: Se } = _, { data: Ye, dims: rt } = re(Se, U);
        let mt = new b.Tensor("int64", Ye, rt);
        if (T) {
          const xt = -(be ?? Fe).dims.at(1);
          mt = mt.slice(null, [xt, null]);
        }
        return mt;
      }
      function Ee(_, T, U, be) {
        if (U.past_key_values) {
          const Fe = Object.values(U.past_key_values)[0].dims.at(-2), { input_ids: Se, attention_mask: Ye } = U;
          if (!(Ye && Ye.dims[1] > Se.dims[1])) {
            if (Fe < Se.dims[1])
              U.input_ids = Se.slice(null, [Fe, null]);
            else if (
              // NOTE: Only used by VLMs (!= so that null matches undefined)
              _.config.image_token_index != null && // Equivalent to `self.config.image_token_index in input_ids` (== so that int matches bigint)
              Se.data.some((rt) => rt == _.config.image_token_index)
            ) {
              const rt = _.config.num_image_tokens;
              if (!rt)
                throw new Error("`num_image_tokens` is missing in the model configuration.");
              const mt = Se.dims[1] - (Fe - rt);
              U.input_ids = Se.slice(null, [-mt, null]), U.attention_mask = (0, b.ones)([1, Fe + mt]);
            }
          }
        }
        return U;
      }
      function Be(_, T, U, be) {
        return U.past_key_values && (T = T.map((Fe) => [Fe.at(-1)])), {
          ...U,
          decoder_input_ids: Ce(T)
        };
      }
      function et(_, ...T) {
        return _.config.is_encoder_decoder ? Be(_, ...T) : Ee(_, ...T);
      }
      function Xe(_, T, U, be) {
        const Fe = !!U.past_key_values;
        return be.guidance_scale !== null && be.guidance_scale > 1 && (Fe ? U.input_ids = (0, b.cat)([
          U.input_ids,
          U.input_ids
        ], 0) : (U.input_ids = (0, b.cat)([
          U.input_ids,
          (0, b.full_like)(U.input_ids, BigInt(be.pad_token_id))
        ], 0), U.attention_mask = (0, b.cat)([
          U.attention_mask,
          (0, b.full_like)(U.attention_mask, 0n)
        ], 0))), (Fe || !U.pixel_values) && (U.pixel_values = (0, b.full)([0, 0, 3, 384, 384], 1)), Fe && (U.images_seq_mask = new b.Tensor(
          "bool",
          new Array(1).fill(!0).fill(!1, 0, 1),
          [1, 1]
        ), U.images_emb_mask = new b.Tensor(
          "bool",
          new Array(0).fill(!1),
          [1, 1, 0]
        )), U;
      }
      class ie extends J.Callable {
        /**
         * Creates a new instance of the `PreTrainedModel` class.
         * @param {import('./configs.js').PretrainedConfig} config The model configuration.
         * @param {Record<string, any>} sessions The inference sessions for the model.
         * @param {Record<string, Object>} configs Additional configuration files (e.g., generation_config.json).
         */
        constructor(U, be, Fe) {
          super();
          ge(this, "main_input_name", "input_ids");
          ge(this, "forward_params", ["input_ids", "attention_mask"]);
          this.config = U, this.sessions = be, this.configs = Fe;
          const Se = C.get(this.constructor), Ye = $.get(Se);
          switch (this.can_generate = !1, this._forward = null, this._prepare_inputs_for_generation = null, Ye) {
            case O.DecoderOnly:
              this.can_generate = !0, this._forward = Ue, this._prepare_inputs_for_generation = Ee;
              break;
            case O.Seq2Seq:
            case O.Vision2Seq:
            case O.Musicgen:
              this.can_generate = !0, this._forward = Le, this._prepare_inputs_for_generation = Be;
              break;
            case O.EncoderDecoder:
              this._forward = Le;
              break;
            case O.ImageTextToText:
              this.can_generate = !0, this._forward = de, this._prepare_inputs_for_generation = et;
              break;
            case O.Phi3V:
              this.can_generate = !0, this._prepare_inputs_for_generation = et;
              break;
            case O.MultiModality:
              this.can_generate = !0, this._prepare_inputs_for_generation = Xe;
              break;
            default:
              this._forward = qe;
              break;
          }
          this.can_generate && this.forward_params.push("past_key_values"), this.custom_config = this.config["transformers.js_config"] ?? {};
        }
        /**
        * Disposes of all the ONNX sessions that were created during inference.
        * @returns {Promise<unknown[]>} An array of promises, one for each ONNX session that is being disposed.
        * @todo Use https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/FinalizationRegistry
        */
        async dispose() {
          var be;
          const U = [];
          for (const Fe of Object.values(this.sessions))
            (be = Fe == null ? void 0 : Fe.handler) != null && be.dispose && U.push(Fe.handler.dispose());
          return await Promise.all(U);
        }
        /**
         * Instantiate one of the model classes of the library from a pretrained model.
         * 
         * The model class to instantiate is selected based on the `model_type` property of the config object
         * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing model weights, e.g., `./my_model_directory/`.
         * @param {import('./utils/hub.js').PretrainedModelOptions} options Additional options for loading the model.
         * 
         * @returns {Promise<PreTrainedModel>} A new instance of the `PreTrainedModel` class.
         */
        static async from_pretrained(U, {
          progress_callback: be = null,
          config: Fe = null,
          cache_dir: Se = null,
          local_files_only: Ye = !1,
          revision: rt = "main",
          model_file_name: mt = null,
          subfolder: xt = "onnx",
          device: Lt = null,
          dtype: Ut = null,
          use_external_data_format: Dt = null,
          session_options: Wt = {}
        } = {}) {
          let Zt = {
            progress_callback: be,
            config: Fe,
            cache_dir: Se,
            local_files_only: Ye,
            revision: rt,
            model_file_name: mt,
            subfolder: xt,
            device: Lt,
            dtype: Ut,
            use_external_data_format: Dt,
            session_options: Wt
          };
          const sr = C.get(this), qt = $.get(sr);
          Fe = Zt.config = await f.AutoConfig.from_pretrained(U, Zt);
          let ir;
          if (qt === O.DecoderOnly)
            ir = await Promise.all([
              ee(U, {
                model: Zt.model_file_name ?? "model"
              }, Zt),
              X(U, {
                generation_config: "generation_config.json"
              }, Zt)
            ]);
          else if (qt === O.Seq2Seq || qt === O.Vision2Seq)
            ir = await Promise.all([
              ee(U, {
                model: "encoder_model",
                decoder_model_merged: "decoder_model_merged"
              }, Zt),
              X(U, {
                generation_config: "generation_config.json"
              }, Zt)
            ]);
          else if (qt === O.MaskGeneration)
            ir = await Promise.all([
              ee(U, {
                model: "vision_encoder",
                prompt_encoder_mask_decoder: "prompt_encoder_mask_decoder"
              }, Zt)
            ]);
          else if (qt === O.EncoderDecoder)
            ir = await Promise.all([
              ee(U, {
                model: "encoder_model",
                decoder_model_merged: "decoder_model_merged"
              }, Zt)
            ]);
          else if (qt === O.ImageTextToText) {
            const Sr = {
              embed_tokens: "embed_tokens",
              vision_encoder: "vision_encoder",
              decoder_model_merged: "decoder_model_merged"
            };
            Fe.is_encoder_decoder && (Sr.model = "encoder_model"), ir = await Promise.all([
              ee(U, Sr, Zt),
              X(U, {
                generation_config: "generation_config.json"
              }, Zt)
            ]);
          } else if (qt === O.Musicgen)
            ir = await Promise.all([
              ee(U, {
                model: "text_encoder",
                decoder_model_merged: "decoder_model_merged",
                encodec_decode: "encodec_decode"
              }, Zt),
              X(U, {
                generation_config: "generation_config.json"
              }, Zt)
            ]);
          else if (qt === O.MultiModality)
            ir = await Promise.all([
              ee(U, {
                prepare_inputs_embeds: "prepare_inputs_embeds",
                model: "language_model",
                lm_head: "lm_head",
                gen_head: "gen_head",
                gen_img_embeds: "gen_img_embeds",
                image_decode: "image_decode"
              }, Zt),
              X(U, {
                generation_config: "generation_config.json"
              }, Zt)
            ]);
          else if (qt === O.Phi3V)
            ir = await Promise.all([
              ee(U, {
                prepare_inputs_embeds: "prepare_inputs_embeds",
                model: "model",
                vision_encoder: "vision_encoder"
              }, Zt),
              X(U, {
                generation_config: "generation_config.json"
              }, Zt)
            ]);
          else {
            if (qt !== O.EncoderOnly) {
              const Sr = sr ?? (Fe == null ? void 0 : Fe.model_type);
              Sr !== "custom" && console.warn(`Model type for '${Sr}' not found, assuming encoder-only architecture. Please report this at ${x.GITHUB_ISSUE_URL}.`);
            }
            ir = await Promise.all([
              ee(U, {
                model: Zt.model_file_name ?? "model"
              }, Zt)
            ]);
          }
          return new this(Fe, ...ir);
        }
        /**
         * Runs the model with the provided inputs
         * @param {Object} model_inputs Object containing input tensors
         * @returns {Promise<Object>} Object containing output tensors
         */
        async _call(U) {
          return await this.forward(U);
        }
        /**
         * Forward method for a pretrained model. If not overridden by a subclass, the correct forward method
         * will be chosen based on the model type.
         * @param {Object} model_inputs The input data to the model in the format specified in the ONNX model.
         * @returns {Promise<Object>} The output data from the model in the format specified in the ONNX model.
         * @throws {Error} This method must be implemented in subclasses.
         */
        async forward(U) {
          return await this._forward(this, U);
        }
        /**
         * Get the model's generation config, if it exists.
         * @returns {GenerationConfig|null} The model's generation config if it exists, otherwise `null`.
         */
        get generation_config() {
          var U;
          return ((U = this.configs) == null ? void 0 : U.generation_config) ?? null;
        }
        /**
         * This function returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsWarper`]
         * instances used for multinomial sampling.
         * @param {GenerationConfig} generation_config The generation config.
         * @returns {LogitsProcessorList} generation_config 
         */
        _get_logits_warper(U) {
          const be = new y.LogitsProcessorList();
          return U.temperature !== null && U.temperature !== 1 && be.push(new y.TemperatureLogitsWarper(U.temperature)), U.top_k !== null && U.top_k !== 0 && be.push(new y.TopKLogitsWarper(U.top_k)), U.top_p !== null && U.top_p < 1 && be.push(new y.TopPLogitsWarper(U.top_p)), be;
        }
        /**
         * @param {GenerationConfig} generation_config 
         * @param {number} input_ids_seq_length The starting sequence length for the input ids.
         * @returns {LogitsProcessorList}
         * @private
         */
        _get_logits_processor(U, be, Fe = null) {
          const Se = new y.LogitsProcessorList();
          if (U.repetition_penalty !== null && U.repetition_penalty !== 1 && Se.push(new y.RepetitionPenaltyLogitsProcessor(U.repetition_penalty)), U.no_repeat_ngram_size !== null && U.no_repeat_ngram_size > 0 && Se.push(new y.NoRepeatNGramLogitsProcessor(U.no_repeat_ngram_size)), U.bad_words_ids !== null && Se.push(new y.NoBadWordsLogitsProcessor(U.bad_words_ids, U.eos_token_id)), U.min_length !== null && U.eos_token_id !== null && U.min_length > 0 && Se.push(new y.MinLengthLogitsProcessor(U.min_length, U.eos_token_id)), U.min_new_tokens !== null && U.eos_token_id !== null && U.min_new_tokens > 0 && Se.push(new y.MinNewTokensLengthLogitsProcessor(
            be,
            U.min_new_tokens,
            U.eos_token_id
          )), U.forced_bos_token_id !== null && Se.push(new y.ForcedBOSTokenLogitsProcessor(U.forced_bos_token_id)), U.forced_eos_token_id !== null && Se.push(new y.ForcedEOSTokenLogitsProcessor(
            U.max_length,
            U.forced_eos_token_id
          )), U.begin_suppress_tokens !== null) {
            const Ye = be > 1 || U.forced_bos_token_id === null ? be : be + 1;
            Se.push(new y.SuppressTokensAtBeginLogitsProcessor(U.begin_suppress_tokens, Ye));
          }
          return U.guidance_scale !== null && U.guidance_scale > 1 && Se.push(new y.ClassifierFreeGuidanceLogitsProcessor(U.guidance_scale)), Fe !== null && Se.extend(Fe), Se;
        }
        /**
         * This function merges multiple generation configs together to form a final generation config to be used by the model for text generation.
         * It first creates an empty `GenerationConfig` object, then it applies the model's own `generation_config` property to it. Finally, if a `generation_config` object was passed in the arguments, it overwrites the corresponding properties in the final config with those of the passed config object.
         * @param {GenerationConfig|null} generation_config A `GenerationConfig` object containing generation parameters.
         * @param {Object} kwargs Additional generation parameters to be used in place of those in the `generation_config` object.
         * @returns {GenerationConfig} The final generation config object to be used by the model for text generation.
         */
        _prepare_generation_config(U, be, Fe = M.GenerationConfig) {
          const Se = { ...this.config };
          for (const rt of ["decoder", "generator", "text_config"])
            rt in Se && Object.assign(Se, Se[rt]);
          const Ye = new Fe(Se);
          return Object.assign(Ye, this.generation_config ?? {}), U && Object.assign(Ye, U), be && Object.assign(Ye, (0, W.pick)(be, Object.getOwnPropertyNames(Ye))), Ye;
        }
        /**
         * 
         * @param {GenerationConfig} generation_config 
         * @param {StoppingCriteriaList} [stopping_criteria=null] 
         */
        _get_stopping_criteria(U, be = null) {
          const Fe = new se.StoppingCriteriaList();
          return U.max_length !== null && Fe.push(new se.MaxLengthCriteria(
            U.max_length,
            this.config.max_position_embeddings ?? null
          )), U.eos_token_id !== null && Fe.push(new se.EosTokenCriteria(U.eos_token_id)), be && Fe.extend(be), Fe;
        }
        /**
         * Confirms that the model class is compatible with generation.
         * If not, raises an exception that points to the right class to use.
         */
        _validate_model_class() {
          if (!this.can_generate) {
            const U = [
              ea,
              // MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING, // TODO
              xi,
              Zo,
              Jo
            ], be = C.get(this.constructor), Fe = /* @__PURE__ */ new Set(), Se = this.config.model_type;
            for (const rt of U) {
              const mt = rt.get(Se);
              mt && Fe.add(mt[0]);
            }
            let Ye = `The current model class (${be}) is not compatible with \`.generate()\`, as it doesn't have a language model head.`;
            throw Fe.size > 0 && (Ye += ` Please use the following class instead: ${[...Fe].join(", ")}`), Error(Ye);
          }
        }
        prepare_inputs_for_generation(...U) {
          return this._prepare_inputs_for_generation(this, ...U);
        }
        /**
         * 
         * @param {Object} inputs
         * @param {bigint[][]} inputs.generated_input_ids
         * @param {Object} inputs.outputs
         * @param {Object} inputs.model_inputs
         * @param {boolean} inputs.is_encoder_decoder
         * @returns {Object} The updated model inputs for the next generation iteration.
         */
        _update_model_kwargs_for_generation({ generated_input_ids: U, outputs: be, model_inputs: Fe, is_encoder_decoder: Se }) {
          return Fe.past_key_values = this.getPastKeyValues(be, Fe.past_key_values), Fe.input_ids = new b.Tensor("int64", U.flat(), [U.length, 1]), Se || (Fe.attention_mask = (0, b.cat)(
            [
              Fe.attention_mask,
              (0, b.ones)([Fe.attention_mask.dims[0], 1])
            ],
            1
          )), Fe.position_ids = null, Fe;
        }
        /**
         * This function extracts the model-specific `inputs` for generation.
         * @param {Object} params
         * @param {Tensor} [params.inputs=null]
         * @param {number} [params.bos_token_id=null]
         * @param {Record<string, Tensor|number[]>} [params.model_kwargs]
         * @returns {{inputs_tensor: Tensor, model_inputs: Record<string, Tensor>, model_input_name: string}} The model-specific inputs for generation.
         */
        _prepare_model_inputs({ inputs: U, bos_token_id: be, model_kwargs: Fe }) {
          const Se = (0, W.pick)(Fe, this.forward_params), Ye = this.main_input_name;
          if (Ye in Se) {
            if (U)
              throw new Error(
                "`inputs`: {inputs}` were passed alongside {input_name} which is not allowed. Make sure to either pass {inputs} or {input_name}=..."
              );
          } else
            Se[Ye] = U;
          return { inputs_tensor: Se[Ye], model_inputs: Se, model_input_name: Ye };
        }
        async _prepare_encoder_decoder_kwargs_for_generation({ inputs_tensor: U, model_inputs: be, model_input_name: Fe, generation_config: Se }) {
          if (this.sessions.model.inputNames.includes("inputs_embeds") && !be.inputs_embeds && "_prepare_inputs_embeds" in this) {
            const { input_ids: rt, pixel_values: mt, attention_mask: xt, ...Lt } = be, Ut = await this._prepare_inputs_embeds(be);
            be = {
              ...Lt,
              ...(0, W.pick)(Ut, ["inputs_embeds", "attention_mask"])
            };
          }
          let { last_hidden_state: Ye } = await qe(this, be);
          if (Se.guidance_scale !== null && Se.guidance_scale > 1)
            Ye = (0, b.cat)([
              Ye,
              (0, b.full_like)(Ye, 0)
            ], 0), "attention_mask" in be && (be.attention_mask = (0, b.cat)([
              be.attention_mask,
              (0, b.zeros_like)(be.attention_mask)
            ], 0));
          else if (be.decoder_input_ids) {
            const rt = Ce(be.decoder_input_ids).dims[0];
            if (rt !== Ye.dims[0]) {
              if (Ye.dims[0] !== 1)
                throw new Error(
                  `The encoder outputs have a different batch size (${Ye.dims[0]}) than the decoder inputs (${rt}).`
                );
              Ye = (0, b.cat)(Array.from({ length: rt }, () => Ye), 0);
            }
          }
          return be.encoder_outputs = Ye, be;
        }
        /**
         * Prepares `decoder_input_ids` for generation with encoder-decoder models
         * @param {*} param0 
         */
        _prepare_decoder_input_ids_for_generation({ batch_size: U, model_input_name: be, model_kwargs: Fe, decoder_start_token_id: Se, bos_token_id: Ye, generation_config: rt }) {
          let { decoder_input_ids: mt, ...xt } = Fe;
          if (!(mt instanceof b.Tensor)) {
            if (mt)
              Array.isArray(mt[0]) || (mt = Array.from({
                length: U
              }, () => mt));
            else if (Se ?? (Se = Ye), this.config.model_type === "musicgen")
              mt = Array.from({
                length: U * this.config.decoder.num_codebooks
              }, () => [Se]);
            else if (Array.isArray(Se)) {
              if (Se.length !== U)
                throw new Error(
                  `\`decoder_start_token_id\` expcted to have length ${U} but got ${Se.length}`
                );
              mt = Se;
            } else
              mt = Array.from({
                length: U
              }, () => [Se]);
            mt = Ce(mt);
          }
          return Fe.decoder_attention_mask = (0, b.ones_like)(mt), { input_ids: mt, model_inputs: xt };
        }
        /**
         * Generates sequences of token ids for models with a language modeling head.
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         * @returns {Promise<ModelOutput|Tensor>} The output of the model, which can contain the generated token ids, attentions, and scores.
         */
        async generate({
          inputs: U = null,
          generation_config: be = null,
          logits_processor: Fe = null,
          stopping_criteria: Se = null,
          streamer: Ye = null,
          // inputs_attention_mask = null,
          ...rt
        }) {
          this._validate_model_class(), be = this._prepare_generation_config(be, rt);
          let { inputs_tensor: mt, model_inputs: xt, model_input_name: Lt } = this._prepare_model_inputs({
            inputs: U,
            model_kwargs: rt
          });
          const Ut = this.config.is_encoder_decoder;
          Ut && ("encoder_outputs" in xt || (xt = await this._prepare_encoder_decoder_kwargs_for_generation(
            { inputs_tensor: mt, model_inputs: xt, model_input_name: Lt, generation_config: be }
          )));
          let Dt;
          Ut ? { input_ids: Dt, model_inputs: xt } = this._prepare_decoder_input_ids_for_generation({
            batch_size: xt[Lt].dims.at(0),
            model_input_name: Lt,
            model_kwargs: xt,
            decoder_start_token_id: be.decoder_start_token_id,
            bos_token_id: be.bos_token_id,
            generation_config: be
          }) : Dt = xt[Lt];
          let Wt = Dt.dims.at(-1);
          be.max_new_tokens !== null && (be.max_length = Wt + be.max_new_tokens);
          const Zt = this._get_logits_processor(
            be,
            Wt,
            Fe
          ), sr = this._get_stopping_criteria(
            be,
            Se
          ), qt = xt[Lt].dims.at(0), ir = oe.LogitsSampler.getSampler(be), Sr = new Array(qt).fill(0), xr = Dt.tolist();
          Ye && Ye.put(xr);
          let dr, Pr = {};
          for (; ; ) {
            if (xt = this.prepare_inputs_for_generation(xr, xt, be), dr = await this.forward(xt), be.output_attentions && be.return_dict_in_generate) {
              const ns = this.getAttentions(dr);
              for (const Ps in ns)
                Ps in Pr || (Pr[Ps] = []), Pr[Ps].push(ns[Ps]);
            }
            const Ur = dr.logits.slice(null, -1, null), gs = Zt(xr, Ur), jn = [];
            for (let ns = 0; ns < gs.dims.at(0); ++ns) {
              const Ps = gs[ns], la = await ir(Ps);
              for (const [Ti, ua] of la) {
                const Ei = BigInt(Ti);
                Sr[ns] += ua, xr[ns].push(Ei), jn.push([Ei]);
                break;
              }
            }
            if (Ye && Ye.put(jn), sr(xr).every((ns) => ns))
              break;
            xt = this._update_model_kwargs_for_generation({
              generated_input_ids: jn,
              outputs: dr,
              model_inputs: xt,
              is_encoder_decoder: Ut
            });
          }
          Ye && Ye.end();
          const $r = this.getPastKeyValues(dr, xt.past_key_values, !0), Gr = new b.Tensor("int64", xr.flat(), [xr.length, xr[0].length]);
          if (be.return_dict_in_generate)
            return {
              sequences: Gr,
              past_key_values: $r,
              ...Pr
              // TODO:
              // scores,
              // logits,
            };
          for (const Ur of Object.values(dr))
            Ur.location === "gpu-buffer" && Ur.dispose();
          return Gr;
        }
        /**
         * Returns an object containing past key values from the given decoder results object.
         *
         * @param {Object} decoderResults The decoder results object.
         * @param {Object} pastKeyValues The previous past key values.
         * @returns {Object} An object containing past key values.
         */
        getPastKeyValues(U, be, Fe = !1) {
          const Se = /* @__PURE__ */ Object.create(null);
          for (const Ye in U)
            if (Ye.startsWith("present")) {
              const rt = Ye.replace("present", "past_key_values"), mt = Ye.includes("encoder");
              if (mt && be ? Se[rt] = be[rt] : Se[rt] = U[Ye], be && (!mt || Fe)) {
                const xt = be[rt];
                xt.location === "gpu-buffer" && xt.dispose();
              }
            }
          return Se;
        }
        /**
         * Returns an object containing attentions from the given model output object.
         *
         * @param {Object} model_output The output of the model.
         * @returns {{cross_attentions?: Tensor[]}} An object containing attentions.
         */
        getAttentions(U) {
          const be = {};
          for (const Fe of ["cross_attentions", "encoder_attentions", "decoder_attentions"])
            for (const Se in U)
              Se.startsWith(Fe) && (Fe in be || (be[Fe] = []), be[Fe].push(U[Se]));
          return be;
        }
        /**
         * Adds past key values to the decoder feeds object. If pastKeyValues is null, creates new tensors for past key values.
         *
         * @param {Object} decoderFeeds The decoder feeds object to add past key values to.
         * @param {Object} pastKeyValues An object containing past key values.
         */
        addPastKeyValues(U, be) {
          var Fe, Se, Ye;
          if (be)
            Object.assign(U, be);
          else {
            const rt = this.sessions.decoder_model_merged ?? this.sessions.model, mt = ((Fe = rt == null ? void 0 : rt.config) == null ? void 0 : Fe.kv_cache_dtype) ?? "float32", xt = mt === "float16" ? new Uint16Array() : [], Lt = ((Ye = (Se = U[this.main_input_name] ?? U.attention_mask) == null ? void 0 : Se.dims) == null ? void 0 : Ye[0]) ?? 1, Ut = (0, f.getKeyValueShapes)(this.config, { batch_size: Lt });
            for (const Dt in Ut)
              U[Dt] = new b.Tensor(mt, xt, Ut[Dt]);
          }
        }
        async encode_image({ pixel_values: U }) {
          const be = (await ue(this.sessions.vision_encoder, { pixel_values: U })).image_features;
          return this.config.num_image_tokens || (console.warn(
            `The number of image tokens was not set in the model configuration. Setting it to the number of features detected by the vision encoder (${be.dims[1]}).`
          ), this.config.num_image_tokens = be.dims[1]), be;
        }
        async encode_text({ input_ids: U }) {
          return (await ue(this.sessions.embed_tokens, { input_ids: U })).inputs_embeds;
        }
      }
      class Je {
      }
      class De extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.last_hidden_state Sequence of hidden-states at the output of the last layer of the model.
         * @param {Tensor} [output.hidden_states] Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.
         * @param {Tensor} [output.attentions] Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.
         */
        constructor({ last_hidden_state: T, hidden_states: U = null, attentions: be = null }) {
          super(), this.last_hidden_state = T, this.hidden_states = U, this.attentions = be;
        }
      }
      class ce extends ie {
      }
      class ve extends ce {
      }
      class Re extends ce {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class je extends ce {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Ve extends ce {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class Ne extends ce {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Ze extends ie {
      }
      class at extends Ze {
      }
      class ft extends Ze {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class dt extends Ze {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class gt extends Ze {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class F extends ie {
      }
      class ne extends F {
      }
      class K extends ie {
      }
      class pe extends K {
      }
      class Oe extends K {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class Qe extends K {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class st extends K {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class pt extends K {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class It extends ie {
      }
      class St extends It {
      }
      class Ot extends It {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class At extends It {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class nr extends It {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class gr extends It {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class kr extends ie {
      }
      class Ar extends kr {
      }
      class Qr extends kr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class is extends kr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Xs extends kr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class zs extends kr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Ms extends ie {
      }
      class Nt extends Ms {
      }
      class Qs extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class ks extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Bs extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class Ss extends Ms {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class os extends ie {
      }
      class $s extends os {
      }
      class cs extends os {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class As extends os {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Ys extends os {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class as extends os {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class nt extends ie {
      }
      class _t extends nt {
      }
      class Ft extends nt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class lr extends nt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class bs extends nt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class tr extends nt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class ts extends ie {
      }
      class Rs extends ts {
      }
      class Js extends ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Ns extends ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class vs extends ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Pn extends ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class js extends ie {
      }
      class Cn extends js {
      }
      class Xn extends js {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class Us extends js {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class xs extends js {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class ls extends ie {
      }
      class mn extends ls {
      }
      class Zs extends ls {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class _n extends ls {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class en extends ls {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Ts extends ie {
      }
      class zt extends Ts {
      }
      class fn extends Ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class kn extends Ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Sn extends Ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class $n extends Ts {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Ws extends ie {
      }
      class An extends Ws {
      }
      class gn extends Ws {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class In extends Ws {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class or extends Ws {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Yr extends ie {
      }
      class wn extends Yr {
      }
      class Fn extends Yr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class yn extends Yr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Mn extends Yr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class Te extends ie {
        constructor() {
          super(...arguments);
          ge(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "encoder_outputs",
            "decoder_input_ids",
            "decoder_attention_mask",
            "past_key_values"
          ]);
        }
      }
      class P extends Te {
      }
      class H extends Te {
      }
      class ae extends ie {
      }
      class Me extends ae {
      }
      class Pe extends ae {
      }
      class He extends ie {
      }
      class ct extends He {
      }
      class yt extends He {
      }
      class ht extends ie {
      }
      class ot extends ht {
      }
      class Pt extends ht {
      }
      class hr extends ht {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class rr extends ie {
      }
      class $e extends rr {
      }
      class wr extends rr {
      }
      class Rr extends rr {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Jr extends rr {
      }
      class Zr extends ie {
      }
      class Bt extends Zr {
      }
      class Nr extends Zr {
      }
      class ps extends ie {
      }
      class er extends ps {
      }
      class mr extends ps {
      }
      class vt extends ie {
      }
      class yr extends vt {
      }
      class Es extends vt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class Or extends vt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class qr extends vt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class Mt extends vt {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class br extends ie {
      }
      class ze extends br {
      }
      class wt extends br {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class rs extends br {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class tn extends br {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class Qn extends br {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class bn extends ie {
      }
      class Qt extends bn {
      }
      class ya extends bn {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<MaskedLMOutput>} returned object
         */
        async _call(T) {
          return new Vr(await super._call(T));
        }
      }
      class zi extends bn {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} returned object
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Ma extends bn {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class ba extends bn {
        /**
         * Calls the model on new inputs.
         *
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<QuestionAnsweringModelOutput>} returned object
         */
        async _call(T) {
          return new Xr(await super._call(T));
        }
      }
      class Bi extends ie {
      }
      class va extends Bi {
      }
      class ss extends Bi {
      }
      class Ri extends ie {
        constructor() {
          super(...arguments);
          ge(this, "requires_attention_mask", !1);
          ge(this, "main_input_name", "input_features");
          ge(this, "forward_params", [
            "input_features",
            "attention_mask",
            "decoder_input_ids",
            "decoder_attention_mask",
            "past_key_values"
          ]);
        }
      }
      class xa extends Ri {
      }
      class Vs extends Ri {
        _prepare_generation_config(T, U) {
          return (
            /** @type {WhisperGenerationConfig} */
            super._prepare_generation_config(T, U, V.WhisperGenerationConfig)
          );
        }
        /**
         * 
         * @param {WhisperGenerationConfig} generation_config 
         */
        _retrieve_init_tokens(T) {
          const U = [T.decoder_start_token_id];
          let be = T.language;
          const Fe = T.task;
          if (T.is_multilingual) {
            be || (console.warn("No language specified - defaulting to English (en)."), be = "en");
            const Ye = `<|${(0, Y.whisper_language_to_code)(be)}|>`;
            U.push(T.lang_to_id[Ye]), U.push(T.task_to_id[Fe ?? "transcribe"]);
          } else if (be || Fe)
            throw new Error(
              "Cannot specify `task` or `language` for an English-only model. If the model is intended to be multilingual, pass `is_multilingual=true` to generate, or update the generation config."
            );
          return !T.return_timestamps && T.no_timestamps_token_id && U.at(-1) !== T.no_timestamps_token_id ? U.push(T.no_timestamps_token_id) : T.return_timestamps && U.at(-1) === T.no_timestamps_token_id && (console.warn("<|notimestamps|> prompt token is removed from generation_config since `return_timestamps` is set to `true`."), U.pop()), U.filter((Se) => Se != null);
        }
        /**
         * Transcribes or translates log-mel input features to a sequence of auto-regressively generated token ids.
         * @param {import('./models/whisper/generation_whisper.js').WhisperGenerationFunctionParameters} options
         * @returns {Promise<ModelOutput|Tensor>} The output of the model, which can contain the generated token ids, attentions, and scores.
         */
        async generate({
          inputs: T = null,
          generation_config: U = null,
          logits_processor: be = null,
          stopping_criteria: Fe = null,
          // Whisper-specific options (passed to kwargs)
          // prompt_ids = null,
          // language = null,
          // task = null,
          ...Se
        }) {
          U = this._prepare_generation_config(U, Se);
          const Ye = Se.decoder_input_ids ?? this._retrieve_init_tokens(U);
          if (U.return_timestamps && (be ?? (be = new y.LogitsProcessorList()), be.push(
            new y.WhisperTimeStampLogitsProcessor(U, Ye)
          )), U.begin_suppress_tokens && (be ?? (be = new y.LogitsProcessorList()), be.push(
            new y.SuppressTokensAtBeginLogitsProcessor(U.begin_suppress_tokens, Ye.length)
          )), U.return_token_timestamps) {
            if (!U.alignment_heads)
              throw new Error(
                "Model generation config has no `alignment_heads`, token-level timestamps not available. See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config."
              );
            U.task === "translate" && console.warn("Token-level timestamps may not be reliable for task 'translate'."), U.output_attentions = !0, U.return_dict_in_generate = !0;
          }
          const rt = await super.generate({
            inputs: T,
            generation_config: U,
            logits_processor: be,
            decoder_input_ids: Ye,
            ...Se
          });
          return U.return_token_timestamps && (rt.token_timestamps = this._extract_token_timestamps(
            rt,
            U.alignment_heads,
            U.num_frames
          )), rt;
        }
        /**
         * Calculates token-level timestamps using the encoder-decoder cross-attentions and
         * dynamic time-warping (DTW) to map each output token to a position in the input audio.
         * If `num_frames` is specified, the encoder-decoder cross-attentions will be cropped before applying DTW.
         * @param {Object} generate_outputs Outputs generated by the model
         * @param {Tensor[][]} generate_outputs.cross_attentions The cross attentions output by the model
         * @param {Tensor} generate_outputs.sequences The sequences output by the model
         * @param {number[][]} alignment_heads Alignment heads of the model
         * @param {number} [num_frames=null] Number of frames in the input audio.
         * @param {number} [time_precision=0.02] Precision of the timestamps in seconds
         * @returns {Tensor} tensor containing the timestamps in seconds for each predicted token
         */
        _extract_token_timestamps(T, U, be = null, Fe = 0.02) {
          if (!T.cross_attentions)
            throw new Error(
              "Model outputs must contain cross attentions to extract timestamps. This is most likely because the model was not exported with `output_attentions=True`."
            );
          be == null && console.warn(
            "`num_frames` has not been set, meaning the entire audio will be analyzed. This may lead to inaccurate token-level timestamps for short audios (< 30 seconds)."
          );
          let Se = this.config.median_filter_width;
          Se === void 0 && (console.warn("Model config has no `median_filter_width`, using default value of 7."), Se = 7);
          const Ye = T.cross_attentions, rt = Array.from(
            { length: this.config.decoder_layers },
            // Concatenate the cross attentions for each layer across sequence length dimension.
            (sr, qt) => (0, b.cat)(Ye.map((ir) => ir[qt]), 2)
          ), mt = (0, b.stack)(U.map(([sr, qt]) => {
            if (sr >= rt.length)
              throw new Error(`Layer index ${sr} is out of bounds for cross attentions (length ${rt.length}).`);
            return be ? rt[sr].slice(null, qt, null, [0, be]) : rt[sr].slice(null, qt);
          })).transpose(1, 0, 2, 3), [xt, Lt] = (0, b.std_mean)(mt, -2, 0, !0), Ut = mt.clone();
          for (let sr = 0; sr < Ut.dims[0]; ++sr) {
            const qt = Ut[sr];
            for (let ir = 0; ir < qt.dims[0]; ++ir) {
              const Sr = qt[ir], xr = xt[sr][ir][0].data, dr = Lt[sr][ir][0].data;
              for (let Pr = 0; Pr < Sr.dims[0]; ++Pr) {
                let $r = Sr[Pr].data;
                for (let Gr = 0; Gr < $r.length; ++Gr)
                  $r[Gr] = ($r[Gr] - dr[Gr]) / xr[Gr];
                $r.set((0, q.medianFilter)($r, Se));
              }
            }
          }
          const Dt = [(0, b.mean)(Ut, 1)], Wt = T.sequences.dims, Zt = new b.Tensor(
            "float32",
            new Float32Array(Wt[0] * Wt[1]),
            Wt
          );
          for (let sr = 0; sr < Wt[0]; ++sr) {
            const qt = Dt[sr].neg().squeeze_(0), [ir, Sr] = (0, q.dynamic_time_warping)(qt.tolist()), xr = Array.from({ length: ir.length - 1 }, ($r, Gr) => ir[Gr + 1] - ir[Gr]), dr = (0, W.mergeArrays)([1], xr).map(($r) => !!$r), Pr = [];
            for (let $r = 0; $r < dr.length; ++$r)
              dr[$r] && Pr.push(Sr[$r] * Fe);
            Zt[sr].data.set(Pr, 1);
          }
          return Zt;
        }
      }
      class Ni extends ie {
        constructor() {
          super(...arguments);
          ge(this, "requires_attention_mask", !1);
          ge(this, "main_input_name", "input_values");
          ge(this, "forward_params", [
            "input_values",
            "decoder_input_ids",
            "past_key_values"
          ]);
        }
      }
      class $c extends Ni {
      }
      class Ta extends Ni {
      }
      class ji extends ie {
        constructor() {
          super(...arguments);
          ge(this, "main_input_name", "pixel_values");
          ge(this, "forward_params", [
            // Encoder inputs
            "pixel_values",
            // Decoder inpputs
            "decoder_input_ids",
            "encoder_hidden_states",
            "past_key_values"
          ]);
        }
      }
      class Ea extends ie {
        constructor() {
          super(...arguments);
          ge(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "pixel_values",
            "position_ids",
            "past_key_values"
          ]);
        }
      }
      class Yn extends Ea {
        _merge_input_ids_with_image_features({
          inputs_embeds: T,
          image_features: U,
          input_ids: be,
          attention_mask: Fe
        }) {
          const Se = this.config.image_token_index, rt = be.tolist().map((Dt) => Dt.findIndex((Wt) => Wt == Se)), mt = rt.every((Dt) => Dt === -1), xt = rt.every((Dt) => Dt !== -1);
          if (!mt && !xt)
            throw new Error("Every input should contain either 0 or 1 image token.");
          if (mt)
            return {
              inputs_embeds: T,
              attention_mask: Fe
            };
          const Lt = [], Ut = [];
          for (let Dt = 0; Dt < rt.length; ++Dt) {
            const Wt = rt[Dt], Zt = T[Dt], sr = U[Dt], qt = Fe[Dt];
            Lt.push(
              (0, b.cat)([
                Zt.slice([0, Wt]),
                sr,
                Zt.slice([Wt + 1, Zt.dims[0]])
              ], 0)
            ), Ut.push(
              (0, b.cat)([
                qt.slice([0, Wt]),
                (0, b.ones)([sr.dims[0]]),
                qt.slice([Wt + 1, qt.dims[0]])
              ], 0)
            );
          }
          return {
            inputs_embeds: (0, b.stack)(Lt, 0),
            attention_mask: (0, b.stack)(Ut, 0)
          };
        }
      }
      class Pa extends Yn {
      }
      class Ca extends Yn {
      }
      class ka extends ie {
        constructor() {
          super(...arguments);
          ge(this, "forward_params", [
            // Encoder inputs
            "input_ids",
            "inputs_embeds",
            "attention_mask",
            "pixel_values",
            // Decoder inputs
            "encoder_outputs",
            "decoder_input_ids",
            "decoder_inputs_embeds",
            "decoder_attention_mask",
            "past_key_values"
          ]);
          ge(this, "main_input_name", "inputs_embeds");
        }
      }
      class Sa extends ka {
        _merge_input_ids_with_image_features({
          inputs_embeds: T,
          image_features: U,
          input_ids: be,
          attention_mask: Fe
        }) {
          return {
            inputs_embeds: (0, b.cat)([
              U,
              // image embeds
              T
              // task prefix embeds
            ], 1),
            attention_mask: (0, b.cat)([
              (0, b.ones)(U.dims.slice(0, 2)),
              // image attention mask
              Fe
              // task prefix attention mask
            ], 1)
          };
        }
        async _prepare_inputs_embeds({ input_ids: T, pixel_values: U, inputs_embeds: be, attention_mask: Fe }) {
          if (!T && !U)
            throw new Error("Either `input_ids` or `pixel_values` should be provided.");
          let Se, Ye;
          return T && (Se = await this.encode_text({ input_ids: T })), U && (Ye = await this.encode_image({ pixel_values: U })), Se && Ye ? { inputs_embeds: be, attention_mask: Fe } = this._merge_input_ids_with_image_features({
            inputs_embeds: Se,
            image_features: Ye,
            input_ids: T,
            attention_mask: Fe
          }) : be = Se || Ye, { inputs_embeds: be, attention_mask: Fe };
        }
        async forward({
          input_ids: T,
          pixel_values: U,
          attention_mask: be,
          decoder_input_ids: Fe,
          decoder_attention_mask: Se,
          encoder_outputs: Ye,
          past_key_values: rt,
          inputs_embeds: mt,
          decoder_inputs_embeds: xt
        }) {
          if (mt || ({ inputs_embeds: mt, attention_mask: be } = await this._prepare_inputs_embeds({ input_ids: T, pixel_values: U, inputs_embeds: mt, attention_mask: be })), !Ye) {
            let { last_hidden_state: Dt } = await qe(this, { inputs_embeds: mt, attention_mask: be });
            Ye = Dt;
          }
          if (!xt) {
            if (!Fe)
              throw new Error("Either `decoder_input_ids` or `decoder_inputs_embeds` should be provided.");
            xt = await this.encode_text({ input_ids: Fe });
          }
          return await Ue(this, {
            inputs_embeds: xt,
            attention_mask: Se,
            encoder_attention_mask: be,
            encoder_hidden_states: Ye,
            past_key_values: rt
          }, !0);
        }
      }
      class hs extends ie {
        constructor() {
          super(...arguments);
          ge(this, "forward_params", [
            "input_ids",
            // 'inputs_embeds',
            "attention_mask",
            "pixel_values",
            "position_ids",
            "past_key_values"
          ]);
        }
      }
      class $a extends hs {
        _merge_input_ids_with_image_features(T) {
          const U = T.image_features.dims.at(-1), be = T.image_features.view(-1, U);
          return ut({
            // @ts-ignore
            image_token_id: this.config.image_token_index,
            ...T,
            image_features: be
          });
        }
      }
      class Aa extends ie {
        constructor() {
          super(...arguments);
          ge(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "pixel_values",
            "pixel_attention_mask",
            "position_ids",
            "past_key_values"
          ]);
        }
      }
      class Ui extends Aa {
        async encode_image({ pixel_values: T, pixel_attention_mask: U }) {
          return (await ue(this.sessions.vision_encoder, { pixel_values: T, pixel_attention_mask: U })).image_features;
        }
        _merge_input_ids_with_image_features(T) {
          const U = T.image_features.dims.at(-1), be = T.image_features.view(-1, U);
          return ut({
            // @ts-ignore
            image_token_id: this.config.image_token_id,
            ...T,
            image_features: be
          });
        }
      }
      class Ia extends ie {
        constructor() {
          super(...arguments);
          ge(this, "forward_params", [
            "input_ids",
            "inputs_embeds",
            "attention_mask",
            "position_ids",
            "pixel_values",
            "image_sizes",
            "past_key_values"
          ]);
        }
      }
      class Wi extends Ia {
        async forward({
          // Produced by the tokenizer/processor:
          input_ids: T = null,
          attention_mask: U = null,
          pixel_values: be = null,
          image_sizes: Fe = null,
          // Used during generation:
          position_ids: Se = null,
          inputs_embeds: Ye = null,
          past_key_values: rt = null,
          // Generic generation parameters
          generation_config: mt = null,
          logits_processor: xt = null,
          // TODO: needed?
          ...Lt
        }) {
          if (!Ye) {
            let Dt;
            if (be && T.dims[1] !== 1) {
              if (!Fe)
                throw new Error("`image_sizes` must be provided when `pixel_values` is provided.");
              ({ image_features: Dt } = await ue(this.sessions.vision_encoder, {
                pixel_values: be,
                image_sizes: Fe
              }));
            } else {
              const Wt = this.config.normalized_config.hidden_size;
              Dt = new b.Tensor(
                "float32",
                [],
                [0, Wt]
              );
            }
            ({ inputs_embeds: Ye } = await ue(this.sessions.prepare_inputs_embeds, {
              input_ids: T,
              image_features: Dt
            }));
          }
          return await Ue(this, {
            inputs_embeds: Ye,
            past_key_values: rt,
            attention_mask: U,
            position_ids: Se,
            generation_config: mt,
            logits_processor: xt
          }, !1);
        }
      }
      class rn extends ie {
      }
      class Fa extends rn {
      }
      class Ac extends rn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "text_model",
            ...U
          });
        }
      }
      class Oa extends rn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "text_model",
            ...U
          });
        }
      }
      class Ic extends rn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "vision_model",
            ...U
          });
        }
      }
      class Fc extends rn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "vision_model",
            ...U
          });
        }
      }
      class us extends ie {
      }
      class Da extends us {
      }
      class Jn extends us {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "text_model",
            ...U
          });
        }
      }
      class Vi extends rn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "vision_model",
            ...U
          });
        }
      }
      class ms extends ie {
      }
      class La extends ms {
      }
      class Zn extends ie {
      }
      class za extends Zn {
        async forward(T) {
          const U = !T.input_ids, be = !T.pixel_values;
          if (U && be)
            throw new Error("Either `input_ids` or `pixel_values` should be provided.");
          if (U && (T.input_ids = (0, b.ones)([T.pixel_values.dims[0], 1])), be) {
            const { image_size: xt } = this.config.vision_config;
            T.pixel_values = (0, b.full)([0, 3, xt, xt], 0);
          }
          const { text_embeddings: Fe, image_embeddings: Se, l2norm_text_embeddings: Ye, l2norm_image_embeddings: rt } = await super.forward(T), mt = {};
          return U || (mt.text_embeddings = Fe, mt.l2norm_text_embeddings = Ye), be || (mt.image_embeddings = Se, mt.l2norm_image_embeddings = rt), mt;
        }
      }
      class Ba extends Zn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "text_model",
            ...U
          });
        }
      }
      class Ra extends Zn {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "vision_model",
            ...U
          });
        }
      }
      class Gi extends ie {
      }
      class Na extends Gi {
      }
      class ja extends Gi {
      }
      class Ki extends ie {
      }
      class Ua extends Ki {
      }
      class _s extends Ki {
      }
      class Hi extends ie {
      }
      class Wa extends Hi {
      }
      class Va extends Hi {
      }
      class qi extends ie {
      }
      class Ga extends qi {
      }
      class Ka extends qi {
      }
      class Xi extends ie {
      }
      class Ha extends Xi {
      }
      class qa extends Xi {
      }
      class Qi extends ie {
      }
      class Yi extends Qi {
      }
      class Ji extends Qi {
      }
      class Zi extends ie {
      }
      class Xa extends Zi {
      }
      class eo extends Zi {
      }
      class Qa extends ie {
      }
      class Ya extends Qa {
      }
      class ei extends Qa {
      }
      class to extends ie {
      }
      class Ja extends to {
      }
      class Za extends to {
      }
      class vn extends ie {
      }
      class el extends vn {
      }
      class tl extends vn {
      }
      class ti extends ie {
      }
      class rl extends ti {
      }
      class sl extends ti {
      }
      class ro extends ie {
      }
      class nl extends ro {
      }
      class Oc extends ro {
      }
      class so extends ie {
      }
      class il extends so {
      }
      class ol extends so {
      }
      class al extends ie {
      }
      class ll extends al {
      }
      class ur extends al {
      }
      class no extends ie {
      }
      class ul extends no {
      }
      class dl extends no {
      }
      class io extends ie {
      }
      class cl extends io {
      }
      class pl extends io {
      }
      class oo extends ie {
      }
      class hl extends oo {
      }
      class ml extends oo {
      }
      class ao extends ie {
      }
      class _l extends ao {
      }
      class fl extends ao {
      }
      class lo extends ie {
      }
      class gl extends lo {
      }
      class On extends lo {
      }
      class wl extends ie {
        constructor() {
          super(...arguments);
          ge(this, "forward_params", [
            // Text inputs
            "input_ids",
            "attention_mask",
            "position_ids",
            "past_key_values",
            // Vision inputs
            "pixel_values",
            "image_grid_thw"
          ]);
        }
      }
      class ri extends wl {
        /**
         * Calculate the 3D rope index based on image and video's temporal, height and width in LLM.
         *
         * Explanation:
         *     Each embedding sequence contains vision embedding and text embedding or just contains text embedding.
         *
         *     For pure text embedding sequence, the rotary position embedding has no difference with mordern LLMs.
         *     Examples:
         *         input_ids: [T T T T T], here T is for text.
         *         temporal position_ids: [0, 1, 2, 3, 4]
         *         height position_ids: [0, 1, 2, 3, 4]
         *         width position_ids: [0, 1, 2, 3, 4]
         *
         *     For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part
         *     and 1D rotary position embeddin for text part.
         *     Examples:
         *         Assume we have a video input with 3 temporal patches, 2 height patches and 2 width patches.
         *         input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision.
         *         vision temporal position_ids: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]
         *         vision height position_ids: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]
         *         vision width position_ids: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
         *         text temporal position_ids: [3, 4, 5, 6, 7]
         *         text height position_ids: [3, 4, 5, 6, 7]
         *         text width position_ids: [3, 4, 5, 6, 7]
         *         Here we calculate the text start position_ids as the max vision position_ids plus 1.
         * 
         * @param {Tensor} input_ids Indices of input sequence tokens in the vocabulary. Tensor of shape `(batch_size, sequence_length)`.
         * @param {Tensor} image_grid_thw (Optional) The temporal, height and width of feature shape of each image in LLM. Tensor of shape `(num_images, 3)`.
         * @param {Tensor} video_grid_thw (Optional) The temporal, height and width of feature shape of each video in LLM. Tensor of shape `(num_videos, 3)`.
         * @param {Tensor} attention_mask (Optional) Mask to avoid performing attention on padding token indices. Tensor of shape `(batch_size, sequence_length)`. Mask values selected in `[0, 1]`:
         * - 1 for tokens that are **not masked**,
         * - 0 for tokens that are **masked**.
         * @returns {[Tensor, Tensor]} [position_ids, mrope_position_deltas] with:
         * - position_ids: Tensor of shape `(3, batch_size, sequence_length)`.
         * - mrope_position_deltas: Tensor of shape `(batch_size)`.
         */
        get_rope_index(T, U, be, Fe) {
          const { vision_config: Se, image_token_id: Ye, video_token_id: rt, vision_start_token_id: mt } = this.config, xt = Se.spatial_merge_size ?? 2, Lt = [];
          if (U || be) {
            let Ut = T.tolist();
            Fe || (Fe = (0, b.ones_like)(T));
            const Dt = Fe.tolist(), Wt = Array.from({ length: 3 }, (Sr) => Array.from({ length: T.dims[0] }, (xr) => Array.from({ length: T.dims[1] }, (dr) => 1))), Zt = U ? U.tolist() : [], sr = be ? be.tolist() : [];
            let qt = 0, ir = 0;
            for (let Sr = 0; Sr < Ut.length; ++Sr) {
              const xr = Ut[Sr].filter((Tr, Dr) => Dt[Sr][Dr] == 1), Pr = xr.reduce((Tr, Dr, Ds) => (Dr == mt && Tr.push(Ds), Tr), []).map((Tr) => xr[Tr + 1]), $r = Pr.filter((Tr) => Tr == Ye).length, Gr = Pr.filter((Tr) => Tr == rt).length;
              let Ur = [], gs = 0, jn = $r, aa = Gr;
              for (let Tr = 0; Tr < Pr.length; ++Tr) {
                const Dr = xr.findIndex((qs, Kr) => Kr > gs && qs == Ye), Ds = xr.findIndex((qs, Kr) => Kr > gs && qs == rt), Hs = jn > 0 && Dr !== -1 ? Dr : xr.length + 1, ln = aa > 0 && Ds !== -1 ? Ds : xr.length + 1;
                let Pi, da, gc, Ci;
                Hs < ln ? ([da, gc, Ci] = Zt[qt], ++qt, --jn, Pi = Hs) : ([da, gc, Ci] = sr[ir], ++ir, --aa, Pi = ln);
                const [mp, ca, pa] = [
                  Number(da),
                  Math.floor(Number(gc) / xt),
                  Math.floor(Number(Ci) / xt)
                ], ha = Pi - gs, ki = Ur.length > 0 ? (0, q.max)(Ur.at(-1))[0] + 1 : 0;
                Ur.push(
                  Array.from({ length: 3 * ha }, (qs, Kr) => ki + Kr % ha)
                );
                const ma = ha + ki, Si = mp * ca * pa, Bp = Array.from({ length: Si }, (qs, Kr) => ma + Math.floor(Kr / (ca * pa))), _p = Array.from({ length: Si }, (qs, Kr) => ma + Math.floor(Kr / pa) % ca), wc = Array.from({ length: Si }, (qs, Kr) => ma + Kr % pa);
                Ur.push([Bp, _p, wc].flat()), gs = Pi + Si;
              }
              if (gs < xr.length) {
                const Tr = Ur.length > 0 ? (0, q.max)(Ur.at(-1))[0] + 1 : 0, Dr = xr.length - gs;
                Ur.push(
                  Array.from({ length: 3 * Dr }, (Ds, Hs) => Tr + Hs % Dr)
                );
              }
              const ns = Ur.reduce((Tr, Dr) => Tr + Dr.length, 0), Ps = new Array(ns);
              let la = 0;
              for (let Tr = 0; Tr < 3; ++Tr)
                for (let Dr = 0; Dr < Ur.length; ++Dr) {
                  const Ds = Ur[Dr], Hs = Ds.length / 3;
                  for (let ln = Tr * Hs; ln < (Tr + 1) * Hs; ++ln)
                    Ps[la++] = Ds[ln];
                }
              let Ti = 0;
              const ua = Dt[Sr];
              for (let Tr = 0; Tr < ua.length; ++Tr)
                if (ua[Tr] == 1) {
                  for (let Dr = 0; Dr < 3; ++Dr)
                    Wt[Dr][Sr][Tr] = Ps[Dr * ns / 3 + Ti];
                  ++Ti;
                }
              const Ei = (0, q.max)(Ps)[0];
              Lt.push(Ei + 1 - Ut[Sr].length);
            }
            return [
              new b.Tensor("int64", Wt.flat(1 / 0), [3, T.dims[0], T.dims[1]]),
              new b.Tensor("int64", Lt, [Lt.length, 1])
            ];
          } else if (Fe) {
            const { data: Ut, dims: Dt } = re(Fe), Wt = BigInt64Array.from(
              { length: 3 * Ut.length },
              (sr, qt) => Ut[qt % Ut.length]
            ), Zt = Array.from(
              { length: Dt[0] },
              (sr, qt) => (0, q.max)(Ut.subarray(Dt[1] * qt, Dt[1] * (qt + 1)))[0] + 1 + Dt[1]
            );
            return [
              new b.Tensor("int64", Wt, [3, ...Dt]),
              new b.Tensor("int64", Zt, [Zt.length, 1])
            ];
          } else {
            const [Ut, Dt] = T.dims, Wt = BigInt64Array.from(
              { length: 3 * Ut * Dt },
              (Zt, sr) => BigInt(Math.floor(sr % Dt / Ut))
            );
            return [
              new b.Tensor("int64", Wt, [3, ...T.dims]),
              (0, b.zeros)([Ut, 1])
            ];
          }
        }
        async encode_image({ pixel_values: T, image_grid_thw: U }) {
          return (await ue(this.sessions.vision_encoder, { pixel_values: T, grid_thw: U })).image_features;
        }
        _merge_input_ids_with_image_features(T) {
          return ut({
            // @ts-ignore
            image_token_id: this.config.image_token_id,
            ...T
          });
        }
        prepare_inputs_for_generation(T, U, be) {
          if (U.attention_mask && !U.position_ids)
            if (!U.past_key_values)
              [U.position_ids, U.rope_deltas] = this.get_rope_index(
                U.input_ids,
                U.image_grid_thw,
                U.video_grid_thw,
                U.attention_mask
              );
            else {
              U.pixel_values = null;
              const Fe = BigInt(Object.values(U.past_key_values)[0].dims.at(-2)), Se = U.rope_deltas.map((Ye) => Fe + Ye);
              U.position_ids = (0, b.stack)([Se, Se, Se], 0);
            }
          return U;
        }
      }
      class uo extends ie {
      }
      class yl extends uo {
      }
      class Ml extends uo {
      }
      class co extends ie {
      }
      class bl extends co {
      }
      class vl extends co {
      }
      class po extends ie {
      }
      class xl extends po {
      }
      class Tl extends po {
      }
      class ho extends ie {
      }
      class El extends ho {
      }
      class Pl extends ho {
      }
      class mo extends ie {
      }
      class Cl extends mo {
      }
      class kl extends mo {
      }
      class _o extends ie {
      }
      class fo extends _o {
      }
      class Sl extends _o {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class si extends ie {
      }
      class go extends si {
      }
      class $l extends si {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Al extends ie {
      }
      class Il extends Al {
      }
      class wo extends ie {
      }
      class Fl extends wo {
      }
      class Ol extends wo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class yo extends ie {
      }
      class Dl extends yo {
      }
      class Mo extends ie {
      }
      class Ll extends Mo {
      }
      class Dc extends Mo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class zl extends ie {
      }
      class Bl extends zl {
      }
      class ds extends ie {
      }
      class Rl extends ds {
      }
      class Nl extends ds {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class jl extends ie {
      }
      class Ul extends jl {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new hp(await super._call(T));
        }
      }
      class bo extends ie {
      }
      class Wl extends bo {
      }
      class Vl extends bo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class vo extends ie {
      }
      class Gl extends vo {
      }
      class Kl extends vo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Hl extends ie {
      }
      class ql extends Hl {
      }
      class Xl extends Hl {
      }
      class xo extends ie {
      }
      class Ql extends xo {
      }
      class Yl extends xo {
      }
      class To extends ie {
      }
      class Lc extends To {
      }
      class sn extends To {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Is extends ie {
      }
      class nn extends Is {
      }
      class Eo extends Is {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Wr(await super._call(T));
        }
      }
      class Gs extends Is {
        /**
         * Runs the model with the provided inputs
         * @param {Object} model_inputs Model inputs
         * @returns {Promise<DetrSegmentationOutput>} Object containing segmentation outputs
         */
        async _call(T) {
          return new Jl(await super._call(T));
        }
      }
      class Wr extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification logits (including no-object) for all queries.
         * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).
         * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).
         */
        constructor({ logits: T, pred_boxes: U }) {
          super(), this.logits = T, this.pred_boxes = U;
        }
      }
      class Jl extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits The output logits of the model.
         * @param {Tensor} output.pred_boxes Predicted boxes.
         * @param {Tensor} output.pred_masks Predicted masks.
         */
        constructor({ logits: T, pred_boxes: U, pred_masks: be }) {
          super(), this.logits = T, this.pred_boxes = U, this.pred_masks = be;
        }
      }
      class ni extends ie {
      }
      class Zl extends ni {
      }
      class zc extends ni {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Dn(await super._call(T));
        }
      }
      class Dn extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification logits (including no-object) for all queries.
         * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).
         * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).
         */
        constructor({ logits: T, pred_boxes: U }) {
          super(), this.logits = T, this.pred_boxes = U;
        }
      }
      class ii extends ie {
      }
      class Po extends ii {
      }
      class eu extends ii {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new tu(await super._call(T));
        }
      }
      class tu extends Wr {
      }
      class oi extends ie {
      }
      class Co extends oi {
      }
      class ru extends oi {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class ai extends ie {
      }
      class su extends ai {
      }
      class li extends ai {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class ui extends ie {
      }
      class nu extends ui {
      }
      class iu extends ui {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class ou extends ie {
      }
      class au extends ou {
      }
      class ko extends ou {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class xn extends ie {
      }
      class lu extends xn {
      }
      class So extends xn {
      }
      class $o extends ie {
      }
      class uu extends $o {
      }
      class du extends $o {
      }
      class Bc extends ie {
      }
      class cu extends Bc {
      }
      class di extends ie {
      }
      class Rc extends di {
      }
      class pu extends di {
      }
      class ci extends di {
      }
      class hu extends ie {
      }
      class pi extends hu {
      }
      class hi extends ie {
      }
      class Ao extends hi {
      }
      class mu extends hi {
      }
      class Io extends ie {
      }
      class Fo extends Io {
      }
      class Nc extends Io {
      }
      class _u extends ie {
      }
      class jc extends _u {
      }
      class Oo extends ie {
      }
      class fu extends Oo {
      }
      class gu extends Oo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class mi extends ie {
      }
      class wu extends mi {
      }
      class yu extends mi {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class _i extends ie {
      }
      class Mu extends _i {
      }
      class bu extends _i {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class vu extends ie {
      }
      class xu extends vu {
      }
      class Tu extends vu {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Eu(await super._call(T));
        }
      }
      class Eu extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification logits (including no-object) for all queries.
         * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).
         * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).
         */
        constructor({ logits: T, pred_boxes: U }) {
          super(), this.logits = T, this.pred_boxes = U;
        }
      }
      class Uc extends ie {
      }
      class Pu extends Uc {
        /**
         * Compute image embeddings and positional image embeddings, given the pixel values of an image.
         * @param {Object} model_inputs Object containing the model inputs.
         * @param {Tensor} model_inputs.pixel_values Pixel values obtained using a `SamProcessor`.
         * @returns {Promise<{ image_embeddings: Tensor, image_positional_embeddings: Tensor }>} The image embeddings and positional image embeddings.
         */
        async get_image_embeddings({ pixel_values: T }) {
          return await qe(this, { pixel_values: T });
        }
        /**
         * @typedef {Object} SamModelInputs Object containing the model inputs.
         * @property {Tensor} pixel_values Pixel values as a Tensor with shape `(batch_size, num_channels, height, width)`.
         * These can be obtained using a `SamProcessor`.
         * @property {Tensor} [input_points] Input 2D spatial points with shape `(batch_size, num_points, 2)`.
         * This is used by the prompt encoder to encode the prompt.
         * @property {Tensor} [input_labels] Input labels for the points, as a Tensor of shape `(batch_size, point_batch_size, num_points)`.
         * This is used by the prompt encoder to encode the prompt. There are 4 types of labels:
         *  - `1`: the point is a point that contains the object of interest
         *  - `0`: the point is a point that does not contain the object of interest
         *  - `-1`: the point corresponds to the background
         *  - `-10`: the point is a padding point, thus should be ignored by the prompt encoder
         * @property {Tensor} [input_boxes] Input bounding boxes with shape `(batch_size, num_boxes, 4)`.
         * @property {Tensor} [image_embeddings] Image embeddings used by the mask decoder.
         * @property {Tensor} [image_positional_embeddings] Image positional embeddings used by the mask decoder.
         */
        /**
         * @param {SamModelInputs} model_inputs Object containing the model inputs.
         * @returns {Promise<Object>} The output of the model.
         */
        async forward(T) {
          if ((!T.image_embeddings || !T.image_positional_embeddings) && (T = {
            ...T,
            ...await this.get_image_embeddings(T)
          }), !T.input_labels && T.input_points) {
            const be = T.input_points.dims.slice(0, -1), Fe = be.reduce((Se, Ye) => Se * Ye, 1);
            T.input_labels = new b.Tensor(
              "int64",
              new BigInt64Array(Fe).fill(1n),
              be
            );
          }
          const U = {
            image_embeddings: T.image_embeddings,
            image_positional_embeddings: T.image_positional_embeddings
          };
          return T.input_points && (U.input_points = T.input_points), T.input_labels && (U.input_labels = T.input_labels), T.input_boxes && (U.input_boxes = T.input_boxes), await ue(this.sessions.prompt_encoder_mask_decoder, U);
        }
        /**
         * Runs the model with the provided inputs
         * @param {Object} model_inputs Model inputs
         * @returns {Promise<SamImageSegmentationOutput>} Object containing segmentation outputs
         */
        async _call(T) {
          return new Cu(await super._call(T));
        }
      }
      class Cu extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.iou_scores The output logits of the model.
         * @param {Tensor} output.pred_masks Predicted boxes.
         */
        constructor({ iou_scores: T, pred_masks: U }) {
          super(), this.iou_scores = T, this.pred_masks = U;
        }
      }
      class Do extends ie {
      }
      class ku extends Do {
      }
      class Su extends Do {
      }
      class $u extends ie {
      }
      class fi extends $u {
      }
      class Ln extends $u {
      }
      class Fs extends ie {
      }
      class Au extends Fs {
      }
      class Iu extends Fs {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new an(await super._call(T));
        }
      }
      class Fu extends Fs {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Ou extends Fs {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class gi extends ie {
      }
      class Du extends gi {
      }
      class Lu extends gi {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class zu extends ie {
      }
      class Wc extends zu {
      }
      class wi extends ie {
      }
      class Lo extends wi {
      }
      class Bu extends wi {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new an(await super._call(T));
        }
      }
      class Ru extends wi {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class zn extends ie {
      }
      class Vc extends zn {
      }
      class Nu extends zn {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new an(await super._call(T));
        }
      }
      class ju extends zn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Gc extends zn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class yi extends ie {
      }
      class Uu extends yi {
      }
      class Wu extends yi {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_features Float values of input mel-spectrogram.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new an(await super._call(T));
        }
      }
      class Vu extends yi {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Dp extends ie {
      }
      class Gu extends Fs {
      }
      class Ku extends Fs {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new an(await super._call(T));
        }
      }
      class Hu extends Fs {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Bn extends ie {
      }
      class qu extends Bn {
      }
      class Xu extends Bn {
        /**
         * @param {Object} model_inputs
         * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.
         * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]
         */
        async _call(T) {
          return new an(await super._call(T));
        }
      }
      class Qu extends Bn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Yu extends Bn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<XVectorOutput>} An object containing the model's output logits and speaker embeddings.
         */
        async _call(T) {
          return new _c(await super._call(T));
        }
      }
      class Kc extends Bn {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for sequence classification.
         */
        async _call(T) {
          return new jr(await super._call(T));
        }
      }
      class Mi extends ie {
      }
      class Hc extends Mi {
      }
      class Ju extends Mi {
      }
      class Zu extends Mi {
        /**
         * @typedef {Object} SpeechOutput
         * @property {Tensor} [spectrogram] The predicted log-mel spectrogram of shape
         * `(output_sequence_length, config.num_mel_bins)`. Returned when no `vocoder` is provided
         * @property {Tensor} [waveform] The predicted waveform of shape `(num_frames,)`. Returned when a `vocoder` is provided.
         * @property {Tensor} [cross_attentions] The outputs of the decoder's cross-attention layers of shape
         * `(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)`. returned when `output_cross_attentions` is `true`.
         */
        /**
         * Converts a sequence of input tokens into a sequence of mel spectrograms, which are subsequently turned into a speech waveform using a vocoder.
         * @param {Tensor} input_values Indices of input sequence tokens in the vocabulary.
         * @param {Tensor} speaker_embeddings Tensor containing the speaker embeddings.
         * @param {Object} options Optional parameters for generating speech.
         * @param {number} [options.threshold=0.5] The generated sequence ends when the predicted stop token probability exceeds this value.
         * @param {number} [options.minlenratio=0.0] Used to calculate the minimum required length for the output sequence.
         * @param {number} [options.maxlenratio=20.0] Used to calculate the maximum allowed length for the output sequence.
         * @param {Object} [options.vocoder=null] The vocoder that converts the mel spectrogram into a speech waveform. If `null`, the output is the mel spectrogram.
         * @param {boolean} [options.output_cross_attentions=false] Whether or not to return the attentions tensors of the decoder's cross-attention layers.
         * @returns {Promise<SpeechOutput>} A promise which resolves to an object containing the spectrogram, waveform, and cross-attention tensors.
         */
        async generate_speech(T, U, {
          threshold: be = 0.5,
          minlenratio: Fe = 0,
          maxlenratio: Se = 20,
          vocoder: Ye = null
          // output_cross_attentions = false, // TODO add
        } = {}) {
          const rt = {
            input_ids: T
          }, { encoder_outputs: mt, encoder_attention_mask: xt } = await qe(this, rt), Lt = mt.dims[1] / this.config.reduction_factor, Ut = Math.floor(Lt * Se), Dt = Math.floor(Lt * Fe), Wt = this.config.num_mel_bins;
          let Zt = [], sr = null, qt = null, ir = 0;
          for (; ; ) {
            ++ir;
            const dr = xe(!!qt);
            let Pr;
            qt ? Pr = qt.output_sequence_out : Pr = new b.Tensor(
              "float32",
              new Float32Array(Wt),
              [1, 1, Wt]
            );
            let $r = {
              use_cache_branch: dr,
              output_sequence: Pr,
              encoder_attention_mask: xt,
              speaker_embeddings: U,
              encoder_hidden_states: mt
            };
            this.addPastKeyValues($r, sr), qt = await ue(this.sessions.decoder_model_merged, $r), sr = this.getPastKeyValues(qt, sr);
            const { prob: Gr, spectrum: Ur } = qt;
            if (Zt.push(Ur), ir >= Dt && // Finished when stop token or maximum length is reached.
            (Array.from(Gr.data).filter((gs) => gs >= be).length > 0 || ir >= Ut))
              break;
          }
          const Sr = (0, b.cat)(Zt), { waveform: xr } = await ue(Ye.sessions.model, { spectrogram: Sr });
          return {
            spectrogram: Sr,
            waveform: xr
            // cross_attentions: null, // TODO add
          };
        }
      }
      class qc extends ie {
        constructor() {
          super(...arguments);
          ge(this, "main_input_name", "spectrogram");
        }
      }
      class ed extends ie {
      }
      class td extends ed {
      }
      class zo extends ie {
      }
      class rd extends zo {
      }
      class Xc extends zo {
      }
      class fs extends ie {
      }
      class Os extends fs {
      }
      class on extends fs {
      }
      class Ks extends ie {
      }
      class sd extends Ks {
      }
      class nd extends Ks {
      }
      class bi extends ie {
      }
      class id extends bi {
      }
      class od extends bi {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "text_model",
            ...U
          });
        }
      }
      class ad extends bi {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, U = {}) {
          return super.from_pretrained(T, {
            // Update default model file name if not provided
            model_file_name: "audio_model",
            ...U
          });
        }
      }
      class ld extends ie {
      }
      class Bo extends ld {
        /**
         * Calls the model on new inputs.
         * @param {Object} model_inputs The inputs to the model.
         * @returns {Promise<VitsModelOutput>} The outputs for the VITS model.
         */
        async _call(T) {
          return new fc(await super._call(T));
        }
      }
      class vi extends ie {
      }
      class Qc extends vi {
      }
      class Yc extends vi {
      }
      class es extends vi {
      }
      class Ro extends ie {
      }
      class ud extends Ro {
      }
      class No extends Ro {
      }
      class jo extends ie {
      }
      class Rn extends jo {
      }
      class dd extends jo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Uo extends ie {
      }
      class Jc extends Uo {
      }
      class Zc extends Uo {
      }
      class Wo extends ie {
        constructor() {
          super(...arguments);
          // NOTE: not MusicgenPreTrainedModel
          ge(this, "forward_params", [
            "input_ids",
            "attention_mask",
            "encoder_outputs",
            "decoder_input_ids",
            "decoder_attention_mask",
            "past_key_values"
          ]);
        }
        /**
         * Apply the pattern mask to the final ids,
         * then revert the pattern delay mask by filtering the pad token id in a single step.
         * @param {Tensor} outputs The output tensor from the model.
         * @returns {Tensor} The filtered output tensor.
         */
        _apply_and_filter_by_delay_pattern_mask(U) {
          const [be, Fe] = U.dims, Se = this.config.decoder.num_codebooks, Ye = Fe - Se;
          let rt = 0;
          for (let Lt = 0; Lt < U.size; ++Lt) {
            if (U.data[Lt] === this.config.decoder.pad_token_id)
              continue;
            const Ut = Lt % Fe, Dt = Math.floor(Lt / Fe) % Se, Wt = Ut - Dt;
            Wt > 0 && Wt <= Ye && (U.data[rt++] = U.data[Lt]);
          }
          const mt = Math.floor(be / Se), xt = rt / (mt * Se);
          return new b.Tensor(
            U.type,
            U.data.slice(0, rt),
            [mt, Se, xt]
          );
        }
        prepare_inputs_for_generation(U, be, Fe) {
          let Se = structuredClone(U);
          for (let rt = 0; rt < Se.length; ++rt)
            for (let mt = 0; mt < Se[rt].length; ++mt)
              rt % this.config.decoder.num_codebooks >= mt && (Se[rt][mt] = BigInt(this.config.decoder.pad_token_id));
          return Fe.guidance_scale !== null && Fe.guidance_scale > 1 && (Se = Se.concat(Se)), super.prepare_inputs_for_generation(Se, be, Fe);
        }
        /**
         * Generates sequences of token ids for models with a language modeling head.
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         * @returns {Promise<ModelOutput|Tensor>} The output of the model, which can contain the generated token ids, attentions, and scores.
         */
        async generate(U) {
          const be = await super.generate(U), Fe = this._apply_and_filter_by_delay_pattern_mask(
            /** @type {Tensor} */
            be
          ).unsqueeze_(0), { audio_values: Se } = await ue(this.sessions.encodec_decode, { audio_codes: Fe });
          return Se;
        }
      }
      class Vo extends ie {
      }
      class Go extends Vo {
      }
      class cd extends Vo {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Ko extends ie {
      }
      class pd extends Ko {
      }
      class hd extends Ko {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class Ho extends ie {
      }
      class qo extends Ho {
      }
      class md extends Ho {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class _d extends ie {
      }
      class Xo extends _d {
      }
      class fd extends _d {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Xt(await super._call(T));
        }
      }
      class gd extends ie {
      }
      class wd extends gd {
      }
      class ep extends ie {
      }
      class yd extends ep {
        constructor(...U) {
          super(...U);
          ge(this, "forward_params", [
            // prepare_inputs_embeds
            "input_ids",
            "pixel_values",
            "images_seq_mask",
            "images_emb_mask",
            // language_model
            "attention_mask",
            "position_ids",
            "past_key_values"
          ]);
          this._generation_mode = "text";
        }
        async forward(U) {
          const be = this._generation_mode ?? "text";
          let Fe;
          if (be === "text" || !U.past_key_values) {
            const xt = this.sessions.prepare_inputs_embeds, Lt = (0, W.pick)(U, xt.inputNames);
            Fe = await ue(xt, Lt);
          } else {
            const xt = this.sessions.gen_img_embeds, Lt = (0, W.pick)({
              image_ids: U.input_ids
            }, xt.inputNames);
            Fe = await ue(xt, Lt);
          }
          const Se = { ...U, ...Fe }, Ye = await Ue(this, Se), rt = this.sessions[be === "text" ? "lm_head" : "gen_head"];
          if (!rt)
            throw new Error(`Unable to find "${rt}" generation head`);
          const mt = await ue(rt, (0, W.pick)(Ye, rt.inputNames));
          return {
            ...Fe,
            ...Ye,
            ...mt
          };
        }
        /**
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         */
        async generate(U) {
          return this._generation_mode = "text", super.generate(U);
        }
        /**
         * @param {import('./generation/parameters.js').GenerationFunctionParameters} options
         */
        async generate_images(U) {
          this._generation_mode = "image";
          const be = (U.inputs ?? U[this.main_input_name]).dims[1], Se = (
            /** @type {Tensor} */
            (await super.generate(U)).slice(null, [be, null])
          ), Ye = this.sessions.image_decode, { decoded_image: rt } = await ue(Ye, {
            generated_tokens: Se
          }), mt = rt.add_(1).mul_(255 / 2).clamp_(0, 255).to("uint8"), xt = [];
          for (const Lt of mt) {
            const Ut = D.RawImage.fromTensor(Lt);
            xt.push(Ut);
          }
          return xt;
        }
      }
      class Md extends Je {
        constructor({ char_logits: T, bpe_logits: U, wp_logits: be }) {
          super(), this.char_logits = T, this.bpe_logits = U, this.wp_logits = be;
        }
        get logits() {
          return [this.char_logits, this.bpe_logits, this.wp_logits];
        }
      }
      class bd extends ie {
      }
      class tp extends bd {
        /**
         * @param {any} model_inputs
         */
        async _call(T) {
          return new Md(await super._call(T));
        }
      }
      class Qo extends ie {
      }
      class vd extends Qo {
      }
      class rp extends Qo {
      }
      class Yo extends ie {
      }
      class xd extends Yo {
      }
      class Td extends Yo {
      }
      class _r {
        /** @type {typeof PreTrainedModel.from_pretrained} */
        static async from_pretrained(T, {
          progress_callback: U = null,
          config: be = null,
          cache_dir: Fe = null,
          local_files_only: Se = !1,
          revision: Ye = "main",
          model_file_name: rt = null,
          subfolder: mt = "onnx",
          device: xt = null,
          dtype: Lt = null,
          use_external_data_format: Ut = null,
          session_options: Dt = {}
        } = {}) {
          const Wt = {
            progress_callback: U,
            config: be,
            cache_dir: Fe,
            local_files_only: Se,
            revision: Ye,
            model_file_name: rt,
            subfolder: mt,
            device: xt,
            dtype: Lt,
            use_external_data_format: Ut,
            session_options: Dt
          };
          if (Wt.config = await f.AutoConfig.from_pretrained(T, Wt), !this.MODEL_CLASS_MAPPINGS)
            throw new Error("`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: " + this.name);
          for (const Zt of this.MODEL_CLASS_MAPPINGS) {
            const sr = Zt.get(Wt.config.model_type);
            if (sr)
              return await sr[1].from_pretrained(T, Wt);
          }
          if (this.BASE_IF_FAIL)
            return console.warn(`Unknown model class "${Wt.config.model_type}", attempting to construct from base class.`), await ie.from_pretrained(T, Wt);
          throw Error(`Unsupported model type: ${Wt.config.model_type}`);
        }
      }
      /**
       * Mapping from model type to model class.
       * @type {Map<string, Object>[]}
       */
      ge(_r, "MODEL_CLASS_MAPPINGS", null), /**
       * Whether to attempt to instantiate the base class (`PretrainedModel`) if 
       * the model type is not found in the mapping.
       */
      ge(_r, "BASE_IF_FAIL", !1);
      const sp = /* @__PURE__ */ new Map([
        ["bert", ["BertModel", ve]],
        ["modernbert", ["ModernBertModel", at]],
        ["nomic_bert", ["NomicBertModel", ne]],
        ["roformer", ["RoFormerModel", pe]],
        ["electra", ["ElectraModel", Ar]],
        ["esm", ["EsmModel", Cn]],
        ["convbert", ["ConvBertModel", St]],
        ["camembert", ["CamembertModel", Nt]],
        ["deberta", ["DebertaModel", $s]],
        ["deberta-v2", ["DebertaV2Model", _t]],
        ["mpnet", ["MPNetModel", zt]],
        ["albert", ["AlbertModel", wn]],
        ["distilbert", ["DistilBertModel", Rs]],
        ["roberta", ["RobertaModel", yr]],
        ["xlm", ["XLMModel", ze]],
        ["xlm-roberta", ["XLMRobertaModel", Qt]],
        ["clap", ["ClapModel", id]],
        ["clip", ["CLIPModel", Fa]],
        ["clipseg", ["CLIPSegModel", Na]],
        ["chinese_clip", ["ChineseCLIPModel", La]],
        ["siglip", ["SiglipModel", Da]],
        ["jina_clip", ["JinaCLIPModel", za]],
        ["mobilebert", ["MobileBertModel", mn]],
        ["squeezebert", ["SqueezeBertModel", An]],
        ["wav2vec2", ["Wav2Vec2Model", Au]],
        ["wav2vec2-bert", ["Wav2Vec2BertModel", Uu]],
        ["unispeech", ["UniSpeechModel", Lo]],
        ["unispeech-sat", ["UniSpeechSatModel", Vc]],
        ["hubert", ["HubertModel", Gu]],
        ["wavlm", ["WavLMModel", qu]],
        ["audio-spectrogram-transformer", ["ASTModel", va]],
        ["vits", ["VitsModel", Bo]],
        ["pyannote", ["PyAnnoteModel", Du]],
        ["wespeaker-resnet", ["WeSpeakerResNetModel", Wc]],
        ["detr", ["DetrModel", nn]],
        ["rt_detr", ["RTDetrModel", Zl]],
        ["table-transformer", ["TableTransformerModel", Po]],
        ["vit", ["ViTModel", fo]],
        ["ijepa", ["IJepaModel", go]],
        ["pvt", ["PvtModel", Fl]],
        ["vit_msn", ["ViTMSNModel", Ll]],
        ["vit_mae", ["ViTMAEModel", Dl]],
        ["groupvit", ["GroupViTModel", Bl]],
        ["fastvit", ["FastViTModel", Rl]],
        ["mobilevit", ["MobileViTModel", Wl]],
        ["mobilevitv2", ["MobileViTV2Model", Gl]],
        ["owlvit", ["OwlViTModel", ql]],
        ["owlv2", ["Owlv2Model", Ql]],
        ["beit", ["BeitModel", Lc]],
        ["deit", ["DeiTModel", Co]],
        ["hiera", ["HieraModel", su]],
        ["convnext", ["ConvNextModel", fu]],
        ["convnextv2", ["ConvNextV2Model", wu]],
        ["dinov2", ["Dinov2Model", Mu]],
        ["resnet", ["ResNetModel", nu]],
        ["swin", ["SwinModel", au]],
        ["swin2sr", ["Swin2SRModel", lu]],
        ["donut-swin", ["DonutSwinModel", jc]],
        ["yolos", ["YolosModel", xu]],
        ["dpt", ["DPTModel", uu]],
        ["glpn", ["GLPNModel", Fo]],
        ["hifigan", ["SpeechT5HifiGan", qc]],
        ["efficientnet", ["EfficientNetModel", Rn]],
        ["decision_transformer", ["DecisionTransformerModel", wd]],
        ["patchtst", ["PatchTSTForPrediction", vd]],
        ["patchtsmixer", ["PatchTSMixerForPrediction", xd]],
        ["mobilenet_v1", ["MobileNetV1Model", Go]],
        ["mobilenet_v2", ["MobileNetV2Model", pd]],
        ["mobilenet_v3", ["MobileNetV3Model", qo]],
        ["mobilenet_v4", ["MobileNetV4Model", Xo]],
        ["maskformer", ["MaskFormerModel", Ao]],
        ["mgp-str", ["MgpstrForSceneTextRecognition", tp]]
      ]), Lp = /* @__PURE__ */ new Map([
        ["t5", ["T5Model", P]],
        ["longt5", ["LongT5Model", Me]],
        ["mt5", ["MT5Model", ct]],
        ["bart", ["BartModel", ot]],
        ["mbart", ["MBartModel", $e]],
        ["marian", ["MarianModel", ku]],
        ["whisper", ["WhisperModel", xa]],
        ["m2m_100", ["M2M100Model", fi]],
        ["blenderbot", ["BlenderbotModel", Bt]],
        ["blenderbot-small", ["BlenderbotSmallModel", er]]
      ]), np = /* @__PURE__ */ new Map([
        ["bloom", ["BloomModel", xl]],
        ["jais", ["JAISModel", Wa]],
        ["gpt2", ["GPT2Model", Ua]],
        ["gptj", ["GPTJModel", Yi]],
        ["gpt_bigcode", ["GPTBigCodeModel", Xa]],
        ["gpt_neo", ["GPTNeoModel", Ga]],
        ["gpt_neox", ["GPTNeoXModel", Ha]],
        ["codegen", ["CodeGenModel", Ya]],
        ["llama", ["LlamaModel", Ja]],
        ["exaone", ["ExaoneModel", el]],
        ["olmo", ["OlmoModel", nl]],
        ["olmo2", ["Olmo2Model", il]],
        ["mobilellm", ["MobileLLMModel", rl]],
        ["granite", ["GraniteModel", ll]],
        ["cohere", ["CohereModel", ul]],
        ["gemma", ["GemmaModel", cl]],
        ["gemma2", ["Gemma2Model", hl]],
        ["openelm", ["OpenELMModel", _l]],
        ["qwen2", ["Qwen2Model", gl]],
        ["phi", ["PhiModel", yl]],
        ["phi3", ["Phi3Model", bl]],
        ["mpt", ["MptModel", El]],
        ["opt", ["OPTModel", Cl]],
        ["mistral", ["MistralModel", rd]],
        ["starcoder2", ["Starcoder2Model", Os]],
        ["falcon", ["FalconModel", sd]],
        ["stablelm", ["StableLmModel", ud]]
      ]), Jo = /* @__PURE__ */ new Map([
        ["speecht5", ["SpeechT5ForSpeechToText", Ju]],
        ["whisper", ["WhisperForConditionalGeneration", Vs]],
        ["moonshine", ["MoonshineForConditionalGeneration", Ta]]
      ]), Ed = /* @__PURE__ */ new Map([
        ["speecht5", ["SpeechT5ForTextToSpeech", Zu]]
      ]), Pd = /* @__PURE__ */ new Map([
        ["vits", ["VitsModel", Bo]],
        ["musicgen", ["MusicgenForConditionalGeneration", Wo]]
      ]), Cd = /* @__PURE__ */ new Map([
        ["bert", ["BertForSequenceClassification", je]],
        ["modernbert", ["ModernBertForSequenceClassification", dt]],
        ["roformer", ["RoFormerForSequenceClassification", Qe]],
        ["electra", ["ElectraForSequenceClassification", is]],
        ["esm", ["EsmForSequenceClassification", Us]],
        ["convbert", ["ConvBertForSequenceClassification", At]],
        ["camembert", ["CamembertForSequenceClassification", ks]],
        ["deberta", ["DebertaForSequenceClassification", As]],
        ["deberta-v2", ["DebertaV2ForSequenceClassification", lr]],
        ["mpnet", ["MPNetForSequenceClassification", kn]],
        ["albert", ["AlbertForSequenceClassification", Fn]],
        ["distilbert", ["DistilBertForSequenceClassification", Js]],
        ["roberta", ["RobertaForSequenceClassification", Or]],
        ["xlm", ["XLMForSequenceClassification", rs]],
        ["xlm-roberta", ["XLMRobertaForSequenceClassification", zi]],
        ["bart", ["BartForSequenceClassification", hr]],
        ["mbart", ["MBartForSequenceClassification", Rr]],
        ["mobilebert", ["MobileBertForSequenceClassification", _n]],
        ["squeezebert", ["SqueezeBertForSequenceClassification", In]]
      ]), kd = /* @__PURE__ */ new Map([
        ["bert", ["BertForTokenClassification", Ve]],
        ["modernbert", ["ModernBertForTokenClassification", gt]],
        ["roformer", ["RoFormerForTokenClassification", st]],
        ["electra", ["ElectraForTokenClassification", Xs]],
        ["esm", ["EsmForTokenClassification", xs]],
        ["convbert", ["ConvBertForTokenClassification", nr]],
        ["camembert", ["CamembertForTokenClassification", Bs]],
        ["deberta", ["DebertaForTokenClassification", Ys]],
        ["deberta-v2", ["DebertaV2ForTokenClassification", bs]],
        ["mpnet", ["MPNetForTokenClassification", Sn]],
        ["distilbert", ["DistilBertForTokenClassification", Ns]],
        ["roberta", ["RobertaForTokenClassification", qr]],
        ["xlm", ["XLMForTokenClassification", tn]],
        ["xlm-roberta", ["XLMRobertaForTokenClassification", Ma]]
      ]), Zo = /* @__PURE__ */ new Map([
        ["t5", ["T5ForConditionalGeneration", H]],
        ["longt5", ["LongT5ForConditionalGeneration", Pe]],
        ["mt5", ["MT5ForConditionalGeneration", yt]],
        ["bart", ["BartForConditionalGeneration", Pt]],
        ["mbart", ["MBartForConditionalGeneration", wr]],
        ["marian", ["MarianMTModel", Su]],
        ["m2m_100", ["M2M100ForConditionalGeneration", Ln]],
        ["blenderbot", ["BlenderbotForConditionalGeneration", Nr]],
        ["blenderbot-small", ["BlenderbotSmallForConditionalGeneration", mr]]
      ]), ea = /* @__PURE__ */ new Map([
        ["bloom", ["BloomForCausalLM", Tl]],
        ["gpt2", ["GPT2LMHeadModel", _s]],
        ["jais", ["JAISLMHeadModel", Va]],
        ["gptj", ["GPTJForCausalLM", Ji]],
        ["gpt_bigcode", ["GPTBigCodeForCausalLM", eo]],
        ["gpt_neo", ["GPTNeoForCausalLM", Ka]],
        ["gpt_neox", ["GPTNeoXForCausalLM", qa]],
        ["codegen", ["CodeGenForCausalLM", ei]],
        ["llama", ["LlamaForCausalLM", Za]],
        ["exaone", ["ExaoneForCausalLM", tl]],
        ["olmo", ["OlmoForCausalLM", Oc]],
        ["olmo2", ["Olmo2ForCausalLM", ol]],
        ["mobilellm", ["MobileLLMForCausalLM", sl]],
        ["granite", ["GraniteForCausalLM", ur]],
        ["cohere", ["CohereForCausalLM", dl]],
        ["gemma", ["GemmaForCausalLM", pl]],
        ["gemma2", ["Gemma2ForCausalLM", ml]],
        ["openelm", ["OpenELMForCausalLM", fl]],
        ["qwen2", ["Qwen2ForCausalLM", On]],
        ["phi", ["PhiForCausalLM", Ml]],
        ["phi3", ["Phi3ForCausalLM", vl]],
        ["mpt", ["MptForCausalLM", Pl]],
        ["opt", ["OPTForCausalLM", kl]],
        ["mbart", ["MBartForCausalLM", Jr]],
        ["mistral", ["MistralForCausalLM", Xc]],
        ["starcoder2", ["Starcoder2ForCausalLM", on]],
        ["falcon", ["FalconForCausalLM", nd]],
        ["trocr", ["TrOCRForCausalLM", td]],
        ["stablelm", ["StableLmForCausalLM", No]],
        // Also image-text-to-text
        ["phi3_v", ["Phi3VForCausalLM", Wi]]
      ]), ip = /* @__PURE__ */ new Map([
        ["multi_modality", ["MultiModalityCausalLM", yd]]
      ]), op = /* @__PURE__ */ new Map([
        ["bert", ["BertForMaskedLM", Re]],
        ["modernbert", ["ModernBertForMaskedLM", ft]],
        ["roformer", ["RoFormerForMaskedLM", Oe]],
        ["electra", ["ElectraForMaskedLM", Qr]],
        ["esm", ["EsmForMaskedLM", Xn]],
        ["convbert", ["ConvBertForMaskedLM", Ot]],
        ["camembert", ["CamembertForMaskedLM", Qs]],
        ["deberta", ["DebertaForMaskedLM", cs]],
        ["deberta-v2", ["DebertaV2ForMaskedLM", Ft]],
        ["mpnet", ["MPNetForMaskedLM", fn]],
        ["albert", ["AlbertForMaskedLM", Mn]],
        ["distilbert", ["DistilBertForMaskedLM", Pn]],
        ["roberta", ["RobertaForMaskedLM", Es]],
        ["xlm", ["XLMWithLMHeadModel", wt]],
        ["xlm-roberta", ["XLMRobertaForMaskedLM", ya]],
        ["mobilebert", ["MobileBertForMaskedLM", Zs]],
        ["squeezebert", ["SqueezeBertForMaskedLM", gn]]
      ]), Nn = /* @__PURE__ */ new Map([
        ["bert", ["BertForQuestionAnswering", Ne]],
        ["roformer", ["RoFormerForQuestionAnswering", pt]],
        ["electra", ["ElectraForQuestionAnswering", zs]],
        ["convbert", ["ConvBertForQuestionAnswering", gr]],
        ["camembert", ["CamembertForQuestionAnswering", Ss]],
        ["deberta", ["DebertaForQuestionAnswering", as]],
        ["deberta-v2", ["DebertaV2ForQuestionAnswering", tr]],
        ["mpnet", ["MPNetForQuestionAnswering", $n]],
        ["albert", ["AlbertForQuestionAnswering", yn]],
        ["distilbert", ["DistilBertForQuestionAnswering", vs]],
        ["roberta", ["RobertaForQuestionAnswering", Mt]],
        ["xlm", ["XLMForQuestionAnswering", Qn]],
        ["xlm-roberta", ["XLMRobertaForQuestionAnswering", ba]],
        ["mobilebert", ["MobileBertForQuestionAnswering", en]],
        ["squeezebert", ["SqueezeBertForQuestionAnswering", or]]
      ]), xi = /* @__PURE__ */ new Map([
        ["vision-encoder-decoder", ["VisionEncoderDecoderModel", ji]],
        ["idefics3", ["Idefics3ForConditionalGeneration", Ui]]
      ]), Sd = /* @__PURE__ */ new Map([
        ["llava", ["LlavaForConditionalGeneration", Yn]],
        ["llava_onevision", ["LlavaOnevisionForConditionalGeneration", Pa]],
        ["moondream1", ["Moondream1ForConditionalGeneration", Ca]],
        ["florence2", ["Florence2ForConditionalGeneration", Sa]],
        ["qwen2-vl", ["Qwen2VLForConditionalGeneration", ri]],
        ["idefics3", ["Idefics3ForConditionalGeneration", Ui]],
        ["paligemma", ["PaliGemmaForConditionalGeneration", $a]]
      ]), $d = /* @__PURE__ */ new Map([
        ["vision-encoder-decoder", ["VisionEncoderDecoderModel", ji]]
      ]), ta = /* @__PURE__ */ new Map([
        ["vit", ["ViTForImageClassification", Sl]],
        ["ijepa", ["IJepaForImageClassification", $l]],
        ["pvt", ["PvtForImageClassification", Ol]],
        ["vit_msn", ["ViTMSNForImageClassification", Dc]],
        ["fastvit", ["FastViTForImageClassification", Nl]],
        ["mobilevit", ["MobileViTForImageClassification", Vl]],
        ["mobilevitv2", ["MobileViTV2ForImageClassification", Kl]],
        ["beit", ["BeitForImageClassification", sn]],
        ["deit", ["DeiTForImageClassification", ru]],
        ["hiera", ["HieraForImageClassification", li]],
        ["convnext", ["ConvNextForImageClassification", gu]],
        ["convnextv2", ["ConvNextV2ForImageClassification", yu]],
        ["dinov2", ["Dinov2ForImageClassification", bu]],
        ["resnet", ["ResNetForImageClassification", iu]],
        ["swin", ["SwinForImageClassification", ko]],
        ["segformer", ["SegformerForImageClassification", Yc]],
        ["efficientnet", ["EfficientNetForImageClassification", dd]],
        ["mobilenet_v1", ["MobileNetV1ForImageClassification", cd]],
        ["mobilenet_v2", ["MobileNetV2ForImageClassification", hd]],
        ["mobilenet_v3", ["MobileNetV3ForImageClassification", md]],
        ["mobilenet_v4", ["MobileNetV4ForImageClassification", fd]]
      ]), Ad = /* @__PURE__ */ new Map([
        ["detr", ["DetrForObjectDetection", Eo]],
        ["rt_detr", ["RTDetrForObjectDetection", zc]],
        ["table-transformer", ["TableTransformerForObjectDetection", eu]],
        ["yolos", ["YolosForObjectDetection", Tu]]
      ]), Id = /* @__PURE__ */ new Map([
        ["owlvit", ["OwlViTForObjectDetection", Xl]],
        ["owlv2", ["Owlv2ForObjectDetection", Yl]]
      ]), ra = /* @__PURE__ */ new Map([
        // TODO: Do not add new models here
        ["detr", ["DetrForSegmentation", Gs]],
        ["clipseg", ["CLIPSegForImageSegmentation", ja]]
      ]), sa = /* @__PURE__ */ new Map([
        ["segformer", ["SegformerForSemanticSegmentation", es]],
        ["sapiens", ["SapiensForSemanticSegmentation", Rc]]
      ]), Fd = /* @__PURE__ */ new Map([
        ["detr", ["DetrForSegmentation", Gs]],
        ["maskformer", ["MaskFormerForInstanceSegmentation", mu]]
      ]), Od = /* @__PURE__ */ new Map([
        ["sam", ["SamModel", Pu]]
      ]), na = /* @__PURE__ */ new Map([
        ["wav2vec2", ["Wav2Vec2ForCTC", Iu]],
        ["wav2vec2-bert", ["Wav2Vec2BertForCTC", Wu]],
        ["unispeech", ["UniSpeechForCTC", Bu]],
        ["unispeech-sat", ["UniSpeechSatForCTC", Nu]],
        ["wavlm", ["WavLMForCTC", Xu]],
        ["hubert", ["HubertForCTC", Ku]]
      ]), Dd = /* @__PURE__ */ new Map([
        ["wav2vec2", ["Wav2Vec2ForSequenceClassification", Fu]],
        ["wav2vec2-bert", ["Wav2Vec2BertForSequenceClassification", Vu]],
        ["unispeech", ["UniSpeechForSequenceClassification", Ru]],
        ["unispeech-sat", ["UniSpeechSatForSequenceClassification", ju]],
        ["wavlm", ["WavLMForSequenceClassification", Qu]],
        ["hubert", ["HubertForSequenceClassification", Hu]],
        ["audio-spectrogram-transformer", ["ASTForAudioClassification", ss]]
      ]), Ld = /* @__PURE__ */ new Map([
        ["wavlm", ["WavLMForXVector", Yu]]
      ]), ia = /* @__PURE__ */ new Map([
        ["unispeech-sat", ["UniSpeechSatForAudioFrameClassification", Gc]],
        ["wavlm", ["WavLMForAudioFrameClassification", Kc]],
        ["wav2vec2", ["Wav2Vec2ForAudioFrameClassification", Ou]],
        ["pyannote", ["PyAnnoteForAudioFrameClassification", Lu]]
      ]), zd = /* @__PURE__ */ new Map([
        ["vitmatte", ["VitMatteForImageMatting", Ul]]
      ]), ap = /* @__PURE__ */ new Map([
        ["patchtst", ["PatchTSTForPrediction", rp]],
        ["patchtsmixer", ["PatchTSMixerForPrediction", Td]]
      ]), Bd = /* @__PURE__ */ new Map([
        ["swin2sr", ["Swin2SRForImageSuperResolution", So]]
      ]), Rd = /* @__PURE__ */ new Map([
        ["dpt", ["DPTForDepthEstimation", du]],
        ["depth_anything", ["DepthAnythingForDepthEstimation", cu]],
        ["glpn", ["GLPNForDepthEstimation", Nc]],
        ["sapiens", ["SapiensForDepthEstimation", pu]],
        ["depth_pro", ["DepthProForDepthEstimation", pi]]
      ]), lp = /* @__PURE__ */ new Map([
        ["sapiens", ["SapiensForNormalEstimation", ci]]
      ]), Nd = /* @__PURE__ */ new Map([
        ["vitpose", ["VitPoseForPoseEstimation", Il]]
      ]), jd = /* @__PURE__ */ new Map([
        ["clip", ["CLIPVisionModelWithProjection", Fc]],
        ["siglip", ["SiglipVisionModel", Vi]],
        ["jina_clip", ["JinaCLIPVisionModel", Ra]]
      ]), Ud = [
        [sp, O.EncoderOnly],
        [Lp, O.EncoderDecoder],
        [np, O.DecoderOnly],
        [Cd, O.EncoderOnly],
        [kd, O.EncoderOnly],
        [Zo, O.Seq2Seq],
        [Jo, O.Seq2Seq],
        [ea, O.DecoderOnly],
        [ip, O.MultiModality],
        [op, O.EncoderOnly],
        [Nn, O.EncoderOnly],
        [xi, O.Vision2Seq],
        [Sd, O.ImageTextToText],
        [ta, O.EncoderOnly],
        [ra, O.EncoderOnly],
        [Fd, O.EncoderOnly],
        [sa, O.EncoderOnly],
        [zd, O.EncoderOnly],
        [ap, O.EncoderOnly],
        [Bd, O.EncoderOnly],
        [Rd, O.EncoderOnly],
        [lp, O.EncoderOnly],
        [Nd, O.EncoderOnly],
        [Ad, O.EncoderOnly],
        [Id, O.EncoderOnly],
        [Od, O.MaskGeneration],
        [na, O.EncoderOnly],
        [Dd, O.EncoderOnly],
        [Ed, O.Seq2Seq],
        [Pd, O.EncoderOnly],
        [Ld, O.EncoderOnly],
        [ia, O.EncoderOnly],
        // Custom:
        [jd, O.EncoderOnly]
      ];
      for (const [_, T] of Ud)
        for (const [U, be] of _.values())
          $.set(U, T), C.set(be, U), g.set(U, be);
      const up = [
        // OVERRIDE:
        // TODO: Refactor to allow class to specify model
        ["MusicgenForConditionalGeneration", Wo, O.Musicgen],
        ["Phi3VForCausalLM", Wi, O.Phi3V],
        ["CLIPTextModelWithProjection", Oa, O.EncoderOnly],
        ["SiglipTextModel", Jn, O.EncoderOnly],
        ["JinaCLIPTextModel", Ba, O.EncoderOnly],
        ["ClapTextModelWithProjection", od, O.EncoderOnly],
        ["ClapAudioModelWithProjection", ad, O.EncoderOnly]
      ];
      for (const [_, T, U] of up)
        $.set(_, U), C.set(T, _), g.set(_, T);
      class Wd extends _r {
      }
      /** @type {Map<string, Object>[]} */
      // @ts-ignore
      ge(Wd, "MODEL_CLASS_MAPPINGS", Ud.map((T) => T[0])), ge(Wd, "BASE_IF_FAIL", !0);
      class Vd extends _r {
      }
      ge(Vd, "MODEL_CLASS_MAPPINGS", [Cd]);
      class Gd extends _r {
      }
      ge(Gd, "MODEL_CLASS_MAPPINGS", [kd]);
      class Kd extends _r {
      }
      ge(Kd, "MODEL_CLASS_MAPPINGS", [Zo]);
      class dp extends _r {
      }
      ge(dp, "MODEL_CLASS_MAPPINGS", [Jo]);
      class Hd extends _r {
      }
      ge(Hd, "MODEL_CLASS_MAPPINGS", [Ed]);
      class qd extends _r {
      }
      ge(qd, "MODEL_CLASS_MAPPINGS", [Pd]);
      class Xd extends _r {
      }
      ge(Xd, "MODEL_CLASS_MAPPINGS", [ea]);
      class Qd extends _r {
      }
      ge(Qd, "MODEL_CLASS_MAPPINGS", [op]);
      class cp extends _r {
      }
      ge(cp, "MODEL_CLASS_MAPPINGS", [Nn]);
      class Yd extends _r {
      }
      ge(Yd, "MODEL_CLASS_MAPPINGS", [xi]);
      class Jd extends _r {
      }
      ge(Jd, "MODEL_CLASS_MAPPINGS", [ta]);
      class Zd extends _r {
      }
      ge(Zd, "MODEL_CLASS_MAPPINGS", [ra]);
      class ec extends _r {
      }
      ge(ec, "MODEL_CLASS_MAPPINGS", [sa]);
      class tc extends _r {
      }
      ge(tc, "MODEL_CLASS_MAPPINGS", [Fd]);
      class rc extends _r {
      }
      ge(rc, "MODEL_CLASS_MAPPINGS", [Ad]);
      class sc extends _r {
      }
      ge(sc, "MODEL_CLASS_MAPPINGS", [Id]);
      class nc extends _r {
      }
      ge(nc, "MODEL_CLASS_MAPPINGS", [Od]);
      class ic extends _r {
      }
      ge(ic, "MODEL_CLASS_MAPPINGS", [na]);
      class oc extends _r {
      }
      ge(oc, "MODEL_CLASS_MAPPINGS", [Dd]);
      class ac extends _r {
      }
      ge(ac, "MODEL_CLASS_MAPPINGS", [Ld]);
      class oa extends _r {
      }
      ge(oa, "MODEL_CLASS_MAPPINGS", [ia]);
      class lc extends _r {
      }
      ge(lc, "MODEL_CLASS_MAPPINGS", [$d]);
      class uc extends _r {
      }
      ge(uc, "MODEL_CLASS_MAPPINGS", [zd]);
      class dc extends _r {
      }
      ge(dc, "MODEL_CLASS_MAPPINGS", [Bd]);
      class cc extends _r {
      }
      ge(cc, "MODEL_CLASS_MAPPINGS", [Rd]);
      class pc extends _r {
      }
      ge(pc, "MODEL_CLASS_MAPPINGS", [lp]);
      class hc extends _r {
      }
      ge(hc, "MODEL_CLASS_MAPPINGS", [Nd]);
      class mc extends _r {
      }
      ge(mc, "MODEL_CLASS_MAPPINGS", [jd]);
      class zp extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits The output logits of the model.
         * @param {Tensor} output.past_key_values An tensor of key/value pairs that represent the previous state of the model.
         * @param {Tensor} output.encoder_outputs The output of the encoder in a sequence-to-sequence model.
         * @param {Tensor} [output.decoder_attentions] Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the self-attention heads.
         * @param {Tensor} [output.cross_attentions] Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.
         */
        constructor({ logits: T, past_key_values: U, encoder_outputs: be, decoder_attentions: Fe = null, cross_attentions: Se = null }) {
          super(), this.logits = T, this.past_key_values = U, this.encoder_outputs = be, this.decoder_attentions = Fe, this.cross_attentions = Se;
        }
      }
      class Xt extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits classification (or regression if config.num_labels==1) scores (before SoftMax).
         */
        constructor({ logits: T }) {
          super(), this.logits = T;
        }
      }
      class _c extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification hidden states before AMSoftmax, of shape `(batch_size, config.xvector_output_dim)`.
         * @param {Tensor} output.embeddings Utterance embeddings used for vector similarity-based retrieval, of shape `(batch_size, config.xvector_output_dim)`.
         */
        constructor({ logits: T, embeddings: U }) {
          super(), this.logits = T, this.embeddings = U;
        }
      }
      class jr extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Classification scores (before SoftMax).
         */
        constructor({ logits: T }) {
          super(), this.logits = T;
        }
      }
      class Vr extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
         */
        constructor({ logits: T }) {
          super(), this.logits = T;
        }
      }
      class Xr extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.start_logits Span-start scores (before SoftMax).
         * @param {Tensor} output.end_logits Span-end scores (before SoftMax).
         */
        constructor({ start_logits: T, end_logits: U }) {
          super(), this.start_logits = T, this.end_logits = U;
        }
      }
      class an extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).
         */
        constructor({ logits: T }) {
          super(), this.logits = T;
        }
      }
      class pp extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).
         * @param {Tensor} output.past_key_values Contains pre-computed hidden-states (key and values in the self-attention blocks)
         * that can be used (see `past_key_values` input) to speed up sequential decoding.
         */
        constructor({ logits: T, past_key_values: U }) {
          super(), this.logits = T, this.past_key_values = U;
        }
      }
      class hp extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.alphas Estimated alpha values, of shape `(batch_size, num_channels, height, width)`.
         */
        constructor({ alphas: T }) {
          super(), this.alphas = T;
        }
      }
      class fc extends Je {
        /**
         * @param {Object} output The output of the model.
         * @param {Tensor} output.waveform The final audio waveform predicted by the model, of shape `(batch_size, sequence_length)`.
         * @param {Tensor} output.spectrogram The log-mel spectrogram predicted at the output of the flow model.
         * This spectrogram is passed to the Hi-Fi GAN decoder model to obtain the final audio waveform.
         */
        constructor({ waveform: T, spectrogram: U }) {
          super(), this.waveform = T, this.spectrogram = U;
        }
      }
    }
  ),
  /***/
  "./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js": (
    /*!******************************************************************************************************!*\
      !*** ./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js ***!
      \******************************************************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ASTFeatureExtractor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var L = s(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class j extends f.FeatureExtractor {
        constructor(W) {
          super(W);
          const w = this.config.sampling_rate, x = (0, L.mel_filter_bank)(
            256,
            // num_frequency_bins
            this.config.num_mel_bins,
            // num_mel_filters
            20,
            // min_frequency
            Math.floor(w / 2),
            // max_frequency
            w,
            // sampling_rate
            null,
            // norm
            "kaldi",
            // mel_scale
            !0
            // triangularize_in_mel_space
          );
          for (let y = 0; y < x.length; ++y)
            x[y].push(0);
          this.mel_filters = x, this.window = (0, L.window_function)(400, "hann", {
            periodic: !1
          }), this.mean = this.config.mean, this.std = this.config.std;
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @param {number} max_length The maximum number of frames to return.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(W, w) {
          return (0, L.spectrogram)(
            W,
            this.window,
            // window
            400,
            // frame_length
            160,
            // hop_length
            {
              fft_length: 512,
              power: 2,
              center: !1,
              preemphasis: 0.97,
              mel_filters: this.mel_filters,
              log_mel: "log",
              mel_floor: 1192092955078125e-22,
              remove_dc_offset: !0,
              // Custom
              max_num_frames: w,
              transpose: !0
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(W) {
          (0, f.validate_audio_inputs)(W, "ASTFeatureExtractor");
          const w = await this._extract_fbank_features(W, this.config.max_length);
          if (this.config.do_normalize) {
            const x = this.std * 2, y = w.data;
            for (let M = 0; M < y.length; ++M)
              y[M] = (y[M] - this.mean) / x;
          }
          return {
            input_values: w.unsqueeze_(0)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/auto/feature_extraction_auto.js": (
    /*!****************************************************!*\
      !*** ./src/models/auto/feature_extraction_auto.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        AutoFeatureExtractor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../utils/constants.js */
        "./src/utils/constants.js"
      ), L = s(
        /*! ../../utils/hub.js */
        "./src/utils/hub.js"
      );
      s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      var j = s(
        /*! ../feature_extractors.js */
        "./src/models/feature_extractors.js"
      );
      class J {
        /**
         * Instantiate one of the feature extractor classes of the library from a pretrained model.
         * 
         * The processor class to instantiate is selected based on the `feature_extractor_type` property of
         * the config object (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.
         * @param {import('../../utils/hub.js').PretrainedOptions} options Additional options for loading the processor.
         * 
         * @returns {Promise<AllFeatureExtractors.ImageProcessor>} A new instance of the Processor class.
         */
        /** @type {typeof FeatureExtractor.from_pretrained} */
        static async from_pretrained(w, x = {}) {
          const y = await (0, L.getModelJSON)(w, f.FEATURE_EXTRACTOR_NAME, !0, x), M = y.feature_extractor_type, b = j[M];
          if (!b)
            throw new Error(`Unknown feature_extractor_type: '${M}'. Please report this at ${f.GITHUB_ISSUE_URL}.`);
          return new b(y);
        }
      }
    }
  ),
  /***/
  "./src/models/auto/image_processing_auto.js": (
    /*!**************************************************!*\
      !*** ./src/models/auto/image_processing_auto.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        AutoImageProcessor: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../utils/constants.js */
        "./src/utils/constants.js"
      ), L = s(
        /*! ../../utils/hub.js */
        "./src/utils/hub.js"
      ), j = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), J = s(
        /*! ../image_processors.js */
        "./src/models/image_processors.js"
      );
      class W {
        /** @type {typeof ImageProcessor.from_pretrained} */
        static async from_pretrained(x, y = {}) {
          const M = await (0, L.getModelJSON)(x, f.IMAGE_PROCESSOR_NAME, !0, y), b = M.image_processor_type ?? M.feature_extractor_type;
          let D = J[b];
          return D || (b !== void 0 && console.warn(`Image processor type '${b}' not found, assuming base ImageProcessor. Please report this at ${f.GITHUB_ISSUE_URL}.`), D = j.ImageProcessor), new D(M);
        }
      }
    }
  ),
  /***/
  "./src/models/auto/processing_auto.js": (
    /*!********************************************!*\
      !*** ./src/models/auto/processing_auto.js ***!
      \********************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        AutoProcessor: () => (
          /* binding */
          x
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../utils/constants.js */
        "./src/utils/constants.js"
      ), L = s(
        /*! ../../utils/hub.js */
        "./src/utils/hub.js"
      ), j = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), J = s(
        /*! ../processors.js */
        "./src/models/processors.js"
      ), W = s(
        /*! ../image_processors.js */
        "./src/models/image_processors.js"
      ), w = s(
        /*! ../feature_extractors.js */
        "./src/models/feature_extractors.js"
      );
      class x {
        /**
         * Instantiate one of the processor classes of the library from a pretrained model.
         * 
         * The processor class to instantiate is selected based on the `image_processor_type` (or `feature_extractor_type`; legacy)
         * property of the config object (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained processor hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing processor files, e.g., `./my_model_directory/`.
         * @param {import('../../utils/hub.js').PretrainedOptions} options Additional options for loading the processor.
         * 
         * @returns {Promise<Processor>} A new instance of the Processor class.
         */
        /** @type {typeof Processor.from_pretrained} */
        static async from_pretrained(M, b = {}) {
          const D = await (0, L.getModelJSON)(M, f.IMAGE_PROCESSOR_NAME, !0, b), { image_processor_type: q, feature_extractor_type: se, processor_class: oe } = D;
          if (oe && J[oe])
            return J[oe].from_pretrained(M, b);
          if (!q && !se)
            throw new Error("No `image_processor_type` or `feature_extractor_type` found in the config.");
          const z = {};
          if (q) {
            const Y = W[q];
            if (!Y)
              throw new Error(`Unknown image_processor_type: '${q}'.`);
            z.image_processor = new Y(D);
          }
          if (se) {
            const Y = W[se];
            if (Y)
              z.image_processor = new Y(D);
            else {
              const O = w[se];
              if (!O)
                throw new Error(`Unknown feature_extractor_type: '${se}'.`);
              z.feature_extractor = new O(D);
            }
          }
          const V = {};
          return new j.Processor(V, z);
        }
      }
    }
  ),
  /***/
  "./src/models/beit/image_processing_beit.js": (
    /*!**************************************************!*\
      !*** ./src/models/beit/image_processing_beit.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        BeitFeatureExtractor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/bit/image_processing_bit.js": (
    /*!************************************************!*\
      !*** ./src/models/bit/image_processing_bit.js ***!
      \************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        BitImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/chinese_clip/image_processing_chinese_clip.js": (
    /*!******************************************************************!*\
      !*** ./src/models/chinese_clip/image_processing_chinese_clip.js ***!
      \******************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ChineseCLIPFeatureExtractor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/clap/feature_extraction_clap.js": (
    /*!****************************************************!*\
      !*** ./src/models/clap/feature_extraction_clap.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ClapFeatureExtractor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var L = s(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class j extends f.FeatureExtractor {
        constructor(W) {
          super(W), this.mel_filters = (0, L.mel_filter_bank)(
            this.config.nb_frequency_bins,
            // num_frequency_bins
            this.config.feature_size,
            // num_mel_filters
            this.config.frequency_min,
            // min_frequency
            this.config.frequency_max,
            // max_frequency
            this.config.sampling_rate,
            // sampling_rate
            null,
            // norm
            "htk"
            // mel_scale
          ), this.mel_filters_slaney = (0, L.mel_filter_bank)(
            this.config.nb_frequency_bins,
            // num_frequency_bins
            this.config.feature_size,
            // num_mel_filters
            this.config.frequency_min,
            // min_frequency
            this.config.frequency_max,
            // max_frequency
            this.config.sampling_rate,
            // sampling_rate
            "slaney",
            // norm
            "slaney"
            // mel_scale
          ), this.window = (0, L.window_function)(this.config.fft_window_size, "hann");
        }
        /**
         * Extracts the mel spectrogram and prepares it for the mode based on the `truncation` and `padding` arguments.
         * 
         * Four different path are possible:
         *   - `truncation="fusion"` and the length of the waveform is greater than the max length: the mel spectrogram
         *     will be computed on the entire audio. 3 random crops and a dowsampled version of the full mel spectrogram
         *     are then stacked together. They will later be used for `feature_fusion`.
         *   - `truncation="rand_trunc"` and the length of the waveform is smaller than the max length: the audio is
         *     padded based on `padding`.
         *   - `truncation="fusion"` and the length of the waveform is smaller than the max length: the audio is padded
         *     based on `padding`, and is repeated `4` times.
         *   - `truncation="rand_trunc"` and the length of the waveform is greater than the max length: the mel
         *     spectrogram will be computed on a random crop of the waveform.
         * 
         * @param {Float32Array|Float64Array} waveform The input waveform.
         * @param {number} max_length The maximum length of the waveform.
         * @param {string} truncation The truncation strategy to use.
         * @param {string} padding The padding strategy to use.
         * @returns {Promise<Tensor>} An object containing the mel spectrogram data as a Float32Array, its dimensions as an array of numbers, and a boolean indicating whether the waveform was longer than the max length.
         * @private
         */
        async _get_input_mel(W, w, x, y) {
          let M;
          const b = W.length - w;
          if (b > 0)
            if (x === "rand_trunc") {
              const D = Math.floor(Math.random() * (b + 1));
              W = W.subarray(D, D + w), M = await this._extract_fbank_features(W, this.mel_filters_slaney, this.config.nb_max_samples);
            } else
              throw new Error(`Truncation strategy "${x}" not implemented`);
          else {
            if (b < 0) {
              let D = new Float64Array(w);
              if (D.set(W), y === "repeat")
                for (let q = W.length; q < w; q += W.length)
                  D.set(W.subarray(0, Math.min(W.length, w - q)), q);
              else if (y === "repeatpad")
                for (let q = W.length; q < -b; q += W.length)
                  D.set(W, q);
              W = D;
            }
            if (x === "fusion")
              throw new Error(`Truncation strategy "${x}" not implemented`);
            M = await this._extract_fbank_features(W, this.mel_filters_slaney, this.config.nb_max_samples);
          }
          return M.unsqueeze_(0);
        }
        /**
         * Compute the log-mel spectrogram of the provided `waveform` using the Hann window.
         * In CLAP, two different filter banks are used depending on the truncation pattern:
         *  - `self.mel_filters`: they correspond to the default parameters of `torchaudio` which can be obtained from
         *    calling `torchaudio.transforms.MelSpectrogram().mel_scale.fb`. These filters are used when `truncation`
         *    is set to `"fusion"`.
         *  - `self.mel_filteres_slaney` : they correspond to the default parameters of `librosa` which used
         *    `librosa.filters.mel` when computing the mel spectrogram. These filters were only used in the original
         *    implementation when the truncation mode is not `"fusion"`.
         * 
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @param {number[][]} mel_filters The mel filters to use.
         * @param {number} [max_length=null] The maximum number of frames to return.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(W, w, x = null) {
          return (0, L.spectrogram)(
            W,
            this.window,
            // window
            this.config.fft_window_size,
            // frame_length
            this.config.hop_length,
            // hop_length
            {
              power: 2,
              mel_filters: w,
              log_mel: "dB",
              // Custom
              max_num_frames: x,
              do_pad: !1,
              transpose: !0
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(W, {
          max_length: w = null
        } = {}) {
          return (0, f.validate_audio_inputs)(W, "ClapFeatureExtractor"), {
            input_features: (await this._get_input_mel(
              W,
              w ?? this.config.nb_max_samples,
              this.config.truncation,
              this.config.padding
            )).unsqueeze_(0)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/clip/image_processing_clip.js": (
    /*!**************************************************!*\
      !*** ./src/models/clip/image_processing_clip.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        CLIPFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        CLIPImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/convnext/image_processing_convnext.js": (
    /*!**********************************************************!*\
      !*** ./src/models/convnext/image_processing_convnext.js ***!
      \**********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ConvNextFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        ConvNextImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        constructor(W) {
          super(W), this.crop_pct = this.config.crop_pct ?? 0.875;
        }
        async resize(W) {
          var x;
          const w = (x = this.size) == null ? void 0 : x.shortest_edge;
          if (w === void 0)
            throw new Error("Size dictionary must contain 'shortest_edge' key.");
          if (w < 384) {
            const y = Math.floor(w / this.crop_pct), [M, b] = this.get_resize_output_image_size(W, {
              shortest_edge: y
            });
            W = await W.resize(M, b, {
              resample: this.resample
            }), W = await W.center_crop(w, w);
          } else
            W = await W.resize(w, w, {
              resample: this.resample
            });
          return W;
        }
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/deit/image_processing_deit.js": (
    /*!**************************************************!*\
      !*** ./src/models/deit/image_processing_deit.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        DeiTFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        DeiTImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/detr/image_processing_detr.js": (
    /*!**************************************************!*\
      !*** ./src/models/detr/image_processing_detr.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        DetrFeatureExtractor: () => (
          /* binding */
          J
        ),
        /* harmony export */
        DetrImageProcessor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class j extends f.ImageProcessor {
        /**
         * Calls the feature extraction process on an array of images, preprocesses
         * each image, and concatenates the resulting features into a single Tensor.
         * @param {import('../../utils/image.js').RawImage[]} images The image(s) to extract features from.
         * @returns {Promise<DetrFeatureExtractorResult>} An object containing the concatenated pixel values of the preprocessed images.
         */
        async _call(w) {
          const x = await super._call(w), y = [x.pixel_values.dims[0], 64, 64], M = (0, L.full)(y, 1n);
          return { ...x, pixel_mask: M };
        }
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...w) {
          return (0, f.post_process_object_detection)(...w);
        }
        /** @type {typeof post_process_panoptic_segmentation} */
        post_process_panoptic_segmentation(...w) {
          return (0, f.post_process_panoptic_segmentation)(...w);
        }
        /** @type {typeof post_process_instance_segmentation} */
        post_process_instance_segmentation(...w) {
          return (0, f.post_process_instance_segmentation)(...w);
        }
      }
      class J extends j {
      }
    }
  ),
  /***/
  "./src/models/donut/image_processing_donut.js": (
    /*!****************************************************!*\
      !*** ./src/models/donut/image_processing_donut.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        DonutFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        DonutImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        pad_image(W, w, x, y = {}) {
          const [M, b, D] = w;
          let q = this.image_mean;
          Array.isArray(this.image_mean) || (q = new Array(D).fill(q));
          let se = this.image_std;
          Array.isArray(se) || (se = new Array(D).fill(q));
          const oe = q.map((z, V) => -z / se[V]);
          return super.pad_image(W, w, x, {
            center: !0,
            // Since normalization is done after padding, we need to use certain constant values to ensure the same behaviour is observed.
            // For more information, see https://github.com/huggingface/transformers/blob/main/src/transformers/models/donut/image_processing_donut.py#L433-L451
            constant_values: oe,
            ...y
          });
        }
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/dpt/image_processing_dpt.js": (
    /*!************************************************!*\
      !*** ./src/models/dpt/image_processing_dpt.js ***!
      \************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        DPTFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        DPTImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/efficientnet/image_processing_efficientnet.js": (
    /*!******************************************************************!*\
      !*** ./src/models/efficientnet/image_processing_efficientnet.js ***!
      \******************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        EfficientNetImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        constructor(J) {
          super(J), this.include_top = this.config.include_top ?? !0, this.include_top && (this.image_std = this.image_std.map((W) => W * W));
        }
      }
    }
  ),
  /***/
  "./src/models/feature_extractors.js": (
    /*!******************************************!*\
      !*** ./src/models/feature_extractors.js ***!
      \******************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ASTFeatureExtractor: () => (
          /* reexport safe */
          f.ASTFeatureExtractor
        ),
        /* harmony export */
        ClapFeatureExtractor: () => (
          /* reexport safe */
          L.ClapFeatureExtractor
        ),
        /* harmony export */
        ImageFeatureExtractor: () => (
          /* reexport safe */
          b.ImageProcessor
        ),
        /* harmony export */
        MoonshineFeatureExtractor: () => (
          /* reexport safe */
          j.MoonshineFeatureExtractor
        ),
        /* harmony export */
        PyAnnoteFeatureExtractor: () => (
          /* reexport safe */
          J.PyAnnoteFeatureExtractor
        ),
        /* harmony export */
        SeamlessM4TFeatureExtractor: () => (
          /* reexport safe */
          W.SeamlessM4TFeatureExtractor
        ),
        /* harmony export */
        SpeechT5FeatureExtractor: () => (
          /* reexport safe */
          w.SpeechT5FeatureExtractor
        ),
        /* harmony export */
        Wav2Vec2FeatureExtractor: () => (
          /* reexport safe */
          x.Wav2Vec2FeatureExtractor
        ),
        /* harmony export */
        WeSpeakerFeatureExtractor: () => (
          /* reexport safe */
          y.WeSpeakerFeatureExtractor
        ),
        /* harmony export */
        WhisperFeatureExtractor: () => (
          /* reexport safe */
          M.WhisperFeatureExtractor
        )
        /* harmony export */
      });
      var f = s(
        /*! ./audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js */
        "./src/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.js"
      ), L = s(
        /*! ./clap/feature_extraction_clap.js */
        "./src/models/clap/feature_extraction_clap.js"
      ), j = s(
        /*! ./moonshine/feature_extraction_moonshine.js */
        "./src/models/moonshine/feature_extraction_moonshine.js"
      ), J = s(
        /*! ./pyannote/feature_extraction_pyannote.js */
        "./src/models/pyannote/feature_extraction_pyannote.js"
      ), W = s(
        /*! ./seamless_m4t/feature_extraction_seamless_m4t.js */
        "./src/models/seamless_m4t/feature_extraction_seamless_m4t.js"
      ), w = s(
        /*! ./speecht5/feature_extraction_speecht5.js */
        "./src/models/speecht5/feature_extraction_speecht5.js"
      ), x = s(
        /*! ./wav2vec2/feature_extraction_wav2vec2.js */
        "./src/models/wav2vec2/feature_extraction_wav2vec2.js"
      ), y = s(
        /*! ./wespeaker/feature_extraction_wespeaker.js */
        "./src/models/wespeaker/feature_extraction_wespeaker.js"
      ), M = s(
        /*! ./whisper/feature_extraction_whisper.js */
        "./src/models/whisper/feature_extraction_whisper.js"
      ), b = s(
        /*! ../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
    }
  ),
  /***/
  "./src/models/florence2/processing_florence2.js": (
    /*!******************************************************!*\
      !*** ./src/models/florence2/processing_florence2.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Florence2Processor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      class J extends f.Processor {
        constructor(w, x) {
          super(w, x);
          const {
            tasks_answer_post_processing_type: y,
            task_prompts_without_inputs: M,
            task_prompts_with_input: b
          } = this.image_processor.config;
          this.tasks_answer_post_processing_type = new Map(Object.entries(y ?? {})), this.task_prompts_without_inputs = new Map(Object.entries(M ?? {})), this.task_prompts_with_input = new Map(Object.entries(b ?? {})), this.regexes = {
            quad_boxes: /(.+?)<loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)>/gm,
            bboxes: /([^<]+)?<loc_(\d+)><loc_(\d+)><loc_(\d+)><loc_(\d+)>/gm
          }, this.size_per_bin = 1e3;
        }
        /**
         * Helper function to construct prompts from input texts
         * @param {string|string[]} text
         * @returns {string[]}
         */
        construct_prompts(w) {
          typeof w == "string" && (w = [w]);
          const x = [];
          for (const y of w)
            if (this.task_prompts_without_inputs.has(y))
              x.push(this.task_prompts_without_inputs.get(y));
            else {
              for (const [M, b] of this.task_prompts_with_input)
                if (y.includes(M)) {
                  x.push(b.replaceAll("{input}", y).replaceAll(M, ""));
                  break;
                }
              x.length !== w.length && x.push(y);
            }
          return x;
        }
        /**
         * Post-process the output of the model to each of the task outputs.
         * @param {string} text The text to post-process.
         * @param {string} task The task to post-process the text for.
         * @param {[number, number]} image_size The size of the image. height x width.
         */
        post_process_generation(w, x, y) {
          const M = this.tasks_answer_post_processing_type.get(x) ?? "pure_text";
          w = w.replaceAll("<s>", "").replaceAll("</s>", "");
          let b;
          switch (M) {
            case "pure_text":
              b = w;
              break;
            case "description_with_bboxes":
            case "bboxes":
            case "phrase_grounding":
            case "ocr":
              const D = M === "ocr" ? "quad_boxes" : "bboxes", q = w.matchAll(this.regexes[D]), se = [], oe = [];
              for (const [z, V, ...Y] of q)
                se.push(V ? V.trim() : se.at(-1) ?? ""), oe.push(
                  Y.map((O, $) => (
                    // NOTE: Add 0.5 to use the center position of the bin as the coordinate.
                    (Number(O) + 0.5) / this.size_per_bin * y[$ % 2]
                  ))
                );
              b = { labels: se, [D]: oe };
              break;
            default:
              throw new Error(`Task "${x}" (of type "${M}") not yet implemented.`);
          }
          return { [x]: b };
        }
        // NOTE: images and text are switched from the python version
        // `images` is required, `text` is optional
        async _call(w, x = null, y = {}) {
          if (!w && !x)
            throw new Error("Either text or images must be provided");
          const M = await this.image_processor(w, y), b = x ? this.tokenizer(x, y) : {};
          return {
            ...M,
            ...b
          };
        }
      }
      ge(J, "tokenizer_class", j.AutoTokenizer), ge(J, "image_processor_class", L.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/glpn/image_processing_glpn.js": (
    /*!**************************************************!*\
      !*** ./src/models/glpn/image_processing_glpn.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        GLPNFeatureExtractor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/idefics3/image_processing_idefics3.js": (
    /*!**********************************************************!*\
      !*** ./src/models/idefics3/image_processing_idefics3.js ***!
      \**********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Idefics3ImageProcessor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class j extends f.ImageProcessor {
        constructor(W) {
          super(W), this.do_image_splitting = W.do_image_splitting ?? !0, this.max_image_size = W.max_image_size;
        }
        /**
         * @typedef {import('../../utils/image.js').RawImage} RawImage
         * @typedef {import('../../utils/tensor.js').Tensor} Tensor
         */
        /**
         * Calculate size to resize images to, to be multiples of `vision_encoder_max_size` while preserving the aspect ratio.
         * @param {Tensor} pixel_values Tensor of the image to resize.
         * @param {number} vision_encoder_max_size Maximum size of the output image. If the image is larger than this size,
         * it will be split into patches of this size, and the original image will be concatenated with the patches, resized to max_size.
         */
        get_resize_for_vision_encoder(W, w) {
          let [x, y] = W.dims.slice(-2);
          const M = y / x;
          return y >= x ? (y = Math.ceil(y / w) * w, x = Math.floor(y / M), x = Math.ceil(x / w) * w) : (x = Math.ceil(x / w) * w, y = Math.floor(x * M), y = Math.ceil(y / w) * w), { height: x, width: y };
        }
        /** @param {RawImage|RawImage[]|RawImage[][]} images */
        async _call(W, {
          do_image_splitting: w = null,
          return_row_col_info: x = !1
        } = {}) {
          let y;
          if (!Array.isArray(W))
            y = [[W]];
          else {
            if (W.length === 0 || !W[0])
              throw new Error("No images provided.");
            Array.isArray(W[0]) ? y = /** @type {RawImage[][]} */
            W : y = [
              /** @type {RawImage[]} */
              W
            ];
          }
          let M = [], b = [], D = [];
          const q = [], se = [];
          for (const C of y) {
            let v = await Promise.all(C.map((le) => this.preprocess(le)));
            q.push(...v.map((le) => le.original_size)), se.push(...v.map((le) => le.reshaped_input_size)), v.forEach((le) => le.pixel_values.unsqueeze_(0));
            const { longest_edge: ee } = this.max_image_size;
            let X;
            if (w ?? this.do_image_splitting) {
              let le = new Array(v.length), ue = new Array(v.length);
              X = await Promise.all(v.map(async (fe, Ce) => {
                const xe = this.get_resize_for_vision_encoder(fe.pixel_values, ee), Le = await (0, L.interpolate_4d)(fe.pixel_values, {
                  size: [xe.height, xe.width]
                }), { frames: qe, num_splits_h: Ue, num_splits_w: ut } = await this.split_image(Le, this.max_image_size);
                return le[Ce] = Ue, ue[Ce] = ut, (0, L.cat)(qe, 0);
              })), b.push(le), D.push(ue);
            } else {
              const le = [ee, ee];
              X = await Promise.all(
                v.map((ue) => (0, L.interpolate_4d)(ue.pixel_values, { size: le }))
              ), b.push(new Array(v.length).fill(0)), D.push(new Array(v.length).fill(0));
            }
            M.push((0, L.cat)(X, 0));
          }
          const oe = M.length, [z, V, Y, O] = M[0].dims;
          let $, g;
          if (oe === 1)
            $ = M[0].unsqueeze_(0), g = (0, L.full)([oe, z, Y, O], !0);
          else {
            const C = Math.max(...M.map((X) => X.dims.at(0)));
            g = (0, L.full)([oe, C, Y, O], !0);
            const v = g.data, ee = C * Y * O;
            for (let X = 0; X < oe; ++X) {
              const le = M[X].dims[0];
              if (le < C) {
                M[X] = (0, L.cat)([
                  M[X],
                  (0, L.full)([C - le, V, Y, O], 0)
                ], 0);
                const ue = X * ee + le * Y * O, fe = (X + 1) * ee;
                v.fill(!1, ue, fe);
              }
            }
            $ = (0, L.stack)(M, 0);
          }
          return {
            pixel_values: $,
            pixel_attention_mask: g,
            original_sizes: q,
            reshaped_input_sizes: se,
            ...x ? { rows: b, cols: D } : {}
          };
        }
        async split_image(W, { longest_edge: w }) {
          const x = w, y = w, M = [], [b, D] = W.dims.slice(-2);
          let q = 0, se = 0;
          if (b > x || D > y) {
            q = Math.ceil(b / x), se = Math.ceil(D / y);
            const oe = Math.ceil(b / q), z = Math.ceil(D / se);
            for (let O = 0; O < q; ++O)
              for (let $ = 0; $ < se; ++$) {
                let g, C, v, ee;
                O === q - 1 ? (C = b - oe, ee = b) : (C = O * oe, ee = (O + 1) * oe), $ === se - 1 ? (g = D - z, v = D) : (g = $ * z, v = ($ + 1) * z);
                const X = [C, g], le = [ee, v], ue = await (0, L.slice)(W, X, le, [2, 3]);
                M.push(ue);
              }
            const V = x, Y = y;
            (b !== V || D !== Y) && (W = await (0, L.interpolate_4d)(W, {
              size: [V, Y]
            }));
          }
          return M.push(W), { frames: M, num_splits_h: q, num_splits_w: se };
        }
      }
    }
  ),
  /***/
  "./src/models/idefics3/processing_idefics3.js": (
    /*!****************************************************!*\
      !*** ./src/models/idefics3/processing_idefics3.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Idefics3Processor: () => (
          /* binding */
          y
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      s(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      var J = s(
        /*! ../../utils/core.js */
        "./src/utils/core.js"
      );
      function W(M, b, D, q, se, oe) {
        let z = "";
        for (let V = 0; V < b; ++V) {
          for (let Y = 0; Y < D; ++Y)
            z += q + `<row_${V + 1}_col_${Y + 1}>` + se.repeat(M);
          z += `
`;
        }
        return z += `
${q}${oe}` + se.repeat(M) + `${q}`, z;
      }
      function w(M, b, D, q) {
        return `${b}${q}` + D.repeat(M) + `${b}`;
      }
      function x(M, b, D, q, se, oe) {
        return M === 0 && b === 0 ? w(
          D,
          q,
          se,
          oe
        ) : W(
          D,
          M,
          b,
          q,
          se,
          oe
        );
      }
      class y extends f.Processor {
        constructor() {
          super(...arguments);
          ge(this, "fake_image_token", "<fake_token_around_image>");
          ge(this, "image_token", "<image>");
          ge(this, "global_img_token", "<global-img>");
        }
        /**
         * 
         * @param {string|string[]} text 
         * @param {RawImage|RawImage[]|RawImage[][]} images  
         * @returns {Promise<any>}
         */
        async _call(D, q = null, se = {}) {
          se.return_row_col_info ?? (se.return_row_col_info = !0);
          let oe;
          q && (oe = await this.image_processor(q, se)), Array.isArray(D) || (D = [D]);
          const z = oe.rows ?? [new Array(D.length).fill(0)], V = oe.cols ?? [new Array(D.length).fill(0)], Y = this.config.image_seq_len, O = [], $ = [];
          for (let C = 0; C < D.length; ++C) {
            const v = D[C], ee = z[C], X = V[C];
            O.push((0, J.count)(v, this.image_token));
            const le = ee.map(
              (Ce, xe) => x(
                Ce,
                X[xe],
                Y,
                this.fake_image_token,
                this.image_token,
                this.global_img_token
              )
            ), ue = v.split(this.image_token);
            if (ue.length === 0)
              throw new Error("The image token should be present in the text.");
            let fe = ue[0];
            for (let Ce = 0; Ce < le.length; ++Ce)
              fe += le[Ce] + ue[Ce + 1];
            $.push(fe);
          }
          return {
            ...this.tokenizer($),
            ...oe
          };
        }
      }
      ge(y, "image_processor_class", L.AutoImageProcessor), ge(y, "tokenizer_class", j.AutoTokenizer), ge(y, "uses_processor_config", !0);
    }
  ),
  /***/
  "./src/models/image_processors.js": (
    /*!****************************************!*\
      !*** ./src/models/image_processors.js ***!
      \****************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        BeitFeatureExtractor: () => (
          /* reexport safe */
          f.BeitFeatureExtractor
        ),
        /* harmony export */
        BitImageProcessor: () => (
          /* reexport safe */
          L.BitImageProcessor
        ),
        /* harmony export */
        CLIPFeatureExtractor: () => (
          /* reexport safe */
          J.CLIPFeatureExtractor
        ),
        /* harmony export */
        CLIPImageProcessor: () => (
          /* reexport safe */
          J.CLIPImageProcessor
        ),
        /* harmony export */
        ChineseCLIPFeatureExtractor: () => (
          /* reexport safe */
          j.ChineseCLIPFeatureExtractor
        ),
        /* harmony export */
        ConvNextFeatureExtractor: () => (
          /* reexport safe */
          W.ConvNextFeatureExtractor
        ),
        /* harmony export */
        ConvNextImageProcessor: () => (
          /* reexport safe */
          W.ConvNextImageProcessor
        ),
        /* harmony export */
        DPTFeatureExtractor: () => (
          /* reexport safe */
          M.DPTFeatureExtractor
        ),
        /* harmony export */
        DPTImageProcessor: () => (
          /* reexport safe */
          M.DPTImageProcessor
        ),
        /* harmony export */
        DeiTFeatureExtractor: () => (
          /* reexport safe */
          w.DeiTFeatureExtractor
        ),
        /* harmony export */
        DeiTImageProcessor: () => (
          /* reexport safe */
          w.DeiTImageProcessor
        ),
        /* harmony export */
        DetrFeatureExtractor: () => (
          /* reexport safe */
          x.DetrFeatureExtractor
        ),
        /* harmony export */
        DetrImageProcessor: () => (
          /* reexport safe */
          x.DetrImageProcessor
        ),
        /* harmony export */
        DonutFeatureExtractor: () => (
          /* reexport safe */
          y.DonutFeatureExtractor
        ),
        /* harmony export */
        DonutImageProcessor: () => (
          /* reexport safe */
          y.DonutImageProcessor
        ),
        /* harmony export */
        EfficientNetImageProcessor: () => (
          /* reexport safe */
          b.EfficientNetImageProcessor
        ),
        /* harmony export */
        GLPNFeatureExtractor: () => (
          /* reexport safe */
          D.GLPNFeatureExtractor
        ),
        /* harmony export */
        Idefics3ImageProcessor: () => (
          /* reexport safe */
          q.Idefics3ImageProcessor
        ),
        /* harmony export */
        JinaCLIPImageProcessor: () => (
          /* reexport safe */
          oe.JinaCLIPImageProcessor
        ),
        /* harmony export */
        LlavaOnevisionImageProcessor: () => (
          /* reexport safe */
          z.LlavaOnevisionImageProcessor
        ),
        /* harmony export */
        Mask2FormerImageProcessor: () => (
          /* reexport safe */
          V.Mask2FormerImageProcessor
        ),
        /* harmony export */
        MaskFormerFeatureExtractor: () => (
          /* reexport safe */
          Y.MaskFormerFeatureExtractor
        ),
        /* harmony export */
        MaskFormerImageProcessor: () => (
          /* reexport safe */
          Y.MaskFormerImageProcessor
        ),
        /* harmony export */
        MobileNetV1FeatureExtractor: () => (
          /* reexport safe */
          O.MobileNetV1FeatureExtractor
        ),
        /* harmony export */
        MobileNetV1ImageProcessor: () => (
          /* reexport safe */
          O.MobileNetV1ImageProcessor
        ),
        /* harmony export */
        MobileNetV2FeatureExtractor: () => (
          /* reexport safe */
          $.MobileNetV2FeatureExtractor
        ),
        /* harmony export */
        MobileNetV2ImageProcessor: () => (
          /* reexport safe */
          $.MobileNetV2ImageProcessor
        ),
        /* harmony export */
        MobileNetV3FeatureExtractor: () => (
          /* reexport safe */
          g.MobileNetV3FeatureExtractor
        ),
        /* harmony export */
        MobileNetV3ImageProcessor: () => (
          /* reexport safe */
          g.MobileNetV3ImageProcessor
        ),
        /* harmony export */
        MobileNetV4FeatureExtractor: () => (
          /* reexport safe */
          C.MobileNetV4FeatureExtractor
        ),
        /* harmony export */
        MobileNetV4ImageProcessor: () => (
          /* reexport safe */
          C.MobileNetV4ImageProcessor
        ),
        /* harmony export */
        MobileViTFeatureExtractor: () => (
          /* reexport safe */
          v.MobileViTFeatureExtractor
        ),
        /* harmony export */
        MobileViTImageProcessor: () => (
          /* reexport safe */
          v.MobileViTImageProcessor
        ),
        /* harmony export */
        NougatImageProcessor: () => (
          /* reexport safe */
          ee.NougatImageProcessor
        ),
        /* harmony export */
        OwlViTFeatureExtractor: () => (
          /* reexport safe */
          le.OwlViTFeatureExtractor
        ),
        /* harmony export */
        OwlViTImageProcessor: () => (
          /* reexport safe */
          le.OwlViTImageProcessor
        ),
        /* harmony export */
        Owlv2ImageProcessor: () => (
          /* reexport safe */
          X.Owlv2ImageProcessor
        ),
        /* harmony export */
        Phi3VImageProcessor: () => (
          /* reexport safe */
          ue.Phi3VImageProcessor
        ),
        /* harmony export */
        PvtImageProcessor: () => (
          /* reexport safe */
          fe.PvtImageProcessor
        ),
        /* harmony export */
        Qwen2VLImageProcessor: () => (
          /* reexport safe */
          Ce.Qwen2VLImageProcessor
        ),
        /* harmony export */
        RTDetrImageProcessor: () => (
          /* reexport safe */
          xe.RTDetrImageProcessor
        ),
        /* harmony export */
        SamImageProcessor: () => (
          /* reexport safe */
          Le.SamImageProcessor
        ),
        /* harmony export */
        SegformerFeatureExtractor: () => (
          /* reexport safe */
          qe.SegformerFeatureExtractor
        ),
        /* harmony export */
        SegformerImageProcessor: () => (
          /* reexport safe */
          qe.SegformerImageProcessor
        ),
        /* harmony export */
        SiglipImageProcessor: () => (
          /* reexport safe */
          Ue.SiglipImageProcessor
        ),
        /* harmony export */
        Swin2SRImageProcessor: () => (
          /* reexport safe */
          ut.Swin2SRImageProcessor
        ),
        /* harmony export */
        VLMImageProcessor: () => (
          /* reexport safe */
          se.VLMImageProcessor
        ),
        /* harmony export */
        ViTFeatureExtractor: () => (
          /* reexport safe */
          de.ViTFeatureExtractor
        ),
        /* harmony export */
        ViTImageProcessor: () => (
          /* reexport safe */
          de.ViTImageProcessor
        ),
        /* harmony export */
        VitMatteImageProcessor: () => (
          /* reexport safe */
          re.VitMatteImageProcessor
        ),
        /* harmony export */
        VitPoseImageProcessor: () => (
          /* reexport safe */
          he.VitPoseImageProcessor
        ),
        /* harmony export */
        YolosFeatureExtractor: () => (
          /* reexport safe */
          Ee.YolosFeatureExtractor
        ),
        /* harmony export */
        YolosImageProcessor: () => (
          /* reexport safe */
          Ee.YolosImageProcessor
        )
        /* harmony export */
      });
      var f = s(
        /*! ./beit/image_processing_beit.js */
        "./src/models/beit/image_processing_beit.js"
      ), L = s(
        /*! ./bit/image_processing_bit.js */
        "./src/models/bit/image_processing_bit.js"
      ), j = s(
        /*! ./chinese_clip/image_processing_chinese_clip.js */
        "./src/models/chinese_clip/image_processing_chinese_clip.js"
      ), J = s(
        /*! ./clip/image_processing_clip.js */
        "./src/models/clip/image_processing_clip.js"
      ), W = s(
        /*! ./convnext/image_processing_convnext.js */
        "./src/models/convnext/image_processing_convnext.js"
      ), w = s(
        /*! ./deit/image_processing_deit.js */
        "./src/models/deit/image_processing_deit.js"
      ), x = s(
        /*! ./detr/image_processing_detr.js */
        "./src/models/detr/image_processing_detr.js"
      ), y = s(
        /*! ./donut/image_processing_donut.js */
        "./src/models/donut/image_processing_donut.js"
      ), M = s(
        /*! ./dpt/image_processing_dpt.js */
        "./src/models/dpt/image_processing_dpt.js"
      ), b = s(
        /*! ./efficientnet/image_processing_efficientnet.js */
        "./src/models/efficientnet/image_processing_efficientnet.js"
      ), D = s(
        /*! ./glpn/image_processing_glpn.js */
        "./src/models/glpn/image_processing_glpn.js"
      ), q = s(
        /*! ./idefics3/image_processing_idefics3.js */
        "./src/models/idefics3/image_processing_idefics3.js"
      ), se = s(
        /*! ./janus/image_processing_janus.js */
        "./src/models/janus/image_processing_janus.js"
      ), oe = s(
        /*! ./jina_clip/image_processing_jina_clip.js */
        "./src/models/jina_clip/image_processing_jina_clip.js"
      ), z = s(
        /*! ./llava_onevision/image_processing_llava_onevision.js */
        "./src/models/llava_onevision/image_processing_llava_onevision.js"
      ), V = s(
        /*! ./mask2former/image_processing_mask2former.js */
        "./src/models/mask2former/image_processing_mask2former.js"
      ), Y = s(
        /*! ./maskformer/image_processing_maskformer.js */
        "./src/models/maskformer/image_processing_maskformer.js"
      ), O = s(
        /*! ./mobilenet_v1/image_processing_mobilenet_v1.js */
        "./src/models/mobilenet_v1/image_processing_mobilenet_v1.js"
      ), $ = s(
        /*! ./mobilenet_v2/image_processing_mobilenet_v2.js */
        "./src/models/mobilenet_v2/image_processing_mobilenet_v2.js"
      ), g = s(
        /*! ./mobilenet_v3/image_processing_mobilenet_v3.js */
        "./src/models/mobilenet_v3/image_processing_mobilenet_v3.js"
      ), C = s(
        /*! ./mobilenet_v4/image_processing_mobilenet_v4.js */
        "./src/models/mobilenet_v4/image_processing_mobilenet_v4.js"
      ), v = s(
        /*! ./mobilevit/image_processing_mobilevit.js */
        "./src/models/mobilevit/image_processing_mobilevit.js"
      ), ee = s(
        /*! ./nougat/image_processing_nougat.js */
        "./src/models/nougat/image_processing_nougat.js"
      ), X = s(
        /*! ./owlv2/image_processing_owlv2.js */
        "./src/models/owlv2/image_processing_owlv2.js"
      ), le = s(
        /*! ./owlvit/image_processing_owlvit.js */
        "./src/models/owlvit/image_processing_owlvit.js"
      ), ue = s(
        /*! ./phi3_v/image_processing_phi3_v.js */
        "./src/models/phi3_v/image_processing_phi3_v.js"
      ), fe = s(
        /*! ./pvt/image_processing_pvt.js */
        "./src/models/pvt/image_processing_pvt.js"
      ), Ce = s(
        /*! ./qwen2_vl/image_processing_qwen2_vl.js */
        "./src/models/qwen2_vl/image_processing_qwen2_vl.js"
      ), xe = s(
        /*! ./rt_detr/image_processing_rt_detr.js */
        "./src/models/rt_detr/image_processing_rt_detr.js"
      ), Le = s(
        /*! ./sam/image_processing_sam.js */
        "./src/models/sam/image_processing_sam.js"
      ), qe = s(
        /*! ./segformer/image_processing_segformer.js */
        "./src/models/segformer/image_processing_segformer.js"
      ), Ue = s(
        /*! ./siglip/image_processing_siglip.js */
        "./src/models/siglip/image_processing_siglip.js"
      ), ut = s(
        /*! ./swin2sr/image_processing_swin2sr.js */
        "./src/models/swin2sr/image_processing_swin2sr.js"
      ), de = s(
        /*! ./vit/image_processing_vit.js */
        "./src/models/vit/image_processing_vit.js"
      ), re = s(
        /*! ./vitmatte/image_processing_vitmatte.js */
        "./src/models/vitmatte/image_processing_vitmatte.js"
      ), he = s(
        /*! ./vitpose/image_processing_vitpose.js */
        "./src/models/vitpose/image_processing_vitpose.js"
      ), Ee = s(
        /*! ./yolos/image_processing_yolos.js */
        "./src/models/yolos/image_processing_yolos.js"
      );
    }
  ),
  /***/
  "./src/models/janus/image_processing_janus.js": (
    /*!****************************************************!*\
      !*** ./src/models/janus/image_processing_janus.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        VLMImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        constructor(J) {
          super({
            do_pad: !0,
            pad_size: {
              width: J.image_size,
              height: J.image_size
            },
            ...J
          }), this.constant_values = this.config.background_color.map((W) => W * this.rescale_factor);
        }
        pad_image(J, W, w, x) {
          return super.pad_image(J, W, w, {
            constant_values: this.constant_values,
            center: !0,
            ...x
          });
        }
      }
    }
  ),
  /***/
  "./src/models/janus/processing_janus.js": (
    /*!**********************************************!*\
      !*** ./src/models/janus/processing_janus.js ***!
      \**********************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        VLChatProcessor: () => (
          /* binding */
          x
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), J = s(
        /*! ../../utils/core.js */
        "./src/utils/core.js"
      ), W = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      ), w = s(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      class x extends f.Processor {
        constructor(M, b) {
          super(M, b), this.image_tag = this.config.image_tag, this.image_start_tag = this.config.image_start_tag, this.image_end_tag = this.config.image_end_tag, this.num_image_tokens = this.config.num_image_tokens;
        }
        /**
         * @typedef {Object} MultimodalMessageProperties Additional properties for multimodal messages.
         * @property {(RawImage | string | URL)[]} [images] The images in the message.
         * @typedef {(import('../../tokenizers.js').Message & MultimodalMessageProperties)[]} MultimodalConversation The conversation possibly containing multimodal inputs.
         */
        /**
         * @typedef {Object} VLCChatProcessorResult The processed input.
         * @property {Tensor} input_ids The input IDs.
         * @property {Tensor} attention_mask The attention mask.
         * @property {Tensor} images_seq_mask The image sequence mask.
         * @property {Tensor} images_emb_mask The image embedding mask.
         */
        /**
         * @param {MultimodalConversation} conversation The chat messages to process.
         * @param {Object} options Additional options for processing.
         * @param {RawImage|RawImage[]} [options.images] The images to process, if not set in the conversation.
         * @param {string} [options.chat_template="default"] The chat template to use.
         * @returns {Promise<VLCChatProcessorResult | VLCChatProcessorResult & import('../../base/image_processors_utils.js').ImageProcessorResult>} The processed input.
         */
        async _call(M, {
          images: b = null,
          chat_template: D = "default"
        } = {}) {
          b ? Array.isArray(b) || (b = [b]) : b = await Promise.all(
            M.filter((X) => X.images).flatMap((X) => X.images).map((X) => w.RawImage.read(X))
          );
          const q = this.tokenizer, se = q.apply_chat_template(M, {
            tokenize: !1,
            add_generation_prompt: !0,
            chat_template: D
          }), oe = (X) => q.encode(X, { add_special_tokens: !1 }), z = (
            /** @type {string} */
            se.split(this.image_tag)
          ), V = z.length - 1;
          if (b.length !== V)
            throw new Error(`Number of images provided (${b.length}) does not match number of "${this.image_tag}" image tags (${V})`);
          const [
            Y,
            O,
            $
          ] = q.model.convert_tokens_to_ids([
            this.image_tag,
            this.image_start_tag,
            this.image_end_tag
          ]);
          let g = oe(z[0]), C = new Array(g.length).fill(!1);
          for (let X = 1; X < z.length; ++X) {
            const le = new Array(this.num_image_tokens).fill(Y), ue = oe(z[X]);
            g = (0, J.mergeArrays)(
              g,
              [O],
              le,
              [$],
              ue
            );
            const fe = new Array(this.num_image_tokens).fill(!0);
            C = (0, J.mergeArrays)(
              C,
              [!1],
              fe,
              [!1],
              new Array(ue.length).fill(!1)
            );
          }
          const v = [1, g.length], ee = {
            input_ids: new W.Tensor("int64", g, v),
            attention_mask: new W.Tensor("int64", new Array(g.length).fill(1), v),
            images_seq_mask: new W.Tensor("bool", C, v),
            images_emb_mask: new W.Tensor(
              "bool",
              new Array(V * this.num_image_tokens).fill(!0),
              [1, V, this.num_image_tokens]
            )
          };
          if (b && b.length > 0) {
            const X = await this.image_processor(b);
            return X.pixel_values.unsqueeze_(0), { ...ee, ...X };
          }
          return ee;
        }
      }
      ge(x, "image_processor_class", L.AutoImageProcessor), ge(x, "tokenizer_class", j.AutoTokenizer), ge(x, "uses_processor_config", !0);
    }
  ),
  /***/
  "./src/models/jina_clip/image_processing_jina_clip.js": (
    /*!************************************************************!*\
      !*** ./src/models/jina_clip/image_processing_jina_clip.js ***!
      \************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        JinaCLIPImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        constructor(J) {
          const { resize_mode: W, fill_color: w, interpolation: x, size: y, ...M } = J, b = W === "squash" ? { width: y, height: y } : W === "shortest" ? { shortest_edge: y } : { longest_edge: y }, D = x === "bicubic" ? 3 : 2;
          super({
            ...M,
            size: b,
            resample: D,
            do_center_crop: !0,
            crop_size: y,
            do_normalize: !0
          });
        }
      }
    }
  ),
  /***/
  "./src/models/jina_clip/processing_jina_clip.js": (
    /*!******************************************************!*\
      !*** ./src/models/jina_clip/processing_jina_clip.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        JinaCLIPProcessor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      class J extends f.Processor {
        async _call(w = null, x = null, y = {}) {
          if (!w && !x)
            throw new Error("Either text or images must be provided");
          const M = w ? this.tokenizer(w, y) : {}, b = x ? await this.image_processor(x, y) : {};
          return {
            ...M,
            ...b
          };
        }
      }
      ge(J, "tokenizer_class", j.AutoTokenizer), ge(J, "image_processor_class", L.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/llava_onevision/image_processing_llava_onevision.js": (
    /*!************************************************************************!*\
      !*** ./src/models/llava_onevision/image_processing_llava_onevision.js ***!
      \************************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        LlavaOnevisionImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/mask2former/image_processing_mask2former.js": (
    /*!****************************************************************!*\
      !*** ./src/models/mask2former/image_processing_mask2former.js ***!
      \****************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Mask2FormerImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../maskformer/image_processing_maskformer.js */
        "./src/models/maskformer/image_processing_maskformer.js"
      );
      class L extends f.MaskFormerImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/maskformer/image_processing_maskformer.js": (
    /*!**************************************************************!*\
      !*** ./src/models/maskformer/image_processing_maskformer.js ***!
      \**************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MaskFormerFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        MaskFormerImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        /** @type {typeof post_process_panoptic_segmentation} */
        post_process_panoptic_segmentation(...W) {
          return (0, f.post_process_panoptic_segmentation)(...W);
        }
        /** @type {typeof post_process_instance_segmentation} */
        post_process_instance_segmentation(...W) {
          return (0, f.post_process_instance_segmentation)(...W);
        }
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/mgp_str/processing_mgp_str.js": (
    /*!**************************************************!*\
      !*** ./src/models/mgp_str/processing_mgp_str.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MgpstrProcessor: () => (
          /* binding */
          w
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), J = s(
        /*! ../../utils/maths.js */
        "./src/utils/maths.js"
      );
      const W = {
        char: ["char_decode", 1],
        bpe: ["bpe_decode", 2],
        wp: ["wp_decode", 102]
      };
      class w extends f.Processor {
        /**
         * @returns {import('../../tokenizers.js').MgpstrTokenizer} The character tokenizer.
         */
        get char_tokenizer() {
          return this.components.char_tokenizer;
        }
        /**
         * @returns {import('../../tokenizers.js').GPT2Tokenizer} The BPE tokenizer.
         */
        get bpe_tokenizer() {
          return this.components.bpe_tokenizer;
        }
        /**
         * @returns {import('../../tokenizers.js').BertTokenizer} The WordPiece tokenizer.
         */
        get wp_tokenizer() {
          return this.components.wp_tokenizer;
        }
        /**
         * Helper function to decode the model prediction logits.
         * @param {import('../../utils/tensor.js').Tensor} pred_logits Model prediction logits.
         * @param {string} format Type of model prediction. Must be one of ['char', 'bpe', 'wp'].
         * @returns {[string[], number[]]} The decoded sentences and their confidence scores.
         */
        _decode_helper(y, M) {
          if (!W.hasOwnProperty(M))
            throw new Error(`Format ${M} is not supported.`);
          const [b, D] = W[M], q = this[b].bind(this), [se, oe] = y.dims, z = [], V = [], Y = y.tolist();
          for (let $ = 0; $ < se; ++$) {
            const g = Y[$], C = [], v = [];
            for (let X = 1; X < oe; ++X) {
              const [le, ue] = (0, J.max)((0, J.softmax)(g[X]));
              if (v.push(le), ue == D)
                break;
              C.push(ue);
            }
            const ee = v.length > 0 ? v.reduce((X, le) => X * le, 1) : 0;
            V.push(C), z.push(ee);
          }
          return [q(V), z];
        }
        /**
         * Convert a list of lists of char token ids into a list of strings by calling char tokenizer.
         * @param {number[][]} sequences List of tokenized input ids.
         * @returns {string[]} The list of char decoded sentences.
         */
        char_decode(y) {
          return this.char_tokenizer.batch_decode(y).map((M) => M.replaceAll(" ", ""));
        }
        /**
         * Convert a list of lists of BPE token ids into a list of strings by calling BPE tokenizer.
         * @param {number[][]} sequences List of tokenized input ids.
         * @returns {string[]} The list of BPE decoded sentences.
         */
        bpe_decode(y) {
          return this.bpe_tokenizer.batch_decode(y);
        }
        /**
         * Convert a list of lists of word piece token ids into a list of strings by calling word piece tokenizer.
         * @param {number[][]} sequences List of tokenized input ids.
         * @returns {string[]} The list of wp decoded sentences.
         */
        wp_decode(y) {
          return this.wp_tokenizer.batch_decode(y).map((M) => M.replaceAll(" ", ""));
        }
        /**
         * Convert a list of lists of token ids into a list of strings by calling decode.
         * @param {import('../../utils/tensor.js').Tensor[]} sequences List of tokenized input ids.
         * @returns {{generated_text: string[], scores: number[], char_preds: string[], bpe_preds: string[], wp_preds: string[]}}
         * Dictionary of all the outputs of the decoded results.
         * - generated_text: The final results after fusion of char, bpe, and wp.
         * - scores: The final scores after fusion of char, bpe, and wp.
         * - char_preds: The list of character decoded sentences.
         * - bpe_preds: The list of BPE decoded sentences.
         * - wp_preds: The list of wp decoded sentences.
         */
        batch_decode([y, M, b]) {
          const [D, q] = this._decode_helper(y, "char"), [se, oe] = this._decode_helper(M, "bpe"), [z, V] = this._decode_helper(b, "wp"), Y = [], O = [];
          for (let $ = 0; $ < D.length; ++$) {
            const [g, C] = (0, J.max)([q[$], oe[$], V[$]]);
            Y.push([D[$], se[$], z[$]][C]), O.push(g);
          }
          return {
            generated_text: Y,
            scores: O,
            char_preds: D,
            bpe_preds: se,
            wp_preds: z
          };
        }
        /** @type {typeof Processor.from_pretrained} */
        static async from_pretrained(...y) {
          const M = await super.from_pretrained(...y), b = await j.AutoTokenizer.from_pretrained("Xenova/gpt2"), D = await j.AutoTokenizer.from_pretrained("Xenova/bert-base-uncased");
          return M.components = {
            image_processor: M.image_processor,
            char_tokenizer: M.tokenizer,
            bpe_tokenizer: b,
            wp_tokenizer: D
          }, M;
        }
        async _call(y, M = null) {
          const b = await this.image_processor(y);
          return M && (b.labels = this.tokenizer(M).input_ids), b;
        }
      }
      ge(w, "tokenizer_class", j.AutoTokenizer), ge(w, "image_processor_class", L.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/mobilenet_v1/image_processing_mobilenet_v1.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v1/image_processing_mobilenet_v1.js ***!
      \******************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MobileNetV1FeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        MobileNetV1ImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/mobilenet_v2/image_processing_mobilenet_v2.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v2/image_processing_mobilenet_v2.js ***!
      \******************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MobileNetV2FeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        MobileNetV2ImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/mobilenet_v3/image_processing_mobilenet_v3.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v3/image_processing_mobilenet_v3.js ***!
      \******************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MobileNetV3FeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        MobileNetV3ImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/mobilenet_v4/image_processing_mobilenet_v4.js": (
    /*!******************************************************************!*\
      !*** ./src/models/mobilenet_v4/image_processing_mobilenet_v4.js ***!
      \******************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MobileNetV4FeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        MobileNetV4ImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/mobilevit/image_processing_mobilevit.js": (
    /*!************************************************************!*\
      !*** ./src/models/mobilevit/image_processing_mobilevit.js ***!
      \************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MobileViTFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        MobileViTImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/moonshine/feature_extraction_moonshine.js": (
    /*!**************************************************************!*\
      !*** ./src/models/moonshine/feature_extraction_moonshine.js ***!
      \**************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MoonshineFeatureExtractor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class j extends f.FeatureExtractor {
        /**
         * Asynchronously extracts input values from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor; }>} The extracted input values.
         */
        async _call(W) {
          (0, f.validate_audio_inputs)(W, "MoonshineFeatureExtractor"), W instanceof Float64Array && (W = new Float32Array(W));
          const w = [
            1,
            /* batch_size */
            W.length
            /* num_samples */
          ];
          return {
            input_values: new L.Tensor("float32", W, w)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/moonshine/processing_moonshine.js": (
    /*!******************************************************!*\
      !*** ./src/models/moonshine/processing_moonshine.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        MoonshineProcessor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      ), L = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), j = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      class J extends j.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(w) {
          return await this.feature_extractor(w);
        }
      }
      ge(J, "tokenizer_class", L.AutoTokenizer), ge(J, "feature_extractor_class", f.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/nougat/image_processing_nougat.js": (
    /*!******************************************************!*\
      !*** ./src/models/nougat/image_processing_nougat.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        NougatImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../donut/image_processing_donut.js */
        "./src/models/donut/image_processing_donut.js"
      );
      class L extends f.DonutImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/owlv2/image_processing_owlv2.js": (
    /*!****************************************************!*\
      !*** ./src/models/owlv2/image_processing_owlv2.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Owlv2ImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../owlvit/image_processing_owlvit.js */
        "./src/models/owlvit/image_processing_owlvit.js"
      );
      class L extends f.OwlViTImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/owlvit/image_processing_owlvit.js": (
    /*!******************************************************!*\
      !*** ./src/models/owlvit/image_processing_owlvit.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        OwlViTFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        OwlViTImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...W) {
          return (0, f.post_process_object_detection)(...W);
        }
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/owlvit/processing_owlvit.js": (
    /*!************************************************!*\
      !*** ./src/models/owlvit/processing_owlvit.js ***!
      \************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        OwlViTProcessor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      class J extends f.Processor {
      }
      ge(J, "tokenizer_class", j.AutoTokenizer), ge(J, "image_processor_class", L.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/paligemma/processing_paligemma.js": (
    /*!******************************************************!*\
      !*** ./src/models/paligemma/processing_paligemma.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        PaliGemmaProcessor: () => (
          /* binding */
          w
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      const J = "<image>";
      function W(x, y, M, b, D) {
        return `${b.repeat(M * D)}${y}${x}
`;
      }
      class w extends f.Processor {
        /**
         * @typedef {import('../../utils/image.js').RawImage} RawImage
         */
        // `images` is required, `text` is optional
        async _call(y, M = null, b = {}) {
          M || (console.warn(
            "You are using PaliGemma without a text prefix. It will perform as a picture-captioning model."
          ), M = ""), Array.isArray(y) || (y = [y]), Array.isArray(M) || (M = [M]);
          const D = this.tokenizer.bos_token, q = this.image_processor.config.image_seq_length;
          let se;
          M.some((V) => V.includes(J)) ? se = M.map(
            (V) => {
              const Y = V.replaceAll(J, J.repeat(q)), O = Y.lastIndexOf(J), $ = O === -1 ? 0 : O + J.length;
              return Y.slice(0, $) + D + Y.slice($) + `
`;
            }
          ) : (console.warn(
            "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens."
          ), se = M.map(
            (V) => W(
              V,
              D,
              q,
              J,
              y.length
            )
          ));
          const oe = this.tokenizer(se, b);
          return {
            ...await this.image_processor(y, b),
            ...oe
          };
        }
      }
      ge(w, "tokenizer_class", j.AutoTokenizer), ge(w, "image_processor_class", L.AutoImageProcessor), ge(w, "uses_processor_config", !1);
    }
  ),
  /***/
  "./src/models/phi3_v/image_processing_phi3_v.js": (
    /*!******************************************************!*\
      !*** ./src/models/phi3_v/image_processing_phi3_v.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Phi3VImageProcessor: () => (
          /* binding */
          y
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      const j = 336, J = [2, 3], { ceil: W, floor: w, sqrt: x } = Math;
      class y extends f.ImageProcessor {
        constructor(b) {
          super({
            ...b,
            do_normalize: !0,
            do_pad: !0,
            pad_size: "custom",
            do_convert_rgb: !0,
            do_resize: !0
            // Smart resizing "hd_transform"
          }), this._num_crops = b.num_crops;
        }
        calc_num_image_tokens_from_image_size(b, D) {
          const { num_img_tokens: q } = this.config;
          return w((w(D / j) * w(b / j) + 1) * q + 1 + (w(D / j) + 1) * x(q));
        }
        /** @type {ImageProcessor['get_resize_output_image_size']} */
        get_resize_output_image_size(b, D) {
          const q = this._num_crops, [se, oe] = b.size;
          let z = se / oe, V = 1;
          for (; V * Math.ceil(V / z) <= q; )
            V += 1;
          V -= 1;
          const Y = Math.floor(V * 336), O = Math.floor(Y / z);
          return [Y, O];
        }
        /** @type {ImageProcessor['pad_image']} */
        pad_image(b, D, q, se = {}) {
          const [oe, z] = D, V = j * W(oe / j), Y = j * W(z / j), O = [1, 1, 1].map(($, g) => ($ - this.image_mean[g]) / this.image_std[g]);
          return super.pad_image(b, D, { width: Y, height: V }, {
            center: !0,
            constant_values: O,
            ...se
          });
        }
        async _call(b, {
          num_crops: D = null
        } = {}) {
          if (this._num_crops = D ?? (D = this.config.num_crops), D < 4 || x(D) % 1 !== 0)
            throw new Error("num_crops must be a square number >= 4");
          Array.isArray(b) || (b = [b]);
          const q = b.length, se = await Promise.all(b.map((C) => this.preprocess(C))), oe = se.map((C) => C.original_size), z = se.map((C) => C.reshaped_input_size), V = [];
          for (const { pixel_values: C } of se) {
            C.unsqueeze_(0);
            const [v, ee] = C.dims.slice(-2), X = await (0, L.interpolate_4d)(C, {
              size: [j, j],
              mode: "bicubic"
            });
            if (D > 0) {
              const le = [], ue = x(D), fe = w(ee / ue), Ce = w(v / ue);
              for (let Le = 0; Le < ue; ++Le)
                for (let qe = 0; qe < ue; ++qe) {
                  let Ue, ut, de, re;
                  Le === ue - 1 ? (ut = v - Ce, re = v) : (ut = Le * Ce, re = (Le + 1) * Ce), qe === ue - 1 ? (Ue = ee - fe, de = ee) : (Ue = qe * fe, de = (qe + 1) * fe);
                  const he = [ut, Ue], Ee = [re, de], Be = await (0, L.slice)(C, he, Ee, J);
                  le.push(Be);
                }
              const xe = await (0, L.interpolate_4d)((0, L.cat)(le, 0), {
                size: [j, j],
                mode: "bicubic"
              });
              V.push((0, L.cat)([X, xe], 0));
            } else
              V.push(X);
          }
          const Y = (0, L.stack)(V, 0), O = z.map((C) => C.map((v) => j * W(v / j))), $ = new L.Tensor(
            "int64",
            O.flat(),
            [q, 2]
          ), g = O.map(
            ([C, v]) => this.calc_num_image_tokens_from_image_size(v, C)
          );
          return { pixel_values: Y, original_sizes: oe, reshaped_input_sizes: z, image_sizes: $, num_img_tokens: g };
        }
      }
    }
  ),
  /***/
  "./src/models/phi3_v/processing_phi3_v.js": (
    /*!************************************************!*\
      !*** ./src/models/phi3_v/processing_phi3_v.js ***!
      \************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Phi3VProcessor: () => (
          /* binding */
          w
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      s(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      const J = "<|image|>", W = /<\|image_\d+\|>/g;
      class w extends f.Processor {
        /**
         * 
         * @param {string|string[]} text 
         * @param {RawImage|RawImage[]} images 
         * @param  {...any} args 
         * @returns {Promise<any>}
         */
        async _call(y, M = null, {
          padding: b = !0,
          truncation: D = !0,
          num_crops: q = null
        } = {}) {
          Array.isArray(y) || (y = [y]);
          let se, oe;
          if (M) {
            oe = await this.image_processor(M, { num_crops: q });
            const { num_img_tokens: z } = oe, V = y.map((O, $) => O.split(W).join(J.repeat(z[$])));
            se = this.tokenizer(V, { padding: b, truncation: D });
            const Y = this.tokenizer.model.convert_tokens_to_ids([J])[0];
            se.input_ids.map_((O) => O == Y ? -O : O);
          } else
            se = this.tokenizer(y);
          return {
            ...se,
            ...oe
          };
        }
      }
      ge(w, "image_processor_class", L.AutoImageProcessor), ge(w, "tokenizer_class", j.AutoTokenizer);
    }
  ),
  /***/
  "./src/models/processors.js": (
    /*!**********************************!*\
      !*** ./src/models/processors.js ***!
      \**********************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Florence2Processor: () => (
          /* reexport safe */
          f.Florence2Processor
        ),
        /* harmony export */
        Idefics3Processor: () => (
          /* reexport safe */
          J.Idefics3Processor
        ),
        /* harmony export */
        JinaCLIPProcessor: () => (
          /* reexport safe */
          w.JinaCLIPProcessor
        ),
        /* harmony export */
        MgpstrProcessor: () => (
          /* reexport safe */
          L.MgpstrProcessor
        ),
        /* harmony export */
        MoonshineProcessor: () => (
          /* reexport safe */
          j.MoonshineProcessor
        ),
        /* harmony export */
        OwlViTProcessor: () => (
          /* reexport safe */
          x.OwlViTProcessor
        ),
        /* harmony export */
        PaliGemmaProcessor: () => (
          /* reexport safe */
          M.PaliGemmaProcessor
        ),
        /* harmony export */
        Phi3VProcessor: () => (
          /* reexport safe */
          y.Phi3VProcessor
        ),
        /* harmony export */
        PyAnnoteProcessor: () => (
          /* reexport safe */
          b.PyAnnoteProcessor
        ),
        /* harmony export */
        Qwen2VLProcessor: () => (
          /* reexport safe */
          D.Qwen2VLProcessor
        ),
        /* harmony export */
        SamProcessor: () => (
          /* reexport safe */
          q.SamProcessor
        ),
        /* harmony export */
        SpeechT5Processor: () => (
          /* reexport safe */
          se.SpeechT5Processor
        ),
        /* harmony export */
        VLChatProcessor: () => (
          /* reexport safe */
          W.VLChatProcessor
        ),
        /* harmony export */
        Wav2Vec2ProcessorWithLM: () => (
          /* reexport safe */
          oe.Wav2Vec2ProcessorWithLM
        ),
        /* harmony export */
        WhisperProcessor: () => (
          /* reexport safe */
          z.WhisperProcessor
        )
        /* harmony export */
      });
      var f = s(
        /*! ./florence2/processing_florence2.js */
        "./src/models/florence2/processing_florence2.js"
      ), L = s(
        /*! ./mgp_str/processing_mgp_str.js */
        "./src/models/mgp_str/processing_mgp_str.js"
      ), j = s(
        /*! ./moonshine/processing_moonshine.js */
        "./src/models/moonshine/processing_moonshine.js"
      ), J = s(
        /*! ./idefics3/processing_idefics3.js */
        "./src/models/idefics3/processing_idefics3.js"
      ), W = s(
        /*! ./janus/processing_janus.js */
        "./src/models/janus/processing_janus.js"
      ), w = s(
        /*! ./jina_clip/processing_jina_clip.js */
        "./src/models/jina_clip/processing_jina_clip.js"
      ), x = s(
        /*! ./owlvit/processing_owlvit.js */
        "./src/models/owlvit/processing_owlvit.js"
      ), y = s(
        /*! ./phi3_v/processing_phi3_v.js */
        "./src/models/phi3_v/processing_phi3_v.js"
      ), M = s(
        /*! ./paligemma/processing_paligemma.js */
        "./src/models/paligemma/processing_paligemma.js"
      ), b = s(
        /*! ./pyannote/processing_pyannote.js */
        "./src/models/pyannote/processing_pyannote.js"
      ), D = s(
        /*! ./qwen2_vl/processing_qwen2_vl.js */
        "./src/models/qwen2_vl/processing_qwen2_vl.js"
      ), q = s(
        /*! ./sam/processing_sam.js */
        "./src/models/sam/processing_sam.js"
      ), se = s(
        /*! ./speecht5/processing_speecht5.js */
        "./src/models/speecht5/processing_speecht5.js"
      ), oe = s(
        /*! ./wav2vec2/processing_wav2vec2.js */
        "./src/models/wav2vec2/processing_wav2vec2.js"
      ), z = s(
        /*! ./whisper/processing_whisper.js */
        "./src/models/whisper/processing_whisper.js"
      );
    }
  ),
  /***/
  "./src/models/pvt/image_processing_pvt.js": (
    /*!************************************************!*\
      !*** ./src/models/pvt/image_processing_pvt.js ***!
      \************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        PvtImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/pyannote/feature_extraction_pyannote.js": (
    /*!************************************************************!*\
      !*** ./src/models/pyannote/feature_extraction_pyannote.js ***!
      \************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        PyAnnoteFeatureExtractor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      ), j = s(
        /*! ../../utils/maths.js */
        "./src/utils/maths.js"
      );
      class J extends f.FeatureExtractor {
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor; }>} The extracted input features.
         */
        async _call(w) {
          (0, f.validate_audio_inputs)(w, "PyAnnoteFeatureExtractor"), w instanceof Float64Array && (w = new Float32Array(w));
          const x = [
            1,
            /* batch_size */
            1,
            /* num_channels */
            w.length
            /* num_samples */
          ];
          return {
            input_values: new L.Tensor("float32", w, x)
          };
        }
        /**
         * NOTE: Can return fractional values. `Math.ceil` will ensure correct value.
         * @param {number} samples The number of frames in the audio.
         * @returns {number} The number of frames in the audio.
         */
        samples_to_frames(w) {
          return (w - this.config.offset) / this.config.step;
        }
        /**
         * Post-processes the speaker diarization logits output by the model.
         * @param {import('../../utils/tensor.js').Tensor} logits The speaker diarization logits output by the model.
         * @param {number} num_samples Number of samples in the input audio.
         * @returns {Array<Array<{ id: number, start: number, end: number, confidence: number }>>} The post-processed speaker diarization results.
         */
        post_process_speaker_diarization(w, x) {
          const y = x / this.samples_to_frames(x) / this.config.sampling_rate, M = [];
          for (const b of w.tolist()) {
            const D = [];
            let q = -1;
            for (let se = 0; se < b.length; ++se) {
              const oe = (0, j.softmax)(b[se]), [z, V] = (0, j.max)(oe), [Y, O] = [se, se + 1];
              V !== q ? (q = V, D.push({ id: V, start: Y, end: O, score: z })) : (D.at(-1).end = O, D.at(-1).score += z);
            }
            M.push(D.map(
              // Convert frame-space to time-space
              // and compute the confidence
              ({ id: se, start: oe, end: z, score: V }) => ({
                id: se,
                start: oe * y,
                end: z * y,
                confidence: V / (z - oe)
              })
            ));
          }
          return M;
        }
      }
    }
  ),
  /***/
  "./src/models/pyannote/processing_pyannote.js": (
    /*!****************************************************!*\
      !*** ./src/models/pyannote/processing_pyannote.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        PyAnnoteProcessor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ./feature_extraction_pyannote.js */
        "./src/models/pyannote/feature_extraction_pyannote.js"
      );
      class j extends f.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(W) {
          return await this.feature_extractor(W);
        }
        /** @type {PyAnnoteFeatureExtractor['post_process_speaker_diarization']} */
        post_process_speaker_diarization(...W) {
          return (
            /** @type {PyAnnoteFeatureExtractor} */
            this.feature_extractor.post_process_speaker_diarization(...W)
          );
        }
        get sampling_rate() {
          return this.feature_extractor.config.sampling_rate;
        }
      }
      ge(j, "feature_extractor_class", L.PyAnnoteFeatureExtractor);
    }
  ),
  /***/
  "./src/models/qwen2_vl/image_processing_qwen2_vl.js": (
    /*!**********************************************************!*\
      !*** ./src/models/qwen2_vl/image_processing_qwen2_vl.js ***!
      \**********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Qwen2VLImageProcessor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class j extends f.ImageProcessor {
        async _call(W, ...w) {
          const { pixel_values: x, original_sizes: y, reshaped_input_sizes: M } = await super._call(W, ...w);
          let b = x;
          const { temporal_patch_size: D, merge_size: q, patch_size: se } = this.config;
          b.dims[0] === 1 && (b = (0, L.cat)(Array.from({ length: D }, () => b), 0));
          const oe = b.dims[0] / D, z = b.dims[1], V = Math.floor(b.dims[2] / se), Y = Math.floor(b.dims[3] / se), O = b.view(
            oe,
            D,
            z,
            Math.floor(V / q),
            q,
            se,
            Math.floor(Y / q),
            q,
            se
          ).permute(0, 3, 6, 4, 7, 2, 1, 5, 8).view(
            oe * V * Y,
            z * D * se * se
          ), $ = new L.Tensor("int64", [oe, V, Y], [1, 3]);
          return {
            pixel_values: O,
            image_grid_thw: $,
            original_sizes: y,
            reshaped_input_sizes: M
          };
        }
      }
    }
  ),
  /***/
  "./src/models/qwen2_vl/processing_qwen2_vl.js": (
    /*!****************************************************!*\
      !*** ./src/models/qwen2_vl/processing_qwen2_vl.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Qwen2VLProcessor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      ), j = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      );
      s(
        /*! ../../utils/image.js */
        "./src/utils/image.js"
      );
      class J extends f.Processor {
        /**
         * 
         * @param {string|string[]} text 
         * @param {RawImage|RawImage[]} images 
         * @param  {...any} args 
         * @returns {Promise<any>}
         */
        async _call(w, x = null, ...y) {
          Array.isArray(w) || (w = [w]);
          let M, b;
          if (x && (M = await this.image_processor(x), b = M.image_grid_thw), b) {
            let q = this.image_processor.config.merge_size ** 2, se = 0;
            const oe = b.tolist();
            w = w.map((z) => {
              for (; z.includes("<|image_pad|>"); ) {
                const V = Number(oe[se++].reduce((Y, O) => Y * O, 1n));
                z = z.replace("<|image_pad|>", "<|placeholder|>".repeat(Math.floor(V / q)));
              }
              return z.replaceAll("<|placeholder|>", "<|image_pad|>");
            });
          }
          return {
            ...this.tokenizer(w),
            ...M
            // TODO: ...videos_inputs,
          };
        }
      }
      ge(J, "image_processor_class", L.AutoImageProcessor), ge(J, "tokenizer_class", j.AutoTokenizer);
    }
  ),
  /***/
  "./src/models/rt_detr/image_processing_rt_detr.js": (
    /*!********************************************************!*\
      !*** ./src/models/rt_detr/image_processing_rt_detr.js ***!
      \********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        RTDetrImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...J) {
          return (0, f.post_process_object_detection)(...J);
        }
      }
    }
  ),
  /***/
  "./src/models/sam/image_processing_sam.js": (
    /*!************************************************!*\
      !*** ./src/models/sam/image_processing_sam.js ***!
      \************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        SamImageProcessor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), L = s(
        /*! ../../utils/core.js */
        "./src/utils/core.js"
      ), j = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class J extends f.ImageProcessor {
        /**
         * 
         * @param {any} input_points 
         * @param {import("../../base/image_processors_utils.js").HeightWidth[]} original_sizes 
         * @param {import("../../base/image_processors_utils.js").HeightWidth[]} reshaped_input_sizes 
         * @returns {Tensor}
         */
        reshape_input_points(w, x, y, M = !1) {
          w = structuredClone(w);
          let b = (0, L.calculateDimensions)(w);
          if (b.length === 3)
            M || (b = [1, ...b]), w = [w];
          else if (b.length !== 4)
            throw Error("The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.");
          for (let D = 0; D < w.length; ++D) {
            let q = x[D], se = y[D], oe = [
              se[0] / q[0],
              se[1] / q[1]
            ];
            for (let z = 0; z < w[D].length; ++z)
              for (let V = 0; V < w[D][z].length; ++V)
                for (let Y = 0; Y < w[D][z][V].length; ++Y)
                  w[D][z][V][Y] *= oe[Y % 2];
          }
          return new j.Tensor(
            "float32",
            Float32Array.from(w.flat(1 / 0)),
            b
          );
        }
        /**
         * 
         * @param {any} input_labels 
         * @param {Tensor} input_points 
         * @returns {Tensor}
         */
        add_input_labels(w, x) {
          let y = (0, L.calculateDimensions)(w);
          if (y.length === 2)
            y = [1, ...y], w = [w];
          else if (y.length !== 3)
            throw Error("The input_points must be a 4D tensor of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.");
          if (y.some((M, b) => M !== x.dims[b]))
            throw Error(`The first ${y.length} dimensions of 'input_points' and 'input_labels' must be the same.`);
          return new j.Tensor(
            "int64",
            w.flat(1 / 0).map(BigInt),
            y
          );
        }
        /**
         * @param {any[]} images The URL(s) of the image(s) to extract features from.
         * @param {Object} [options] Additional options for the processor.
         * @param {any} [options.input_points=null] A 3D or 4D array, representing the input points provided by the user.
         * - 3D: `[point_batch_size, nb_points_per_image, 2]`. In this case, `batch_size` is assumed to be 1.
         * - 4D: `[batch_size, point_batch_size, nb_points_per_image, 2]`.
         * @param {any} [options.input_labels=null] A 2D or 3D array, representing the input labels for the points, used by the prompt encoder to encode the prompt.
         * - 2D: `[point_batch_size, nb_points_per_image]`. In this case, `batch_size` is assumed to be 1.
         * - 3D: `[batch_size, point_batch_size, nb_points_per_image]`.
         * @param {number[][][]} [options.input_boxes=null] A 3D array of shape `(batch_size, num_boxes, 4)`, representing the input boxes provided by the user.
         * This is used by the prompt encoder to encode the prompt. Generally yields to much better generated masks.
         * The processor will generate a tensor, with each dimension corresponding respectively to the image batch size,
         * the number of boxes per image and the coordinates of the top left and botton right point of the box.
         * In the order (`x1`, `y1`, `x2`, `y2`):
         * - `x1`: the x coordinate of the top left point of the input box
         * - `y1`: the y coordinate of the top left point of the input box
         * - `x2`: the x coordinate of the bottom right point of the input box
         * - `y2`: the y coordinate of the bottom right point of the input box
         * @returns {Promise<SamImageProcessorResult>}
         */
        async _call(w, {
          input_points: x = null,
          input_labels: y = null,
          input_boxes: M = null
        } = {}) {
          const b = await super._call(w);
          if (x && (b.input_points = this.reshape_input_points(
            x,
            b.original_sizes,
            b.reshaped_input_sizes
          )), y) {
            if (!b.input_points)
              throw Error("`input_points` must be provided if `input_labels` are provided.");
            b.input_labels = this.add_input_labels(y, b.input_points);
          }
          return M && (b.input_boxes = this.reshape_input_points(
            M,
            b.original_sizes,
            b.reshaped_input_sizes,
            !0
          )), b;
        }
        /**
         * Remove padding and upscale masks to the original image size.
         * @param {Tensor} masks Batched masks from the mask_decoder in (batch_size, num_channels, height, width) format.
         * @param {[number, number][]} original_sizes The original sizes of each image before it was resized to the model's expected input shape, in (height, width) format.
         * @param {[number, number][]} reshaped_input_sizes The size of each image as it is fed to the model, in (height, width) format. Used to remove padding.
         * @param {Object} options Optional parameters for post-processing.
         * @param {number} [options.mask_threshold] The threshold to use for binarizing the masks.
         * @param {boolean} [options.binarize] Whether to binarize the masks.
         * @param {Object} [options.pad_size] The target size the images were padded to before being passed to the model. If `null`, the target size is assumed to be the processor's `pad_size`.
         * @param {number} [options.pad_size.height] The height the images were padded to.
         * @param {number} [options.pad_size.width] The width the images were padded to.
         * @returns {Promise<Tensor[]>} Batched masks in batch_size, num_channels, height, width) format, where (height, width) is given by original_size.
         */
        async post_process_masks(w, x, y, {
          mask_threshold: M = 0,
          binarize: b = !0,
          pad_size: D = null
        } = {}) {
          const q = [];
          D = D ?? this.pad_size;
          const se = [D.height, D.width];
          for (let oe = 0; oe < x.length; ++oe) {
            const z = x[oe], V = y[oe];
            let Y = await (0, j.interpolate_4d)(
              w[oe],
              { mode: "bilinear", size: se }
            );
            if (Y = Y.slice(null, null, [0, V[0]], [0, V[1]]), Y = await (0, j.interpolate_4d)(
              Y,
              { mode: "bilinear", size: z }
            ), b) {
              const O = Y.data, $ = new Uint8Array(O.length);
              for (let g = 0; g < O.length; ++g)
                O[g] > M && ($[g] = 1);
              Y = new j.Tensor(
                "bool",
                $,
                Y.dims
              );
            }
            q.push(Y);
          }
          return q;
        }
        /**
         * Generates a list of crop boxes of different sizes. Each layer has (2**i)**2 boxes for the ith layer.
         * @param {import("../../utils/image.js").RawImage} image Input original image
         * @param {number} target_size Target size of the resized image
         * @param {Object} options Options for generating crop boxes 
         * @param {number} [options.crop_n_layers] If >0, mask prediction will be run again on crops of the image.
         * Sets the number of layers to run, where each layer has 2**i_layer number of image crops.
         * @param {number} [options.overlap_ratio] Sets the degree to which crops overlap. In the first crop layer,
         * crops will overlap by this fraction of the image length. Later layers with more crops scale down this overlap.
         * @param {number} [options.points_per_crop] Number of points to sample from each crop.
         * @param {number} [options.crop_n_points_downscale_factor] The number of points-per-side sampled in layer n is
         * scaled down by crop_n_points_downscale_factor**n.
         * @returns {Object} An object containing the crop boxes, number of points per crop, cropped images, and input labels.
         */
        generate_crop_boxes(w, x, {
          crop_n_layers: y = 0,
          overlap_ratio: M = 0.3413333333333333,
          points_per_crop: b = 32,
          crop_n_points_downscale_factor: D = 1
        } = {}) {
        }
      }
    }
  ),
  /***/
  "./src/models/sam/processing_sam.js": (
    /*!******************************************!*\
      !*** ./src/models/sam/processing_sam.js ***!
      \******************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        SamProcessor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/image_processing_auto.js */
        "./src/models/auto/image_processing_auto.js"
      );
      class j extends f.Processor {
        async _call(...W) {
          return await this.image_processor(...W);
        }
        post_process_masks(...W) {
          return this.image_processor.post_process_masks(...W);
        }
        reshape_input_points(...W) {
          return this.image_processor.reshape_input_points(...W);
        }
      }
      ge(j, "image_processor_class", L.AutoImageProcessor);
    }
  ),
  /***/
  "./src/models/seamless_m4t/feature_extraction_seamless_m4t.js": (
    /*!********************************************************************!*\
      !*** ./src/models/seamless_m4t/feature_extraction_seamless_m4t.js ***!
      \********************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        SeamlessM4TFeatureExtractor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      ), j = s(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class J extends f.FeatureExtractor {
        constructor(w) {
          super(w);
          const x = this.config.sampling_rate, y = (0, j.mel_filter_bank)(
            256,
            // num_frequency_bins
            this.config.num_mel_bins,
            // num_mel_filters
            20,
            // min_frequency
            Math.floor(x / 2),
            // max_frequency
            x,
            // sampling_rate
            null,
            // norm
            "kaldi",
            // mel_scale
            !0
            // triangularize_in_mel_space
          );
          for (let M = 0; M < y.length; ++M)
            y[M].push(0);
          this.mel_filters = y, this.window = (0, j.window_function)(400, "povey", {
            periodic: !1
          });
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @param {number} max_length The maximum number of frames to return.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(w, x) {
          return w = w.map((y) => y * 32768), (0, j.spectrogram)(
            w,
            this.window,
            // window
            400,
            // frame_length
            160,
            // hop_length
            {
              fft_length: 512,
              power: 2,
              center: !1,
              preemphasis: 0.97,
              mel_filters: this.mel_filters,
              log_mel: "log",
              mel_floor: 1192092955078125e-22,
              remove_dc_offset: !0,
              // Custom
              max_num_frames: x,
              transpose: !0
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @param {Object} options Optional parameters for feature extraction.
         * @param {boolean} [options.padding=true] Whether to pad the sequence to a multiple of `pad_to_multiple_of`.
         * @param {number} [options.pad_to_multiple_of=2] The number to pad the sequence to a multiple of.
         * @param {boolean} [options.do_normalize_per_mel_bins=true] Whether or not to zero-mean unit-variance normalize the input per mel-channel.
         * @param {boolean} [options.return_attention_mask=true] Whether to return the attention mask.
         * @returns {Promise<{ input_features: Tensor, attention_mask?: Tensor }>} A Promise resolving to an object containing the extracted input features and attention masks as Tensors.
         */
        async _call(w, {
          padding: x = !0,
          pad_to_multiple_of: y = 2,
          do_normalize_per_mel_bins: M = !0,
          return_attention_mask: b = !0
        } = {}) {
          (0, f.validate_audio_inputs)(w, "SeamlessM4TFeatureExtractor");
          let D = await this._extract_fbank_features(w, this.config.max_length);
          if (M) {
            const [$, g] = D.dims, C = D.data;
            for (let v = 0; v < g; ++v) {
              let ee = 0;
              for (let fe = 0; fe < $; ++fe)
                ee += C[fe * g + v];
              const X = ee / $;
              let le = 0;
              for (let fe = 0; fe < $; ++fe)
                le += (C[fe * g + v] - X) ** 2;
              le /= $ - 1;
              const ue = Math.sqrt(le + 1e-7);
              for (let fe = 0; fe < $; ++fe) {
                const Ce = fe * g + v;
                C[Ce] = (C[Ce] - X) / ue;
              }
            }
          }
          let q;
          if (x) {
            const [$, g] = D.dims, C = (
              /** @type {Float32Array} */
              D.data
            ), v = $ % y;
            if (v > 0) {
              const ee = new Float32Array(g * ($ + v));
              ee.set(C), ee.fill(this.config.padding_value, C.length);
              const X = $ + v;
              D = new L.Tensor(
                D.type,
                ee,
                [X, g]
              ), b && (q = new L.Tensor(
                "int64",
                new BigInt64Array(X),
                [1, X]
              ), q.data.fill(1n, 0, $));
            }
          }
          const [se, oe] = D.dims, z = this.config.stride;
          if (se % z !== 0)
            throw new Error(`The number of frames (${se}) must be a multiple of the stride (${z}).`);
          const Y = D.view(
            1,
            Math.floor(se / z),
            oe * z
          ), O = { input_features: Y };
          if (b) {
            const $ = Y.dims[1], g = new BigInt64Array($);
            if (q) {
              const C = q.data;
              for (let v = 1, ee = 0; v < se; v += z, ++ee)
                g[ee] = C[v];
            } else
              g.fill(1n);
            O.attention_mask = new L.Tensor(
              "int64",
              g,
              [1, $]
            );
          }
          return O;
        }
      }
    }
  ),
  /***/
  "./src/models/segformer/image_processing_segformer.js": (
    /*!************************************************************!*\
      !*** ./src/models/segformer/image_processing_segformer.js ***!
      \************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        SegformerFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        SegformerImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        /** @type {typeof post_process_semantic_segmentation} */
        post_process_semantic_segmentation(...W) {
          return (0, f.post_process_semantic_segmentation)(...W);
        }
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/siglip/image_processing_siglip.js": (
    /*!******************************************************!*\
      !*** ./src/models/siglip/image_processing_siglip.js ***!
      \******************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        SiglipImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
    }
  ),
  /***/
  "./src/models/speecht5/feature_extraction_speecht5.js": (
    /*!************************************************************!*\
      !*** ./src/models/speecht5/feature_extraction_speecht5.js ***!
      \************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        SpeechT5FeatureExtractor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      class L extends f.FeatureExtractor {
      }
    }
  ),
  /***/
  "./src/models/speecht5/processing_speecht5.js": (
    /*!****************************************************!*\
      !*** ./src/models/speecht5/processing_speecht5.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        SpeechT5Processor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), j = s(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      );
      class J extends f.Processor {
        /**
         * Calls the feature_extractor function with the given input.
         * @param {any} input The input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(w) {
          return await this.feature_extractor(w);
        }
      }
      ge(J, "tokenizer_class", L.AutoTokenizer), ge(J, "feature_extractor_class", j.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/swin2sr/image_processing_swin2sr.js": (
    /*!********************************************************!*\
      !*** ./src/models/swin2sr/image_processing_swin2sr.js ***!
      \********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Swin2SRImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        pad_image(J, W, w, x = {}) {
          const [y, M, b] = W;
          return super.pad_image(J, W, {
            // NOTE: For Swin2SR models, the original python implementation adds padding even when the image's width/height is already
            // a multiple of `pad_size`. However, this is most likely a bug (PR: https://github.com/mv-lab/swin2sr/pull/19).
            // For this reason, we only add padding when the image's width/height is not a multiple of `pad_size`.
            width: M + (w - M % w) % w,
            height: y + (w - y % w) % w
          }, {
            mode: "symmetric",
            center: !1,
            constant_values: -1,
            ...x
          });
        }
      }
    }
  ),
  /***/
  "./src/models/vit/image_processing_vit.js": (
    /*!************************************************!*\
      !*** ./src/models/vit/image_processing_vit.js ***!
      \************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        ViTFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        ViTImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/models/vitmatte/image_processing_vitmatte.js": (
    /*!**********************************************************!*\
      !*** ./src/models/vitmatte/image_processing_vitmatte.js ***!
      \**********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        VitMatteImageProcessor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class j extends f.ImageProcessor {
        /**
         * Calls the feature extraction process on an array of images, preprocesses
         * each image, and concatenates the resulting features into a single Tensor.
         * @param {import("../../utils/image.js").RawImage[]} images The image(s) to extract features from.
         * @param {import("../../utils/image.js").RawImage[]} trimaps The trimaps(s) to extract features from.
         * @returns {Promise<import("../../base/image_processors_utils.js").ImageProcessorResult>} An object containing the concatenated pixel values of the preprocessed images.
         */
        async _call(W, w) {
          Array.isArray(W) || (W = [W]), Array.isArray(w) || (w = [w]);
          const x = await Promise.all(W.map((b) => this.preprocess(b))), y = await Promise.all(w.map((b) => this.preprocess(b, {
            do_normalize: !1,
            do_convert_rgb: !1,
            do_convert_grayscale: !0
          })));
          return {
            pixel_values: (0, L.stack)(x.map(
              // Concatenate images and trimaps
              (b, D) => (0, L.cat)([b.pixel_values, y[D].pixel_values], 0)
            ), 0),
            // Original sizes of images
            original_sizes: x.map((b) => b.original_size),
            // Reshaped sizes of images, before padding or cropping
            reshaped_input_sizes: x.map((b) => b.reshaped_input_size)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/vitpose/image_processing_vitpose.js": (
    /*!********************************************************!*\
      !*** ./src/models/vitpose/image_processing_vitpose.js ***!
      \********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        VitPoseImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        /**
         * Transform the heatmaps into keypoint predictions and transform them back to the image.
         * NOTE: This is a naive implementation and does not include advanced post-processing techniques,
         * so the results may not be as accurate as the original implementation.
         * @param {import('../../utils/tensor.js').Tensor} outputs The model outputs.
         * @param {[number, number, number, number][][]} boxes List or array of bounding boxes for each image.
         * Each box should be a list of 4 floats representing the bounding box coordinates in COCO format (top_left_x, top_left_y, width, height).
         * @returns {{
         *   bbox: [number, number, number, number],
         *   scores: number[],
         *   labels: number[],
         *   keypoints: [number, number][]
         * }[][]} List of keypoints predictions for each image.
         */
        post_process_pose_estimation(J, W, {
          threshold: w = null
          // TODO:
          // kernel_size = 11,
          // target_sizes = null,
        } = {}) {
          const x = J.tolist(), [y, M, b, D] = J.dims, q = [];
          for (let se = 0; se < y; ++se) {
            const oe = x[se], z = W[se], V = [];
            for (let Y = 0; Y < z.length; ++Y) {
              const O = z[Y], $ = [], g = [], C = [], v = O.at(-2) / D, ee = O.at(-1) / b;
              for (let X = 0; X < oe.length; ++X) {
                let [le, ue] = [0, 0], fe = 0, Ce = -1 / 0;
                const xe = oe[X];
                for (let qe = 0; qe < xe.length; ++qe) {
                  const Ue = xe[qe];
                  for (let ut = 0; ut < Ue.length; ++ut) {
                    const de = Ue[ut];
                    fe += de, Ce = Math.max(Ce, de), le += (ut + 0.5) * de, ue += qe * de;
                  }
                }
                if (w != null && Ce < w) continue;
                const Le = [
                  v * le / fe,
                  ee * ue / fe
                ];
                $.push(Le), C.push(X), g.push(Ce);
              }
              V.push({
                bbox: O,
                scores: g,
                labels: C,
                keypoints: $
              });
            }
            q.push(V);
          }
          return q;
        }
      }
    }
  ),
  /***/
  "./src/models/wav2vec2/feature_extraction_wav2vec2.js": (
    /*!************************************************************!*\
      !*** ./src/models/wav2vec2/feature_extraction_wav2vec2.js ***!
      \************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Wav2Vec2FeatureExtractor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      ), L = s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      class j extends f.FeatureExtractor {
        /**
         * @param {Float32Array} input_values 
         * @returns {Float32Array} 
         */
        _zero_mean_unit_var_norm(W) {
          const x = W.reduce((M, b) => M + b, 0) / W.length, y = W.reduce((M, b) => M + (b - x) ** 2, 0) / W.length;
          return W.map((M) => (M - x) / Math.sqrt(y + 1e-7));
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_values: Tensor; attention_mask: Tensor }>} A Promise resolving to an object containing the extracted input features and attention mask as Tensors.
         */
        async _call(W) {
          (0, f.validate_audio_inputs)(W, "Wav2Vec2FeatureExtractor"), W instanceof Float64Array && (W = new Float32Array(W));
          let w = W;
          this.config.do_normalize && (w = this._zero_mean_unit_var_norm(w));
          const x = [1, w.length];
          return {
            input_values: new L.Tensor("float32", w, x),
            attention_mask: new L.Tensor("int64", new BigInt64Array(w.length).fill(1n), x)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/wav2vec2/processing_wav2vec2.js": (
    /*!****************************************************!*\
      !*** ./src/models/wav2vec2/processing_wav2vec2.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Wav2Vec2ProcessorWithLM: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      ), L = s(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      );
      class j extends f.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(W) {
          return await this.feature_extractor(W);
        }
      }
      ge(j, "feature_extractor_class", L.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/wespeaker/feature_extraction_wespeaker.js": (
    /*!**************************************************************!*\
      !*** ./src/models/wespeaker/feature_extraction_wespeaker.js ***!
      \**************************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        WeSpeakerFeatureExtractor: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var L = s(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      );
      class j extends f.FeatureExtractor {
        constructor(W) {
          super(W);
          const w = this.config.sampling_rate, x = (0, L.mel_filter_bank)(
            256,
            // num_frequency_bins
            this.config.num_mel_bins,
            // num_mel_filters
            20,
            // min_frequency
            Math.floor(w / 2),
            // max_frequency
            w,
            // sampling_rate
            null,
            // norm
            "kaldi",
            // mel_scale
            !0
            // triangularize_in_mel_space
          );
          for (let y = 0; y < x.length; ++y)
            x[y].push(0);
          this.mel_filters = x, this.window = (0, L.window_function)(400, "hamming", {
            periodic: !1
          }), this.min_num_frames = this.config.min_num_frames;
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(W) {
          return W = W.map((w) => w * 32768), (0, L.spectrogram)(
            W,
            this.window,
            // window
            400,
            // frame_length
            160,
            // hop_length
            {
              fft_length: 512,
              power: 2,
              center: !1,
              preemphasis: 0.97,
              mel_filters: this.mel_filters,
              log_mel: "log",
              mel_floor: 1192092955078125e-22,
              remove_dc_offset: !0,
              // Custom
              transpose: !0,
              min_num_frames: this.min_num_frames
            }
          );
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(W) {
          (0, f.validate_audio_inputs)(W, "WeSpeakerFeatureExtractor");
          const w = (await this._extract_fbank_features(W)).unsqueeze_(0);
          if (this.config.fbank_centering_span === null) {
            const x = (
              /** @type {Float32Array} */
              w.mean(1).data
            ), y = (
              /** @type {Float32Array} */
              w.data
            ), [M, b, D] = w.dims;
            for (let q = 0; q < M; ++q) {
              const se = q * b * D, oe = q * D;
              for (let z = 0; z < b; ++z) {
                const V = se + z * D;
                for (let Y = 0; Y < D; ++Y)
                  y[V + Y] -= x[oe + Y];
              }
            }
          }
          return {
            input_features: w
          };
        }
      }
    }
  ),
  /***/
  "./src/models/whisper/common_whisper.js": (
    /*!**********************************************!*\
      !*** ./src/models/whisper/common_whisper.js ***!
      \**********************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        WHISPER_LANGUAGE_MAPPING: () => (
          /* binding */
          L
        ),
        /* harmony export */
        WHISPER_TO_LANGUAGE_CODE_MAPPING: () => (
          /* binding */
          j
        ),
        /* harmony export */
        whisper_language_to_code: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      const f = [
        ["en", "english"],
        ["zh", "chinese"],
        ["de", "german"],
        ["es", "spanish"],
        ["ru", "russian"],
        ["ko", "korean"],
        ["fr", "french"],
        ["ja", "japanese"],
        ["pt", "portuguese"],
        ["tr", "turkish"],
        ["pl", "polish"],
        ["ca", "catalan"],
        ["nl", "dutch"],
        ["ar", "arabic"],
        ["sv", "swedish"],
        ["it", "italian"],
        ["id", "indonesian"],
        ["hi", "hindi"],
        ["fi", "finnish"],
        ["vi", "vietnamese"],
        ["he", "hebrew"],
        ["uk", "ukrainian"],
        ["el", "greek"],
        ["ms", "malay"],
        ["cs", "czech"],
        ["ro", "romanian"],
        ["da", "danish"],
        ["hu", "hungarian"],
        ["ta", "tamil"],
        ["no", "norwegian"],
        ["th", "thai"],
        ["ur", "urdu"],
        ["hr", "croatian"],
        ["bg", "bulgarian"],
        ["lt", "lithuanian"],
        ["la", "latin"],
        ["mi", "maori"],
        ["ml", "malayalam"],
        ["cy", "welsh"],
        ["sk", "slovak"],
        ["te", "telugu"],
        ["fa", "persian"],
        ["lv", "latvian"],
        ["bn", "bengali"],
        ["sr", "serbian"],
        ["az", "azerbaijani"],
        ["sl", "slovenian"],
        ["kn", "kannada"],
        ["et", "estonian"],
        ["mk", "macedonian"],
        ["br", "breton"],
        ["eu", "basque"],
        ["is", "icelandic"],
        ["hy", "armenian"],
        ["ne", "nepali"],
        ["mn", "mongolian"],
        ["bs", "bosnian"],
        ["kk", "kazakh"],
        ["sq", "albanian"],
        ["sw", "swahili"],
        ["gl", "galician"],
        ["mr", "marathi"],
        ["pa", "punjabi"],
        ["si", "sinhala"],
        ["km", "khmer"],
        ["sn", "shona"],
        ["yo", "yoruba"],
        ["so", "somali"],
        ["af", "afrikaans"],
        ["oc", "occitan"],
        ["ka", "georgian"],
        ["be", "belarusian"],
        ["tg", "tajik"],
        ["sd", "sindhi"],
        ["gu", "gujarati"],
        ["am", "amharic"],
        ["yi", "yiddish"],
        ["lo", "lao"],
        ["uz", "uzbek"],
        ["fo", "faroese"],
        ["ht", "haitian creole"],
        ["ps", "pashto"],
        ["tk", "turkmen"],
        ["nn", "nynorsk"],
        ["mt", "maltese"],
        ["sa", "sanskrit"],
        ["lb", "luxembourgish"],
        ["my", "myanmar"],
        ["bo", "tibetan"],
        ["tl", "tagalog"],
        ["mg", "malagasy"],
        ["as", "assamese"],
        ["tt", "tatar"],
        ["haw", "hawaiian"],
        ["ln", "lingala"],
        ["ha", "hausa"],
        ["ba", "bashkir"],
        ["jw", "javanese"],
        ["su", "sundanese"]
      ], L = new Map(f), j = new Map([
        ...f.map(([W, w]) => [w, W]),
        ["burmese", "my"],
        ["valencian", "ca"],
        ["flemish", "nl"],
        ["haitian", "ht"],
        ["letzeburgesch", "lb"],
        ["pushto", "ps"],
        ["panjabi", "pa"],
        ["moldavian", "ro"],
        ["moldovan", "ro"],
        ["sinhalese", "si"],
        ["castilian", "es"]
      ]);
      function J(W) {
        W = W.toLowerCase();
        let w = j.get(W);
        if (w === void 0)
          if (L.has(W))
            w = W;
          else {
            const y = W.length === 2 ? L.keys() : L.values();
            throw new Error(`Language "${W}" is not supported. Must be one of: ${JSON.stringify(y)}`);
          }
        return w;
      }
    }
  ),
  /***/
  "./src/models/whisper/feature_extraction_whisper.js": (
    /*!**********************************************************!*\
      !*** ./src/models/whisper/feature_extraction_whisper.js ***!
      \**********************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        WhisperFeatureExtractor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/feature_extraction_utils.js */
        "./src/base/feature_extraction_utils.js"
      );
      s(
        /*! ../../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      var L = s(
        /*! ../../utils/audio.js */
        "./src/utils/audio.js"
      ), j = s(
        /*! ../../utils/maths.js */
        "./src/utils/maths.js"
      );
      class J extends f.FeatureExtractor {
        constructor(w) {
          var x;
          super(w), (x = this.config).mel_filters ?? (x.mel_filters = (0, L.mel_filter_bank)(
            Math.floor(1 + this.config.n_fft / 2),
            // num_frequency_bins
            this.config.feature_size,
            // num_mel_filters
            0,
            // min_frequency
            8e3,
            // max_frequency
            this.config.sampling_rate,
            // sampling_rate
            "slaney",
            // norm
            "slaney"
            // mel_scale
          )), this.window = (0, L.window_function)(this.config.n_fft, "hann");
        }
        /**
         * Computes the log-Mel spectrogram of the provided audio waveform.
         * @param {Float32Array|Float64Array} waveform The audio waveform to process.
         * @returns {Promise<Tensor>} An object containing the log-Mel spectrogram data as a Float32Array and its dimensions as an array of numbers.
         */
        async _extract_fbank_features(w) {
          const x = await (0, L.spectrogram)(
            w,
            this.window,
            // window
            this.config.n_fft,
            // frame_length
            this.config.hop_length,
            // hop_length
            {
              power: 2,
              mel_filters: this.config.mel_filters,
              log_mel: "log10",
              // Custom
              max_num_frames: this.config.nb_max_frames
              // 3000
            }
          ), y = x.data, M = (0, j.max)(y)[0];
          for (let b = 0; b < y.length; ++b)
            y[b] = (Math.max(y[b], M - 8) + 4) / 4;
          return x;
        }
        /**
         * Asynchronously extracts features from a given audio using the provided configuration.
         * @param {Float32Array|Float64Array} audio The audio data as a Float32Array/Float64Array.
         * @returns {Promise<{ input_features: Tensor }>} A Promise resolving to an object containing the extracted input features as a Tensor.
         */
        async _call(w) {
          (0, f.validate_audio_inputs)(w, "WhisperFeatureExtractor");
          let x;
          return w.length > this.config.n_samples ? (console.warn(
            "Attempting to extract features for audio longer than 30 seconds. If using a pipeline to extract transcript from a long audio clip, remember to specify `chunk_length_s` and/or `stride_length_s`."
          ), x = w.slice(0, this.config.n_samples)) : (x = new Float32Array(this.config.n_samples), x.set(w)), {
            input_features: (await this._extract_fbank_features(x)).unsqueeze_(0)
          };
        }
      }
    }
  ),
  /***/
  "./src/models/whisper/generation_whisper.js": (
    /*!**************************************************!*\
      !*** ./src/models/whisper/generation_whisper.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        WhisperGenerationConfig: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../generation/configuration_utils.js */
        "./src/generation/configuration_utils.js"
      );
      class L extends f.GenerationConfig {
        constructor() {
          super(...arguments);
          /**
           * Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.
           * @type {boolean}
           */
          ge(this, "return_timestamps", null);
          /**
           * Whether to return token-level timestamps
           * with the text. This can be used with or without the `return_timestamps` option. To get word-level
           * timestamps, use the tokenizer to group the tokens into words.
           * @type {boolean}
           */
          ge(this, "return_token_timestamps", null);
          /**
           * The number of audio frames available in this chunk. This is only used generating word-level timestamps.
           * @type {number}
           */
          ge(this, "num_frames", null);
          /**
           * Alignment heads to predict word-level timestamps. This is a list of [layer, head] pairs that
           * select the cross-attention heads that are highly correlated to word-level timing.
           * @type {[number, number][]}
           */
          ge(this, "alignment_heads", null);
          /**
           * Task to use for generation, either "translate" or "transcribe".
           * @type {string}
           */
          ge(this, "task", null);
          /**
           * Language token to use for generation, can be either in the form of `<|en|>`, `en` or `english`.
           * You can find all the possible language tokens in the `model.generation_config.lang_to_id` dictionary.
           * @type {string}
           */
          ge(this, "language", null);
          /**
           * The id of the `"<|notimestamps|>"` token.
           * @type {number}
           */
          ge(this, "no_timestamps_token_id", null);
          /**
           * Rank-1 list of token IDs created by passing text to [`~WhisperProcessor.get_prompt_ids`] that is
           * provided as a prompt to each chunk. This can be used to provide or "prompt-engineer" a context for
           * transcription, e.g. custom vocabularies or proper nouns to make it more likely to predict those words
           * correctly. It cannot be used in conjunction with `decoder_start_token_id` as it overwrites this value.
           * @type {number[]}
           */
          ge(this, "prompt_ids", null);
          /**
           * Whether the model is multilingual or not.
           * @type {boolean}
           */
          ge(this, "is_multilingual", null);
          /**
           * (Optional) A mapping from language tokens to their corresponding IDs.
           * Only required if the model is multilingual.
           * @type {Record<string, number>|null}
           */
          ge(this, "lang_to_id", null);
          /**
           * (Optional) A mapping from task tokens to their corresponding IDs.
           * @type {Record<string, number>|null}
           */
          ge(this, "task_to_id", null);
          /**
           * Used to set the maximum value of the initial timestamp. This is used to prevent the model from
           * predicting timestamps that are too far in the future.
           * @type {number}
           */
          ge(this, "max_initial_timestamp_index", 1);
        }
      }
    }
  ),
  /***/
  "./src/models/whisper/processing_whisper.js": (
    /*!**************************************************!*\
      !*** ./src/models/whisper/processing_whisper.js ***!
      \**************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        WhisperProcessor: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../auto/feature_extraction_auto.js */
        "./src/models/auto/feature_extraction_auto.js"
      ), L = s(
        /*! ../../tokenizers.js */
        "./src/tokenizers.js"
      ), j = s(
        /*! ../../base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      class J extends j.Processor {
        /**
         * Calls the feature_extractor function with the given audio input.
         * @param {any} audio The audio input to extract features from.
         * @returns {Promise<any>} A Promise that resolves with the extracted features.
         */
        async _call(w) {
          return await this.feature_extractor(w);
        }
      }
      ge(J, "tokenizer_class", L.AutoTokenizer), ge(J, "feature_extractor_class", f.AutoFeatureExtractor);
    }
  ),
  /***/
  "./src/models/yolos/image_processing_yolos.js": (
    /*!****************************************************!*\
      !*** ./src/models/yolos/image_processing_yolos.js ***!
      \****************************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        YolosFeatureExtractor: () => (
          /* binding */
          j
        ),
        /* harmony export */
        YolosImageProcessor: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      var f = s(
        /*! ../../base/image_processors_utils.js */
        "./src/base/image_processors_utils.js"
      );
      class L extends f.ImageProcessor {
        /** @type {typeof post_process_object_detection} */
        post_process_object_detection(...W) {
          return (0, f.post_process_object_detection)(...W);
        }
      }
      class j extends L {
      }
    }
  ),
  /***/
  "./src/ops/registry.js": (
    /*!*****************************!*\
      !*** ./src/ops/registry.js ***!
      \*****************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        TensorOpRegistry: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      var f = s(
        /*! ../backends/onnx.js */
        "./src/backends/onnx.js"
      ), L = s(
        /*! ../utils/tensor.js */
        "./src/utils/tensor.js"
      );
      const j = async (W, w, x) => {
        const y = await (0, f.createInferenceSession)(
          new Uint8Array(W),
          w
        );
        return (
          /** @type {any} */
          async (M) => {
            const b = (0, f.isONNXProxy)(), D = Object.fromEntries(Object.entries(M).map(([se, oe]) => [se, (b ? oe.clone() : oe).ort_tensor])), q = await y.run(D);
            return Array.isArray(x) ? x.map((se) => new L.Tensor(q[se])) : new L.Tensor(q[
              /** @type {string} */
              x
            ]);
          }
        );
      };
      class J {
        static get bilinear_interpolate_4d() {
          return this._bilinear_interpolate_4d || (this._bilinear_interpolate_4d = j(
            [8, 9, 18, 0, 58, 128, 1, 10, 40, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 17, 10, 4, 109, 111, 100, 101, 34, 6, 108, 105, 110, 101, 97, 114, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 20],
            this.session_options,
            "y"
          )), this._bilinear_interpolate_4d;
        }
        static get bicubic_interpolate_4d() {
          return this._bicubic_interpolate_4d || (this._bicubic_interpolate_4d = j(
            [8, 9, 18, 0, 58, 127, 10, 39, 10, 1, 120, 10, 0, 10, 0, 10, 1, 115, 18, 1, 121, 34, 6, 82, 101, 115, 105, 122, 101, 42, 16, 10, 4, 109, 111, 100, 101, 34, 5, 99, 117, 98, 105, 99, 160, 1, 3, 18, 1, 114, 90, 31, 10, 1, 120, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 90, 15, 10, 1, 115, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 4, 98, 31, 10, 1, 121, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 99, 10, 3, 18, 1, 104, 10, 3, 18, 1, 119, 66, 2, 16, 20],
            this.session_options,
            "y"
          )), this._bicubic_interpolate_4d;
        }
        static get matmul() {
          return this._matmul || (this._matmul = j(
            [8, 9, 18, 0, 58, 55, 10, 17, 10, 1, 97, 10, 1, 98, 18, 1, 99, 34, 6, 77, 97, 116, 77, 117, 108, 18, 1, 114, 90, 9, 10, 1, 97, 18, 4, 10, 2, 8, 1, 90, 9, 10, 1, 98, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 99, 18, 4, 10, 2, 8, 1, 66, 2, 16, 20],
            this.session_options,
            "c"
          )), this._matmul;
        }
        static get stft() {
          return this._stft || (this._stft = j(
            [8, 7, 18, 0, 58, 148, 1, 10, 38, 10, 1, 115, 10, 1, 106, 10, 1, 119, 10, 1, 108, 18, 1, 111, 34, 4, 83, 84, 70, 84, 42, 15, 10, 8, 111, 110, 101, 115, 105, 100, 101, 100, 24, 1, 160, 1, 2, 18, 1, 115, 90, 26, 10, 1, 115, 18, 21, 10, 19, 8, 1, 18, 15, 10, 3, 18, 1, 98, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 90, 11, 10, 1, 106, 18, 6, 10, 4, 8, 7, 18, 0, 90, 16, 10, 1, 119, 18, 11, 10, 9, 8, 1, 18, 5, 10, 3, 18, 1, 119, 90, 11, 10, 1, 108, 18, 6, 10, 4, 8, 7, 18, 0, 98, 31, 10, 1, 111, 18, 26, 10, 24, 8, 1, 18, 20, 10, 3, 18, 1, 98, 10, 3, 18, 1, 102, 10, 3, 18, 1, 100, 10, 3, 18, 1, 99, 66, 2, 16, 17],
            this.session_options,
            "o"
          )), this._stft;
        }
        static get rfft() {
          return this._rfft || (this._rfft = j(
            [8, 9, 18, 0, 58, 97, 10, 33, 10, 1, 120, 10, 0, 10, 1, 97, 18, 1, 121, 34, 3, 68, 70, 84, 42, 15, 10, 8, 111, 110, 101, 115, 105, 100, 101, 100, 24, 1, 160, 1, 2, 18, 1, 100, 90, 21, 10, 1, 120, 18, 16, 10, 14, 8, 1, 18, 10, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 90, 11, 10, 1, 97, 18, 6, 10, 4, 8, 7, 18, 0, 98, 21, 10, 1, 121, 18, 16, 10, 14, 8, 1, 18, 10, 10, 3, 18, 1, 115, 10, 3, 18, 1, 99, 66, 2, 16, 20],
            this.session_options,
            "y"
          )), this._rfft;
        }
        static get top_k() {
          return this._top_k || (this._top_k = j(
            [8, 10, 18, 0, 58, 73, 10, 18, 10, 1, 120, 10, 1, 107, 18, 1, 118, 18, 1, 105, 34, 4, 84, 111, 112, 75, 18, 1, 116, 90, 9, 10, 1, 120, 18, 4, 10, 2, 8, 1, 90, 15, 10, 1, 107, 18, 10, 10, 8, 8, 7, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 118, 18, 4, 10, 2, 8, 1, 98, 9, 10, 1, 105, 18, 4, 10, 2, 8, 7, 66, 2, 16, 21],
            this.session_options,
            [
              /* Values */
              "v",
              /* Indices */
              "i"
            ]
          )), this._top_k;
        }
        static get slice() {
          return this._slice || (this._slice = j(
            [8, 7, 18, 0, 58, 96, 10, 25, 10, 1, 120, 10, 1, 115, 10, 1, 101, 10, 1, 97, 10, 1, 116, 18, 1, 121, 34, 5, 83, 108, 105, 99, 101, 18, 1, 114, 90, 9, 10, 1, 120, 18, 4, 10, 2, 8, 1, 90, 9, 10, 1, 115, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 101, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 97, 18, 4, 10, 2, 8, 7, 90, 9, 10, 1, 116, 18, 4, 10, 2, 8, 7, 98, 9, 10, 1, 121, 18, 4, 10, 2, 8, 1, 66, 2, 16, 13],
            this.session_options,
            "y"
          )), this._slice;
        }
      }
      ge(J, "session_options", {
        // TODO: Allow for multiple execution providers
        // executionProviders: ['webgpu'],
      });
    }
  ),
  /***/
  "./src/pipelines.js": (
    /*!**************************!*\
      !*** ./src/pipelines.js ***!
      \**************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        AudioClassificationPipeline: () => (
          /* binding */
          ue
        ),
        /* harmony export */
        AutomaticSpeechRecognitionPipeline: () => (
          /* binding */
          Ce
        ),
        /* harmony export */
        DepthEstimationPipeline: () => (
          /* binding */
          Be
        ),
        /* harmony export */
        DocumentQuestionAnsweringPipeline: () => (
          /* binding */
          re
        ),
        /* harmony export */
        FeatureExtractionPipeline: () => (
          /* binding */
          X
        ),
        /* harmony export */
        FillMaskPipeline: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        ImageClassificationPipeline: () => (
          /* binding */
          Le
        ),
        /* harmony export */
        ImageFeatureExtractionPipeline: () => (
          /* binding */
          le
        ),
        /* harmony export */
        ImageSegmentationPipeline: () => (
          /* binding */
          qe
        ),
        /* harmony export */
        ImageToImagePipeline: () => (
          /* binding */
          Ee
        ),
        /* harmony export */
        ImageToTextPipeline: () => (
          /* binding */
          xe
        ),
        /* harmony export */
        ObjectDetectionPipeline: () => (
          /* binding */
          ut
        ),
        /* harmony export */
        Pipeline: () => (
          /* binding */
          se
        ),
        /* harmony export */
        QuestionAnsweringPipeline: () => (
          /* binding */
          V
        ),
        /* harmony export */
        SummarizationPipeline: () => (
          /* binding */
          $
        ),
        /* harmony export */
        Text2TextGenerationPipeline: () => (
          /* binding */
          O
        ),
        /* harmony export */
        TextClassificationPipeline: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        TextGenerationPipeline: () => (
          /* binding */
          v
        ),
        /* harmony export */
        TextToAudioPipeline: () => (
          /* binding */
          he
        ),
        /* harmony export */
        TokenClassificationPipeline: () => (
          /* binding */
          z
        ),
        /* harmony export */
        TranslationPipeline: () => (
          /* binding */
          g
        ),
        /* harmony export */
        ZeroShotAudioClassificationPipeline: () => (
          /* binding */
          fe
        ),
        /* harmony export */
        ZeroShotClassificationPipeline: () => (
          /* binding */
          ee
        ),
        /* harmony export */
        ZeroShotImageClassificationPipeline: () => (
          /* binding */
          Ue
        ),
        /* harmony export */
        ZeroShotObjectDetectionPipeline: () => (
          /* binding */
          de
        ),
        /* harmony export */
        pipeline: () => (
          /* binding */
          ie
        )
        /* harmony export */
      });
      var f = s(
        /*! ./tokenizers.js */
        "./src/tokenizers.js"
      ), L = s(
        /*! ./models.js */
        "./src/models.js"
      ), j = s(
        /*! ./models/auto/processing_auto.js */
        "./src/models/auto/processing_auto.js"
      );
      s(
        /*! ./base/processing_utils.js */
        "./src/base/processing_utils.js"
      );
      var J = s(
        /*! ./utils/generic.js */
        "./src/utils/generic.js"
      ), W = s(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), w = s(
        /*! ./utils/maths.js */
        "./src/utils/maths.js"
      ), x = s(
        /*! ./utils/audio.js */
        "./src/utils/audio.js"
      ), y = s(
        /*! ./utils/tensor.js */
        "./src/utils/tensor.js"
      ), M = s(
        /*! ./utils/image.js */
        "./src/utils/image.js"
      );
      async function b(De) {
        return Array.isArray(De) || (De = [De]), await Promise.all(De.map((ce) => M.RawImage.read(ce)));
      }
      async function D(De, ce) {
        return Array.isArray(De) || (De = [De]), await Promise.all(De.map((ve) => typeof ve == "string" || ve instanceof URL ? (0, x.read_audio)(ve, ce) : ve instanceof Float64Array ? new Float32Array(ve) : ve));
      }
      function q(De, ce) {
        ce && (De = De.map((Ne) => Ne | 0));
        const [ve, Re, je, Ve] = De;
        return { xmin: ve, ymin: Re, xmax: je, ymax: Ve };
      }
      class se extends J.Callable {
        /**
         * Create a new Pipeline.
         * @param {Object} options An object containing the following properties:
         * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.
         * @param {PreTrainedModel} [options.model] The model used by the pipeline.
         * @param {PreTrainedTokenizer} [options.tokenizer=null] The tokenizer used by the pipeline (if any).
         * @param {Processor} [options.processor=null] The processor used by the pipeline (if any).
         */
        constructor({ task: ce, model: ve, tokenizer: Re = null, processor: je = null }) {
          super(), this.task = ce, this.model = ve, this.tokenizer = Re, this.processor = je;
        }
        /** @type {DisposeType} */
        async dispose() {
          await this.model.dispose();
        }
      }
      class oe extends /** @type {new (options: TextPipelineConstructorArgs) => TextClassificationPipelineType} */
      se {
        /**
         * Create a new TextClassificationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {TextClassificationPipelineCallback} */
        async _call(ce, {
          top_k: ve = 1
        } = {}) {
          const Re = this.tokenizer(ce, {
            padding: !0,
            truncation: !0
          }), je = await this.model(Re), Ve = this.model.config.problem_type === "multi_label_classification" ? (at) => at.sigmoid() : (at) => new y.Tensor(
            "float32",
            (0, w.softmax)(at.data),
            at.dims
          ), Ne = this.model.config.id2label, Ze = [];
          for (const at of je.logits) {
            const ft = Ve(at), dt = await (0, y.topk)(ft, ve), gt = dt[0].tolist(), ne = dt[1].tolist().map((K, pe) => ({
              label: Ne ? Ne[K] : `LABEL_${K}`,
              score: gt[pe]
            }));
            ve === 1 ? Ze.push(...ne) : Ze.push(ne);
          }
          return Array.isArray(ce) || ve === 1 ? (
            /** @type {TextClassificationOutput} */
            Ze
          ) : (
            /** @type {TextClassificationOutput[]} */
            Ze[0]
          );
        }
      }
      class z extends /** @type {new (options: TextPipelineConstructorArgs) => TokenClassificationPipelineType} */
      se {
        /**
         * Create a new TokenClassificationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {TokenClassificationPipelineCallback} */
        async _call(ce, {
          ignore_labels: ve = ["O"]
        } = {}) {
          const Re = Array.isArray(ce), je = this.tokenizer(Re ? ce : [ce], {
            padding: !0,
            truncation: !0
          }), Ne = (await this.model(je)).logits, Ze = this.model.config.id2label, at = [];
          for (let ft = 0; ft < Ne.dims[0]; ++ft) {
            const dt = je.input_ids[ft], gt = Ne[ft], F = [];
            for (let ne = 0; ne < gt.dims[0]; ++ne) {
              const K = gt[ne], pe = (0, w.max)(K.data)[1], Oe = Ze ? Ze[pe] : `LABEL_${pe}`;
              if (ve.includes(Oe))
                continue;
              const Qe = this.tokenizer.decode([dt[ne].item()], { skip_special_tokens: !0 });
              if (Qe === "")
                continue;
              const st = (0, w.softmax)(K.data);
              F.push({
                entity: Oe,
                score: st[pe],
                index: ne,
                word: Qe
                // TODO: Add support for start and end
                // start: null,
                // end: null,
              });
            }
            at.push(F);
          }
          return Re ? at : at[0];
        }
      }
      class V extends /** @type {new (options: TextPipelineConstructorArgs) => QuestionAnsweringPipelineType} */
      se {
        /**
         * Create a new QuestionAnsweringPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {QuestionAnsweringPipelineCallback} */
        async _call(ce, ve, {
          top_k: Re = 1
        } = {}) {
          const je = this.tokenizer(ce, {
            text_pair: ve,
            padding: !0,
            truncation: !0
          }), { start_logits: Ve, end_logits: Ne } = await this.model(je), Ze = je.input_ids.tolist(), at = je.attention_mask.tolist(), ft = this.tokenizer.all_special_ids, dt = [];
          for (let gt = 0; gt < Ve.dims[0]; ++gt) {
            const F = Ze[gt], ne = F.findIndex(
              (pt) => (
                // We use == to match bigint with number
                // @ts-ignore
                pt == this.tokenizer.sep_token_id
              )
            );
            at[gt].map((pt, It) => pt == 1 && (It === 0 || It > ne && ft.findIndex((St) => St == F[It]) === -1));
            const K = Ve[gt].tolist(), pe = Ne[gt].tolist();
            for (let pt = 1; pt < K.length; ++pt)
              (at[gt] == 0 || pt <= ne || ft.findIndex((It) => It == F[pt]) !== -1) && (K[pt] = -1 / 0, pe[pt] = -1 / 0);
            const Oe = (0, w.softmax)(K).map((pt, It) => [pt, It]), Qe = (0, w.softmax)(pe).map((pt, It) => [pt, It]);
            Oe[0][0] = 0, Qe[0][0] = 0;
            const st = (0, W.product)(Oe, Qe).filter((pt) => pt[0][1] <= pt[1][1]).map((pt) => [pt[0][1], pt[1][1], pt[0][0] * pt[1][0]]).sort((pt, It) => It[2] - pt[2]);
            for (let pt = 0; pt < Math.min(st.length, Re); ++pt) {
              const [It, St, Ot] = st[pt], At = F.slice(It, St + 1), nr = this.tokenizer.decode(At, {
                skip_special_tokens: !0
              });
              dt.push({
                answer: nr,
                score: Ot
              });
            }
          }
          return Re === 1 ? dt[0] : dt;
        }
      }
      class Y extends /** @type {new (options: TextPipelineConstructorArgs) => FillMaskPipelineType} */
      se {
        /**
         * Create a new FillMaskPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {FillMaskPipelineCallback} */
        async _call(ce, {
          top_k: ve = 5
        } = {}) {
          const Re = this.tokenizer(ce, {
            padding: !0,
            truncation: !0
          }), { logits: je } = await this.model(Re), Ve = [], Ne = Re.input_ids.tolist();
          for (let Ze = 0; Ze < Ne.length; ++Ze) {
            const at = Ne[Ze], ft = at.findIndex(
              (K) => (
                // We use == to match bigint with number
                // @ts-ignore
                K == this.tokenizer.mask_token_id
              )
            );
            if (ft === -1)
              throw Error(`Mask token (${this.tokenizer.mask_token}) not found in text.`);
            const dt = je[Ze][ft], gt = await (0, y.topk)(new y.Tensor(
              "float32",
              (0, w.softmax)(dt.data),
              dt.dims
            ), ve), F = gt[0].tolist(), ne = gt[1].tolist();
            Ve.push(ne.map((K, pe) => {
              const Oe = at.slice();
              return Oe[ft] = K, {
                score: F[pe],
                token: Number(K),
                token_str: this.tokenizer.decode([K]),
                sequence: this.tokenizer.decode(Oe, { skip_special_tokens: !0 })
              };
            }));
          }
          return Array.isArray(ce) ? Ve : Ve[0];
        }
      }
      class O extends /** @type {new (options: TextPipelineConstructorArgs) => Text2TextGenerationPipelineType} */
      se {
        /**
         * Create a new Text2TextGenerationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ve) {
          super(ve);
          /** @type {'generated_text'} */
          ge(this, "_key", "generated_text");
        }
        /** @type {Text2TextGenerationPipelineCallback} */
        async _call(ve, Re = {}) {
          Array.isArray(ve) || (ve = [ve]), this.model.config.prefix && (ve = ve.map((ft) => this.model.config.prefix + ft));
          const je = this.model.config.task_specific_params;
          je && je[this.task] && je[this.task].prefix && (ve = ve.map((ft) => je[this.task].prefix + ft));
          const Ve = this.tokenizer, Ne = {
            padding: !0,
            truncation: !0
          };
          let Ze;
          this instanceof g && "_build_translation_inputs" in Ve ? Ze = Ve._build_translation_inputs(ve, Ne, Re) : Ze = Ve(ve, Ne);
          const at = await this.model.generate({ ...Ze, ...Re });
          return Ve.batch_decode(
            /** @type {Tensor} */
            at,
            {
              skip_special_tokens: !0
            }
          ).map((ft) => ({ [this._key]: ft }));
        }
      }
      class $ extends /** @type {new (options: TextPipelineConstructorArgs) => SummarizationPipelineType} */
      /** @type {any} */
      O {
        /**
         * Create a new SummarizationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ve) {
          super(ve);
          /** @type {'summary_text'} */
          ge(this, "_key", "summary_text");
        }
      }
      class g extends /** @type {new (options: TextPipelineConstructorArgs) => TranslationPipelineType} */
      /** @type {any} */
      O {
        /**
         * Create a new TranslationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ve) {
          super(ve);
          /** @type {'translation_text'} */
          ge(this, "_key", "translation_text");
        }
      }
      function C(De) {
        return Array.isArray(De) && De.every((ce) => "role" in ce && "content" in ce);
      }
      class v extends /** @type {new (options: TextPipelineConstructorArgs) => TextGenerationPipelineType} */
      se {
        /**
         * Create a new TextGenerationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {TextGenerationPipelineCallback} */
        async _call(ce, ve = {}) {
          let Re = !1, je = !1, Ve;
          if (typeof ce == "string")
            Ve = ce = [ce];
          else if (Array.isArray(ce) && ce.every((ne) => typeof ne == "string"))
            Re = !0, Ve = /** @type {string[]} */
            ce;
          else {
            if (C(ce))
              ce = [
                /** @type {Chat} */
                ce
              ];
            else if (Array.isArray(ce) && ce.every(C))
              Re = !0;
            else
              throw new Error("Input must be a string, an array of strings, a Chat, or an array of Chats");
            je = !0, Ve = /** @type {string[]} */
            /** @type {Chat[]} */
            ce.map(
              (ne) => this.tokenizer.apply_chat_template(ne, {
                tokenize: !1,
                add_generation_prompt: !0
              })
            );
          }
          const Ne = ve.add_special_tokens ?? !1, Ze = je ? !1 : ve.return_full_text ?? !0;
          this.tokenizer.padding_side = "left";
          const at = this.tokenizer(Ve, {
            add_special_tokens: Ne,
            padding: !0,
            truncation: !0
          }), ft = (
            /** @type {Tensor} */
            await this.model.generate({
              ...at,
              ...ve
            })
          ), dt = this.tokenizer.batch_decode(ft, {
            skip_special_tokens: !0
          });
          let gt;
          !Ze && at.input_ids.dims.at(-1) > 0 && (gt = this.tokenizer.batch_decode(at.input_ids, {
            skip_special_tokens: !0
          }).map((ne) => ne.length));
          const F = Array.from({ length: ce.length }, (ne) => []);
          for (let ne = 0; ne < dt.length; ++ne) {
            const K = Math.floor(ne / ft.dims[0] * ce.length);
            gt && (dt[ne] = dt[ne].slice(gt[K])), F[K].push({
              generated_text: je ? [
                .../** @type {Chat[]} */
                ce[K],
                { role: "assistant", content: dt[ne] }
              ] : dt[ne]
            });
          }
          return !Re && F.length === 1 ? F[0] : F;
        }
      }
      class ee extends /** @type {new (options: TextPipelineConstructorArgs) => ZeroShotClassificationPipelineType} */
      se {
        /**
         * Create a new ZeroShotClassificationPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce), this.label2id = Object.fromEntries(
            Object.entries(
              /** @type {any} */
              this.model.config.label2id
            ).map(
              ([ve, Re]) => [ve.toLowerCase(), Re]
            )
          ), this.entailment_id = this.label2id.entailment, this.entailment_id === void 0 && (console.warn("Could not find 'entailment' in label2id mapping. Using 2 as entailment_id."), this.entailment_id = 2), this.contradiction_id = this.label2id.contradiction ?? this.label2id.not_entailment, this.contradiction_id === void 0 && (console.warn("Could not find 'contradiction' in label2id mapping. Using 0 as contradiction_id."), this.contradiction_id = 0);
        }
        /** @type {ZeroShotClassificationPipelineCallback} */
        async _call(ce, ve, {
          hypothesis_template: Re = "This example is {}.",
          multi_label: je = !1
        } = {}) {
          const Ve = Array.isArray(ce);
          Ve || (ce = [
            /** @type {string} */
            ce
          ]), Array.isArray(ve) || (ve = [ve]);
          const Ne = ve.map(
            (ft) => Re.replace("{}", ft)
          ), Ze = je || ve.length === 1, at = [];
          for (const ft of ce) {
            const dt = [];
            for (const ne of Ne) {
              const K = this.tokenizer(ft, {
                text_pair: ne,
                padding: !0,
                truncation: !0
              }), pe = await this.model(K);
              Ze ? dt.push([
                pe.logits.data[this.contradiction_id],
                pe.logits.data[this.entailment_id]
              ]) : dt.push(pe.logits.data[this.entailment_id]);
            }
            const F = (Ze ? dt.map((ne) => (0, w.softmax)(ne)[1]) : (0, w.softmax)(dt)).map((ne, K) => [ne, K]).sort((ne, K) => K[0] - ne[0]);
            at.push({
              sequence: ft,
              labels: F.map((ne) => ve[ne[1]]),
              scores: F.map((ne) => ne[0])
            });
          }
          return Ve ? at : at[0];
        }
      }
      class X extends /** @type {new (options: TextPipelineConstructorArgs) => FeatureExtractionPipelineType} */
      se {
        /**
         * Create a new FeatureExtractionPipeline.
         * @param {TextPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {FeatureExtractionPipelineCallback} */
        async _call(ce, {
          pooling: ve = (
            /** @type {'none'} */
            "none"
          ),
          normalize: Re = !1,
          quantize: je = !1,
          precision: Ve = (
            /** @type {'binary'} */
            "binary"
          )
        } = {}) {
          const Ne = this.tokenizer(ce, {
            padding: !0,
            truncation: !0
          }), Ze = await this.model(Ne);
          let at = Ze.last_hidden_state ?? Ze.logits ?? Ze.token_embeddings;
          if (ve !== "none") if (ve === "mean")
            at = (0, y.mean_pooling)(at, Ne.attention_mask);
          else if (ve === "cls")
            at = at.slice(null, 0);
          else
            throw Error(`Pooling method '${ve}' not supported.`);
          return Re && (at = at.normalize(2, -1)), je && (at = (0, y.quantize_embeddings)(at, Ve)), at;
        }
      }
      class le extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageFeatureExtractionPipelineType} */
      se {
        /**
         * Create a new ImageFeatureExtractionPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ImageFeatureExtractionPipelineCallback} */
        async _call(ce, {
          pool: ve = null
        } = {}) {
          const Re = await b(ce), { pixel_values: je } = await this.processor(Re), Ve = await this.model({ pixel_values: je });
          let Ne;
          if (ve) {
            if (!("pooler_output" in Ve))
              throw Error("No pooled output was returned. Make sure the model has a 'pooler' layer when using the 'pool' option.");
            Ne = Ve.pooler_output;
          } else
            Ne = Ve.last_hidden_state ?? Ve.logits ?? Ve.image_embeds;
          return Ne;
        }
      }
      class ue extends /** @type {new (options: AudioPipelineConstructorArgs) => AudioClassificationPipelineType} */
      se {
        /**
         * Create a new AudioClassificationPipeline.
         * @param {AudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {AudioClassificationPipelineCallback} */
        async _call(ce, {
          top_k: ve = 5
        } = {}) {
          const Re = this.processor.feature_extractor.config.sampling_rate, je = await D(ce, Re), Ve = this.model.config.id2label, Ne = [];
          for (const Ze of je) {
            const at = await this.processor(Ze), dt = (await this.model(at)).logits[0], gt = await (0, y.topk)(new y.Tensor(
              "float32",
              (0, w.softmax)(dt.data),
              dt.dims
            ), ve), F = gt[0].tolist(), K = gt[1].tolist().map((pe, Oe) => ({
              label: (
                /** @type {string} */
                Ve ? Ve[pe] : `LABEL_${pe}`
              ),
              score: (
                /** @type {number} */
                F[Oe]
              )
            }));
            Ne.push(K);
          }
          return Array.isArray(ce) ? Ne : Ne[0];
        }
      }
      class fe extends /** @type {new (options: TextAudioPipelineConstructorArgs) => ZeroShotAudioClassificationPipelineType} */
      se {
        /**
         * Create a new ZeroShotAudioClassificationPipeline.
         * @param {TextAudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ZeroShotAudioClassificationPipelineCallback} */
        async _call(ce, ve, {
          hypothesis_template: Re = "This is a sound of {}."
        } = {}) {
          const je = !Array.isArray(ce);
          je && (ce = [
            /** @type {AudioInput} */
            ce
          ]);
          const Ve = ve.map(
            (dt) => Re.replace("{}", dt)
          ), Ne = this.tokenizer(Ve, {
            padding: !0,
            truncation: !0
          }), Ze = this.processor.feature_extractor.config.sampling_rate, at = await D(ce, Ze), ft = [];
          for (const dt of at) {
            const gt = await this.processor(dt), F = await this.model({ ...Ne, ...gt }), ne = (0, w.softmax)(F.logits_per_audio.data);
            ft.push([...ne].map((K, pe) => ({
              score: K,
              label: ve[pe]
            })));
          }
          return je ? ft[0] : ft;
        }
      }
      class Ce extends /** @type {new (options: TextAudioPipelineConstructorArgs) => AutomaticSpeechRecognitionPipelineType} */
      se {
        /**
         * Create a new AutomaticSpeechRecognitionPipeline.
         * @param {TextAudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {AutomaticSpeechRecognitionPipelineCallback} */
        async _call(ce, ve = {}) {
          switch (this.model.config.model_type) {
            case "whisper":
              return this._call_whisper(ce, ve);
            case "wav2vec2":
            case "wav2vec2-bert":
            case "unispeech":
            case "unispeech-sat":
            case "hubert":
              return this._call_wav2vec2(ce, ve);
            case "moonshine":
              return this._call_moonshine(ce, ve);
            default:
              throw new Error(`AutomaticSpeechRecognitionPipeline does not support model type '${this.model.config.model_type}'.`);
          }
        }
        /**
         * @type {AutomaticSpeechRecognitionPipelineCallback}
         * @private
         */
        async _call_wav2vec2(ce, ve) {
          ve.language && console.warn('`language` parameter is not yet supported for `wav2vec2` models, defaulting to "English".'), ve.task && console.warn('`task` parameter is not yet supported for `wav2vec2` models, defaulting to "transcribe".');
          const Re = !Array.isArray(ce);
          Re && (ce = [
            /** @type {AudioInput} */
            ce
          ]);
          const je = this.processor.feature_extractor.config.sampling_rate, Ve = await D(ce, je), Ne = [];
          for (const Ze of Ve) {
            const at = await this.processor(Ze), dt = (await this.model(at)).logits[0], gt = [];
            for (const ne of dt)
              gt.push((0, w.max)(ne.data)[1]);
            const F = this.tokenizer.decode(gt);
            Ne.push({ text: F });
          }
          return Re ? Ne[0] : Ne;
        }
        /**
         * @type {AutomaticSpeechRecognitionPipelineCallback}
         * @private
         */
        async _call_whisper(ce, ve) {
          const Re = ve.return_timestamps ?? !1, je = ve.chunk_length_s ?? 0, Ve = ve.force_full_sequences ?? !1;
          let Ne = ve.stride_length_s ?? null;
          const Ze = { ...ve };
          Re === "word" && (Ze.return_token_timestamps = !0, Ze.return_timestamps = !1);
          const at = !Array.isArray(ce);
          at && (ce = [
            /** @type {AudioInput} */
            ce
          ]);
          const ft = this.processor.feature_extractor.config.chunk_length / this.model.config.max_source_positions, dt = this.processor.feature_extractor.config.hop_length, gt = this.processor.feature_extractor.config.sampling_rate, F = await D(ce, gt), ne = [];
          for (const K of F) {
            let pe = [];
            if (je > 0) {
              if (Ne === null)
                Ne = je / 6;
              else if (je <= Ne)
                throw Error("`chunk_length_s` must be larger than `stride_length_s`.");
              const st = gt * je, pt = gt * Ne, It = st - 2 * pt;
              let St = 0;
              for (; ; ) {
                const Ot = St + st, At = K.subarray(St, Ot), nr = await this.processor(At), gr = St === 0, kr = Ot >= K.length;
                if (pe.push({
                  stride: [
                    At.length,
                    gr ? 0 : pt,
                    kr ? 0 : pt
                  ],
                  input_features: nr.input_features,
                  is_last: kr
                }), kr) break;
                St += It;
              }
            } else
              pe = [{
                stride: [K.length, 0, 0],
                input_features: (await this.processor(K)).input_features,
                is_last: !0
              }];
            for (const st of pe) {
              Ze.num_frames = Math.floor(st.stride[0] / dt);
              const pt = await this.model.generate({
                inputs: st.input_features,
                ...Ze
              });
              Re === "word" ? (st.tokens = pt.sequences.tolist()[0], st.token_timestamps = pt.token_timestamps.tolist()[0].map(
                (It) => (0, w.round)(It, 2)
              )) : st.tokens = /** @type {Tensor} */
              pt[0].tolist(), st.stride = st.stride.map((It) => It / gt);
            }
            const [Oe, Qe] = this.tokenizer._decode_asr(pe, {
              time_precision: ft,
              return_timestamps: Re,
              force_full_sequences: Ve
            });
            ne.push({ text: Oe, ...Qe });
          }
          return at ? ne[0] : ne;
        }
        /**
         * @type {AutomaticSpeechRecognitionPipelineCallback}
         * @private
         */
        async _call_moonshine(ce, ve) {
          const Re = !Array.isArray(ce);
          Re && (ce = [
            /** @type {AudioInput} */
            ce
          ]);
          const je = this.processor.feature_extractor.config.sampling_rate, Ve = await D(ce, je), Ne = [];
          for (const Ze of Ve) {
            const at = await this.processor(Ze), ft = Math.floor(Ze.length / je) * 6, dt = await this.model.generate({ max_new_tokens: ft, ...ve, ...at }), gt = this.processor.batch_decode(dt, { skip_special_tokens: !0 })[0];
            Ne.push({ text: gt });
          }
          return Re ? Ne[0] : Ne;
        }
      }
      class xe extends /** @type {new (options: TextImagePipelineConstructorArgs) => ImageToTextPipelineType} */
      se {
        /**
         * Create a new ImageToTextPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ImageToTextPipelineCallback} */
        async _call(ce, ve = {}) {
          const Re = Array.isArray(ce), je = await b(ce), { pixel_values: Ve } = await this.processor(je), Ne = [];
          for (const Ze of Ve) {
            Ze.dims = [1, ...Ze.dims];
            const at = await this.model.generate({ inputs: Ze, ...ve }), ft = this.tokenizer.batch_decode(
              /** @type {Tensor} */
              at,
              {
                skip_special_tokens: !0
              }
            ).map((dt) => ({ generated_text: dt.trim() }));
            Ne.push(ft);
          }
          return Re ? Ne : Ne[0];
        }
      }
      class Le extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageClassificationPipelineType} */
      se {
        /**
         * Create a new ImageClassificationPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ImageClassificationPipelineCallback} */
        async _call(ce, {
          top_k: ve = 5
        } = {}) {
          const Re = await b(ce), { pixel_values: je } = await this.processor(Re), Ve = await this.model({ pixel_values: je }), Ne = this.model.config.id2label, Ze = [];
          for (const at of Ve.logits) {
            const ft = await (0, y.topk)(new y.Tensor(
              "float32",
              (0, w.softmax)(at.data),
              at.dims
            ), ve), dt = ft[0].tolist(), F = ft[1].tolist().map((ne, K) => ({
              label: (
                /** @type {string} */
                Ne ? Ne[ne] : `LABEL_${ne}`
              ),
              score: (
                /** @type {number} */
                dt[K]
              )
            }));
            Ze.push(F);
          }
          return Array.isArray(ce) ? Ze : Ze[0];
        }
      }
      class qe extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageSegmentationPipelineType} */
      se {
        /**
         * Create a new ImageSegmentationPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce), this.subtasks_mapping = {
            // Mapping of subtasks to their corresponding post-processing function names.
            panoptic: "post_process_panoptic_segmentation",
            instance: "post_process_instance_segmentation",
            semantic: "post_process_semantic_segmentation"
          };
        }
        /** @type {ImageSegmentationPipelineCallback} */
        async _call(ce, {
          threshold: ve = 0.5,
          mask_threshold: Re = 0.5,
          overlap_mask_area_threshold: je = 0.8,
          label_ids_to_fuse: Ve = null,
          target_sizes: Ne = null,
          subtask: Ze = null
        } = {}) {
          if (Array.isArray(ce) && ce.length !== 1)
            throw Error("Image segmentation pipeline currently only supports a batch size of 1.");
          const ft = await b(ce), dt = ft.map((Qe) => [Qe.height, Qe.width]), { pixel_values: gt, pixel_mask: F } = await this.processor(ft), ne = await this.model({ pixel_values: gt, pixel_mask: F });
          let K = null;
          if (Ze !== null)
            K = this.subtasks_mapping[Ze];
          else
            for (let [Qe, st] of Object.entries(this.subtasks_mapping))
              if (st in this.processor.image_processor) {
                K = this.processor.image_processor[st].bind(this.processor.image_processor), Ze = Qe;
                break;
              }
          const pe = this.model.config.id2label, Oe = [];
          if (Ze === "panoptic" || Ze === "instance") {
            const Qe = K(
              ne,
              ve,
              Re,
              je,
              Ve,
              Ne ?? dt
              // TODO FIX?
            )[0], st = Qe.segmentation;
            for (const pt of Qe.segments_info) {
              const It = new Uint8ClampedArray(st.data.length);
              for (let Ot = 0; Ot < st.data.length; ++Ot)
                st.data[Ot] === pt.id && (It[Ot] = 255);
              const St = new M.RawImage(It, st.dims[1], st.dims[0], 1);
              Oe.push({
                score: pt.score,
                label: pe[pt.label_id],
                mask: St
              });
            }
          } else if (Ze === "semantic") {
            const { segmentation: Qe, labels: st } = K(ne, Ne ?? dt)[0];
            for (const pt of st) {
              const It = new Uint8ClampedArray(Qe.data.length);
              for (let Ot = 0; Ot < Qe.data.length; ++Ot)
                Qe.data[Ot] === pt && (It[Ot] = 255);
              const St = new M.RawImage(It, Qe.dims[1], Qe.dims[0], 1);
              Oe.push({
                score: null,
                label: pe[pt],
                mask: St
              });
            }
          } else
            throw Error(`Subtask ${Ze} not supported.`);
          return Oe;
        }
      }
      class Ue extends /** @type {new (options: TextImagePipelineConstructorArgs) => ZeroShotImageClassificationPipelineType} */
      se {
        /**
         * Create a new ZeroShotImageClassificationPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ZeroShotImageClassificationPipelineCallback} */
        async _call(ce, ve, {
          hypothesis_template: Re = "This is a photo of {}"
        } = {}) {
          const je = Array.isArray(ce), Ve = await b(ce), Ne = ve.map(
            (F) => Re.replace("{}", F)
          ), Ze = this.tokenizer(Ne, {
            padding: this.model.config.model_type === "siglip" ? "max_length" : !0,
            truncation: !0
          }), { pixel_values: at } = await this.processor(Ve), ft = await this.model({ ...Ze, pixel_values: at }), dt = this.model.config.model_type === "siglip" ? (F) => F.sigmoid().data : (F) => (0, w.softmax)(F.data), gt = [];
          for (const F of ft.logits_per_image) {
            const K = [...dt(F)].map((pe, Oe) => ({
              score: pe,
              label: ve[Oe]
            }));
            K.sort((pe, Oe) => Oe.score - pe.score), gt.push(K);
          }
          return je ? gt : gt[0];
        }
      }
      class ut extends /** @type {new (options: ImagePipelineConstructorArgs) => ObjectDetectionPipelineType} */
      se {
        /**
         * Create a new ObjectDetectionPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ObjectDetectionPipelineCallback} */
        async _call(ce, {
          threshold: ve = 0.9,
          percentage: Re = !1
        } = {}) {
          const je = Array.isArray(ce);
          if (je && ce.length !== 1)
            throw Error("Object detection pipeline currently only supports a batch size of 1.");
          const Ve = await b(ce), Ne = Re ? null : Ve.map((ne) => [ne.height, ne.width]), { pixel_values: Ze, pixel_mask: at } = await this.processor(Ve), ft = await this.model({ pixel_values: Ze, pixel_mask: at }), dt = this.processor.image_processor.post_process_object_detection(ft, ve, Ne), gt = this.model.config.id2label, F = dt.map((ne) => ne.boxes.map((K, pe) => ({
            score: ne.scores[pe],
            label: gt[ne.classes[pe]],
            box: q(K, !Re)
          })));
          return je ? F : F[0];
        }
      }
      class de extends /** @type {new (options: TextImagePipelineConstructorArgs) => ZeroShotObjectDetectionPipelineType} */
      se {
        /**
         * Create a new ZeroShotObjectDetectionPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ZeroShotObjectDetectionPipelineCallback} */
        async _call(ce, ve, {
          threshold: Re = 0.1,
          top_k: je = null,
          percentage: Ve = !1
        } = {}) {
          const Ne = Array.isArray(ce), Ze = await b(ce), at = this.tokenizer(ve, {
            padding: !0,
            truncation: !0
          }), ft = await this.processor(Ze), dt = [];
          for (let gt = 0; gt < Ze.length; ++gt) {
            const F = Ze[gt], ne = Ve ? null : [[F.height, F.width]], K = ft.pixel_values[gt].unsqueeze_(0), pe = await this.model({ ...at, pixel_values: K }), Oe = this.processor.image_processor.post_process_object_detection(pe, Re, ne, !0)[0];
            let Qe = Oe.boxes.map((st, pt) => ({
              score: Oe.scores[pt],
              label: ve[Oe.classes[pt]],
              box: q(st, !Ve)
            })).sort((st, pt) => pt.score - st.score);
            je !== null && (Qe = Qe.slice(0, je)), dt.push(Qe);
          }
          return Ne ? dt : dt[0];
        }
      }
      class re extends /** @type {new (options: TextImagePipelineConstructorArgs) => DocumentQuestionAnsweringPipelineType} */
      se {
        /**
         * Create a new DocumentQuestionAnsweringPipeline.
         * @param {TextImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {DocumentQuestionAnsweringPipelineCallback} */
        async _call(ce, ve, Re = {}) {
          const je = (await b(ce))[0], { pixel_values: Ve } = await this.processor(je), Ne = `<s_docvqa><s_question>${ve}</s_question><s_answer>`, Ze = this.tokenizer(Ne, {
            add_special_tokens: !1,
            padding: !0,
            truncation: !0
          }).input_ids, at = await this.model.generate({
            inputs: Ve,
            max_length: this.model.config.decoder.max_position_embeddings,
            decoder_input_ids: Ze,
            ...Re
          }), dt = this.tokenizer.batch_decode(
            /** @type {Tensor} */
            at
          )[0].match(/<s_answer>(.*?)<\/s_answer>/);
          let gt = null;
          return dt && dt.length >= 2 && (gt = dt[1].trim()), [{ answer: gt }];
        }
      }
      class he extends /** @type {new (options: TextToAudioPipelineConstructorArgs) => TextToAudioPipelineType} */
      se {
        /**
         * Create a new TextToAudioPipeline.
         * @param {TextToAudioPipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ve) {
          super(ve);
          ge(this, "DEFAULT_VOCODER_ID", "Xenova/speecht5_hifigan");
          this.vocoder = ve.vocoder ?? null;
        }
        /** @type {TextToAudioPipelineCallback} */
        async _call(ve, {
          speaker_embeddings: Re = null
        } = {}) {
          return this.processor ? this._call_text_to_spectrogram(ve, { speaker_embeddings: Re }) : this._call_text_to_waveform(ve);
        }
        async _call_text_to_waveform(ve) {
          const Re = this.tokenizer(ve, {
            padding: !0,
            truncation: !0
          }), { waveform: je } = await this.model(Re), Ve = this.model.config.sampling_rate;
          return {
            audio: je.data,
            sampling_rate: Ve
          };
        }
        async _call_text_to_spectrogram(ve, { speaker_embeddings: Re }) {
          if (this.vocoder || (console.log("No vocoder specified, using default HifiGan vocoder."), this.vocoder = await L.AutoModel.from_pretrained(this.DEFAULT_VOCODER_ID, { dtype: "fp32" })), (typeof Re == "string" || Re instanceof URL) && (Re = new Float32Array(
            await (await fetch(Re)).arrayBuffer()
          )), Re instanceof Float32Array)
            Re = new y.Tensor(
              "float32",
              Re,
              [1, Re.length]
            );
          else if (!(Re instanceof y.Tensor))
            throw new Error("Speaker embeddings must be a `Tensor`, `Float32Array`, `string`, or `URL`.");
          const { input_ids: je } = this.tokenizer(ve, {
            padding: !0,
            truncation: !0
          }), { waveform: Ve } = await this.model.generate_speech(je, Re, { vocoder: this.vocoder }), Ne = this.processor.feature_extractor.config.sampling_rate;
          return {
            audio: Ve.data,
            sampling_rate: Ne
          };
        }
      }
      class Ee extends /** @type {new (options: ImagePipelineConstructorArgs) => ImageToImagePipelineType} */
      se {
        /**
         * Create a new ImageToImagePipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {ImageToImagePipelineCallback} */
        async _call(ce) {
          const ve = await b(ce), Re = await this.processor(ve), je = await this.model(Re), Ve = [];
          for (const Ne of je.reconstruction) {
            const Ze = Ne.squeeze().clamp_(0, 1).mul_(255).round_().to("uint8");
            Ve.push(M.RawImage.fromTensor(Ze));
          }
          return Ve.length > 1 ? Ve : Ve[0];
        }
      }
      class Be extends /** @type {new (options: ImagePipelineConstructorArgs) => DepthEstimationPipelineType} */
      se {
        /**
         * Create a new DepthEstimationPipeline.
         * @param {ImagePipelineConstructorArgs} options An object used to instantiate the pipeline.
         */
        constructor(ce) {
          super(ce);
        }
        /** @type {DepthEstimationPipelineCallback} */
        async _call(ce) {
          const ve = await b(ce), Re = await this.processor(ve), { predicted_depth: je } = await this.model(Re), Ve = [];
          for (let Ne = 0; Ne < ve.length; ++Ne) {
            const Ze = (0, y.interpolate)(je[Ne], ve[Ne].size.reverse(), "bilinear", !1), at = Ze.mul_(255 / (0, w.max)(Ze.data)[0]).to("uint8");
            Ve.push({
              predicted_depth: je[Ne],
              depth: M.RawImage.fromTensor(at)
            });
          }
          return Ve.length > 1 ? Ve : Ve[0];
        }
      }
      const et = Object.freeze({
        "text-classification": {
          tokenizer: f.AutoTokenizer,
          pipeline: oe,
          model: L.AutoModelForSequenceClassification,
          default: {
            // TODO: replace with original
            // "model": "distilbert-base-uncased-finetuned-sst-2-english",
            model: "Xenova/distilbert-base-uncased-finetuned-sst-2-english"
          },
          type: "text"
        },
        "token-classification": {
          tokenizer: f.AutoTokenizer,
          pipeline: z,
          model: L.AutoModelForTokenClassification,
          default: {
            // TODO: replace with original
            // "model": "Davlan/bert-base-multilingual-cased-ner-hrl",
            model: "Xenova/bert-base-multilingual-cased-ner-hrl"
          },
          type: "text"
        },
        "question-answering": {
          tokenizer: f.AutoTokenizer,
          pipeline: V,
          model: L.AutoModelForQuestionAnswering,
          default: {
            // TODO: replace with original
            // "model": "distilbert-base-cased-distilled-squad",
            model: "Xenova/distilbert-base-cased-distilled-squad"
          },
          type: "text"
        },
        "fill-mask": {
          tokenizer: f.AutoTokenizer,
          pipeline: Y,
          model: L.AutoModelForMaskedLM,
          default: {
            // TODO: replace with original
            // "model": "bert-base-uncased",
            model: "Xenova/bert-base-uncased"
          },
          type: "text"
        },
        summarization: {
          tokenizer: f.AutoTokenizer,
          pipeline: $,
          model: L.AutoModelForSeq2SeqLM,
          default: {
            // TODO: replace with original
            // "model": "sshleifer/distilbart-cnn-6-6",
            model: "Xenova/distilbart-cnn-6-6"
          },
          type: "text"
        },
        translation: {
          tokenizer: f.AutoTokenizer,
          pipeline: g,
          model: L.AutoModelForSeq2SeqLM,
          default: {
            // TODO: replace with original
            // "model": "t5-small",
            model: "Xenova/t5-small"
          },
          type: "text"
        },
        "text2text-generation": {
          tokenizer: f.AutoTokenizer,
          pipeline: O,
          model: L.AutoModelForSeq2SeqLM,
          default: {
            // TODO: replace with original
            // "model": "google/flan-t5-small",
            model: "Xenova/flan-t5-small"
          },
          type: "text"
        },
        "text-generation": {
          tokenizer: f.AutoTokenizer,
          pipeline: v,
          model: L.AutoModelForCausalLM,
          default: {
            // TODO: replace with original
            // "model": "gpt2",
            model: "Xenova/gpt2"
          },
          type: "text"
        },
        "zero-shot-classification": {
          tokenizer: f.AutoTokenizer,
          pipeline: ee,
          model: L.AutoModelForSequenceClassification,
          default: {
            // TODO: replace with original
            // "model": "typeform/distilbert-base-uncased-mnli",
            model: "Xenova/distilbert-base-uncased-mnli"
          },
          type: "text"
        },
        "audio-classification": {
          pipeline: ue,
          model: L.AutoModelForAudioClassification,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "superb/wav2vec2-base-superb-ks",
            model: "Xenova/wav2vec2-base-superb-ks"
          },
          type: "audio"
        },
        "zero-shot-audio-classification": {
          tokenizer: f.AutoTokenizer,
          pipeline: fe,
          model: L.AutoModel,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "laion/clap-htsat-fused",
            model: "Xenova/clap-htsat-unfused"
          },
          type: "multimodal"
        },
        "automatic-speech-recognition": {
          tokenizer: f.AutoTokenizer,
          pipeline: Ce,
          model: [L.AutoModelForSpeechSeq2Seq, L.AutoModelForCTC],
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "openai/whisper-tiny.en",
            model: "Xenova/whisper-tiny.en"
          },
          type: "multimodal"
        },
        "text-to-audio": {
          tokenizer: f.AutoTokenizer,
          pipeline: he,
          model: [L.AutoModelForTextToWaveform, L.AutoModelForTextToSpectrogram],
          processor: [
            j.AutoProcessor,
            /* Some don't use a processor */
            null
          ],
          default: {
            // TODO: replace with original
            // "model": "microsoft/speecht5_tts",
            model: "Xenova/speecht5_tts"
          },
          type: "text"
        },
        "image-to-text": {
          tokenizer: f.AutoTokenizer,
          pipeline: xe,
          model: L.AutoModelForVision2Seq,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "nlpconnect/vit-gpt2-image-captioning",
            model: "Xenova/vit-gpt2-image-captioning"
          },
          type: "multimodal"
        },
        "image-classification": {
          // no tokenizer
          pipeline: Le,
          model: L.AutoModelForImageClassification,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "google/vit-base-patch16-224",
            model: "Xenova/vit-base-patch16-224"
          },
          type: "multimodal"
        },
        "image-segmentation": {
          // no tokenizer
          pipeline: qe,
          model: [L.AutoModelForImageSegmentation, L.AutoModelForSemanticSegmentation, L.AutoModelForUniversalSegmentation],
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "facebook/detr-resnet-50-panoptic",
            model: "Xenova/detr-resnet-50-panoptic"
          },
          type: "multimodal"
        },
        "zero-shot-image-classification": {
          tokenizer: f.AutoTokenizer,
          pipeline: Ue,
          model: L.AutoModel,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "openai/clip-vit-base-patch32",
            model: "Xenova/clip-vit-base-patch32"
          },
          type: "multimodal"
        },
        "object-detection": {
          // no tokenizer
          pipeline: ut,
          model: L.AutoModelForObjectDetection,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "facebook/detr-resnet-50",
            model: "Xenova/detr-resnet-50"
          },
          type: "multimodal"
        },
        "zero-shot-object-detection": {
          tokenizer: f.AutoTokenizer,
          pipeline: de,
          model: L.AutoModelForZeroShotObjectDetection,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "google/owlvit-base-patch32",
            model: "Xenova/owlvit-base-patch32"
          },
          type: "multimodal"
        },
        "document-question-answering": {
          tokenizer: f.AutoTokenizer,
          pipeline: re,
          model: L.AutoModelForDocumentQuestionAnswering,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "naver-clova-ix/donut-base-finetuned-docvqa",
            model: "Xenova/donut-base-finetuned-docvqa"
          },
          type: "multimodal"
        },
        "image-to-image": {
          // no tokenizer
          pipeline: Ee,
          model: L.AutoModelForImageToImage,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "caidas/swin2SR-classical-sr-x2-64",
            model: "Xenova/swin2SR-classical-sr-x2-64"
          },
          type: "image"
        },
        "depth-estimation": {
          // no tokenizer
          pipeline: Be,
          model: L.AutoModelForDepthEstimation,
          processor: j.AutoProcessor,
          default: {
            // TODO: replace with original
            // "model": "Intel/dpt-large",
            model: "Xenova/dpt-large"
          },
          type: "image"
        },
        // This task serves as a useful interface for dealing with sentence-transformers (https://huggingface.co/sentence-transformers).
        "feature-extraction": {
          tokenizer: f.AutoTokenizer,
          pipeline: X,
          model: L.AutoModel,
          default: {
            // TODO: replace with original
            // "model": "sentence-transformers/all-MiniLM-L6-v2",
            model: "Xenova/all-MiniLM-L6-v2"
          },
          type: "text"
        },
        "image-feature-extraction": {
          processor: j.AutoProcessor,
          pipeline: le,
          model: [L.AutoModelForImageFeatureExtraction, L.AutoModel],
          default: {
            // TODO: replace with original
            // "model": "google/vit-base-patch16-224",
            model: "Xenova/vit-base-patch16-224-in21k"
          },
          type: "image"
        }
      }), Xe = Object.freeze({
        "sentiment-analysis": "text-classification",
        ner: "token-classification",
        // "vqa": "visual-question-answering", // TODO: Add
        asr: "automatic-speech-recognition",
        "text-to-speech": "text-to-audio",
        // Add for backwards compatibility
        embeddings: "feature-extraction"
      });
      async function ie(De, ce = null, {
        progress_callback: ve = null,
        config: Re = null,
        cache_dir: je = null,
        local_files_only: Ve = !1,
        revision: Ne = "main",
        device: Ze = null,
        dtype: at = null,
        model_file_name: ft = null,
        session_options: dt = {}
      } = {}) {
        De = Xe[De] ?? De;
        const gt = et[De.split("_", 1)[0]];
        if (!gt)
          throw Error(`Unsupported pipeline: ${De}. Must be one of [${Object.keys(et)}]`);
        ce || (ce = gt.default.model, console.log(`No model specified. Using default model: "${ce}".`));
        const F = {
          progress_callback: ve,
          config: Re,
          cache_dir: je,
          local_files_only: Ve,
          revision: Ne,
          device: Ze,
          dtype: at,
          model_file_name: ft,
          session_options: dt
        }, ne = /* @__PURE__ */ new Map([
          ["tokenizer", gt.tokenizer],
          ["model", gt.model],
          ["processor", gt.processor]
        ]), K = await Je(ne, ce, F);
        K.task = De, (0, W.dispatchCallback)(ve, {
          status: "ready",
          task: De,
          model: ce
        });
        const pe = gt.pipeline;
        return new pe(K);
      }
      async function Je(De, ce, ve) {
        const Re = /* @__PURE__ */ Object.create(null), je = [];
        for (const [Ve, Ne] of De.entries()) {
          if (!Ne) continue;
          let Ze;
          Array.isArray(Ne) ? Ze = new Promise(async (at, ft) => {
            var gt, F;
            let dt;
            for (const ne of Ne) {
              if (ne === null) {
                at(null);
                return;
              }
              try {
                at(await ne.from_pretrained(ce, ve));
                return;
              } catch (K) {
                if ((gt = K.message) != null && gt.includes("Unsupported model type"))
                  dt = K;
                else if ((F = K.message) != null && F.includes("Could not locate file"))
                  dt = K;
                else {
                  ft(K);
                  return;
                }
              }
            }
            ft(dt);
          }) : Ze = Ne.from_pretrained(ce, ve), Re[Ve] = Ze, je.push(Ze);
        }
        await Promise.all(je);
        for (const [Ve, Ne] of Object.entries(Re))
          Re[Ve] = await Ne;
        return Re;
      }
    }
  ),
  /***/
  "./src/tokenizers.js": (
    /*!***************************!*\
      !*** ./src/tokenizers.js ***!
      \***************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        AlbertTokenizer: () => (
          /* binding */
          ks
        ),
        /* harmony export */
        AutoTokenizer: () => (
          /* binding */
          Mn
        ),
        /* harmony export */
        BartTokenizer: () => (
          /* binding */
          tr
        ),
        /* harmony export */
        BertTokenizer: () => (
          /* binding */
          Qs
        ),
        /* harmony export */
        BlenderbotSmallTokenizer: () => (
          /* binding */
          In
        ),
        /* harmony export */
        BlenderbotTokenizer: () => (
          /* binding */
          gn
        ),
        /* harmony export */
        BloomTokenizer: () => (
          /* binding */
          Ns
        ),
        /* harmony export */
        CLIPTokenizer: () => (
          /* binding */
          Sn
        ),
        /* harmony export */
        CamembertTokenizer: () => (
          /* binding */
          nt
        ),
        /* harmony export */
        CodeGenTokenizer: () => (
          /* binding */
          kn
        ),
        /* harmony export */
        CodeLlamaTokenizer: () => (
          /* binding */
          js
        ),
        /* harmony export */
        CohereTokenizer: () => (
          /* binding */
          Fn
        ),
        /* harmony export */
        ConvBertTokenizer: () => (
          /* binding */
          As
        ),
        /* harmony export */
        DebertaTokenizer: () => (
          /* binding */
          os
        ),
        /* harmony export */
        DebertaV2Tokenizer: () => (
          /* binding */
          $s
        ),
        /* harmony export */
        DistilBertTokenizer: () => (
          /* binding */
          as
        ),
        /* harmony export */
        ElectraTokenizer: () => (
          /* binding */
          Ft
        ),
        /* harmony export */
        EsmTokenizer: () => (
          /* binding */
          ls
        ),
        /* harmony export */
        FalconTokenizer: () => (
          /* binding */
          Us
        ),
        /* harmony export */
        GPT2Tokenizer: () => (
          /* binding */
          bs
        ),
        /* harmony export */
        GPTNeoXTokenizer: () => (
          /* binding */
          xs
        ),
        /* harmony export */
        GemmaTokenizer: () => (
          /* binding */
          Zs
        ),
        /* harmony export */
        Grok1Tokenizer: () => (
          /* binding */
          _n
        ),
        /* harmony export */
        HerbertTokenizer: () => (
          /* binding */
          cs
        ),
        /* harmony export */
        LlamaTokenizer: () => (
          /* binding */
          Pn
        ),
        /* harmony export */
        M2M100Tokenizer: () => (
          /* binding */
          zt
        ),
        /* harmony export */
        MBart50Tokenizer: () => (
          /* binding */
          Rs
        ),
        /* harmony export */
        MBartTokenizer: () => (
          /* binding */
          ts
        ),
        /* harmony export */
        MPNetTokenizer: () => (
          /* binding */
          Xn
        ),
        /* harmony export */
        MarianTokenizer: () => (
          /* binding */
          Ws
        ),
        /* harmony export */
        MgpstrTokenizer: () => (
          /* binding */
          yn
        ),
        /* harmony export */
        MobileBertTokenizer: () => (
          /* binding */
          Bs
        ),
        /* harmony export */
        NllbTokenizer: () => (
          /* binding */
          Ts
        ),
        /* harmony export */
        NougatTokenizer: () => (
          /* binding */
          Yr
        ),
        /* harmony export */
        PreTrainedTokenizer: () => (
          /* binding */
          Nt
        ),
        /* harmony export */
        Qwen2Tokenizer: () => (
          /* binding */
          mn
        ),
        /* harmony export */
        RoFormerTokenizer: () => (
          /* binding */
          Ys
        ),
        /* harmony export */
        RobertaTokenizer: () => (
          /* binding */
          Js
        ),
        /* harmony export */
        SiglipTokenizer: () => (
          /* binding */
          $n
        ),
        /* harmony export */
        SpeechT5Tokenizer: () => (
          /* binding */
          or
        ),
        /* harmony export */
        SqueezeBertTokenizer: () => (
          /* binding */
          Ss
        ),
        /* harmony export */
        T5Tokenizer: () => (
          /* binding */
          lr
        ),
        /* harmony export */
        TokenizerModel: () => (
          /* binding */
          le
        ),
        /* harmony export */
        VitsTokenizer: () => (
          /* binding */
          wn
        ),
        /* harmony export */
        Wav2Vec2CTCTokenizer: () => (
          /* binding */
          An
        ),
        /* harmony export */
        WhisperTokenizer: () => (
          /* binding */
          fn
        ),
        /* harmony export */
        XLMRobertaTokenizer: () => (
          /* binding */
          Cn
        ),
        /* harmony export */
        XLMTokenizer: () => (
          /* binding */
          _t
        ),
        /* harmony export */
        is_chinese_char: () => (
          /* binding */
          Y
        )
        /* harmony export */
      });
      var f = s(
        /*! ./utils/generic.js */
        "./src/utils/generic.js"
      ), L = s(
        /*! ./utils/core.js */
        "./src/utils/core.js"
      ), j = s(
        /*! ./utils/hub.js */
        "./src/utils/hub.js"
      ), J = s(
        /*! ./utils/maths.js */
        "./src/utils/maths.js"
      ), W = s(
        /*! ./utils/tensor.js */
        "./src/utils/tensor.js"
      ), w = s(
        /*! ./utils/data-structures.js */
        "./src/utils/data-structures.js"
      ), x = s(
        /*! @huggingface/jinja */
        "./node_modules/@huggingface/jinja/dist/index.js"
      ), y = s(
        /*! ./models/whisper/common_whisper.js */
        "./src/models/whisper/common_whisper.js"
      );
      s(
        /*! ./utils/constants.js */
        "./src/utils/constants.js"
      );
      async function M(Te, P) {
        const H = await Promise.all([
          (0, j.getModelJSON)(Te, "tokenizer.json", !0, P),
          (0, j.getModelJSON)(Te, "tokenizer_config.json", !0, P)
        ]);
        return P.legacy !== null && (H[1].legacy = P.legacy), H;
      }
      function b(Te, P) {
        const H = [];
        let ae = 0;
        for (const Me of Te.matchAll(P)) {
          const Pe = Me[0];
          ae < Me.index && H.push(Te.slice(ae, Me.index)), Pe.length > 0 && H.push(Pe), ae = Me.index + Pe.length;
        }
        return ae < Te.length && H.push(Te.slice(ae)), H;
      }
      function D(Te, P = !0) {
        if (Te.Regex !== void 0) {
          let H = Te.Regex.replace(/\\([#&~])/g, "$1");
          for (const [ae, Me] of ee)
            H = H.replaceAll(ae, Me);
          return new RegExp(H, "gu");
        } else if (Te.String !== void 0) {
          const H = (0, L.escapeRegExp)(Te.String);
          return new RegExp(P ? H : `(${H})`, "gu");
        } else
          return console.warn("Unknown pattern type:", Te), null;
      }
      function q(Te) {
        return new Map(Object.entries(Te));
      }
      function se(Te) {
        const P = Te.dims;
        switch (P.length) {
          case 1:
            return Te.tolist();
          case 2:
            if (P[0] !== 1)
              throw new Error("Unable to decode tensor with `batch size !== 1`. Use `tokenizer.batch_decode(...)` for batched inputs.");
            return Te.tolist()[0];
          default:
            throw new Error(`Expected tensor to have 1-2 dimensions, got ${P.length}.`);
        }
      }
      function oe(Te) {
        return Te.replace(/ \./g, ".").replace(/ \?/g, "?").replace(/ \!/g, "!").replace(/ ,/g, ",").replace(/ \' /g, "'").replace(/ n\'t/g, "n't").replace(/ \'m/g, "'m").replace(/ \'s/g, "'s").replace(/ \'ve/g, "'ve").replace(/ \'re/g, "'re");
      }
      function z(Te) {
        return Te.replace(new RegExp("\\p{M}", "gu"), "");
      }
      function V(Te) {
        return z(Te.toLowerCase());
      }
      function Y(Te) {
        return Te >= 19968 && Te <= 40959 || Te >= 13312 && Te <= 19903 || Te >= 131072 && Te <= 173791 || Te >= 173824 && Te <= 177983 || Te >= 177984 && Te <= 178207 || Te >= 178208 && Te <= 183983 || Te >= 63744 && Te <= 64255 || Te >= 194560 && Te <= 195103;
      }
      function O(Te, P, H) {
        const ae = [];
        let Me = 0;
        for (; Me < Te.length; ) {
          if (ae.push(Te[Me]), (P.get(Te[Me]) ?? H) !== H) {
            ++Me;
            continue;
          }
          for (; ++Me < Te.length && (P.get(Te[Me]) ?? H) === H; )
            P.get(ae.at(-1)) !== H && (ae[ae.length - 1] += Te[Me]);
        }
        return ae;
      }
      function $(Te) {
        return Te.match(/\S+/g) || [];
      }
      const g = "\\p{P}\\u0021-\\u002F\\u003A-\\u0040\\u005B-\\u0060\\u007B-\\u007E", C = new RegExp(`^[${g}]+$`, "gu"), v = ".,!?…。，、।۔،", ee = /* @__PURE__ */ new Map([
        // This uses the case insensitive group modifier, which is not supported in JavaScript.
        // When parsing the regex, an "Invalid group" error is thrown.
        ["(?i:'s|'t|'re|'ve|'m|'ll|'d)", "(?:'([sS]|[tT]|[rR][eE]|[vV][eE]|[mM]|[lL][lL]|[dD]))"],
        // Used to override the default (invalid) regex of the bloom pretokenizer.
        // For more information, see https://github.com/huggingface/transformers.js/issues/94
        [` ?[^(\\s|[${v}])]+`, ` ?[^\\s${v}]+`]
      ]);
      class X {
        /**
         * Creates a new instance of AddedToken.
         * @param {Object} config Added token configuration object.
         * @param {string} config.content The content of the added token.
         * @param {number} config.id The id of the added token.
         * @param {boolean} [config.single_word=false] Whether this token must be a single word or can break words.
         * @param {boolean} [config.lstrip=false] Whether this token should strip whitespaces on its left.
         * @param {boolean} [config.rstrip=false] Whether this token should strip whitespaces on its right.
         * @param {boolean} [config.normalized=false] Whether this token should be normalized.
         * @param {boolean} [config.special=false] Whether this token is special.
         */
        constructor(P) {
          this.content = P.content, this.id = P.id, this.single_word = P.single_word ?? !1, this.lstrip = P.lstrip ?? !1, this.rstrip = P.rstrip ?? !1, this.special = P.special ?? !1, this.normalized = P.normalized ?? null;
        }
      }
      class le extends f.Callable {
        /**
         * Creates a new instance of TokenizerModel.
         * @param {Object} config The configuration object for the TokenizerModel.
         */
        constructor(P) {
          super(), this.config = P, this.vocab = [], this.tokens_to_ids = /* @__PURE__ */ new Map(), this.unk_token_id = void 0, this.unk_token = void 0, this.end_of_word_suffix = void 0, this.fuse_unk = this.config.fuse_unk ?? !1;
        }
        /**
         * Instantiates a new TokenizerModel instance based on the configuration object provided.
         * @param {Object} config The configuration object for the TokenizerModel.
         * @param {...*} args Optional arguments to pass to the specific TokenizerModel constructor.
         * @returns {TokenizerModel} A new instance of a TokenizerModel.
         * @throws Will throw an error if the TokenizerModel type in the config is not recognized.
         */
        static fromConfig(P, ...H) {
          switch (P.type) {
            case "WordPiece":
              return new ue(P);
            case "Unigram":
              return new fe(P, ...H);
            case "BPE":
              return new Le(P);
            default:
              if (P.vocab)
                return Array.isArray(P.vocab) ? new fe(P, ...H) : new qe(P, ...H);
              throw new Error(`Unknown TokenizerModel type: ${P.type}`);
          }
        }
        /**
         * Internal function to call the TokenizerModel instance.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} The encoded tokens.
         */
        _call(P) {
          return P = this.encode(P), this.fuse_unk && (P = O(P, this.tokens_to_ids, this.unk_token_id)), P;
        }
        /**
         * Encodes a list of tokens into a list of token IDs.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} The encoded tokens.
         * @throws Will throw an error if not implemented in a subclass.
         */
        encode(P) {
          throw Error("encode should be implemented in subclass.");
        }
        /**
         * Converts a list of tokens into a list of token IDs.
         * @param {string[]} tokens The tokens to convert.
         * @returns {number[]} The converted token IDs.
         */
        convert_tokens_to_ids(P) {
          return P.map((H) => this.tokens_to_ids.get(H) ?? this.unk_token_id);
        }
        /**
         * Converts a list of token IDs into a list of tokens.
         * @param {number[]|bigint[]} ids The token IDs to convert.
         * @returns {string[]} The converted tokens.
         */
        convert_ids_to_tokens(P) {
          return P.map((H) => this.vocab[H] ?? this.unk_token);
        }
      }
      class ue extends le {
        /**
         * @param {Object} config The configuration object.
         * @param {Object} config.vocab A mapping of tokens to ids.
         * @param {string} config.unk_token The unknown token string.
         * @param {string} config.continuing_subword_prefix The prefix to use for continuing subwords.
         * @param {number} [config.max_input_chars_per_word=100] The maximum number of characters per word.
         */
        constructor(P) {
          super(P), this.tokens_to_ids = q(P.vocab), this.unk_token_id = this.tokens_to_ids.get(P.unk_token), this.unk_token = P.unk_token, this.max_input_chars_per_word = P.max_input_chars_per_word ?? 100, this.vocab = new Array(this.tokens_to_ids.size);
          for (const [H, ae] of this.tokens_to_ids)
            this.vocab[ae] = H;
        }
        /**
         * Encodes an array of tokens using WordPiece encoding.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} An array of encoded tokens.
         */
        encode(P) {
          const H = [];
          for (const ae of P) {
            const Me = [...ae];
            if (Me.length > this.max_input_chars_per_word) {
              H.push(this.unk_token);
              continue;
            }
            let Pe = !1, He = 0;
            const ct = [];
            for (; He < Me.length; ) {
              let yt = Me.length, ht = null;
              for (; He < yt; ) {
                let ot = Me.slice(He, yt).join("");
                if (He > 0 && (ot = this.config.continuing_subword_prefix + ot), this.tokens_to_ids.has(ot)) {
                  ht = ot;
                  break;
                }
                --yt;
              }
              if (ht === null) {
                Pe = !0;
                break;
              }
              ct.push(ht), He = yt;
            }
            Pe ? H.push(this.unk_token) : H.push(...ct);
          }
          return H;
        }
      }
      class fe extends le {
        /**
         * Create a new Unigram tokenizer model.
         * @param {Object} config The configuration object for the Unigram model.
         * @param {number} config.unk_id The ID of the unknown token
         * @param {any[][]} config.vocab A 2D array representing a mapping of tokens to scores.
         * @param {Object} moreConfig Additional configuration object for the Unigram model.
         */
        constructor(P, H) {
          super(P);
          const ae = P.vocab.length;
          this.vocab = new Array(ae), this.scores = new Array(ae);
          for (let Me = 0; Me < ae; ++Me) {
            const Pe = P.vocab[Me];
            this.vocab[Me] = Pe[0], this.scores[Me] = Pe[1];
          }
          this.unk_token_id = P.unk_id, this.unk_token = this.vocab[P.unk_id], this.tokens_to_ids = new Map(this.vocab.map((Me, Pe) => [Me, Pe])), this.bos_token = " ", this.bos_token_id = this.tokens_to_ids.get(this.bos_token), this.eos_token = H.eos_token, this.eos_token_id = this.tokens_to_ids.get(this.eos_token), this.unk_token = this.vocab[this.unk_token_id], this.minScore = (0, J.min)(this.scores)[0], this.unk_score = this.minScore - 10, this.scores[this.unk_token_id] = this.unk_score, this.trie = new w.CharTrie(), this.trie.extend(this.vocab), this.fuse_unk = !0;
        }
        /**
         * Populates lattice nodes.
         * @param {TokenLattice} lattice The token lattice to populate with nodes.
         */
        populateNodes(P) {
          const H = P.chars, ae = 1;
          let Me = 0;
          for (; Me < H.length; ) {
            let Pe = !1;
            const He = H.slice(Me).join(""), ct = this.trie.commonPrefixSearch(He);
            for (const yt of ct) {
              const ht = this.tokens_to_ids.get(yt), ot = this.scores[ht], Pt = (0, L.len)(yt);
              P.insert(Me, Pt, ot, ht), !Pe && Pt === ae && (Pe = !0);
            }
            Pe || P.insert(Me, ae, this.unk_score, this.unk_token_id), Me += ae;
          }
        }
        /**
         * Encodes an array of tokens into an array of subtokens using the unigram model.
         *
         * @param {string} normalized The normalized string.
         * @returns {string[]} An array of subtokens obtained by encoding the input tokens using the unigram model.
         */
        tokenize(P) {
          const H = new w.TokenLattice(P, this.bos_token_id, this.eos_token_id);
          return this.populateNodes(H), H.tokens();
        }
        /**
         * Encodes an array of tokens using Unigram encoding.
         * @param {string[]} tokens The tokens to encode.
         * @returns {string[]} An array of encoded tokens.
         */
        encode(P) {
          const H = [];
          for (const ae of P) {
            const Me = this.tokenize(ae);
            H.push(...Me);
          }
          return H;
        }
      }
      const Ce = (() => {
        const Te = [
          ...Array.from({ length: 94 }, (Me, Pe) => Pe + 33),
          ...Array.from({ length: 12 }, (Me, Pe) => Pe + 161),
          ...Array.from({ length: 82 }, (Me, Pe) => Pe + 174)
        ], P = Te.slice();
        let H = 0;
        for (let Me = 0; Me < 256; ++Me)
          Te.includes(Me) || (Te.push(Me), P.push(256 + H), H += 1);
        const ae = P.map((Me) => String.fromCharCode(Me));
        return Object.fromEntries(Te.map((Me, Pe) => [Me, ae[Pe]]));
      })(), xe = (0, L.reverseDictionary)(Ce);
      class Le extends le {
        /**
         * Create a BPE instance.
         * @param {Object} config The configuration object for BPE.
         * @param {Object} config.vocab A mapping of tokens to ids.
         * @param {string[]|[string, string][]} config.merges An array of BPE merges as strings.
         * @param {string} config.unk_token The unknown token used for out of vocabulary words.
         * @param {string} config.end_of_word_suffix The suffix to place at the end of each word.
         * @param {string} [config.continuing_subword_suffix] The suffix to insert between words.
         * @param {boolean} [config.byte_fallback=false] Whether to use spm byte-fallback trick (defaults to False)
         * @param {boolean} [config.ignore_merges=false] Whether or not to match tokens with the vocab before using merges.
         */
        constructor(P) {
          super(P), this.tokens_to_ids = q(P.vocab), this.unk_token_id = this.tokens_to_ids.get(P.unk_token), this.unk_token = P.unk_token, this.vocab = new Array(this.tokens_to_ids.size);
          for (const [ae, Me] of this.tokens_to_ids)
            this.vocab[Me] = ae;
          const H = Array.isArray(P.merges[0]);
          this.merges = H ? (
            /** @type {[string, string][]} */
            P.merges
          ) : (
            /** @type {string[]} */
            P.merges.map((ae) => (
              /** @type {[string, string]} */
              ae.split(" ", 2)
            ))
          ), this.bpe_ranks = new Map(this.merges.map((ae, Me) => [JSON.stringify(ae), Me])), this.end_of_word_suffix = P.end_of_word_suffix, this.continuing_subword_suffix = P.continuing_subword_suffix ?? null, this.byte_fallback = this.config.byte_fallback ?? !1, this.byte_fallback && (this.text_encoder = new TextEncoder()), this.ignore_merges = this.config.ignore_merges ?? !1, this.cache = /* @__PURE__ */ new Map();
        }
        /**
         * Apply Byte-Pair-Encoding (BPE) to a given token. Efficient heap-based priority
         * queue implementation adapted from https://github.com/belladoreai/llama-tokenizer-js.
         * @param {string} token The token to encode.
         * @returns {string[]} The BPE encoded tokens.
         */
        bpe(P) {
          if (P.length === 0)
            return [];
          const H = this.cache.get(P);
          if (H !== void 0)
            return H;
          const ae = Array.from(P);
          this.end_of_word_suffix && (ae[ae.length - 1] += this.end_of_word_suffix);
          let Me = [];
          if (ae.length > 1) {
            const Pe = new w.PriorityQueue((yt, ht) => yt.score < ht.score);
            let He = {
              token: ae[0],
              bias: 0,
              prev: null,
              next: null
            }, ct = He;
            for (let yt = 1; yt < ae.length; ++yt) {
              const ht = {
                bias: yt / ae.length,
                // Add fractional component to break ties
                token: ae[yt],
                prev: ct,
                next: null
              };
              ct.next = ht, this._add_node(Pe, ct), ct = ht;
            }
            for (; !Pe.isEmpty(); ) {
              const yt = Pe.pop();
              if (yt.deleted || !yt.next || yt.next.deleted) continue;
              if (yt.deleted = !0, yt.next.deleted = !0, yt.prev) {
                const ot = { ...yt.prev };
                yt.prev.deleted = !0, yt.prev = ot, ot.prev ? ot.prev.next = ot : He = ot;
              }
              const ht = {
                token: yt.token + yt.next.token,
                bias: yt.bias,
                prev: yt.prev,
                next: yt.next.next
              };
              ht.prev ? (ht.prev.next = ht, this._add_node(Pe, ht.prev)) : He = ht, ht.next && (ht.next.prev = ht, this._add_node(Pe, ht));
            }
            for (let yt = He; yt !== null; yt = yt.next)
              Me.push(yt.token);
          } else
            Me = ae;
          if (this.continuing_subword_suffix)
            for (let Pe = 0; Pe < Me.length - 1; ++Pe)
              Me[Pe] += this.continuing_subword_suffix;
          return this.cache.set(P, Me), Me;
        }
        /**
         * Helper function to add a node to the priority queue.
         * @param {PriorityQueue} queue 
         * @param {BPENode} node
         * @private
         */
        _add_node(P, H) {
          const ae = this.bpe_ranks.get(JSON.stringify([H.token, H.next.token]));
          ae !== void 0 && (H.score = ae + H.bias, P.push(H));
        }
        /**
         * Encodes the input sequence of tokens using the BPE algorithm and returns the resulting subword tokens.
         * @param {string[]} tokens The input sequence of tokens to encode.
         * @returns {string[]} The resulting subword tokens after applying the BPE algorithm to the input sequence of tokens.
         */
        encode(P) {
          const H = [];
          for (const ae of P) {
            if (this.ignore_merges && this.tokens_to_ids.has(ae)) {
              H.push(ae);
              continue;
            }
            const Me = this.bpe(ae);
            for (const Pe of Me)
              if (this.tokens_to_ids.has(Pe))
                H.push(Pe);
              else if (this.byte_fallback) {
                const He = Array.from(this.text_encoder.encode(Pe)).map((ct) => `<0x${ct.toString(16).toUpperCase().padStart(2, "0")}>`);
                He.every((ct) => this.tokens_to_ids.has(ct)) ? H.push(...He) : H.push(this.unk_token);
              } else
                H.push(this.unk_token);
          }
          return H;
        }
      }
      class qe extends le {
        /**
         * Create a LegacyTokenizerModel instance.
         * @param {Object} config The configuration object for LegacyTokenizerModel.
         * @param {Object} config.vocab A (possibly nested) mapping of tokens to ids.
         * @param {Object} moreConfig Additional configuration object for the LegacyTokenizerModel model.
         */
        constructor(P, H) {
          super(P), this.tokens_to_ids = q(
            H.target_lang ? P.vocab[H.target_lang] : P.vocab
          ), this.bos_token = H.bos_token, this.bos_token_id = this.tokens_to_ids.get(this.bos_token), this.eos_token = H.eos_token, this.eos_token_id = this.tokens_to_ids.get(this.eos_token), this.pad_token = H.pad_token, this.pad_token_id = this.tokens_to_ids.get(this.pad_token), this.unk_token = H.unk_token, this.unk_token_id = this.tokens_to_ids.get(this.unk_token), this.vocab = new Array(this.tokens_to_ids.size);
          for (const [ae, Me] of this.tokens_to_ids)
            this.vocab[Me] = ae;
        }
        encode(P) {
          return P;
        }
      }
      class Ue extends f.Callable {
        /**
         * @param {Object} config The configuration object for the normalizer.
         */
        constructor(P) {
          super(), this.config = P;
        }
        /**
         * Factory method for creating normalizers from config objects.
         * @static
         * @param {Object} config The configuration object for the normalizer.
         * @returns {Normalizer} A Normalizer object.
         * @throws {Error} If an unknown Normalizer type is specified in the config.
         */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "BertNormalizer":
              return new Je(P);
            case "Precompiled":
              return new gr(P);
            case "Sequence":
              return new ie(P);
            case "Replace":
              return new ut(P);
            case "NFC":
              return new de(P);
            case "NFKC":
              return new re(P);
            case "NFKD":
              return new he(P);
            case "Strip":
              return new Ee(P);
            case "StripAccents":
              return new Be(P);
            case "Lowercase":
              return new et(P);
            case "Prepend":
              return new Xe(P);
            default:
              throw new Error(`Unknown Normalizer type: ${P.type}`);
          }
        }
        /**
         * Normalize the input text.
         * @abstract
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         * @throws {Error} If this method is not implemented in a subclass.
         */
        normalize(P) {
          throw Error("normalize should be implemented in subclass.");
        }
        /**
         * Alias for {@link Normalizer#normalize}.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        _call(P) {
          return this.normalize(P);
        }
      }
      class ut extends Ue {
        /**
         * Normalize the input text by replacing the pattern with the content.
         * @param {string} text The input text to be normalized.
         * @returns {string} The normalized text after replacing the pattern with the content.
         */
        normalize(P) {
          const H = D(this.config.pattern);
          return H === null ? P : P.replaceAll(H, this.config.content);
        }
      }
      class de extends Ue {
        /**
         * Normalize the input text by applying Unicode normalization form C (NFC).
         * @param {string} text The input text to be normalized.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.normalize("NFC"), P;
        }
      }
      class re extends Ue {
        /**
         * Normalize text using NFKC normalization.
         * @param {string} text The text to be normalized.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.normalize("NFKC"), P;
        }
      }
      class he extends Ue {
        /**
         * Normalize text using NFKD normalization.
         * @param {string} text The text to be normalized.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.normalize("NFKD"), P;
        }
      }
      class Ee extends Ue {
        /**
         * Strip leading and/or trailing whitespace from the input text.
         * @param {string} text The input text.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return this.config.strip_left && this.config.strip_right ? P = P.trim() : (this.config.strip_left && (P = P.trimStart()), this.config.strip_right && (P = P.trimEnd())), P;
        }
      }
      class Be extends Ue {
        /**
         * Remove all accents from the text.
         * @param {string} text The input text.
         * @returns {string} The normalized text without accents.
         */
        normalize(P) {
          return P = z(P), P;
        }
      }
      class et extends Ue {
        /**
         * Lowercases the input string.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.toLowerCase(), P;
        }
      }
      class Xe extends Ue {
        /**
         * Prepends the input string.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = this.config.prepend + P, P;
        }
      }
      class ie extends Ue {
        /**
        * Create a new instance of NormalizerSequence.
        * @param {Object} config The configuration object.
        * @param {Object[]} config.normalizers An array of Normalizer configuration objects.
        */
        constructor(P) {
          super(P), this.normalizers = P.normalizers.map((H) => Ue.fromConfig(H));
        }
        /**
        * Apply a sequence of Normalizers to the input text.
        * @param {string} text The text to normalize.
        * @returns {string} The normalized text.
        */
        normalize(P) {
          return this.normalizers.reduce((H, ae) => ae.normalize(H), P);
        }
      }
      class Je extends Ue {
        /**
         * Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the input text.
         *
         * @param {string} text The input text to tokenize.
         * @returns {string} The tokenized text with whitespace added around CJK characters.
         */
        _tokenize_chinese_chars(P) {
          const H = [];
          for (let ae = 0; ae < P.length; ++ae) {
            const Me = P[ae], Pe = Me.charCodeAt(0);
            Y(Pe) ? (H.push(" "), H.push(Me), H.push(" ")) : H.push(Me);
          }
          return H.join("");
        }
        /**
         * Strips accents from the given text.
         * @param {string} text The text to strip accents from.
         * @returns {string} The text with accents removed.
         */
        stripAccents(P) {
          return P.normalize("NFD").replace(new RegExp("\\p{Mn}", "gu"), "");
        }
        /**
         * Checks whether `char` is a control character.
         * @param {string} char The character to check.
         * @returns {boolean} Whether `char` is a control character.
         * @private
         */
        _is_control(P) {
          switch (P) {
            case "	":
            case `
`:
            case "\r":
              return !1;
            default:
              return new RegExp("^\\p{Cc}|\\p{Cf}|\\p{Co}|\\p{Cs}$", "u").test(P);
          }
        }
        /**
         * Performs invalid character removal and whitespace cleanup on text.
         * @param {string} text The text to clean.
         * @returns {string} The cleaned text.
         * @private
         */
        _clean_text(P) {
          const H = [];
          for (const ae of P) {
            const Me = ae.charCodeAt(0);
            Me === 0 || Me === 65533 || this._is_control(ae) || (/^\s$/.test(ae) ? H.push(" ") : H.push(ae));
          }
          return H.join("");
        }
        /**
         * Normalizes the given text based on the configuration.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return this.config.clean_text && (P = this._clean_text(P)), this.config.handle_chinese_chars && (P = this._tokenize_chinese_chars(P)), this.config.lowercase ? (P = P.toLowerCase(), this.config.strip_accents !== !1 && (P = this.stripAccents(P))) : this.config.strip_accents && (P = this.stripAccents(P)), P;
        }
      }
      class De extends f.Callable {
        /**
        * Factory method that returns an instance of a subclass of `PreTokenizer` based on the provided configuration.
        *
        * @static
        * @param {Object} config A configuration object for the pre-tokenizer.
        * @returns {PreTokenizer} An instance of a subclass of `PreTokenizer`.
        * @throws {Error} If the provided configuration object does not correspond to any known pre-tokenizer.
        */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "BertPreTokenizer":
              return new ce(P);
            case "Sequence":
              return new kr(P);
            case "Whitespace":
              return new Ar(P);
            case "WhitespaceSplit":
              return new Qr(P);
            case "Metaspace":
              return new At(P);
            case "ByteLevel":
              return new ve(P);
            case "Split":
              return new Re(P);
            case "Punctuation":
              return new je(P);
            case "Digits":
              return new Ve(P);
            case "Replace":
              return new is(P);
            default:
              throw new Error(`Unknown PreTokenizer type: ${P.type}`);
          }
        }
        /**
         * Method that should be implemented by subclasses to define the specific pre-tokenization logic.
         *
         * @abstract
         * @param {string} text The text to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} The pre-tokenized text.
         * @throws {Error} If the method is not implemented in the subclass.
         */
        pre_tokenize_text(P, H) {
          throw Error("pre_tokenize_text should be implemented in subclass.");
        }
        /**
         * Tokenizes the given text into pre-tokens.
         * @param {string|string[]} text The text or array of texts to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of pre-tokens.
         */
        pre_tokenize(P, H) {
          return (Array.isArray(P) ? P.map((ae) => this.pre_tokenize_text(ae, H)) : this.pre_tokenize_text(P, H)).flat();
        }
        /**
         * Alias for {@link PreTokenizer#pre_tokenize}.
         * @param {string|string[]} text The text or array of texts to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of pre-tokens.
         */
        _call(P, H) {
          return this.pre_tokenize(P, H);
        }
      }
      class ce extends De {
        /**
         * A PreTokenizer that splits text into wordpieces using a basic tokenization scheme
         * similar to that used in the original implementation of BERT.
         * 
         * @param {Object} config The configuration object.
         */
        constructor(P) {
          super(), this.pattern = new RegExp(`[^\\s${g}]+|[${g}]`, "gu");
        }
        /**
         * Tokenizes a single text using the BERT pre-tokenization scheme.
         * 
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, H) {
          return P.trim().match(this.pattern) || [];
        }
      }
      class ve extends De {
        /**
         * Creates a new instance of the `ByteLevelPreTokenizer` class.
         * @param {Object} config The configuration object.
         */
        constructor(P) {
          super(), this.config = P, this.add_prefix_space = this.config.add_prefix_space, this.trim_offsets = this.config.trim_offsets, this.use_regex = this.config.use_regex ?? !0, this.pattern = new RegExp("'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+", "gu"), this.byte_encoder = Ce, this.text_encoder = new TextEncoder();
        }
        /**
         * Tokenizes a single piece of text using byte-level tokenization.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, H) {
          return this.add_prefix_space && !P.startsWith(" ") && (P = " " + P), (this.use_regex ? P.match(this.pattern) || [] : [P]).map(
            (Me) => Array.from(this.text_encoder.encode(Me), (Pe) => this.byte_encoder[Pe]).join("")
          );
        }
      }
      class Re extends De {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.
         * @param {string|undefined} config.pattern.String The string to use for splitting. Only defined if the pattern is a string.
         * @param {string|undefined} config.pattern.Regex The regex to use for splitting. Only defined if the pattern is a regex.
         * @param {SplitDelimiterBehavior} config.behavior The behavior to use when splitting.
         * @param {boolean} config.invert Whether to split (invert=false) or match (invert=true) the pattern.
         */
        constructor(P) {
          super(), this.config = P, this.pattern = D(this.config.pattern, this.config.invert);
        }
        /**
         * Tokenizes text by splitting it using the given pattern.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, H) {
          var ae;
          return this.pattern === null ? [] : this.config.invert ? P.match(this.pattern) || [] : ((ae = this.config.behavior) == null ? void 0 : ae.toLowerCase()) === "removed" ? P.split(this.pattern).filter((Me) => Me) : b(P, this.pattern);
        }
      }
      class je extends De {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {SplitDelimiterBehavior} config.behavior The behavior to use when splitting.
         */
        constructor(P) {
          super(), this.config = P, this.pattern = new RegExp(`[^${g}]+|[${g}]+`, "gu");
        }
        /**
         * Tokenizes text by splitting it using the given pattern.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, H) {
          return P.match(this.pattern) || [];
        }
      }
      class Ve extends De {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {boolean} config.individual_digits Whether to split on individual digits.
         */
        constructor(P) {
          super(), this.config = P;
          const H = `[^\\d]+|\\d${this.config.individual_digits ? "" : "+"}`;
          this.pattern = new RegExp(H, "gu");
        }
        /**
         * Tokenizes text by splitting it using the given pattern.
         * @param {string} text The text to tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens.
         */
        pre_tokenize_text(P, H) {
          return P.match(this.pattern) || [];
        }
      }
      class Ne extends f.Callable {
        /**
         * @param {Object} config The configuration for the post-processor.
         */
        constructor(P) {
          super(), this.config = P;
        }
        /**
         * Factory method to create a PostProcessor object from a configuration object.
         *
         * @param {Object} config Configuration object representing a PostProcessor.
         * @returns {PostProcessor} A PostProcessor object created from the given configuration.
         * @throws {Error} If an unknown PostProcessor type is encountered.
         */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "TemplateProcessing":
              return new ft(P);
            case "ByteLevel":
              return new dt(P);
            case "RobertaProcessing":
              return new at(P);
            case "BertProcessing":
              return new Ze(P);
            case "Sequence":
              return new gt(P);
            default:
              throw new Error(`Unknown PostProcessor type: ${P.type}`);
          }
        }
        /**
         * Method to be implemented in subclass to apply post-processing on the given tokens.
         *
         * @param {Array} tokens The input tokens to be post-processed.
         * @param {...*} args Additional arguments required by the post-processing logic.
         * @returns {PostProcessedOutput} The post-processed tokens.
         * @throws {Error} If the method is not implemented in subclass.
         */
        post_process(P, ...H) {
          throw Error("post_process should be implemented in subclass.");
        }
        /**
         * Alias for {@link PostProcessor#post_process}.
         * @param {Array} tokens The text or array of texts to post-process.
         * @param {...*} args Additional arguments required by the post-processing logic.
         * @returns {PostProcessedOutput} The post-processed tokens.
         */
        _call(P, ...H) {
          return this.post_process(P, ...H);
        }
      }
      class Ze extends Ne {
        /**
         * @param {Object} config The configuration for the post-processor.
         * @param {string[]} config.cls The special tokens to add to the beginning of the input.
         * @param {string[]} config.sep The special tokens to add to the end of the input.
         */
        constructor(P) {
          super(P), this.cls = P.cls[0], this.sep = P.sep[0];
        }
        /**
         * Adds the special tokens to the beginning and end of the input.
         * @param {string[]} tokens The input tokens.
         * @param {string[]} [tokens_pair=null] An optional second set of input tokens.
         * @returns {PostProcessedOutput} The post-processed tokens with the special tokens added to the beginning and end.
         */
        post_process(P, H = null, {
          add_special_tokens: ae = !0
        } = {}) {
          ae && (P = (0, L.mergeArrays)([this.cls], P, [this.sep]));
          let Me = new Array(P.length).fill(0);
          if (H !== null) {
            const Pe = ae && this instanceof at ? [this.sep] : [], He = ae ? [this.sep] : [];
            P = (0, L.mergeArrays)(P, Pe, H, He), Me = (0, L.mergeArrays)(Me, new Array(H.length + Pe.length + He.length).fill(1));
          }
          return { tokens: P, token_type_ids: Me };
        }
      }
      class at extends Ze {
      }
      class ft extends Ne {
        /**
         * Creates a new instance of `TemplateProcessing`.
         * @param {Object} config The configuration options for the post processor.
         * @param {Array} config.single The template for a single sequence of tokens.
         * @param {Array} config.pair The template for a pair of sequences of tokens.
         */
        constructor(P) {
          super(P), this.single = P.single, this.pair = P.pair;
        }
        /**
         * Replaces special tokens in the template with actual tokens.
         * @param {string[]} tokens The list of tokens for the first sequence.
         * @param {string[]} [tokens_pair=null] The list of tokens for the second sequence (optional).
         * @returns {PostProcessedOutput} An object containing the list of tokens with the special tokens replaced with actual tokens.
         */
        post_process(P, H = null, {
          add_special_tokens: ae = !0
        } = {}) {
          const Me = H === null ? this.single : this.pair;
          let Pe = [], He = [];
          for (const ct of Me)
            "SpecialToken" in ct ? ae && (Pe.push(ct.SpecialToken.id), He.push(ct.SpecialToken.type_id)) : "Sequence" in ct && (ct.Sequence.id === "A" ? (Pe = (0, L.mergeArrays)(Pe, P), He = (0, L.mergeArrays)(He, new Array(P.length).fill(ct.Sequence.type_id))) : ct.Sequence.id === "B" && (Pe = (0, L.mergeArrays)(Pe, H), He = (0, L.mergeArrays)(He, new Array(H.length).fill(ct.Sequence.type_id))));
          return { tokens: Pe, token_type_ids: He };
        }
      }
      class dt extends Ne {
        /**
         * Post process the given tokens.
         * @param {string[]} tokens The list of tokens for the first sequence.
         * @param {string[]} [tokens_pair=null] The list of tokens for the second sequence (optional).
         * @returns {PostProcessedOutput} An object containing the post-processed tokens.
         */
        post_process(P, H = null) {
          return H && (P = (0, L.mergeArrays)(P, H)), { tokens: P };
        }
      }
      class gt extends Ne {
        /**
         * Creates a new instance of PostProcessorSequence.
         * @param {Object} config The configuration object.
         * @param {Object[]} config.processors The list of post-processors to apply.
         */
        constructor(P) {
          super(P), this.processors = P.processors.map((H) => Ne.fromConfig(H));
        }
        /**
         * Post process the given tokens.
         * @param {string[]} tokens The list of tokens for the first sequence.
         * @param {string[]} [tokens_pair=null] The list of tokens for the second sequence (optional).
         * @returns {PostProcessedOutput} An object containing the post-processed tokens.
         */
        post_process(P, H = null, ae = {}) {
          let Me;
          for (const Pe of this.processors)
            if (Pe instanceof dt)
              P = Pe.post_process(P).tokens, H && (H = Pe.post_process(H).tokens);
            else {
              const He = Pe.post_process(P, H, ae);
              P = He.tokens, Me = He.token_type_ids;
            }
          return { tokens: P, token_type_ids: Me };
        }
      }
      class F extends f.Callable {
        /**
        * Creates an instance of `Decoder`.
        *
        * @param {Object} config The configuration object.
        */
        constructor(P) {
          super(), this.config = P, this.added_tokens = [], this.end_of_word_suffix = null, this.trim_offsets = P.trim_offsets;
        }
        /**
        * Creates a decoder instance based on the provided configuration.
        *
        * @param {Object} config The configuration object.
        * @returns {Decoder} A decoder instance.
        * @throws {Error} If an unknown decoder type is provided.
        */
        static fromConfig(P) {
          if (P === null) return null;
          switch (P.type) {
            case "WordPiece":
              return new Qe(P);
            case "Metaspace":
              return new nr(P);
            case "ByteLevel":
              return new st(P);
            case "Replace":
              return new ne(P);
            case "ByteFallback":
              return new K(P);
            case "Fuse":
              return new pe(P);
            case "Strip":
              return new Oe(P);
            case "Sequence":
              return new It(P);
            case "CTC":
              return new pt(P);
            case "BPEDecoder":
              return new St(P);
            default:
              throw new Error(`Unknown Decoder type: ${P.type}`);
          }
        }
        /**
        * Calls the `decode` method.
        *
        * @param {string[]} tokens The list of tokens.
        * @returns {string} The decoded string.
        */
        _call(P) {
          return this.decode(P);
        }
        /**
        * Decodes a list of tokens.
        * @param {string[]} tokens The list of tokens.
        * @returns {string} The decoded string.
        */
        decode(P) {
          return this.decode_chain(P).join("");
        }
        /**
         * Apply the decoder to a list of tokens.
         * 
         * @param {string[]} tokens The list of tokens.
         * @returns {string[]} The decoded list of tokens.
         * @throws {Error} If the `decode_chain` method is not implemented in the subclass.
         */
        decode_chain(P) {
          throw Error("`decode_chain` should be implemented in subclass.");
        }
      }
      class ne extends F {
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const H = D(this.config.pattern);
          return H === null ? P : P.map((ae) => ae.replaceAll(H, this.config.content));
        }
      }
      class K extends F {
        constructor(P) {
          super(P), this.text_decoder = new TextDecoder();
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const H = [];
          let ae = [];
          for (const Me of P) {
            let Pe = null;
            if (Me.length === 6 && Me.startsWith("<0x") && Me.endsWith(">")) {
              const He = parseInt(Me.slice(3, 5), 16);
              isNaN(He) || (Pe = He);
            }
            if (Pe !== null)
              ae.push(Pe);
            else {
              if (ae.length > 0) {
                const He = this.text_decoder.decode(Uint8Array.from(ae));
                H.push(He), ae = [];
              }
              H.push(Me);
            }
          }
          if (ae.length > 0) {
            const Me = this.text_decoder.decode(Uint8Array.from(ae));
            H.push(Me), ae = [];
          }
          return H;
        }
      }
      class pe extends F {
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return [P.join("")];
        }
      }
      class Oe extends F {
        constructor(P) {
          super(P), this.content = this.config.content, this.start = this.config.start, this.stop = this.config.stop;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return P.map((H) => {
            let ae = 0;
            for (let Pe = 0; Pe < this.start && H[Pe] === this.content; ++Pe) {
              ae = Pe + 1;
              continue;
            }
            let Me = H.length;
            for (let Pe = 0; Pe < this.stop; ++Pe) {
              const He = H.length - Pe - 1;
              if (H[He] === this.content) {
                Me = He;
                continue;
              } else
                break;
            }
            return H.slice(ae, Me);
          });
        }
      }
      class Qe extends F {
        /**
         * Creates a new instance of WordPieceDecoder.
         * @param {Object} config The configuration object.
         * @param {string} config.prefix The prefix used for WordPiece encoding.
         * @param {boolean} config.cleanup Whether to cleanup the decoded string.
         */
        constructor(P) {
          super(P), this.cleanup = P.cleanup;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return P.map((H, ae) => (ae !== 0 && (H.startsWith(this.config.prefix) ? H = H.replace(this.config.prefix, "") : H = " " + H), this.cleanup && (H = oe(H)), H));
        }
      }
      class st extends F {
        /**
         * Create a `ByteLevelDecoder` object.
         * @param {Object} config Configuration object.
         */
        constructor(P) {
          super(P), this.byte_decoder = xe, this.text_decoder = new TextDecoder("utf-8", {
            fatal: !1,
            ignoreBOM: !0
          }), this.end_of_word_suffix = null;
        }
        /**
         * Convert an array of tokens to string by decoding each byte.
         * @param {string[]} tokens Array of tokens to be decoded.
         * @returns {string} The decoded string.
         */
        convert_tokens_to_string(P) {
          const H = P.join(""), ae = new Uint8Array([...H].map((Pe) => this.byte_decoder[Pe]));
          return this.text_decoder.decode(ae);
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const H = [];
          let ae = [];
          for (const Me of P)
            this.added_tokens.find((Pe) => Pe.content === Me) !== void 0 ? (ae.length > 0 && (H.push(this.convert_tokens_to_string(ae)), ae = []), H.push(Me)) : ae.push(Me);
          return ae.length > 0 && H.push(this.convert_tokens_to_string(ae)), H;
        }
      }
      class pt extends F {
        constructor(P) {
          super(P), this.pad_token = this.config.pad_token, this.word_delimiter_token = this.config.word_delimiter_token, this.cleanup = this.config.cleanup;
        }
        /**
         * Converts a connectionist-temporal-classification (CTC) output tokens into a single string.
         * @param {string[]} tokens Array of tokens to be decoded.
         * @returns {string} The decoded string.
         */
        convert_tokens_to_string(P) {
          if (P.length === 0) return "";
          const H = [P[0]];
          for (let Pe = 1; Pe < P.length; ++Pe)
            P[Pe] !== H.at(-1) && H.push(P[Pe]);
          let Me = H.filter((Pe) => Pe !== this.pad_token).join("");
          return this.cleanup && (Me = oe(Me).replaceAll(this.word_delimiter_token, " ").trim()), Me;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return [this.convert_tokens_to_string(P)];
        }
      }
      class It extends F {
        /**
         * Creates a new instance of DecoderSequence.
         * @param {Object} config The configuration object.
         * @param {Object[]} config.decoders The list of decoders to apply.
         */
        constructor(P) {
          super(P), this.decoders = P.decoders.map((H) => F.fromConfig(H));
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return this.decoders.reduce((H, ae) => ae.decode_chain(H), P);
        }
      }
      class St extends F {
        constructor(P) {
          super(P), this.suffix = this.config.suffix;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          return P.map((H, ae) => H.replaceAll(this.suffix, ae === P.length - 1 ? "" : " "));
        }
      }
      class Ot extends F {
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          let H = "";
          for (let ae = 1; ae < P.length; ae += 2)
            H += P[ae];
          return [H];
        }
      }
      class At extends De {
        /**
         * @param {Object} config The configuration object for the MetaspacePreTokenizer.
         * @param {boolean} config.add_prefix_space Whether to add a prefix space to the first token.
         * @param {string} config.replacement The character to replace spaces with.
         * @param {string} [config.str_rep=config.replacement] An optional string representation of the replacement character.
         * @param {'first'|'never'|'always'} [config.prepend_scheme='always'] The metaspace prepending scheme.
         */
        constructor(P) {
          super(), this.addPrefixSpace = P.add_prefix_space, this.replacement = P.replacement, this.strRep = P.str_rep || this.replacement, this.prepend_scheme = P.prepend_scheme ?? "always";
        }
        /**
         * This method takes a string, replaces spaces with the replacement character,
         * adds a prefix space if requested, and returns a new list of tokens.
         * @param {string} text The text to pre-tokenize.
         * @param {Object} [options] The options for the pre-tokenization.
         * @param {number} [options.section_index] The index of the section to pre-tokenize.
         * @returns {string[]} A new list of pre-tokenized tokens.
         */
        pre_tokenize_text(P, {
          section_index: H = void 0
        } = {}) {
          let ae = P.replaceAll(" ", this.strRep);
          return (
            // We add a prefix space if:
            //  (1) The addPrefixSpace option is enabled and the normalized
            //      token does not already start with the replacement character.
            this.addPrefixSpace && !ae.startsWith(this.replacement) && (this.prepend_scheme === "always" || this.prepend_scheme === "first" && H === 0) && (ae = this.strRep + ae), [ae]
          );
        }
      }
      class nr extends F {
        /**
         * Constructs a new MetaspaceDecoder object.
         * @param {Object} config The configuration object for the MetaspaceDecoder.
         * @param {boolean} config.add_prefix_space Whether to add a prefix space to the decoded string.
         * @param {string} config.replacement The string to replace spaces with.
         */
        constructor(P) {
          super(P), this.addPrefixSpace = P.add_prefix_space, this.replacement = P.replacement;
        }
        /** @type {Decoder['decode_chain']} */
        decode_chain(P) {
          const H = [];
          for (let ae = 0; ae < P.length; ++ae) {
            let Me = P[ae].replaceAll(this.replacement, " ");
            this.addPrefixSpace && ae == 0 && Me.startsWith(" ") && (Me = Me.substring(1)), H.push(Me);
          }
          return H;
        }
      }
      class gr extends Ue {
        /**
         * Create a new instance of Precompiled normalizer.
         * @param {Object} config The configuration object.
         * @param {any} config.precompiled_charsmap Precompiled chars mapping.
         */
        constructor(P) {
          super(P), this.charsmap = P.precompiled_charsmap;
        }
        /**
         * Normalizes the given text by applying the precompiled charsmap.
         * @param {string} text The text to normalize.
         * @returns {string} The normalized text.
         */
        normalize(P) {
          return P = P.replace(/[\u0001-\u0008\u000B\u000E-\u001F\u007F\u008F\u009F]/gm, ""), P = P.replace(/[\u0009\u000A\u000C\u000D\u00A0\u1680\u2000-\u200F\u2028\u2029\u202F\u205F\u2581\u3000\uFEFF\uFFFD]/gm, " "), P.includes("～") ? P = P.split("～").map((ae) => ae.normalize("NFKC")).join("～") : P = P.normalize("NFKC"), P;
        }
      }
      class kr extends De {
        /**
         * Creates an instance of PreTokenizerSequence.
         * @param {Object} config The configuration object for the pre-tokenizer sequence.
         * @param {Object[]} config.pretokenizers An array of pre-tokenizer configurations.
         */
        constructor(P) {
          super(), this.tokenizers = P.pretokenizers.map((H) => De.fromConfig(H));
        }
        /**
         * Applies each pre-tokenizer in the sequence to the input text in turn.
         * @param {string} text The text to pre-tokenize.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} The pre-tokenized text.
         */
        pre_tokenize_text(P, H) {
          return this.tokenizers.reduce((ae, Me) => Me.pre_tokenize(ae, H), [P]);
        }
      }
      class Ar extends De {
        /**
         * Creates an instance of WhitespacePreTokenizer.
         * @param {Object} config The configuration object for the pre-tokenizer.
         */
        constructor(P) {
          super();
        }
        /**
         * Pre-tokenizes the input text by splitting it on word boundaries.
         * @param {string} text The text to be pre-tokenized.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens produced by splitting the input text on whitespace.
         */
        pre_tokenize_text(P, H) {
          return P.match(/\w+|[^\w\s]+/g) || [];
        }
      }
      class Qr extends De {
        /**
         * Creates an instance of WhitespaceSplit.
         * @param {Object} config The configuration object for the pre-tokenizer.
         */
        constructor(P) {
          super();
        }
        /**
         * Pre-tokenizes the input text by splitting it on whitespace characters.
         * @param {string} text The text to be pre-tokenized.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens produced by splitting the input text on whitespace.
         */
        pre_tokenize_text(P, H) {
          return $(P);
        }
      }
      class is extends De {
        /**
         * @param {Object} config The configuration options for the pre-tokenizer.
         * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.
         * @param {string} config.content What to replace the pattern with.
         */
        constructor(P) {
          super(), this.config = P, this.pattern = D(this.config.pattern), this.content = this.config.content;
        }
        /**
         * Pre-tokenizes the input text by replacing certain characters.
         * @param {string} text The text to be pre-tokenized.
         * @param {Object} [options] Additional options for the pre-tokenization logic.
         * @returns {string[]} An array of tokens produced by replacing certain characters.
         */
        pre_tokenize_text(P, H) {
          return this.pattern === null ? [P] : [P.replaceAll(this.pattern, this.config.content)];
        }
      }
      const Xs = [
        "bos_token",
        "eos_token",
        "unk_token",
        "sep_token",
        "pad_token",
        "cls_token",
        "mask_token"
        // additional_special_tokens (TODO)
      ];
      function zs(Te, P, H, ae) {
        for (const Me of Object.keys(Te)) {
          const Pe = P - Te[Me].length, He = H(Me), ct = new Array(Pe).fill(He);
          Te[Me] = ae === "right" ? (0, L.mergeArrays)(Te[Me], ct) : (0, L.mergeArrays)(ct, Te[Me]);
        }
      }
      function Ms(Te, P) {
        for (const H of Object.keys(Te))
          Te[H].length = P;
      }
      class Nt extends f.Callable {
        /**
         * Create a new PreTrainedTokenizer instance.
         * @param {Object} tokenizerJSON The JSON of the tokenizer.
         * @param {Object} tokenizerConfig The config of the tokenizer.
         */
        constructor(H, ae) {
          super();
          ge(this, "return_token_type_ids", !1);
          ge(this, "padding_side", "right");
          this._tokenizer_config = ae, this.normalizer = Ue.fromConfig(H.normalizer), this.pre_tokenizer = De.fromConfig(H.pre_tokenizer), this.model = le.fromConfig(H.model, ae), this.post_processor = Ne.fromConfig(H.post_processor), this.decoder = F.fromConfig(H.decoder), this.special_tokens = [], this.all_special_ids = [], this.added_tokens = [];
          for (const Me of H.added_tokens) {
            const Pe = new X(Me);
            this.added_tokens.push(Pe), this.model.tokens_to_ids.set(Pe.content, Pe.id), this.model.vocab[Pe.id] = Pe.content, Pe.special && (this.special_tokens.push(Pe.content), this.all_special_ids.push(Pe.id));
          }
          if (this.additional_special_tokens = ae.additional_special_tokens ?? [], this.special_tokens.push(...this.additional_special_tokens), this.special_tokens = [...new Set(this.special_tokens)], this.decoder && (this.decoder.added_tokens = this.added_tokens, this.decoder.end_of_word_suffix = this.model.end_of_word_suffix), this.added_tokens_regex = this.added_tokens.length > 0 ? new RegExp(
            this.added_tokens.slice().sort((Me, Pe) => Pe.content.length - Me.content.length).map((Me) => `${Me.lstrip ? "\\s*" : ""}(${(0, L.escapeRegExp)(Me.content)})${Me.rstrip ? "\\s*" : ""}`).join("|")
          ) : null, this.mask_token = this.getToken("mask_token"), this.mask_token_id = this.model.tokens_to_ids.get(this.mask_token), this.pad_token = this.getToken("pad_token", "eos_token"), this.pad_token_id = this.model.tokens_to_ids.get(this.pad_token), this.sep_token = this.getToken("sep_token"), this.sep_token_id = this.model.tokens_to_ids.get(this.sep_token), this.unk_token = this.getToken("unk_token"), this.unk_token_id = this.model.tokens_to_ids.get(this.unk_token), this.bos_token = this.getToken("bos_token"), this.bos_token_id = this.model.tokens_to_ids.get(this.bos_token), this.eos_token = this.getToken("eos_token"), this.eos_token_id = this.model.tokens_to_ids.get(this.eos_token), this.model_max_length = ae.model_max_length, this.remove_space = ae.remove_space, this.clean_up_tokenization_spaces = ae.clean_up_tokenization_spaces ?? !0, this.do_lowercase_and_remove_accent = ae.do_lowercase_and_remove_accent ?? !1, ae.padding_side && (this.padding_side = ae.padding_side), this.legacy = !1, this.chat_template = ae.chat_template ?? null, Array.isArray(this.chat_template)) {
            const Me = /* @__PURE__ */ Object.create(null);
            for (const { name: Pe, template: He } of this.chat_template) {
              if (typeof Pe != "string" || typeof He != "string")
                throw new Error('Chat template must be a list of objects with "name" and "template" properties');
              Me[Pe] = He;
            }
            this.chat_template = Me;
          }
          this._compiled_template_cache = /* @__PURE__ */ new Map();
        }
        /**
         * Returns the value of the first matching key in the tokenizer config object.
         * @param {...string} keys One or more keys to search for in the tokenizer config object.
         * @returns {string|null} The value associated with the first matching key, or null if no match is found.
         * @throws {Error} If an object is found for a matching key and its __type property is not "AddedToken".
         * @private
         */
        getToken(...H) {
          for (const ae of H) {
            const Me = this._tokenizer_config[ae];
            if (Me)
              if (typeof Me == "object") {
                if (Me.__type === "AddedToken")
                  return Me.content;
                throw Error(`Unknown token: ${Me}`);
              } else
                return Me;
          }
          return null;
        }
        /**
         * Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`. 
         * 
         * @param {string} pretrained_model_name_or_path The path to the pre-trained tokenizer.
         * @param {PretrainedTokenizerOptions} options Additional options for loading the tokenizer.
         * 
         * @throws {Error} Throws an error if the tokenizer.json or tokenizer_config.json files are not found in the `pretrained_model_name_or_path`.
         * @returns {Promise<PreTrainedTokenizer>} A new instance of the `PreTrainedTokenizer` class.
         */
        static async from_pretrained(H, {
          progress_callback: ae = null,
          config: Me = null,
          cache_dir: Pe = null,
          local_files_only: He = !1,
          revision: ct = "main",
          legacy: yt = null
        } = {}) {
          const ht = await M(H, {
            progress_callback: ae,
            config: Me,
            cache_dir: Pe,
            local_files_only: He,
            revision: ct,
            legacy: yt
          });
          return new this(...ht);
        }
        /**
         * @typedef {number[]|number[][]|Tensor} BatchEncodingItem
         * 
         * @typedef {Object} BatchEncoding Holds the output of the tokenizer's call function.
         * @property {BatchEncodingItem} input_ids List of token ids to be fed to a model.
         * @property {BatchEncodingItem} attention_mask List of indices specifying which tokens should be attended to by the model.
         * @property {BatchEncodingItem} [token_type_ids] List of token type ids to be fed to a model.
         */
        /**
         * Encode/tokenize the given text(s).
         * @param {string|string[]} text The text to tokenize.
         * @param {Object} options An optional object containing the following properties:
         * @param {string|string[]} [options.text_pair=null] Optional second sequence to be encoded. If set, must be the same type as text.
         * @param {boolean|'max_length'} [options.padding=false] Whether to pad the input sequences.
         * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.
         * @param {boolean} [options.truncation=null] Whether to truncate the input sequences.
         * @param {number} [options.max_length=null] Maximum length of the returned list and optionally padding length.
         * @param {boolean} [options.return_tensor=true] Whether to return the results as Tensors or arrays.
         * @param {boolean} [options.return_token_type_ids=null] Whether to return the token type ids.
         * @returns {BatchEncoding} Object to be passed to the model.
         */
        _call(H, {
          text_pair: ae = null,
          add_special_tokens: Me = !0,
          padding: Pe = !1,
          truncation: He = null,
          max_length: ct = null,
          return_tensor: yt = !0,
          // Different to HF
          return_token_type_ids: ht = null
        } = {}) {
          const ot = Array.isArray(H);
          let Pt;
          if (ot) {
            if (H.length === 0)
              throw Error("text array must be non-empty");
            if (ae !== null) {
              if (Array.isArray(ae)) {
                if (H.length !== ae.length)
                  throw Error("text and text_pair must have the same length");
              } else throw Error("text_pair must also be an array");
              Pt = H.map(
                (rr, $e) => this._encode_plus(rr, { text_pair: ae[$e], add_special_tokens: Me, return_token_type_ids: ht })
              );
            } else
              Pt = H.map((rr) => this._encode_plus(rr, { add_special_tokens: Me, return_token_type_ids: ht }));
          } else {
            if (H == null)
              throw Error("text may not be null or undefined");
            if (Array.isArray(ae))
              throw Error("When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).");
            Pt = [this._encode_plus(H, { text_pair: ae, add_special_tokens: Me, return_token_type_ids: ht })];
          }
          if (ct === null ? Pe === "max_length" ? ct = this.model_max_length : ct = (0, J.max)(Pt.map((rr) => rr.input_ids.length))[0] : He || console.warn("Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=true` to explicitly truncate examples to max length."), ct = Math.min(ct, this.model_max_length ?? 1 / 0), Pe || He)
            for (let rr = 0; rr < Pt.length; ++rr)
              Pt[rr].input_ids.length !== ct && (Pt[rr].input_ids.length > ct ? He && Ms(Pt[rr], ct) : Pe && zs(
                Pt[rr],
                ct,
                ($e) => $e === "input_ids" ? this.pad_token_id : 0,
                this.padding_side
              ));
          const hr = {};
          if (yt) {
            if (!(Pe && He) && Pt.some(($e) => {
              var wr;
              for (const Rr of Object.keys($e))
                if ($e[Rr].length !== ((wr = Pt[0][Rr]) == null ? void 0 : wr.length))
                  return !0;
              return !1;
            }))
              throw Error(
                "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=true' and 'truncation=true' to have batched tensors with the same length."
              );
            const rr = [Pt.length, Pt[0].input_ids.length];
            for (const $e of Object.keys(Pt[0]))
              hr[$e] = new W.Tensor(
                "int64",
                BigInt64Array.from(Pt.flatMap((wr) => wr[$e]).map(BigInt)),
                rr
              );
          } else {
            for (const rr of Object.keys(Pt[0]))
              hr[rr] = Pt.map(($e) => $e[rr]);
            if (!ot)
              for (const rr of Object.keys(hr))
                hr[rr] = hr[rr][0];
          }
          return (
            /** @type {BatchEncoding} */
            hr
          );
        }
        /**
         * Encodes a single text using the preprocessor pipeline of the tokenizer.
         *
         * @param {string|null} text The text to encode.
         * @returns {string[]|null} The encoded tokens.
         */
        _encode_text(H) {
          return H === null ? null : (this.added_tokens_regex ? H.split(this.added_tokens_regex).filter((Pe) => Pe) : [H]).map((Pe, He) => {
            if (this.added_tokens.find((yt) => yt.content === Pe) !== void 0)
              return Pe;
            {
              if (this.remove_space === !0 && (Pe = Pe.trim().split(/\s+/).join(" ")), this.do_lowercase_and_remove_accent && (Pe = V(Pe)), this.normalizer !== null && (Pe = this.normalizer(Pe)), Pe.length === 0)
                return [];
              const yt = this.pre_tokenizer !== null ? this.pre_tokenizer(Pe, {
                section_index: He
              }) : [Pe];
              return this.model(yt);
            }
          }).flat();
        }
        /**
         * Encodes a single text or a pair of texts using the model's tokenizer.
         *
         * @param {string} text The text to encode.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.text_pair=null] The optional second text to encode.
         * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.
         * @param {boolean} [options.return_token_type_ids=null] Whether to return token_type_ids.
         * @returns {EncodingSingle} An object containing the encoded text.
         * @private
         */
        _encode_plus(H, {
          text_pair: ae = null,
          add_special_tokens: Me = !0,
          return_token_type_ids: Pe = null
        } = {}) {
          const { tokens: He, token_type_ids: ct } = this._tokenize_helper(H, { pair: ae, add_special_tokens: Me }), yt = this.model.convert_tokens_to_ids(He), ht = {
            input_ids: yt,
            attention_mask: new Array(yt.length).fill(1)
          };
          return (Pe ?? this.return_token_type_ids) && ct && (ht.token_type_ids = ct), ht;
        }
        /**
         * Internal helper function to tokenize a text, and optionally a pair of texts.
         * @param {string} text The text to tokenize.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.pair=null] The optional second text to tokenize.
         * @param {boolean} [options.add_special_tokens=false] Whether or not to add the special tokens associated with the corresponding model.
         * @returns {{tokens: string[], token_type_ids?: number[]}} An object containing the tokens and optionally the token type IDs.
         */
        _tokenize_helper(H, {
          pair: ae = null,
          add_special_tokens: Me = !1
        } = {}) {
          const Pe = this._encode_text(H), He = this._encode_text(ae);
          return this.post_processor ? this.post_processor(Pe, He, { add_special_tokens: Me }) : { tokens: (0, L.mergeArrays)(Pe ?? [], He ?? []) };
        }
        /**
         * Converts a string into a sequence of tokens.
         * @param {string} text The sequence to be encoded.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.pair] A second sequence to be encoded with the first.
         * @param {boolean} [options.add_special_tokens=false] Whether or not to add the special tokens associated with the corresponding model.
         * @returns {string[]} The list of tokens.
         */
        tokenize(H, {
          pair: ae = null,
          add_special_tokens: Me = !1
        } = {}) {
          return this._tokenize_helper(H, { pair: ae, add_special_tokens: Me }).tokens;
        }
        /**
         * Encodes a single text or a pair of texts using the model's tokenizer.
         *
         * @param {string} text The text to encode.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.text_pair=null] The optional second text to encode.
         * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.
         * @param {boolean} [options.return_token_type_ids=null] Whether to return token_type_ids.
         * @returns {number[]} An array of token IDs representing the encoded text(s).
         */
        encode(H, {
          text_pair: ae = null,
          add_special_tokens: Me = !0,
          return_token_type_ids: Pe = null
        } = {}) {
          return this._encode_plus(H, {
            text_pair: ae,
            add_special_tokens: Me,
            return_token_type_ids: Pe
          }).input_ids;
        }
        /**
         * Decode a batch of tokenized sequences.
         * @param {number[][]|Tensor} batch List/Tensor of tokenized input sequences.
         * @param {Object} decode_args (Optional) Object with decoding arguments.
         * @returns {string[]} List of decoded sequences.
         */
        batch_decode(H, ae = {}) {
          return H instanceof W.Tensor && (H = H.tolist()), H.map((Me) => this.decode(Me, ae));
        }
        /**
         * Decodes a sequence of token IDs back to a string.
         *
         * @param {number[]|bigint[]|Tensor} token_ids List/Tensor of token IDs to decode.
         * @param {Object} [decode_args={}]
         * @param {boolean} [decode_args.skip_special_tokens=false] If true, special tokens are removed from the output string.
         * @param {boolean} [decode_args.clean_up_tokenization_spaces=true] If true, spaces before punctuations and abbreviated forms are removed.
         *
         * @returns {string} The decoded string.
         * @throws {Error} If `token_ids` is not a non-empty array of integers.
         */
        decode(H, ae = {}) {
          if (H instanceof W.Tensor && (H = se(H)), !Array.isArray(H) || H.length === 0 || !(0, L.isIntegralNumber)(H[0]))
            throw Error("token_ids must be a non-empty array of integers.");
          return this.decode_single(H, ae);
        }
        /**
         * Decode a single list of token ids to a string.
         * @param {number[]|bigint[]} token_ids List of token ids to decode
         * @param {Object} decode_args Optional arguments for decoding
         * @param {boolean} [decode_args.skip_special_tokens=false] Whether to skip special tokens during decoding
         * @param {boolean} [decode_args.clean_up_tokenization_spaces=null] Whether to clean up tokenization spaces during decoding.
         * If null, the value is set to `this.decoder.cleanup` if it exists, falling back to `this.clean_up_tokenization_spaces` if it exists, falling back to `true`.
         * @returns {string} The decoded string
         */
        decode_single(H, {
          skip_special_tokens: ae = !1,
          clean_up_tokenization_spaces: Me = null
        }) {
          let Pe = this.model.convert_ids_to_tokens(H);
          ae && (Pe = Pe.filter((ct) => !this.special_tokens.includes(ct)));
          let He = this.decoder ? this.decoder(Pe) : Pe.join(" ");
          return this.decoder && this.decoder.end_of_word_suffix && (He = He.replaceAll(this.decoder.end_of_word_suffix, " "), ae && (He = He.trim())), (Me ?? this.clean_up_tokenization_spaces) && (He = oe(He)), He;
        }
        /**
         * Retrieve the chat template string used for tokenizing chat messages. This template is used
         * internally by the `apply_chat_template` method and can also be used externally to retrieve the model's chat
         * template for better generation tracking.
         * 
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.chat_template=null]
         * A Jinja template or the name of a template to use for this conversion.
         * It is usually not necessary to pass anything to this argument,
         * as the model's template will be used by default.
         * @param {Object[]} [options.tools=null]
         * A list of tools (callable functions) that will be accessible to the model. If the template does not
         * support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,
         * giving the name, description and argument types for the tool. See our
         * [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)
         * for more information.
         * @returns {string} The chat template string.
         */
        get_chat_template({
          chat_template: H = null,
          tools: ae = null
        } = {}) {
          if (this.chat_template && typeof this.chat_template == "object") {
            const Me = this.chat_template;
            if (H !== null && Object.hasOwn(Me, H))
              H = Me[H];
            else if (H === null)
              if (ae !== null && "tool_use" in Me)
                H = Me.tool_use;
              else if ("default" in Me)
                H = Me.default;
              else
                throw Error(
                  `This model has multiple chat templates with no default specified! Please either pass a chat template or the name of the template you wish to use to the 'chat_template' argument. Available template names are ${Object.keys(Me).sort()}.`
                );
          } else if (H === null)
            if (this.chat_template)
              H = this.chat_template;
            else
              throw Error(
                "Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
              );
          return H;
        }
        /**
         * Converts a list of message objects with `"role"` and `"content"` keys to a list of token
         * ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to
         * determine the format and control tokens to use when converting.
         * 
         * See [here](https://huggingface.co/docs/transformers/chat_templating) for more information.
         * 
         * **Example:** Applying a chat template to a conversation.
         * 
         * ```javascript
         * import { AutoTokenizer } from "@huggingface/transformers";
         * 
         * const tokenizer = await AutoTokenizer.from_pretrained("Xenova/mistral-tokenizer-v1");
         * 
         * const chat = [
         *   { "role": "user", "content": "Hello, how are you?" },
         *   { "role": "assistant", "content": "I'm doing great. How can I help you today?" },
         *   { "role": "user", "content": "I'd like to show off how chat templating works!" },
         * ]
         * 
         * const text = tokenizer.apply_chat_template(chat, { tokenize: false });
         * // "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
         * 
         * const input_ids = tokenizer.apply_chat_template(chat, { tokenize: true, return_tensor: false });
         * // [1, 733, 16289, 28793, 22557, 28725, 910, 460, 368, 28804, 733, 28748, 16289, 28793, 28737, 28742, 28719, 2548, 1598, 28723, 1602, 541, 315, 1316, 368, 3154, 28804, 2, 28705, 733, 16289, 28793, 315, 28742, 28715, 737, 298, 1347, 805, 910, 10706, 5752, 1077, 3791, 28808, 733, 28748, 16289, 28793]
         * ```
         * 
         * @param {Message[]} conversation A list of message objects with `"role"` and `"content"` keys,
         * representing the chat history so far.
         * @param {Object} options An optional object containing the following properties:
         * @param {string} [options.chat_template=null] A Jinja template to use for this conversion. If
         * this is not passed, the model's chat template will be used instead.
         * @param {Object[]} [options.tools=null]
         * A list of tools (callable functions) that will be accessible to the model. If the template does not
         * support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,
         * giving the name, description and argument types for the tool. See our
         * [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)
         * for more information.
         * @param {Record<string, string>[]} [options.documents=null]
         * A list of dicts representing documents that will be accessible to the model if it is performing RAG
         * (retrieval-augmented generation). If the template does not support RAG, this argument will have no
         * effect. We recommend that each document should be a dict containing "title" and "text" keys. Please
         * see the RAG section of the [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#arguments-for-RAG)
         * for examples of passing documents with chat templates.
         * @param {boolean} [options.add_generation_prompt=false] Whether to end the prompt with the token(s) that indicate
         * the start of an assistant message. This is useful when you want to generate a response from the model.
         * Note that this argument will be passed to the chat template, and so it must be supported in the
         * template for this argument to have any effect.
         * @param {boolean} [options.tokenize=true] Whether to tokenize the output. If false, the output will be a string.
         * @param {boolean} [options.padding=false] Whether to pad sequences to the maximum length. Has no effect if tokenize is false.
         * @param {boolean} [options.truncation=false] Whether to truncate sequences to the maximum length. Has no effect if tokenize is false.
         * @param {number} [options.max_length=null] Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is false.
         * If not specified, the tokenizer's `max_length` attribute will be used as a default.
         * @param {boolean} [options.return_tensor=true] Whether to return the output as a Tensor or an Array. Has no effect if tokenize is false.
         * @param {boolean} [options.return_dict=true] Whether to return a dictionary with named outputs. Has no effect if tokenize is false.
         * @param {Object} [options.tokenizer_kwargs={}] Additional options to pass to the tokenizer.
         * @returns {string | Tensor | number[]| number[][]|BatchEncoding} The tokenized output.
         */
        apply_chat_template(H, {
          tools: ae = null,
          documents: Me = null,
          chat_template: Pe = null,
          add_generation_prompt: He = !1,
          tokenize: ct = !0,
          padding: yt = !1,
          truncation: ht = !1,
          max_length: ot = null,
          return_tensor: Pt = !0,
          return_dict: hr = !1,
          tokenizer_kwargs: rr = {},
          ...$e
        } = {}) {
          if (Pe = this.get_chat_template({ chat_template: Pe, tools: ae }), typeof Pe != "string")
            throw Error(`chat_template must be a string, but got ${typeof Pe}`);
          let wr = this._compiled_template_cache.get(Pe);
          wr === void 0 && (wr = new x.Template(Pe), this._compiled_template_cache.set(Pe, wr));
          const Rr = /* @__PURE__ */ Object.create(null);
          for (const Zr of Xs) {
            const Bt = this.getToken(Zr);
            Bt && (Rr[Zr] = Bt);
          }
          const Jr = wr.render({
            messages: H,
            add_generation_prompt: He,
            tools: ae,
            documents: Me,
            ...Rr,
            ...$e
          });
          if (ct) {
            const Zr = this._call(Jr, {
              add_special_tokens: !1,
              padding: yt,
              truncation: ht,
              max_length: ot,
              return_tensor: Pt,
              ...rr
            });
            return hr ? Zr : Zr.input_ids;
          }
          return Jr;
        }
      }
      class Qs extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class ks extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class Bs extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class Ss extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class os extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class $s extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class cs extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class As extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class Ys extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class as extends Nt {
      }
      class nt extends Nt {
      }
      class _t extends Nt {
        constructor(H, ae) {
          super(H, ae);
          ge(this, "return_token_type_ids", !0);
          console.warn('WARNING: `XLMTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.');
        }
      }
      class Ft extends Nt {
        constructor() {
          super(...arguments);
          ge(this, "return_token_type_ids", !0);
        }
      }
      class lr extends Nt {
      }
      class bs extends Nt {
      }
      class tr extends Nt {
      }
      class ts extends Nt {
        constructor(P, H) {
          super(P, H), this.languageRegex = /^[a-z]{2}_[A-Z]{2}$/, this.language_codes = this.special_tokens.filter((ae) => this.languageRegex.test(ae)), this.lang_to_token = (ae) => ae;
        }
        /**
         * Helper function to build translation inputs for an `MBartTokenizer`.
         * @param {string|string[]} raw_inputs The text to tokenize.
         * @param {Object} tokenizer_options Options to be sent to the tokenizer
         * @param {Object} generate_kwargs Generation options.
         * @returns {Object} Object to be passed to the model.
         */
        _build_translation_inputs(P, H, ae) {
          return en(this, P, H, ae);
        }
      }
      class Rs extends ts {
      }
      class Js extends Nt {
      }
      class Ns extends Nt {
      }
      const vs = "▁";
      class Pn extends Nt {
        constructor(H, ae) {
          super(H, ae);
          ge(this, "padding_side", "left");
          this.legacy = ae.legacy ?? !0, this.legacy || (this.normalizer = null, this.pre_tokenizer = new At({
            replacement: vs,
            add_prefix_space: !0,
            prepend_scheme: "first"
          }));
        }
        /**
         * Helper function to handle legacy encoding of SPM tokenizers.
         * Adapted from https://github.com/huggingface/transformers/blob/e6dcf8abd6f65bb4b6dfc1831b20d9ba49ce00e2/src/transformers/models/t5/tokenization_t5.py#L374-L387
         * @param {string} text The text to encode.
         * @returns {string[]} The encoded tokens.
         */
        _encode_text(H) {
          if (H === null) return null;
          if (this.legacy || H.length === 0)
            return super._encode_text(H);
          let ae = super._encode_text(vs + H.replaceAll(vs, " "));
          return ae.length > 1 && ae[0] === vs && this.special_tokens.includes(ae[1]) && (ae = ae.slice(1)), ae;
        }
      }
      class js extends Nt {
      }
      class Cn extends Nt {
      }
      class Xn extends Nt {
      }
      class Us extends Nt {
      }
      class xs extends Nt {
      }
      class ls extends Nt {
      }
      class mn extends Nt {
      }
      class Zs extends Nt {
      }
      class _n extends Nt {
      }
      function en(Te, P, H, ae) {
        if (!("language_codes" in Te) || !Array.isArray(Te.language_codes))
          throw new Error("Tokenizer must have `language_codes` attribute set and it should be an array of language ids.");
        if (!("languageRegex" in Te) || !(Te.languageRegex instanceof RegExp))
          throw new Error("Tokenizer must have `languageRegex` attribute set and it should be a regular expression.");
        if (!("lang_to_token" in Te) || typeof Te.lang_to_token != "function")
          throw new Error("Tokenizer must have `lang_to_token` attribute set and it should be a function.");
        const Me = ae.src_lang, Pe = ae.tgt_lang;
        if (!Te.language_codes.includes(Pe))
          throw new Error(`Target language code "${Pe}" is not valid. Must be one of: {${Te.language_codes.join(", ")}}`);
        if (Me !== void 0) {
          if (!Te.language_codes.includes(Me))
            throw new Error(`Source language code "${Me}" is not valid. Must be one of: {${Te.language_codes.join(", ")}}`);
          for (const He of Te.post_processor.config.single)
            if ("SpecialToken" in He && Te.languageRegex.test(He.SpecialToken.id)) {
              He.SpecialToken.id = Te.lang_to_token(Me);
              break;
            }
        }
        return ae.forced_bos_token_id = Te.model.convert_tokens_to_ids([Te.lang_to_token(Pe)])[0], Te._call(P, H);
      }
      class Ts extends Nt {
        constructor(P, H) {
          super(P, H), this.languageRegex = /^[a-z]{3}_[A-Z][a-z]{3}$/, this.language_codes = this.special_tokens.filter((ae) => this.languageRegex.test(ae)), this.lang_to_token = (ae) => ae;
        }
        /**
         * Helper function to build translation inputs for an `NllbTokenizer`.
         * @param {string|string[]} raw_inputs The text to tokenize.
         * @param {Object} tokenizer_options Options to be sent to the tokenizer
         * @param {Object} generate_kwargs Generation options.
         * @returns {Object} Object to be passed to the model.
         */
        _build_translation_inputs(P, H, ae) {
          return en(this, P, H, ae);
        }
      }
      class zt extends Nt {
        constructor(P, H) {
          super(P, H), this.languageRegex = /^__[a-z]{2,3}__$/, this.language_codes = this.special_tokens.filter((ae) => this.languageRegex.test(ae)).map((ae) => ae.slice(2, -2)), this.lang_to_token = (ae) => `__${ae}__`;
        }
        /**
         * Helper function to build translation inputs for an `M2M100Tokenizer`.
         * @param {string|string[]} raw_inputs The text to tokenize.
         * @param {Object} tokenizer_options Options to be sent to the tokenizer
         * @param {Object} generate_kwargs Generation options.
         * @returns {Object} Object to be passed to the model.
         */
        _build_translation_inputs(P, H, ae) {
          return en(this, P, H, ae);
        }
      }
      class fn extends Nt {
        get timestamp_begin() {
          return this.model.convert_tokens_to_ids(["<|notimestamps|>"])[0] + 1;
        }
        /**
         * Decodes automatic speech recognition (ASR) sequences.
         * @param {Array<{tokens: bigint[], token_timestamps?: number[], stride: number[]}>} sequences The sequences to decode.
         * @param {Object} options The options to use for decoding.
         * @returns {Array<string|{chunks?: undefined|Array<{language: string|null, timestamp: Array<number|null>, text: string}>}>} The decoded sequences.
         */
        _decode_asr(P, {
          return_timestamps: H = !1,
          return_language: ae = !1,
          time_precision: Me = null,
          force_full_sequences: Pe = !0
        } = {}) {
          if (Me === null)
            throw Error("Must specify time_precision");
          let He = null;
          const ct = H === "word";
          function yt() {
            return { language: He, timestamp: [null, null], text: "" };
          }
          const ht = [];
          let ot = yt(), Pt = 0;
          const hr = this.timestamp_begin, $e = hr + 1500;
          let wr = [], Rr = [], Jr = !1, Zr = null;
          const Bt = new Set(this.all_special_ids);
          for (const er of P) {
            const mr = er.tokens, vt = ct ? er.token_timestamps : null;
            let yr = null, Es = hr;
            if ("stride" in er) {
              const [Mt, br, ze] = er.stride;
              if (Pt -= br, Zr = Mt - ze, br && (Es = br / Me + hr), ze)
                for (let wt = mr.length - 1; wt >= 0; --wt) {
                  const rs = Number(mr[wt]);
                  if (rs >= hr) {
                    if (yr !== null && (rs - hr) * Me < Zr)
                      break;
                    yr = rs;
                  }
                }
            }
            let Or = [], qr = [];
            for (let Mt = 0; Mt < mr.length; ++Mt) {
              const br = Number(mr[Mt]);
              if (Bt.has(br)) {
                const ze = this.decode([br]), wt = y.WHISPER_LANGUAGE_MAPPING.get(ze.slice(2, -2));
                if (wt !== void 0) {
                  if (He !== null && wt !== He && !H) {
                    wr.push(Or);
                    const rs = this.findLongestCommonSequence(wr)[0], tn = this.decode(rs);
                    ot.text = tn, ht.push(ot), wr = [], Or = [], ot = yt();
                  }
                  He = ot.language = wt;
                }
              } else if (br >= hr && br <= $e) {
                const ze = (br - hr) * Me + Pt, wt = (0, J.round)(ze, 2);
                if (yr !== null && br >= yr)
                  Jr = !0;
                else if (Jr || wr.length > 0 && br < Es)
                  Jr = !1;
                else if (ot.timestamp[0] === null)
                  ot.timestamp[0] = wt;
                else if (wt !== ot.timestamp[0]) {
                  ot.timestamp[1] = wt, wr.push(Or), ct && Rr.push(qr);
                  const [rs, tn] = this.findLongestCommonSequence(
                    wr,
                    Rr
                  ), Qn = this.decode(rs);
                  ot.text = Qn, ct && (ot.words = this.collateWordTimestamps(
                    rs,
                    tn,
                    He
                  )), ht.push(ot), wr = [], Or = [], Rr = [], qr = [], ot = yt();
                }
              } else if (Or.push(br), ct) {
                let ze = (0, J.round)(vt[Mt] + Pt, 2), wt;
                if (Mt + 1 < vt.length) {
                  wt = (0, J.round)(vt[Mt + 1] + Pt, 2);
                  const rs = this.decode([br]);
                  C.test(rs) && (wt = (0, J.round)(Math.min(ze + Me, wt), 2));
                } else
                  wt = null;
                qr.push([ze, wt]);
              }
            }
            if ("stride" in er) {
              const [Mt, br, ze] = er.stride;
              Pt += Mt - ze;
            }
            Or.length > 0 ? (wr.push(Or), ct && Rr.push(qr)) : wr.every((Mt) => Mt.length === 0) && (ot = yt(), wr = [], Or = [], Rr = [], qr = []);
          }
          if (wr.length > 0) {
            if (Pe && H)
              throw new Error(
                "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation."
              );
            const [er, mr] = this.findLongestCommonSequence(wr, Rr), vt = this.decode(er);
            ot.text = vt, ct && (ot.words = this.collateWordTimestamps(
              er,
              mr,
              He
            )), ht.push(ot);
          }
          let Nr = /* @__PURE__ */ Object.create(null);
          const ps = ht.map((er) => er.text).join("");
          if (H || ae) {
            for (let er = 0; er < ht.length; ++er) {
              const mr = ht[er];
              H || delete mr.timestamp, ae || delete mr.language;
            }
            if (ct) {
              const er = [];
              for (const mr of ht)
                for (const vt of mr.words)
                  er.push(vt);
              Nr = { chunks: er };
            } else
              Nr = { chunks: ht };
          }
          return [ps, Nr];
        }
        /**
         * Finds the longest common sequence among the provided sequences.
         * @param {number[][]} sequences An array of sequences of token ids to compare.
         * @returns {number[][]} The longest common sequence found.
         * @throws {Error} If there is a bug within the function.
         * @private
         */
        findLongestCommonSequence(P, H = null) {
          let ae = P[0], Me = ae.length, Pe = [];
          const He = Array.isArray(H) && H.length > 0;
          let ct = He ? [] : null, yt = He ? H[0] : null;
          for (let ht = 1; ht < P.length; ++ht) {
            const ot = P[ht];
            let Pt = 0, hr = [Me, Me, 0, 0];
            const rr = ot.length;
            for (let Nr = 1; Nr < Me + rr; ++Nr) {
              const ps = Math.max(0, Me - Nr), er = Math.min(Me, Me + rr - Nr), mr = ae.slice(ps, er), vt = Math.max(0, Nr - Me), yr = Math.min(rr, Nr), Es = ot.slice(vt, yr);
              if (mr.length !== Es.length)
                throw new Error("There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.");
              let Or;
              He ? Or = mr.filter((br, ze) => br === Es[ze] && yt[ps + ze] <= H[ht][vt + ze]).length : Or = mr.filter((br, ze) => br === Es[ze]).length;
              const qr = Nr / 1e4, Mt = Or / Nr + qr;
              Or > 1 && Mt > Pt && (Pt = Mt, hr = [ps, er, vt, yr]);
            }
            const [$e, wr, Rr, Jr] = hr, Zr = Math.floor((wr + $e) / 2), Bt = Math.floor((Jr + Rr) / 2);
            Pe.push(...ae.slice(0, Zr)), ae = ot.slice(Bt), Me = ae.length, He && (ct.push(...yt.slice(0, Zr)), yt = H[ht].slice(Bt));
          }
          return Pe.push(...ae), He ? (ct.push(...yt), [Pe, ct]) : [Pe, []];
        }
        /** @private */
        collateWordTimestamps(P, H, ae) {
          const [Me, Pe, He] = this.combineTokensIntoWords(P, ae), ct = [];
          for (let yt = 0; yt < Me.length; ++yt) {
            const ht = He[yt];
            ct.push({
              text: Me[yt],
              timestamp: [
                H[ht.at(0)][0],
                H[ht.at(-1)][1]
              ]
            });
          }
          return ct;
        }
        /**
         * Groups tokens by word. Returns a tuple containing a list of strings with the words,
         * and a list of `token_id` sequences with the tokens making up each word.
         * @param {number[]} tokens 
         * @param {string} [language] 
         * @param {string} prepend_punctionations 
         * @param {string} append_punctuations 
         * 
         * @private
         */
        combineTokensIntoWords(P, H, ae = `"'“¡¿([{-`, Me = `"'.。,，!！?？:：”)]}、`) {
          H = H ?? "english";
          let Pe, He, ct;
          return ["chinese", "japanese", "thai", "lao", "myanmar"].includes(H) ? [Pe, He, ct] = this.splitTokensOnUnicode(P) : [Pe, He, ct] = this.splitTokensOnSpaces(P), this.mergePunctuations(Pe, He, ct, ae, Me);
        }
        /** @type {PreTrainedTokenizer['decode']} */
        decode(P, H) {
          let ae;
          return H != null && H.decode_with_timestamps ? (P instanceof W.Tensor && (P = se(P)), ae = this.decodeWithTimestamps(P, H)) : ae = super.decode(P, H), ae;
        }
        /**
         * @param {number[]|bigint[]} token_ids List of token IDs to decode.
         * @param {Object} decode_args Optional arguments for decoding
         * @private
         */
        decodeWithTimestamps(P, H) {
          const ae = (H == null ? void 0 : H.time_precision) ?? 0.02, Me = Array.from(this.all_special_ids).at(-1) + 1;
          let Pe = [[]];
          for (let He of P)
            if (He = Number(He), He >= Me) {
              const ct = ((He - Me) * ae).toFixed(2);
              Pe.push(`<|${ct}|>`), Pe.push([]);
            } else
              Pe[Pe.length - 1].push(He);
          return Pe = Pe.map(
            (He) => typeof He == "string" ? He : super.decode(He, H)
          ), Pe.join("");
        }
        /**
         * Combine tokens into words by splitting at any position where the tokens are decoded as valid unicode points.
         * @param {number[]} tokens 
         * @returns {*}
         * @private
         */
        splitTokensOnUnicode(P) {
          const H = this.decode(P, {
            // @ts-ignore
            decode_with_timestamps: !0
          }), ae = "�", Me = [], Pe = [], He = [];
          let ct = [], yt = [], ht = 0;
          for (let ot = 0; ot < P.length; ++ot) {
            const Pt = P[ot];
            ct.push(Pt), yt.push(ot);
            const hr = this.decode(ct, {
              // @ts-ignore
              decode_with_timestamps: !0
            });
            (!hr.includes(ae) || H[ht + hr.indexOf(ae)] === ae) && (Me.push(hr), Pe.push(ct), He.push(yt), ct = [], yt = [], ht += hr.length);
          }
          return [Me, Pe, He];
        }
        /**
         * Combine tokens into words by splitting at whitespace and punctuation tokens.
         * @param {number[]} tokens 
         * @private
         */
        splitTokensOnSpaces(P) {
          const [H, ae, Me] = this.splitTokensOnUnicode(P), Pe = [], He = [], ct = [], yt = new RegExp(`^[${g}]$`, "gu");
          for (let ht = 0; ht < H.length; ++ht) {
            const ot = H[ht], Pt = ae[ht], hr = Me[ht], rr = Pt[0] >= this.model.tokens_to_ids.get("<|endoftext|>"), $e = ot.startsWith(" "), wr = ot.trim(), Rr = yt.test(wr);
            if (rr || $e || Rr || Pe.length === 0)
              Pe.push(ot), He.push(Pt), ct.push(hr);
            else {
              const Jr = Pe.length - 1;
              Pe[Jr] += ot, He[Jr].push(...Pt), ct[Jr].push(...hr);
            }
          }
          return [Pe, He, ct];
        }
        /**
         * Merges punctuation tokens with neighboring words.
         * @param {string[]} words 
         * @param {number[][]} tokens 
         * @param {number[][]} indices 
         * @param {string} prepended 
         * @param {string} appended 
         * @private
         */
        mergePunctuations(P, H, ae, Me, Pe) {
          const He = structuredClone(P), ct = structuredClone(H), yt = structuredClone(ae);
          let ht = He.length - 2, ot = He.length - 1;
          for (; ht >= 0; )
            He[ht].startsWith(" ") && Me.includes(He[ht].trim()) ? (He[ot] = He[ht] + He[ot], ct[ot] = (0, L.mergeArrays)(ct[ht], ct[ot]), yt[ot] = (0, L.mergeArrays)(yt[ht], yt[ot]), He[ht] = "", ct[ht] = [], yt[ht] = []) : ot = ht, --ht;
          for (ht = 0, ot = 1; ot < He.length; )
            !He[ht].endsWith(" ") && Pe.includes(He[ot]) ? (He[ht] += He[ot], ct[ht] = (0, L.mergeArrays)(ct[ht], ct[ot]), yt[ht] = (0, L.mergeArrays)(yt[ht], yt[ot]), He[ot] = "", ct[ot] = [], yt[ot] = []) : ht = ot, ++ot;
          return [
            He.filter((Pt) => Pt),
            ct.filter((Pt) => Pt.length > 0),
            yt.filter((Pt) => Pt.length > 0)
          ];
        }
      }
      class kn extends Nt {
      }
      class Sn extends Nt {
      }
      class $n extends Nt {
      }
      class Ws extends Nt {
        /**
         * Create a new MarianTokenizer instance.
         * @param {Object} tokenizerJSON The JSON of the tokenizer.
         * @param {Object} tokenizerConfig The config of the tokenizer.
         */
        constructor(P, H) {
          super(P, H), this.languageRegex = /^(>>\w+<<)\s*/g, this.supported_language_codes = this.model.vocab.filter(
            (ae) => this.languageRegex.test(ae)
          ), console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.');
        }
        /**
         * Encodes a single text. Overriding this method is necessary since the language codes
         * must be removed before encoding with sentencepiece model.
         * @see https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213
         *
         * @param {string|null} text The text to encode.
         * @returns {Array} The encoded tokens.
         */
        _encode_text(P) {
          if (P === null) return null;
          const [H, ...ae] = P.trim().split(this.languageRegex);
          if (ae.length === 0)
            return super._encode_text(H);
          if (ae.length === 2) {
            const [Me, Pe] = ae;
            return this.supported_language_codes.includes(Me) || console.warn(`Unsupported language code "${Me}" detected, which may lead to unexpected behavior. Should be one of: ${JSON.stringify(this.supported_language_codes)}`), (0, L.mergeArrays)([Me], super._encode_text(Pe));
          }
        }
      }
      class An extends Nt {
      }
      class gn extends Nt {
      }
      class In extends Nt {
      }
      class or extends Nt {
      }
      class Yr extends Nt {
      }
      class wn extends Nt {
        constructor(P, H) {
          super(P, H), this.decoder = new Ot({});
        }
      }
      class Fn extends Nt {
      }
      class yn extends Nt {
      }
      class Mn {
        /**
         * Instantiate one of the tokenizer classes of the library from a pretrained model.
         * 
         * The tokenizer class to instantiate is selected based on the `tokenizer_class` property of the config object
         * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)
         * 
         * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:
         * - A string, the *model id* of a pretrained tokenizer hosted inside a model repo on huggingface.co.
         *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
         *   user or organization name, like `dbmdz/bert-base-german-cased`.
         * - A path to a *directory* containing tokenizer files, e.g., `./my_model_directory/`.
         * @param {PretrainedTokenizerOptions} options Additional options for loading the tokenizer.
         * 
         * @returns {Promise<PreTrainedTokenizer>} A new instance of the PreTrainedTokenizer class.
         */
        static async from_pretrained(P, {
          progress_callback: H = null,
          config: ae = null,
          cache_dir: Me = null,
          local_files_only: Pe = !1,
          revision: He = "main",
          legacy: ct = null
        } = {}) {
          var hr;
          const [yt, ht] = await M(P, {
            progress_callback: H,
            config: ae,
            cache_dir: Me,
            local_files_only: Pe,
            revision: He,
            legacy: ct
          }), ot = ((hr = ht.tokenizer_class) == null ? void 0 : hr.replace(/Fast$/, "")) ?? "PreTrainedTokenizer";
          let Pt = this.TOKENIZER_CLASS_MAPPING[ot];
          return Pt || (console.warn(`Unknown tokenizer class "${ot}", attempting to construct from base class.`), Pt = Nt), new Pt(yt, ht);
        }
      }
      ge(Mn, "TOKENIZER_CLASS_MAPPING", {
        T5Tokenizer: lr,
        DistilBertTokenizer: as,
        CamembertTokenizer: nt,
        DebertaTokenizer: os,
        DebertaV2Tokenizer: $s,
        BertTokenizer: Qs,
        HerbertTokenizer: cs,
        ConvBertTokenizer: As,
        RoFormerTokenizer: Ys,
        XLMTokenizer: _t,
        ElectraTokenizer: Ft,
        MobileBertTokenizer: Bs,
        SqueezeBertTokenizer: Ss,
        AlbertTokenizer: ks,
        GPT2Tokenizer: bs,
        BartTokenizer: tr,
        MBartTokenizer: ts,
        MBart50Tokenizer: Rs,
        RobertaTokenizer: Js,
        WhisperTokenizer: fn,
        CodeGenTokenizer: kn,
        CLIPTokenizer: Sn,
        SiglipTokenizer: $n,
        MarianTokenizer: Ws,
        BloomTokenizer: Ns,
        NllbTokenizer: Ts,
        M2M100Tokenizer: zt,
        LlamaTokenizer: Pn,
        CodeLlamaTokenizer: js,
        XLMRobertaTokenizer: Cn,
        MPNetTokenizer: Xn,
        FalconTokenizer: Us,
        GPTNeoXTokenizer: xs,
        EsmTokenizer: ls,
        Wav2Vec2CTCTokenizer: An,
        BlenderbotTokenizer: gn,
        BlenderbotSmallTokenizer: In,
        SpeechT5Tokenizer: or,
        NougatTokenizer: Yr,
        VitsTokenizer: wn,
        Qwen2Tokenizer: mn,
        GemmaTokenizer: Zs,
        Grok1Tokenizer: _n,
        CohereTokenizer: Fn,
        MgpstrTokenizer: yn,
        // Base case:
        PreTrainedTokenizer: Nt
      });
    }
  ),
  /***/
  "./src/utils/audio.js": (
    /*!****************************!*\
      !*** ./src/utils/audio.js ***!
      \****************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        hamming: () => (
          /* binding */
          y
        ),
        /* harmony export */
        hanning: () => (
          /* binding */
          x
        ),
        /* harmony export */
        mel_filter_bank: () => (
          /* binding */
          z
        ),
        /* harmony export */
        read_audio: () => (
          /* binding */
          W
        ),
        /* harmony export */
        spectrogram: () => (
          /* binding */
          g
        ),
        /* harmony export */
        window_function: () => (
          /* binding */
          C
        )
        /* harmony export */
      });
      var f = s(
        /*! ./hub.js */
        "./src/utils/hub.js"
      ), L = s(
        /*! ./maths.js */
        "./src/utils/maths.js"
      ), j = s(
        /*! ./core.js */
        "./src/utils/core.js"
      ), J = s(
        /*! ./tensor.js */
        "./src/utils/tensor.js"
      );
      async function W(v, ee) {
        if (typeof AudioContext > "u")
          throw Error(
            "Unable to load audio from path/URL since `AudioContext` is not available in your environment. Instead, audio data should be passed directly to the pipeline/processor. For more information and some example code, see https://huggingface.co/docs/transformers.js/guides/node-audio-processing."
          );
        const X = await (await (0, f.getFile)(v)).arrayBuffer(), le = new AudioContext({ sampleRate: ee });
        typeof ee > "u" && console.warn(`No sampling rate provided, using default of ${le.sampleRate}Hz.`);
        const ue = await le.decodeAudioData(X);
        let fe;
        if (ue.numberOfChannels === 2) {
          const Ce = Math.sqrt(2), xe = ue.getChannelData(0), Le = ue.getChannelData(1);
          fe = new Float32Array(xe.length);
          for (let qe = 0; qe < ue.length; ++qe)
            fe[qe] = Ce * (xe[qe] + Le[qe]) / 2;
        } else
          fe = ue.getChannelData(0);
        return fe;
      }
      function w(v, ee) {
        if (v < 1)
          return new Float64Array();
        if (v === 1)
          return new Float64Array([1]);
        const X = 1 - ee, le = 2 * Math.PI / (v - 1), ue = new Float64Array(v);
        for (let fe = 0; fe < v; ++fe)
          ue[fe] = ee - X * Math.cos(fe * le);
        return ue;
      }
      function x(v) {
        return w(v, 0.5);
      }
      function y(v) {
        return w(v, 0.54);
      }
      const M = {
        htk: (v) => 2595 * Math.log10(1 + v / 700),
        kaldi: (v) => 1127 * Math.log(1 + v / 700),
        slaney: (v, ee = 1e3, X = 15, le = 27 / Math.log(6.4)) => v >= ee ? X + Math.log(v / ee) * le : 3 * v / 200
      };
      function b(v, ee = "htk") {
        const X = M[ee];
        if (!X)
          throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');
        return typeof v == "number" ? X(v) : v.map((le) => X(le));
      }
      const D = {
        htk: (v) => 700 * (10 ** (v / 2595) - 1),
        kaldi: (v) => 700 * (Math.exp(v / 1127) - 1),
        slaney: (v, ee = 1e3, X = 15, le = Math.log(6.4) / 27) => v >= X ? ee * Math.exp(le * (v - X)) : 200 * v / 3
      };
      function q(v, ee = "htk") {
        const X = D[ee];
        if (!X)
          throw new Error('mel_scale should be one of "htk", "slaney" or "kaldi".');
        return typeof v == "number" ? X(v) : v.map((le) => X(le));
      }
      function se(v, ee) {
        const X = Float64Array.from(
          { length: ee.length - 1 },
          (Ce, xe) => ee[xe + 1] - ee[xe]
        ), le = Array.from({
          length: v.length
        }, () => new Array(ee.length));
        for (let Ce = 0; Ce < v.length; ++Ce) {
          const xe = le[Ce];
          for (let Le = 0; Le < ee.length; ++Le)
            xe[Le] = ee[Le] - v[Ce];
        }
        const ue = ee.length - 2, fe = Array.from({ length: ue }, () => new Array(v.length));
        for (let Ce = 0; Ce < v.length; ++Ce) {
          const xe = le[Ce];
          for (let Le = 0; Le < ue; ++Le) {
            const qe = -xe[Le] / X[Le], Ue = xe[Le + 2] / X[Le + 1];
            fe[Le][Ce] = Math.max(0, Math.min(qe, Ue));
          }
        }
        return fe;
      }
      function oe(v, ee, X) {
        const le = (ee - v) / (X - 1);
        return Float64Array.from({ length: X }, (ue, fe) => v + le * fe);
      }
      function z(v, ee, X, le, ue, fe = null, Ce = "htk", xe = !1) {
        if (fe !== null && fe !== "slaney")
          throw new Error('norm must be one of null or "slaney"');
        const Le = b(X, Ce), qe = b(le, Ce), Ue = oe(Le, qe, ee + 2);
        let ut = q(Ue, Ce), de;
        if (xe) {
          const he = ue / (v * 2);
          de = b(Float64Array.from({ length: v }, (Ee, Be) => Be * he), Ce), ut = Ue;
        } else
          de = oe(0, Math.floor(ue / 2), v);
        const re = se(de, ut);
        if (fe !== null && fe === "slaney")
          for (let he = 0; he < ee; ++he) {
            const Ee = re[he], Be = 2 / (ut[he + 2] - ut[he]);
            for (let et = 0; et < v; ++et)
              Ee[et] *= Be;
          }
        return re;
      }
      function V(v, ee, X) {
        const le = new v.constructor(v.length + ee + X), ue = v.length - 1;
        for (let fe = 0; fe < v.length; ++fe)
          le[ee + fe] = v[fe];
        for (let fe = 1; fe <= ee; ++fe)
          le[ee - fe] = v[(0, j.calculateReflectOffset)(fe, ue)];
        for (let fe = 1; fe <= X; ++fe)
          le[ue + ee + fe] = v[(0, j.calculateReflectOffset)(ue - fe, ue)];
        return le;
      }
      function Y(v, ee, X, le, ue) {
        if (X <= 0)
          throw new Error("reference must be greater than zero");
        if (le <= 0)
          throw new Error("min_value must be greater than zero");
        X = Math.max(le, X);
        const fe = Math.log10(X);
        for (let Ce = 0; Ce < v.length; ++Ce)
          v[Ce] = ee * Math.log10(Math.max(le, v[Ce]) - fe);
        if (ue !== null) {
          if (ue <= 0)
            throw new Error("db_range must be greater than zero");
          const Ce = (0, L.max)(v)[0] - ue;
          for (let xe = 0; xe < v.length; ++xe)
            v[xe] = Math.max(v[xe], Ce);
        }
        return v;
      }
      function O(v, ee = 1, X = 1e-5, le = null) {
        return Y(v, 20, ee, X, le);
      }
      function $(v, ee = 1, X = 1e-10, le = null) {
        return Y(v, 10, ee, X, le);
      }
      async function g(v, ee, X, le, {
        fft_length: ue = null,
        power: fe = 1,
        center: Ce = !0,
        pad_mode: xe = "reflect",
        onesided: Le = !0,
        preemphasis: qe = null,
        mel_filters: Ue = null,
        mel_floor: ut = 1e-10,
        log_mel: de = null,
        reference: re = 1,
        min_value: he = 1e-10,
        db_range: Ee = null,
        remove_dc_offset: Be = null,
        // Custom parameters for efficiency reasons
        min_num_frames: et = null,
        max_num_frames: Xe = null,
        do_pad: ie = !0,
        transpose: Je = !1
      } = {}) {
        const De = ee.length;
        if (ue === null && (ue = X), X > ue)
          throw Error(`frame_length (${X}) may not be larger than fft_length (${ue})`);
        if (De !== X)
          throw new Error(`Length of the window (${De}) must equal frame_length (${X})`);
        if (le <= 0)
          throw new Error("hop_length must be greater than zero");
        if (fe === null && Ue !== null)
          throw new Error(
            "You have provided `mel_filters` but `power` is `None`. Mel spectrogram computation is not yet supported for complex-valued spectrogram. Specify `power` to fix this issue."
          );
        if (Ce) {
          if (xe !== "reflect")
            throw new Error(`pad_mode="${xe}" not implemented yet.`);
          const F = Math.floor((ue - 1) / 2) + 1;
          v = V(v, F, F);
        }
        let ce = Math.floor(1 + Math.floor((v.length - X) / le));
        et !== null && ce < et && (ce = et);
        const ve = Le ? Math.floor(ue / 2) + 1 : ue;
        let Re = ce, je = ce;
        Xe !== null && (Xe > ce ? ie && (je = Xe) : je = Re = Xe);
        const Ve = new L.FFT(ue), Ne = new Float64Array(ue), Ze = new Float64Array(Ve.outputBufferSize), at = new Float32Array(ve * je);
        for (let F = 0; F < Re; ++F) {
          const ne = F * le, K = Math.min(v.length - ne, X);
          K !== X && Ne.fill(0, 0, X);
          for (let pe = 0; pe < K; ++pe)
            Ne[pe] = v[ne + pe];
          if (Be) {
            let pe = 0;
            for (let Qe = 0; Qe < K; ++Qe)
              pe += Ne[Qe];
            const Oe = pe / K;
            for (let Qe = 0; Qe < K; ++Qe)
              Ne[Qe] -= Oe;
          }
          if (qe !== null) {
            for (let pe = K - 1; pe >= 1; --pe)
              Ne[pe] -= qe * Ne[pe - 1];
            Ne[0] *= 1 - qe;
          }
          for (let pe = 0; pe < ee.length; ++pe)
            Ne[pe] *= ee[pe];
          Ve.realTransform(Ze, Ne);
          for (let pe = 0; pe < ve; ++pe) {
            const Oe = pe << 1;
            at[pe * je + F] = Ze[Oe] ** 2 + Ze[Oe + 1] ** 2;
          }
        }
        if (fe !== null && fe !== 2) {
          const F = 2 / fe;
          for (let ne = 0; ne < at.length; ++ne)
            at[ne] **= F;
        }
        const ft = Ue.length;
        let dt = await (0, J.matmul)(
          // TODO: Make `mel_filters` a Tensor during initialization
          new J.Tensor("float32", Ue.flat(), [ft, ve]),
          new J.Tensor("float32", at, [ve, je])
        );
        Je && (dt = dt.transpose(1, 0));
        const gt = (
          /** @type {Float32Array} */
          dt.data
        );
        for (let F = 0; F < gt.length; ++F)
          gt[F] = Math.max(ut, gt[F]);
        if (fe !== null && de !== null) {
          const F = Math.min(gt.length, Re * ft);
          switch (de) {
            case "log":
              for (let ne = 0; ne < F; ++ne)
                gt[ne] = Math.log(gt[ne]);
              break;
            case "log10":
              for (let ne = 0; ne < F; ++ne)
                gt[ne] = Math.log10(gt[ne]);
              break;
            case "dB":
              if (fe === 1)
                O(gt, re, he, Ee);
              else if (fe === 2)
                $(gt, re, he, Ee);
              else
                throw new Error(`Cannot use log_mel option '${de}' with power ${fe}`);
              break;
            default:
              throw new Error(`log_mel must be one of null, 'log', 'log10' or 'dB'. Got '${de}'`);
          }
        }
        return dt;
      }
      function C(v, ee, {
        periodic: X = !0,
        frame_length: le = null,
        center: ue = !0
      } = {}) {
        const fe = X ? v + 1 : v;
        let Ce;
        switch (ee) {
          case "boxcar":
            Ce = new Float64Array(fe).fill(1);
            break;
          case "hann":
          case "hann_window":
            Ce = x(fe);
            break;
          case "hamming":
            Ce = y(fe);
            break;
          case "povey":
            Ce = x(fe).map((xe) => Math.pow(xe, 0.85));
            break;
          default:
            throw new Error(`Unknown window type ${ee}.`);
        }
        if (X && (Ce = Ce.subarray(0, v)), le === null)
          return Ce;
        if (v > le)
          throw new Error(`Length of the window (${v}) may not be larger than frame_length (${le})`);
        return Ce;
      }
    }
  ),
  /***/
  "./src/utils/constants.js": (
    /*!********************************!*\
      !*** ./src/utils/constants.js ***!
      \********************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        CHAT_TEMPLATE_NAME: () => (
          /* binding */
          w
        ),
        /* harmony export */
        CONFIG_NAME: () => (
          /* binding */
          L
        ),
        /* harmony export */
        FEATURE_EXTRACTOR_NAME: () => (
          /* binding */
          j
        ),
        /* harmony export */
        GENERATION_CONFIG_NAME: () => (
          /* binding */
          x
        ),
        /* harmony export */
        GITHUB_ISSUE_URL: () => (
          /* binding */
          f
        ),
        /* harmony export */
        IMAGE_PROCESSOR_NAME: () => (
          /* binding */
          J
        ),
        /* harmony export */
        PROCESSOR_NAME: () => (
          /* binding */
          W
        )
        /* harmony export */
      });
      const f = "https://github.com/huggingface/transformers.js/issues/new/choose", L = "config.json", j = "preprocessor_config.json", J = j, W = "processor_config.json", w = "chat_template.json", x = "generation_config.json";
    }
  ),
  /***/
  "./src/utils/core.js": (
    /*!***************************!*\
      !*** ./src/utils/core.js ***!
      \***************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        calculateDimensions: () => (
          /* binding */
          x
        ),
        /* harmony export */
        calculateReflectOffset: () => (
          /* binding */
          D
        ),
        /* harmony export */
        count: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        dispatchCallback: () => (
          /* binding */
          f
        ),
        /* harmony export */
        escapeRegExp: () => (
          /* binding */
          j
        ),
        /* harmony export */
        isIntegralNumber: () => (
          /* binding */
          W
        ),
        /* harmony export */
        isNullishDimension: () => (
          /* binding */
          w
        ),
        /* harmony export */
        isTypedArray: () => (
          /* binding */
          J
        ),
        /* harmony export */
        len: () => (
          /* binding */
          se
        ),
        /* harmony export */
        mergeArrays: () => (
          /* binding */
          M
        ),
        /* harmony export */
        pick: () => (
          /* binding */
          q
        ),
        /* harmony export */
        pop: () => (
          /* binding */
          y
        ),
        /* harmony export */
        product: () => (
          /* binding */
          b
        ),
        /* harmony export */
        reverseDictionary: () => (
          /* binding */
          L
        )
        /* harmony export */
      });
      function f(z, V) {
        z && z(V);
      }
      function L(z) {
        return Object.fromEntries(Object.entries(z).map(([V, Y]) => [Y, V]));
      }
      function j(z) {
        return z.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
      }
      function J(z) {
        var V, Y, O;
        return ((O = (Y = (V = z == null ? void 0 : z.prototype) == null ? void 0 : V.__proto__) == null ? void 0 : Y.constructor) == null ? void 0 : O.name) === "TypedArray";
      }
      function W(z) {
        return Number.isInteger(z) || typeof z == "bigint";
      }
      function w(z) {
        return z == null || z === -1;
      }
      function x(z) {
        const V = [];
        let Y = z;
        for (; Array.isArray(Y); )
          V.push(Y.length), Y = Y[0];
        return V;
      }
      function y(z, V, Y = void 0) {
        const O = z[V];
        if (O !== void 0)
          return delete z[V], O;
        if (Y === void 0)
          throw Error(`Key ${V} does not exist in object.`);
        return Y;
      }
      function M(...z) {
        return Array.prototype.concat.apply([], z);
      }
      function b(...z) {
        return z.reduce((V, Y) => V.flatMap((O) => Y.map(($) => [O, $])));
      }
      function D(z, V) {
        return Math.abs((z + V) % (2 * V) - V);
      }
      function q(z, V) {
        return Object.assign(
          {},
          ...V.map((Y) => {
            if (z[Y] !== void 0)
              return { [Y]: z[Y] };
          })
        );
      }
      function se(z) {
        let V = 0;
        for (const Y of z) ++V;
        return V;
      }
      function oe(z, V) {
        let Y = 0;
        for (const O of z)
          O === V && ++Y;
        return Y;
      }
    }
  ),
  /***/
  "./src/utils/data-structures.js": (
    /*!**************************************!*\
      !*** ./src/utils/data-structures.js ***!
      \**************************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        CharTrie: () => (
          /* binding */
          L
        ),
        /* harmony export */
        PriorityQueue: () => (
          /* binding */
          f
        ),
        /* harmony export */
        TokenLattice: () => (
          /* binding */
          J
        )
        /* harmony export */
      });
      class f {
        /**
         * Create a new PriorityQueue.
         * @param {function(any, any): boolean} comparator Comparator function to determine priority. Defaults to a MaxHeap.
         */
        constructor(x = (M, b) => M > b, y = 1 / 0) {
          this._heap = [], this._comparator = x, this._maxSize = y;
        }
        /**
         * The size of the queue
         */
        get size() {
          return this._heap.length;
        }
        /**
         * Check if the queue is empty.
         * @returns {boolean} `true` if the queue is empty, `false` otherwise.
         */
        isEmpty() {
          return this.size === 0;
        }
        /**
         * Return the element with the highest priority in the queue.
         * @returns {any} The highest priority element in the queue.
         */
        peek() {
          return this._heap[0];
        }
        /**
         * Add one or more elements to the queue.
         * @param  {...any} values The values to push into the queue.
         * @returns {number} The new size of the queue.
         */
        push(...x) {
          return this.extend(x);
        }
        /**
         * Add multiple elements to the queue.
         * @param {any[]} values The values to push into the queue.
         * @returns {number} The new size of the queue.
         */
        extend(x) {
          for (const y of x)
            if (this.size < this._maxSize)
              this._heap.push(y), this._siftUp();
            else {
              const M = this._smallest();
              this._comparator(y, this._heap[M]) && (this._heap[M] = y, this._siftUpFrom(M));
            }
          return this.size;
        }
        /**
         * Remove and return the element with the highest priority in the queue.
         * @returns {any} The element with the highest priority in the queue.
         */
        pop() {
          const x = this.peek(), y = this.size - 1;
          return y > 0 && this._swap(0, y), this._heap.pop(), this._siftDown(), x;
        }
        /**
         * Replace the element with the highest priority in the queue with a new value.
         * @param {*} value The new value.
         * @returns {*} The replaced value.
         */
        replace(x) {
          const y = this.peek();
          return this._heap[0] = x, this._siftDown(), y;
        }
        /**
         * Compute the index for the parent of the node at index `i`.
         * @param {number} i The index of the node to get the parent of.
         * @returns {number} The index of the parent node.
         * @private
         */
        _parent(x) {
          return (x + 1 >>> 1) - 1;
        }
        /**
         * Compute the index for the left child of the node at index `i`.
         * @param {number} i The index of the node to get the left child of.
         * @returns {number} The index of the left child.
         * @private
         */
        _left(x) {
          return (x << 1) + 1;
        }
        /**
         * Compute the index for the right child of the node at index `i`.
         * @param {number} i The index of the node to get the right child of.
         * @returns {number} The index of the right child.
         * @private
         */
        _right(x) {
          return x + 1 << 1;
        }
        /**
         * Check if the element at index `i` is greater than the element at index `j`.
         * @param {number} i The index of the first element to compare.
         * @param {number} j The index of the second element to compare.
         * @returns {boolean} `true` if the element at index `i` is greater than the element at index `j`, `false` otherwise.
         * @private
         */
        _greater(x, y) {
          return this._comparator(this._heap[x], this._heap[y]);
        }
        /**
         * Swap the elements at indices `i` and `j`.
         * @param {number} i The index of the first element to swap.
         * @param {number} j The index of the second element to swap.
         * @private
         */
        _swap(x, y) {
          const M = this._heap[x];
          this._heap[x] = this._heap[y], this._heap[y] = M;
        }
        /**
         * Maintain the heap property by updating positions in the heap,
         * starting at the last element and moving up the heap.
         * @private
         */
        _siftUp() {
          this._siftUpFrom(this.size - 1);
        }
        /**
         * Helper function to sift up from a given node.
         * @param {number} node The index of the node to start sifting up from.
         */
        _siftUpFrom(x) {
          for (; x > 0 && this._greater(x, this._parent(x)); )
            this._swap(x, this._parent(x)), x = this._parent(x);
        }
        /**
         * Maintain the heap property by updating positions in the heap,
         * starting at the first element and moving down the heap.
         * @private
         */
        _siftDown() {
          let x = 0;
          for (; this._left(x) < this.size && this._greater(this._left(x), x) || this._right(x) < this.size && this._greater(this._right(x), x); ) {
            const y = this._right(x) < this.size && this._greater(this._right(x), this._left(x)) ? this._right(x) : this._left(x);
            this._swap(x, y), x = y;
          }
        }
        /**
         * Get the index of the smallest element in the heap. Since we use an array-based heap,
         * the index can be computed without needing to traverse the heap.
         * @private
         */
        _smallest() {
          return 2 ** Math.floor(Math.log2(this.size)) - 1;
        }
      }
      class L {
        constructor() {
          this.root = j.default();
        }
        /**
         * Adds one or more `texts` to the trie.
         * @param {string[]} texts The strings to add to the trie.
         */
        extend(x) {
          for (const y of x)
            this.push(y);
        }
        /**
         * Adds text to the trie.
         * @param {string} text The string to add to the trie.
         */
        push(x) {
          let y = this.root;
          for (const M of x) {
            let b = y.children.get(M);
            b === void 0 && (b = j.default(), y.children.set(M, b)), y = b;
          }
          y.isLeaf = !0;
        }
        /**
         * Searches the trie for all strings with a common prefix of `text`.
         * @param {string} text The common prefix to search for.
         * @yields {string} Each string in the trie that has `text` as a prefix.
         */
        *commonPrefixSearch(x) {
          let y = this.root;
          if (y === void 0) return;
          let M = "";
          for (const b of x) {
            if (M += b, y = y.children.get(b), y === void 0) return;
            y.isLeaf && (yield M);
          }
        }
      }
      class j {
        /**
         * Create a new CharTrieNode.
         * @param {boolean} isLeaf Whether the node is a leaf node or not.
         * @param {Map<string, CharTrieNode>} children A map containing the node's children, where the key is a character and the value is a `CharTrieNode`.
         */
        constructor(x, y) {
          this.isLeaf = x, this.children = y;
        }
        /**
         * Returns a new `CharTrieNode` instance with default values.
         * @returns {CharTrieNode} A new `CharTrieNode` instance with `isLeaf` set to `false` and an empty `children` map.
         */
        static default() {
          return new j(!1, /* @__PURE__ */ new Map());
        }
      }
      class J {
        /**
         * Creates a new TokenLattice instance.
         *
         * @param {string} sentence The input sentence to be tokenized.
         * @param {number} bosTokenId The beginning-of-sequence token ID.
         * @param {number} eosTokenId The end-of-sequence token ID.
         */
        constructor(x, y, M) {
          this.chars = Array.from(x), this.len = this.chars.length, this.bosTokenId = y, this.eosTokenId = M, this.nodes = [], this.beginNodes = Array.from({ length: this.len + 1 }, () => []), this.endNodes = Array.from({ length: this.len + 1 }, () => []);
          const b = new W(this.bosTokenId, 0, 0, 0, 0), D = new W(this.eosTokenId, 1, this.len, 0, 0);
          this.nodes.push(b.clone()), this.nodes.push(D.clone()), this.beginNodes[this.len].push(D), this.endNodes[0].push(b);
        }
        /**
         * Inserts a new token node into the token lattice.
         *
         * @param {number} pos The starting position of the token.
         * @param {number} length The length of the token.
         * @param {number} score The score of the token.
         * @param {number} tokenId The token ID of the token.
         */
        insert(x, y, M, b) {
          const D = this.nodes.length, q = new W(b, D, x, y, M);
          this.beginNodes[x].push(q), this.endNodes[x + y].push(q), this.nodes.push(q);
        }
        /**
         * Implements the Viterbi algorithm to compute the most likely sequence of tokens.
         *
         * @returns {TokenLatticeNode[]} The most likely sequence of tokens.
         */
        viterbi() {
          const x = this.len;
          let y = 0;
          for (; y <= x; ) {
            if (this.beginNodes[y].length == 0)
              return [];
            for (let se of this.beginNodes[y]) {
              se.prev = null;
              let oe = 0, z = null;
              for (let V of this.endNodes[y]) {
                const Y = V.backtraceScore + se.score;
                (z === null || Y > oe) && (z = V.clone(), oe = Y);
              }
              if (z !== null)
                se.prev = z, se.backtraceScore = oe;
              else
                return [];
            }
            ++y;
          }
          const M = [], D = this.beginNodes[x][0].prev;
          if (D === null)
            return [];
          let q = D.clone();
          for (; q.prev !== null; )
            M.push(q.clone()), q = q.clone().prev.clone();
          return M.reverse(), M;
        }
        /**
         * @param {TokenLatticeNode} node
         * @returns {string} The array of nodes representing the most likely sequence of tokens.
         */
        piece(x) {
          return this.chars.slice(x.pos, x.pos + x.length).join("");
        }
        /**
         * @returns {string[]} The most likely sequence of tokens.
         */
        tokens() {
          return this.viterbi().map((y) => this.piece(y));
        }
        /**
         * @returns {number[]} The most likely sequence of token ids.
         */
        tokenIds() {
          return this.viterbi().map((y) => y.tokenId);
        }
      }
      class W {
        /**
         * Represents a node in a token lattice for a given sentence.
         * @param {number} tokenId The ID of the token associated with this node.
         * @param {number} nodeId The ID of this node.
         * @param {number} pos The starting position of the token in the sentence.
         * @param {number} length The length of the token.
         * @param {number} score The score associated with the token.
         */
        constructor(x, y, M, b, D) {
          this.tokenId = x, this.nodeId = y, this.pos = M, this.length = b, this.score = D, this.prev = null, this.backtraceScore = 0;
        }
        /**
         * Returns a clone of this node.
         * @returns {TokenLatticeNode} A clone of this node.
         */
        clone() {
          const x = new W(this.tokenId, this.nodeId, this.pos, this.length, this.score);
          return x.prev = this.prev, x.backtraceScore = this.backtraceScore, x;
        }
      }
    }
  ),
  /***/
  "./src/utils/devices.js": (
    /*!******************************!*\
      !*** ./src/utils/devices.js ***!
      \******************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        DEVICE_TYPES: () => (
          /* binding */
          f
        )
        /* harmony export */
      });
      const f = Object.freeze({
        auto: "auto",
        // Auto-detect based on device and environment
        gpu: "gpu",
        // Auto-detect GPU
        cpu: "cpu",
        // CPU
        wasm: "wasm",
        // WebAssembly
        webgpu: "webgpu",
        // WebGPU
        cuda: "cuda",
        // CUDA
        dml: "dml",
        // DirectML
        webnn: "webnn",
        // WebNN (default)
        "webnn-npu": "webnn-npu",
        // WebNN NPU
        "webnn-gpu": "webnn-gpu",
        // WebNN GPU
        "webnn-cpu": "webnn-cpu"
        // WebNN CPU
      });
    }
  ),
  /***/
  "./src/utils/dtypes.js": (
    /*!*****************************!*\
      !*** ./src/utils/dtypes.js ***!
      \*****************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        DATA_TYPES: () => (
          /* binding */
          J
        ),
        /* harmony export */
        DEFAULT_DEVICE_DTYPE_MAPPING: () => (
          /* binding */
          W
        ),
        /* harmony export */
        DEFAULT_DTYPE_SUFFIX_MAPPING: () => (
          /* binding */
          w
        ),
        /* harmony export */
        isWebGpuFp16Supported: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      var f = s(
        /*! ../env.js */
        "./src/env.js"
      ), L = s(
        /*! ./devices.js */
        "./src/utils/devices.js"
      );
      const j = /* @__PURE__ */ function() {
        let x;
        return async function() {
          if (x === void 0)
            if (!f.apis.IS_WEBGPU_AVAILABLE)
              x = !1;
            else
              try {
                x = (await navigator.gpu.requestAdapter()).features.has("shader-f16");
              } catch {
                x = !1;
              }
          return x;
        };
      }(), J = Object.freeze({
        auto: "auto",
        // Auto-detect based on environment
        fp32: "fp32",
        fp16: "fp16",
        q8: "q8",
        int8: "int8",
        uint8: "uint8",
        q4: "q4",
        bnb4: "bnb4",
        q4f16: "q4f16"
        // fp16 model with int4 block weight quantization
      }), W = Object.freeze({
        // NOTE: If not specified, will default to fp32
        [L.DEVICE_TYPES.wasm]: J.q8
      }), w = Object.freeze({
        [J.fp32]: "",
        [J.fp16]: "_fp16",
        [J.int8]: "_int8",
        [J.uint8]: "_uint8",
        [J.q8]: "_quantized",
        [J.q4]: "_q4",
        [J.q4f16]: "_q4f16",
        [J.bnb4]: "_bnb4"
      });
    }
  ),
  /***/
  "./src/utils/generic.js": (
    /*!******************************!*\
      !*** ./src/utils/generic.js ***!
      \******************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Callable: () => (
          /* binding */
          f
        )
        /* harmony export */
      });
      const f = (
        /** @type {any} */
        class {
          /**
          * Creates a new instance of the Callable class.
          */
          constructor() {
            let L = function(...j) {
              return L._call(...j);
            };
            return Object.setPrototypeOf(L, new.target.prototype);
          }
          /**
           * This method should be implemented in subclasses to provide the
           * functionality of the callable object.
           *
           * @param {any[]} args
           * @throws {Error} If the subclass does not implement the `_call` method.
           */
          _call(...L) {
            throw Error("Must implement _call method in subclass");
          }
        }
      );
    }
  ),
  /***/
  "./src/utils/hub.js": (
    /*!**************************!*\
      !*** ./src/utils/hub.js ***!
      \**************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        getFile: () => (
          /* binding */
          y
        ),
        /* harmony export */
        getModelFile: () => (
          /* binding */
          se
        ),
        /* harmony export */
        getModelJSON: () => (
          /* binding */
          oe
        )
        /* harmony export */
      });
      var f = s(
        /*! fs */
        "?7a2c"
      ), L = s(
        /*! path */
        "?a42a"
      ), j = s(
        /*! ../env.js */
        "./src/env.js"
      ), J = s(
        /*! ./core.js */
        "./src/utils/core.js"
      );
      const W = {
        txt: "text/plain",
        html: "text/html",
        css: "text/css",
        js: "text/javascript",
        json: "application/json",
        png: "image/png",
        jpg: "image/jpeg",
        jpeg: "image/jpeg",
        gif: "image/gif"
      };
      class w {
        /**
         * Creates a new `FileResponse` object.
         * @param {string|URL} filePath
         */
        constructor(O) {
          if (this.filePath = O, this.headers = new Headers(), this.exists = f.existsSync(O), this.exists) {
            this.status = 200, this.statusText = "OK";
            let $ = f.statSync(O);
            this.headers.set("content-length", $.size.toString()), this.updateContentType();
            let g = this;
            this.body = new ReadableStream({
              start(C) {
                g.arrayBuffer().then((v) => {
                  C.enqueue(new Uint8Array(v)), C.close();
                });
              }
            });
          } else
            this.status = 404, this.statusText = "Not Found", this.body = null;
        }
        /**
         * Updates the 'content-type' header property of the response based on the extension of
         * the file specified by the filePath property of the current object.
         * @returns {void}
         */
        updateContentType() {
          const O = this.filePath.toString().split(".").pop().toLowerCase();
          this.headers.set("content-type", W[O] ?? "application/octet-stream");
        }
        /**
         * Clone the current FileResponse object.
         * @returns {FileResponse} A new FileResponse object with the same properties as the current object.
         */
        clone() {
          let O = new w(this.filePath);
          return O.exists = this.exists, O.status = this.status, O.statusText = this.statusText, O.headers = new Headers(this.headers), O;
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with an ArrayBuffer containing the file's contents.
         * @returns {Promise<ArrayBuffer>} A Promise that resolves with an ArrayBuffer containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async arrayBuffer() {
          return (await f.promises.readFile(this.filePath)).buffer;
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with a Blob containing the file's contents.
         * @returns {Promise<Blob>} A Promise that resolves with a Blob containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async blob() {
          const O = await f.promises.readFile(this.filePath);
          return new Blob([O], { type: this.headers.get("content-type") });
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with a string containing the file's contents.
         * @returns {Promise<string>} A Promise that resolves with a string containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async text() {
          return await f.promises.readFile(this.filePath, "utf8");
        }
        /**
         * Reads the contents of the file specified by the filePath property and returns a Promise that
         * resolves with a parsed JavaScript object containing the file's contents.
         * 
         * @returns {Promise<Object>} A Promise that resolves with a parsed JavaScript object containing the file's contents.
         * @throws {Error} If the file cannot be read.
         */
        async json() {
          return JSON.parse(await this.text());
        }
      }
      function x(Y, O = null, $ = null) {
        let g;
        try {
          g = new URL(Y);
        } catch {
          return !1;
        }
        return !(O && !O.includes(g.protocol) || $ && !$.includes(g.hostname));
      }
      async function y(Y) {
        var O, $, g, C;
        if (j.env.useFS && !x(Y, ["http:", "https:", "blob:"]))
          return new w(Y);
        if (typeof process < "u" && ((O = process == null ? void 0 : process.release) == null ? void 0 : O.name) === "node") {
          const v = !!(($ = process.env) != null && $.TESTING_REMOTELY), ee = j.env.version, X = new Headers();
          if (X.set("User-Agent", `transformers.js/${ee}; is_ci/${v};`), x(Y, ["http:", "https:"], ["huggingface.co", "hf.co"])) {
            const ue = ((g = process.env) == null ? void 0 : g.HF_TOKEN) ?? ((C = process.env) == null ? void 0 : C.HF_ACCESS_TOKEN);
            ue && X.set("Authorization", `Bearer ${ue}`);
          }
          return fetch(Y, { headers: X });
        } else
          return fetch(Y);
      }
      const M = {
        // 4xx errors (https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses)
        400: "Bad request error occurred while trying to load file",
        401: "Unauthorized access to file",
        403: "Forbidden access to file",
        404: "Could not locate file",
        408: "Request timeout error occurred while trying to load file",
        // 5xx errors (https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#server_error_responses)
        500: "Internal server error error occurred while trying to load file",
        502: "Bad gateway error occurred while trying to load file",
        503: "Service unavailable error occurred while trying to load file",
        504: "Gateway timeout error occurred while trying to load file"
      };
      function b(Y, O, $) {
        if (!$)
          return null;
        const g = M[Y] ?? `Error (${Y}) occurred while trying to load file`;
        throw Error(`${g}: "${O}".`);
      }
      class D {
        /**
         * Instantiate a `FileCache` object.
         * @param {string} path 
         */
        constructor(O) {
          this.path = O;
        }
        /**
         * Checks whether the given request is in the cache.
         * @param {string} request 
         * @returns {Promise<FileResponse | undefined>}
         */
        async match(O) {
          let $ = L.join(this.path, O), g = new w($);
          if (g.exists)
            return g;
        }
        /**
         * Adds the given response to the cache.
         * @param {string} request 
         * @param {Response|FileResponse} response 
         * @returns {Promise<void>}
         */
        async put(O, $) {
          const g = Buffer.from(await $.arrayBuffer());
          let C = L.join(this.path, O);
          try {
            await f.promises.mkdir(L.dirname(C), { recursive: !0 }), await f.promises.writeFile(C, g);
          } catch (v) {
            console.warn("An error occurred while writing the file to cache:", v);
          }
        }
        // TODO add the rest?
        // addAll(requests: RequestInfo[]): Promise<void>;
        // delete(request: RequestInfo | URL, options?: CacheQueryOptions): Promise<boolean>;
        // keys(request?: RequestInfo | URL, options?: CacheQueryOptions): Promise<ReadonlyArray<Request>>;
        // match(request: RequestInfo | URL, options?: CacheQueryOptions): Promise<Response | undefined>;
        // matchAll(request?: RequestInfo | URL, options?: CacheQueryOptions): Promise<ReadonlyArray<Response>>;
      }
      async function q(Y, ...O) {
        for (let $ of O)
          try {
            let g = await Y.match($);
            if (g) return g;
          } catch {
            continue;
          }
      }
      async function se(Y, O, $ = !0, g = {}) {
        if (!j.env.allowLocalModels) {
          if (g.local_files_only)
            throw Error("Invalid configuration detected: local models are disabled (`env.allowLocalModels=false`) but you have requested to only use local models (`local_files_only=true`).");
          if (!j.env.allowRemoteModels)
            throw Error("Invalid configuration detected: both local and remote models are disabled. Fix by setting `env.allowLocalModels` or `env.allowRemoteModels` to `true`.");
        }
        (0, J.dispatchCallback)(g.progress_callback, {
          status: "initiate",
          name: Y,
          file: O
        });
        let C;
        if (!C && j.env.useBrowserCache) {
          if (typeof caches > "u")
            throw Error("Browser cache is not available in this environment.");
          try {
            C = await caches.open("transformers-cache");
          } catch (ut) {
            console.warn("An error occurred while opening the browser cache:", ut);
          }
        }
        if (!C && j.env.useFSCache && (C = new D(g.cache_dir ?? j.env.cacheDir)), !C && j.env.useCustomCache) {
          if (!j.env.customCache)
            throw Error("`env.useCustomCache=true`, but `env.customCache` is not defined.");
          if (!j.env.customCache.match || !j.env.customCache.put)
            throw new Error(
              "`env.customCache` must be an object which implements the `match` and `put` functions of the Web Cache API. For more information, see https://developer.mozilla.org/en-US/docs/Web/API/Cache"
            );
          C = j.env.customCache;
        }
        const v = g.revision ?? "main";
        let ee = V(Y, O), X = V(j.env.localModelPath, ee), le = V(
          j.env.remoteHost,
          j.env.remotePathTemplate.replaceAll("{model}", Y).replaceAll("{revision}", encodeURIComponent(v)),
          O
        ), ue = v === "main" ? ee : V(Y, v, O), fe, Ce = C instanceof D ? ue : le, xe = !1, Le;
        C && (Le = await q(C, X, Ce));
        const qe = Le !== void 0;
        if (Le === void 0) {
          if (j.env.allowLocalModels)
            if (x(ee, ["http:", "https:"])) {
              if (g.local_files_only)
                throw new Error(`\`local_files_only=true\`, but attempted to load a remote file from: ${ee}.`);
              if (!j.env.allowRemoteModels)
                throw new Error(`\`env.allowRemoteModels=false\`, but attempted to load a remote file from: ${ee}.`);
            } else try {
              Le = await y(X), fe = X;
            } catch (de) {
              console.warn(`Unable to load from local path "${X}": "${de}"`);
            }
          if (Le === void 0 || Le.status === 404) {
            if (g.local_files_only || !j.env.allowRemoteModels) {
              if ($)
                throw Error(`\`local_files_only=true\` or \`env.allowRemoteModels=false\` and file was not found locally at "${X}".`);
              return null;
            }
            if (Le = await y(le), Le.status !== 200)
              return b(Le.status, le, $);
            fe = Ce;
          }
          xe = C && typeof Response < "u" && Le instanceof Response && Le.status === 200;
        }
        (0, J.dispatchCallback)(g.progress_callback, {
          status: "download",
          name: Y,
          file: O
        });
        let Ue;
        return g.progress_callback ? qe && typeof navigator < "u" && /firefox/i.test(navigator.userAgent) ? (Ue = new Uint8Array(await Le.arrayBuffer()), (0, J.dispatchCallback)(g.progress_callback, {
          status: "progress",
          name: Y,
          file: O,
          progress: 100,
          loaded: Ue.length,
          total: Ue.length
        })) : Ue = await z(Le, (ut) => {
          (0, J.dispatchCallback)(g.progress_callback, {
            status: "progress",
            name: Y,
            file: O,
            ...ut
          });
        }) : Ue = new Uint8Array(await Le.arrayBuffer()), // Only cache web responses
        // i.e., do not cache FileResponses (prevents duplication)
        xe && fe && // Check again whether request is in cache. If not, we add the response to the cache
        await C.match(fe) === void 0 && await C.put(fe, new Response(Ue, {
          headers: Le.headers
        })).catch((ut) => {
          console.warn(`Unable to add response to browser cache: ${ut}.`);
        }), (0, J.dispatchCallback)(g.progress_callback, {
          status: "done",
          name: Y,
          file: O
        }), Ue;
      }
      async function oe(Y, O, $ = !0, g = {}) {
        let C = await se(Y, O, $, g);
        if (C === null)
          return {};
        let ee = new TextDecoder("utf-8").decode(C);
        return JSON.parse(ee);
      }
      async function z(Y, O) {
        const $ = Y.headers.get("Content-Length");
        $ === null && console.warn("Unable to determine content-length from response headers. Will expand buffer when needed.");
        let g = parseInt($ ?? "0"), C = new Uint8Array(g), v = 0;
        const ee = Y.body.getReader();
        async function X() {
          const { done: le, value: ue } = await ee.read();
          if (le) return;
          let fe = v + ue.length;
          if (fe > g) {
            g = fe;
            let xe = new Uint8Array(g);
            xe.set(C), C = xe;
          }
          C.set(ue, v), v = fe;
          const Ce = v / g * 100;
          return O({
            progress: Ce,
            loaded: v,
            total: g
          }), X();
        }
        return await X(), C;
      }
      function V(...Y) {
        return Y = Y.map((O, $) => ($ && (O = O.replace(new RegExp("^/"), "")), $ !== Y.length - 1 && (O = O.replace(new RegExp("/$"), "")), O)), Y.join("/");
      }
    }
  ),
  /***/
  "./src/utils/image.js": (
    /*!****************************!*\
      !*** ./src/utils/image.js ***!
      \****************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        RawImage: () => (
          /* binding */
          q
        ),
        /* harmony export */
        load_image: () => (
          /* binding */
          se
        )
        /* harmony export */
      });
      var f = s(
        /*! ./core.js */
        "./src/utils/core.js"
      ), L = s(
        /*! ./hub.js */
        "./src/utils/hub.js"
      ), j = s(
        /*! ../env.js */
        "./src/env.js"
      ), J = s(
        /*! ./tensor.js */
        "./src/utils/tensor.js"
      ), W = s(
        /*! sharp */
        "?2b25"
      );
      let w, x, y;
      const M = j.apis.IS_BROWSER_ENV || j.apis.IS_WEBWORKER_ENV;
      if (M)
        w = (oe, z) => {
          if (!self.OffscreenCanvas)
            throw new Error("OffscreenCanvas not supported by this browser.");
          return new self.OffscreenCanvas(oe, z);
        }, y = self.createImageBitmap, x = self.ImageData;
      else if (W)
        y = async (oe) => {
          const V = (await oe.metadata()).channels, { data: Y, info: O } = await oe.rotate().raw().toBuffer({ resolveWithObject: !0 }), $ = new q(new Uint8ClampedArray(Y), O.width, O.height, O.channels);
          return V !== void 0 && V !== O.channels && $.convert(V), $;
        };
      else
        throw new Error("Unable to load image processing library.");
      const b = {
        0: "nearest",
        1: "lanczos",
        2: "bilinear",
        3: "bicubic",
        4: "box",
        5: "hamming"
      }, D = /* @__PURE__ */ new Map([
        ["png", "image/png"],
        ["jpg", "image/jpeg"],
        ["jpeg", "image/jpeg"],
        ["gif", "image/gif"]
      ]);
      class q {
        /**
         * Create a new `RawImage` object.
         * @param {Uint8ClampedArray|Uint8Array} data The pixel data.
         * @param {number} width The width of the image.
         * @param {number} height The height of the image.
         * @param {1|2|3|4} channels The number of channels.
         */
        constructor(z, V, Y, O) {
          this.data = z, this.width = V, this.height = Y, this.channels = O;
        }
        /**
         * Returns the size of the image (width, height).
         * @returns {[number, number]} The size of the image (width, height).
         */
        get size() {
          return [this.width, this.height];
        }
        /**
         * Helper method for reading an image from a variety of input types.
         * @param {RawImage|string|URL} input
         * @returns The image object.
         *
         * **Example:** Read image from a URL.
         * ```javascript
         * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');
         * // RawImage {
         * //   "data": Uint8ClampedArray [ 25, 25, 25, 19, 19, 19, ... ],
         * //   "width": 800,
         * //   "height": 533,
         * //   "channels": 3
         * // }
         * ```
         */
        static async read(z) {
          if (z instanceof q)
            return z;
          if (typeof z == "string" || z instanceof URL)
            return await this.fromURL(z);
          throw new Error(`Unsupported input type: ${typeof z}`);
        }
        /**
         * Read an image from a canvas.
         * @param {HTMLCanvasElement|OffscreenCanvas} canvas The canvas to read the image from.
         * @returns {RawImage} The image object.
         */
        static fromCanvas(z) {
          if (!M)
            throw new Error("fromCanvas() is only supported in browser environments.");
          const Y = z.getContext("2d").getImageData(0, 0, z.width, z.height).data;
          return new q(Y, z.width, z.height, 4);
        }
        /**
         * Read an image from a URL or file path.
         * @param {string|URL} url The URL or file path to read the image from.
         * @returns {Promise<RawImage>} The image object.
         */
        static async fromURL(z) {
          const V = await (0, L.getFile)(z);
          if (V.status !== 200)
            throw new Error(`Unable to read image from "${z}" (${V.status} ${V.statusText})`);
          const Y = await V.blob();
          return this.fromBlob(Y);
        }
        /**
         * Helper method to create a new Image from a blob.
         * @param {Blob} blob The blob to read the image from.
         * @returns {Promise<RawImage>} The image object.
         */
        static async fromBlob(z) {
          if (M) {
            const V = await y(z), Y = w(V.width, V.height).getContext("2d");
            return Y.drawImage(V, 0, 0), new this(Y.getImageData(0, 0, V.width, V.height).data, V.width, V.height, 4);
          } else {
            const V = W(await z.arrayBuffer());
            return await y(V);
          }
        }
        /**
         * Helper method to create a new Image from a tensor
         * @param {Tensor} tensor
         */
        static fromTensor(z, V = "CHW") {
          if (z.dims.length !== 3)
            throw new Error(`Tensor should have 3 dimensions, but has ${z.dims.length} dimensions.`);
          if (V === "CHW")
            z = z.transpose(1, 2, 0);
          else if (V !== "HWC") throw new Error(`Unsupported channel format: ${V}`);
          if (!(z.data instanceof Uint8ClampedArray || z.data instanceof Uint8Array))
            throw new Error(`Unsupported tensor type: ${z.type}`);
          switch (z.dims[2]) {
            case 1:
            case 2:
            case 3:
            case 4:
              return new q(z.data, z.dims[1], z.dims[0], z.dims[2]);
            default:
              throw new Error(`Unsupported number of channels: ${z.dims[2]}`);
          }
        }
        /**
         * Convert the image to grayscale format.
         * @returns {RawImage} `this` to support chaining.
         */
        grayscale() {
          if (this.channels === 1)
            return this;
          const z = new Uint8ClampedArray(this.width * this.height * 1);
          switch (this.channels) {
            case 3:
            // rgb to grayscale
            case 4:
              for (let V = 0, Y = 0; V < this.data.length; V += this.channels) {
                const O = this.data[V], $ = this.data[V + 1], g = this.data[V + 2];
                z[Y++] = Math.round(0.2989 * O + 0.587 * $ + 0.114 * g);
              }
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this._update(z, this.width, this.height, 1);
        }
        /**
         * Convert the image to RGB format.
         * @returns {RawImage} `this` to support chaining.
         */
        rgb() {
          if (this.channels === 3)
            return this;
          const z = new Uint8ClampedArray(this.width * this.height * 3);
          switch (this.channels) {
            case 1:
              for (let V = 0, Y = 0; V < this.data.length; ++V)
                z[Y++] = this.data[V], z[Y++] = this.data[V], z[Y++] = this.data[V];
              break;
            case 4:
              for (let V = 0, Y = 0; V < this.data.length; V += 4)
                z[Y++] = this.data[V], z[Y++] = this.data[V + 1], z[Y++] = this.data[V + 2];
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this._update(z, this.width, this.height, 3);
        }
        /**
         * Convert the image to RGBA format.
         * @returns {RawImage} `this` to support chaining.
         */
        rgba() {
          if (this.channels === 4)
            return this;
          const z = new Uint8ClampedArray(this.width * this.height * 4);
          switch (this.channels) {
            case 1:
              for (let V = 0, Y = 0; V < this.data.length; ++V)
                z[Y++] = this.data[V], z[Y++] = this.data[V], z[Y++] = this.data[V], z[Y++] = 255;
              break;
            case 3:
              for (let V = 0, Y = 0; V < this.data.length; V += 3)
                z[Y++] = this.data[V], z[Y++] = this.data[V + 1], z[Y++] = this.data[V + 2], z[Y++] = 255;
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this._update(z, this.width, this.height, 4);
        }
        /**
         * Apply an alpha mask to the image. Operates in place.
         * @param {RawImage} mask The mask to apply. It should have a single channel.
         * @returns {RawImage} The masked image.
         * @throws {Error} If the mask is not the same size as the image.
         * @throws {Error} If the image does not have 4 channels.
         * @throws {Error} If the mask is not a single channel.
         */
        putAlpha(z) {
          if (z.width !== this.width || z.height !== this.height)
            throw new Error(`Expected mask size to be ${this.width}x${this.height}, but got ${z.width}x${z.height}`);
          if (z.channels !== 1)
            throw new Error(`Expected mask to have 1 channel, but got ${z.channels}`);
          const V = this.data, Y = z.data, O = this.width * this.height;
          if (this.channels === 3) {
            const $ = new Uint8ClampedArray(O * 4);
            for (let g = 0, C = 0, v = 0; g < O; ++g)
              $[v++] = V[C++], $[v++] = V[C++], $[v++] = V[C++], $[v++] = Y[g];
            return this._update($, this.width, this.height, 4);
          } else if (this.channels === 4) {
            for (let $ = 0; $ < O; ++$)
              V[4 * $ + 3] = Y[$];
            return this;
          }
          throw new Error(`Expected image to have 3 or 4 channels, but got ${this.channels}`);
        }
        /**
         * Resize the image to the given dimensions. This method uses the canvas API to perform the resizing.
         * @param {number} width The width of the new image. `null` or `-1` will preserve the aspect ratio.
         * @param {number} height The height of the new image. `null` or `-1` will preserve the aspect ratio.
         * @param {Object} options Additional options for resizing.
         * @param {0|1|2|3|4|5|string} [options.resample] The resampling method to use.
         * @returns {Promise<RawImage>} `this` to support chaining.
         */
        async resize(z, V, {
          resample: Y = 2
        } = {}) {
          if (this.width === z && this.height === V)
            return this;
          let O = b[Y] ?? Y;
          const $ = (0, f.isNullishDimension)(z), g = (0, f.isNullishDimension)(V);
          if ($ && g)
            return this;
          if ($ ? z = V / this.height * this.width : g && (V = z / this.width * this.height), M) {
            const C = this.channels, v = this.toCanvas(), ee = w(z, V).getContext("2d");
            return ee.drawImage(v, 0, 0, z, V), new q(ee.getImageData(0, 0, z, V).data, z, V, 4).convert(C);
          } else {
            let C = this.toSharp();
            switch (O) {
              case "box":
              case "hamming":
                (O === "box" || O === "hamming") && (console.warn(`Resampling method ${O} is not yet supported. Using bilinear instead.`), O = "bilinear");
              case "nearest":
              case "bilinear":
              case "bicubic":
                C = C.affine([z / this.width, 0, 0, V / this.height], {
                  interpolator: O
                });
                break;
              case "lanczos":
                C = C.resize({
                  width: z,
                  height: V,
                  fit: "fill",
                  kernel: "lanczos3"
                  // PIL Lanczos uses a kernel size of 3
                });
                break;
              default:
                throw new Error(`Resampling method ${O} is not supported.`);
            }
            return await y(C);
          }
        }
        async pad([z, V, Y, O]) {
          if (z = Math.max(z, 0), V = Math.max(V, 0), Y = Math.max(Y, 0), O = Math.max(O, 0), z === 0 && V === 0 && Y === 0 && O === 0)
            return this;
          if (M) {
            const $ = this.channels, g = this.toCanvas(), C = this.width + z + V, v = this.height + Y + O, ee = w(C, v).getContext("2d");
            return ee.drawImage(
              g,
              0,
              0,
              this.width,
              this.height,
              z,
              Y,
              this.width,
              this.height
            ), new q(
              ee.getImageData(0, 0, C, v).data,
              C,
              v,
              4
            ).convert($);
          } else {
            const $ = this.toSharp().extend({ left: z, right: V, top: Y, bottom: O });
            return await y($);
          }
        }
        async crop([z, V, Y, O]) {
          if (z = Math.max(z, 0), V = Math.max(V, 0), Y = Math.min(Y, this.width - 1), O = Math.min(O, this.height - 1), z === 0 && V === 0 && Y === this.width - 1 && O === this.height - 1)
            return this;
          const $ = Y - z + 1, g = O - V + 1;
          if (M) {
            const C = this.channels, v = this.toCanvas(), ee = w($, g).getContext("2d");
            return ee.drawImage(
              v,
              z,
              V,
              $,
              g,
              0,
              0,
              $,
              g
            ), new q(ee.getImageData(0, 0, $, g).data, $, g, 4).convert(C);
          } else {
            const C = this.toSharp().extract({
              left: z,
              top: V,
              width: $,
              height: g
            });
            return await y(C);
          }
        }
        async center_crop(z, V) {
          if (this.width === z && this.height === V)
            return this;
          const Y = (this.width - z) / 2, O = (this.height - V) / 2;
          if (M) {
            const $ = this.channels, g = this.toCanvas(), C = w(z, V).getContext("2d");
            let v = 0, ee = 0, X = 0, le = 0;
            return Y >= 0 ? v = Y : X = -Y, O >= 0 ? ee = O : le = -O, C.drawImage(
              g,
              v,
              ee,
              z,
              V,
              X,
              le,
              z,
              V
            ), new q(C.getImageData(0, 0, z, V).data, z, V, 4).convert($);
          } else {
            let $ = this.toSharp();
            if (Y >= 0 && O >= 0)
              $ = $.extract({
                left: Math.floor(Y),
                top: Math.floor(O),
                width: z,
                height: V
              });
            else if (Y <= 0 && O <= 0) {
              const g = Math.floor(-O), C = Math.floor(-Y);
              $ = $.extend({
                top: g,
                left: C,
                // Ensures the resulting image has the desired dimensions
                right: z - this.width - C,
                bottom: V - this.height - g
              });
            } else {
              let g = [0, 0], C = 0;
              O < 0 ? (g[0] = Math.floor(-O), g[1] = V - this.height - g[0]) : C = Math.floor(O);
              let v = [0, 0], ee = 0;
              Y < 0 ? (v[0] = Math.floor(-Y), v[1] = z - this.width - v[0]) : ee = Math.floor(Y), $ = $.extend({
                top: g[0],
                bottom: g[1],
                left: v[0],
                right: v[1]
              }).extract({
                left: ee,
                top: C,
                width: z,
                height: V
              });
            }
            return await y($);
          }
        }
        async toBlob(z = "image/png", V = 1) {
          if (!M)
            throw new Error("toBlob() is only supported in browser environments.");
          return await this.toCanvas().convertToBlob({ type: z, quality: V });
        }
        toTensor(z = "CHW") {
          let V = new J.Tensor(
            "uint8",
            new Uint8Array(this.data),
            [this.height, this.width, this.channels]
          );
          if (z !== "HWC") if (z === "CHW")
            V = V.permute(2, 0, 1);
          else
            throw new Error(`Unsupported channel format: ${z}`);
          return V;
        }
        toCanvas() {
          if (!M)
            throw new Error("toCanvas() is only supported in browser environments.");
          const z = this.clone().rgba(), V = w(z.width, z.height), Y = new x(z.data, z.width, z.height);
          return V.getContext("2d").putImageData(Y, 0, 0), V;
        }
        /**
         * Split this image into individual bands. This method returns an array of individual image bands from an image.
         * For example, splitting an "RGB" image creates three new images each containing a copy of one of the original bands (red, green, blue).
         * 
         * Inspired by PIL's `Image.split()` [function](https://pillow.readthedocs.io/en/latest/reference/Image.html#PIL.Image.Image.split).
         * @returns {RawImage[]} An array containing bands.
         */
        split() {
          const { data: z, width: V, height: Y, channels: O } = this, $ = (
            /** @type {any} */
            z.constructor
          ), g = z.length / O, C = Array.from(
            { length: O },
            () => new $(g)
          );
          for (let v = 0; v < g; ++v) {
            const ee = O * v;
            for (let X = 0; X < O; ++X)
              C[X][v] = z[ee + X];
          }
          return C.map((v) => new q(v, V, Y, 1));
        }
        /**
         * Helper method to update the image data.
         * @param {Uint8ClampedArray} data The new image data.
         * @param {number} width The new width of the image.
         * @param {number} height The new height of the image.
         * @param {1|2|3|4|null} [channels] The new number of channels of the image.
         * @private
         */
        _update(z, V, Y, O = null) {
          return this.data = z, this.width = V, this.height = Y, O !== null && (this.channels = O), this;
        }
        /**
         * Clone the image
         * @returns {RawImage} The cloned image
         */
        clone() {
          return new q(this.data.slice(), this.width, this.height, this.channels);
        }
        /**
         * Helper method for converting image to have a certain number of channels
         * @param {number} numChannels The number of channels. Must be 1, 3, or 4.
         * @returns {RawImage} `this` to support chaining.
         */
        convert(z) {
          if (this.channels === z) return this;
          switch (z) {
            case 1:
              this.grayscale();
              break;
            case 3:
              this.rgb();
              break;
            case 4:
              this.rgba();
              break;
            default:
              throw new Error(`Conversion failed due to unsupported number of channels: ${this.channels}`);
          }
          return this;
        }
        /**
         * Save the image to the given path.
         * @param {string} path The path to save the image to.
         */
        async save(z) {
          if (M) {
            if (j.apis.IS_WEBWORKER_ENV)
              throw new Error("Unable to save an image from a Web Worker.");
            const V = z.split(".").pop().toLowerCase(), Y = D.get(V) ?? "image/png", O = await this.toBlob(Y), $ = URL.createObjectURL(O), g = document.createElement("a");
            g.href = $, g.download = z, g.click(), g.remove();
          } else {
            if (j.env.useFS)
              return await this.toSharp().toFile(z);
            throw new Error("Unable to save the image because filesystem is disabled in this environment.");
          }
        }
        toSharp() {
          if (M)
            throw new Error("toSharp() is only supported in server-side environments.");
          return W(this.data, {
            raw: {
              width: this.width,
              height: this.height,
              channels: this.channels
            }
          });
        }
      }
      const se = q.read.bind(q);
    }
  ),
  /***/
  "./src/utils/maths.js": (
    /*!****************************!*\
      !*** ./src/utils/maths.js ***!
      \****************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        FFT: () => (
          /* binding */
          se
        ),
        /* harmony export */
        bankers_round: () => (
          /* binding */
          V
        ),
        /* harmony export */
        cos_sim: () => (
          /* binding */
          w
        ),
        /* harmony export */
        dot: () => (
          /* binding */
          W
        ),
        /* harmony export */
        dynamic_time_warping: () => (
          /* binding */
          Y
        ),
        /* harmony export */
        interpolate_data: () => (
          /* binding */
          f
        ),
        /* harmony export */
        log_softmax: () => (
          /* binding */
          J
        ),
        /* harmony export */
        magnitude: () => (
          /* binding */
          x
        ),
        /* harmony export */
        max: () => (
          /* binding */
          M
        ),
        /* harmony export */
        medianFilter: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        min: () => (
          /* binding */
          y
        ),
        /* harmony export */
        permute_data: () => (
          /* binding */
          L
        ),
        /* harmony export */
        round: () => (
          /* binding */
          z
        ),
        /* harmony export */
        softmax: () => (
          /* binding */
          j
        )
        /* harmony export */
      });
      function f(O, [$, g, C], [v, ee], X = "bilinear", le = !1) {
        const ue = ee / C, fe = v / g, Ce = new O.constructor(v * ee * $), xe = g * C, Le = v * ee;
        for (let qe = 0; qe < v; ++qe)
          for (let Ue = 0; Ue < ee; ++Ue) {
            const ut = qe * ee + Ue, de = (Ue + 0.5) / ue - 0.5, re = (qe + 0.5) / fe - 0.5;
            let he = Math.floor(de), Ee = Math.floor(re);
            const Be = Math.min(he + 1, C - 1), et = Math.min(Ee + 1, g - 1);
            he = Math.max(he, 0), Ee = Math.max(Ee, 0);
            const Xe = de - he, ie = re - Ee, Je = (1 - Xe) * (1 - ie), De = Xe * (1 - ie), ce = (1 - Xe) * ie, ve = Xe * ie, Re = Ee * C, je = et * C, Ve = Re + he, Ne = Re + Be, Ze = je + he, at = je + Be;
            for (let ft = 0; ft < $; ++ft) {
              const dt = ft * xe;
              Ce[ft * Le + ut] = Je * O[dt + Ve] + De * O[dt + Ne] + ce * O[dt + Ze] + ve * O[dt + at];
            }
          }
        return Ce;
      }
      function L(O, $, g) {
        const C = new Array(g.length), v = new Array(g.length);
        for (let le = g.length - 1, ue = 1; le >= 0; --le)
          v[le] = ue, C[le] = $[g[le]], ue *= C[le];
        const ee = g.map((le, ue) => v[g.indexOf(ue)]), X = new O.constructor(O.length);
        for (let le = 0; le < O.length; ++le) {
          let ue = 0;
          for (let fe = $.length - 1, Ce = le; fe >= 0; --fe)
            ue += Ce % $[fe] * ee[fe], Ce = Math.floor(Ce / $[fe]);
          X[ue] = O[le];
        }
        return [X, C];
      }
      function j(O) {
        const $ = M(O)[0], g = O.map((ee) => Math.exp(ee - $)), C = g.reduce((ee, X) => ee + X, 0);
        return g.map((ee) => ee / C);
      }
      function J(O) {
        const $ = M(O)[0];
        let g = 0;
        for (let ee = 0; ee < O.length; ++ee)
          g += Math.exp(O[ee] - $);
        const C = Math.log(g);
        return O.map((ee) => ee - $ - C);
      }
      function W(O, $) {
        let g = 0;
        for (let C = 0; C < O.length; ++C)
          g += O[C] * $[C];
        return g;
      }
      function w(O, $) {
        const g = W(O, $), C = x(O), v = x($);
        return g / (C * v);
      }
      function x(O) {
        return Math.sqrt(O.reduce(($, g) => $ + g * g, 0));
      }
      function y(O) {
        if (O.length === 0) throw Error("Array must not be empty");
        let $ = O[0], g = 0;
        for (let C = 1; C < O.length; ++C)
          O[C] < $ && ($ = O[C], g = C);
        return [$, g];
      }
      function M(O) {
        if (O.length === 0) throw Error("Array must not be empty");
        let $ = O[0], g = 0;
        for (let C = 1; C < O.length; ++C)
          O[C] > $ && ($ = O[C], g = C);
        return [Number($), g];
      }
      function b(O) {
        return O > 0 && (O & O - 1) === 0;
      }
      class D {
        /**
         * @param {number} size The size of the input array. Must be a power of two larger than 1.
         * @throws {Error} FFT size must be a power of two larger than 1.
         */
        constructor($) {
          if (this.size = $ | 0, this.size <= 1 || !b(this.size))
            throw new Error("FFT size must be a power of two larger than 1");
          this._csize = $ << 1, this.table = new Float64Array(this.size * 2);
          for (let C = 0; C < this.table.length; C += 2) {
            const v = Math.PI * C / this.size;
            this.table[C] = Math.cos(v), this.table[C + 1] = -Math.sin(v);
          }
          let g = 0;
          for (let C = 1; this.size > C; C <<= 1)
            ++g;
          this._width = g % 2 === 0 ? g - 1 : g, this._bitrev = new Int32Array(1 << this._width);
          for (let C = 0; C < this._bitrev.length; ++C) {
            this._bitrev[C] = 0;
            for (let v = 0; v < this._width; v += 2) {
              const ee = this._width - v - 2;
              this._bitrev[C] |= (C >>> v & 3) << ee;
            }
          }
        }
        /**
         * Create a complex number array with size `2 * size`
         *
         * @returns {Float64Array} A complex number array with size `2 * size`
         */
        createComplexArray() {
          return new Float64Array(this._csize);
        }
        /**
         * Converts a complex number representation stored in a Float64Array to an array of real numbers.
         * 
         * @param {Float64Array} complex The complex number representation to be converted.
         * @param {number[]} [storage] An optional array to store the result in.
         * @returns {number[]} An array of real numbers representing the input complex number representation.
         */
        fromComplexArray($, g) {
          const C = g || new Array($.length >>> 1);
          for (let v = 0; v < $.length; v += 2)
            C[v >>> 1] = $[v];
          return C;
        }
        /**
         * Convert a real-valued input array to a complex-valued output array.
         * @param {Float64Array} input The real-valued input array.
         * @param {Float64Array} [storage] Optional buffer to store the output array.
         * @returns {Float64Array} The complex-valued output array.
         */
        toComplexArray($, g) {
          const C = g || this.createComplexArray();
          for (let v = 0; v < C.length; v += 2)
            C[v] = $[v >>> 1], C[v + 1] = 0;
          return C;
        }
        /**
         * Performs a Fast Fourier Transform (FFT) on the given input data and stores the result in the output buffer.
         * 
         * @param {Float64Array} out The output buffer to store the result.
         * @param {Float64Array} data The input data to transform.
         * 
         * @throws {Error} Input and output buffers must be different.
         * 
         * @returns {void}
         */
        transform($, g) {
          if ($ === g)
            throw new Error("Input and output buffers must be different");
          this._transform4(
            $,
            g,
            1
            /* DONE */
          );
        }
        /**
         * Performs a real-valued forward FFT on the given input buffer and stores the result in the given output buffer.
         * The input buffer must contain real values only, while the output buffer will contain complex values. The input and
         * output buffers must be different.
         *
         * @param {Float64Array} out The output buffer.
         * @param {Float64Array} data The input buffer containing real values.
         *
         * @throws {Error} If the input and output buffers are the same.
         */
        realTransform($, g) {
          if ($ === g)
            throw new Error("Input and output buffers must be different");
          this._realTransform4(
            $,
            g,
            1
            /* DONE */
          );
        }
        /**
         * Performs an inverse FFT transformation on the given `data` array, and stores the result in `out`.
         * The `out` array must be a different buffer than the `data` array. The `out` array will contain the
         * result of the transformation. The `data` array will not be modified.
         * 
         * @param {Float64Array} out The output buffer for the transformed data.
         * @param {Float64Array} data The input data to transform.
         * @throws {Error} If `out` and `data` refer to the same buffer.
         * @returns {void}
         */
        inverseTransform($, g) {
          if ($ === g)
            throw new Error("Input and output buffers must be different");
          this._transform4(
            $,
            g,
            -1
            /* DONE */
          );
          for (let C = 0; C < $.length; ++C)
            $[C] /= this.size;
        }
        /**
         * Performs a radix-4 implementation of a discrete Fourier transform on a given set of data.
         *
         * @param {Float64Array} out The output buffer for the transformed data.
         * @param {Float64Array} data The input buffer of data to be transformed.
         * @param {number} inv A scaling factor to apply to the transform.
         * @returns {void}
         */
        _transform4($, g, C) {
          const v = this._csize;
          let X = 1 << this._width, le = v / X << 1, ue, fe;
          const Ce = this._bitrev;
          if (le === 4)
            for (ue = 0, fe = 0; ue < v; ue += le, ++fe) {
              const Le = Ce[fe];
              this._singleTransform2(g, $, ue, Le, X);
            }
          else
            for (ue = 0, fe = 0; ue < v; ue += le, ++fe) {
              const Le = Ce[fe];
              this._singleTransform4(g, $, ue, Le, X, C);
            }
          const xe = this.table;
          for (X >>= 2; X >= 2; X >>= 2) {
            le = v / X << 1;
            const Le = le >>> 2;
            for (ue = 0; ue < v; ue += le) {
              const qe = ue + Le - 1;
              for (let Ue = ue, ut = 0; Ue < qe; Ue += 2, ut += X) {
                const de = Ue, re = de + Le, he = re + Le, Ee = he + Le, Be = $[de], et = $[de + 1], Xe = $[re], ie = $[re + 1], Je = $[he], De = $[he + 1], ce = $[Ee], ve = $[Ee + 1], Re = xe[ut], je = C * xe[ut + 1], Ve = Xe * Re - ie * je, Ne = Xe * je + ie * Re, Ze = xe[2 * ut], at = C * xe[2 * ut + 1], ft = Je * Ze - De * at, dt = Je * at + De * Ze, gt = xe[3 * ut], F = C * xe[3 * ut + 1], ne = ce * gt - ve * F, K = ce * F + ve * gt, pe = Be + ft, Oe = et + dt, Qe = Be - ft, st = et - dt, pt = Ve + ne, It = Ne + K, St = C * (Ve - ne), Ot = C * (Ne - K);
                $[de] = pe + pt, $[de + 1] = Oe + It, $[re] = Qe + Ot, $[re + 1] = st - St, $[he] = pe - pt, $[he + 1] = Oe - It, $[Ee] = Qe - Ot, $[Ee + 1] = st + St;
              }
            }
          }
        }
        /**
         * Performs a radix-2 implementation of a discrete Fourier transform on a given set of data.
         *
         * @param {Float64Array} data The input buffer of data to be transformed.
         * @param {Float64Array} out The output buffer for the transformed data.
         * @param {number} outOff The offset at which to write the output data.
         * @param {number} off The offset at which to begin reading the input data.
         * @param {number} step The step size for indexing the input data.
         * @returns {void}
         */
        _singleTransform2($, g, C, v, ee) {
          const X = $[v], le = $[v + 1], ue = $[v + ee], fe = $[v + ee + 1];
          g[C] = X + ue, g[C + 1] = le + fe, g[C + 2] = X - ue, g[C + 3] = le - fe;
        }
        /**
         * Performs radix-4 transformation on input data of length 8
         *
         * @param {Float64Array} data Input data array of length 8
         * @param {Float64Array} out Output data array of length 8
         * @param {number} outOff Index of output array to start writing from
         * @param {number} off Index of input array to start reading from
         * @param {number} step Step size between elements in input array
         * @param {number} inv Scaling factor for inverse transform
         * 
         * @returns {void}
         */
        _singleTransform4($, g, C, v, ee, X) {
          const le = ee * 2, ue = ee * 3, fe = $[v], Ce = $[v + 1], xe = $[v + ee], Le = $[v + ee + 1], qe = $[v + le], Ue = $[v + le + 1], ut = $[v + ue], de = $[v + ue + 1], re = fe + qe, he = Ce + Ue, Ee = fe - qe, Be = Ce - Ue, et = xe + ut, Xe = Le + de, ie = X * (xe - ut), Je = X * (Le - de);
          g[C] = re + et, g[C + 1] = he + Xe, g[C + 2] = Ee + Je, g[C + 3] = Be - ie, g[C + 4] = re - et, g[C + 5] = he - Xe, g[C + 6] = Ee - Je, g[C + 7] = Be + ie;
        }
        /**
         * Real input radix-4 implementation
         * @param {Float64Array} out Output array for the transformed data
         * @param {Float64Array} data Input array of real data to be transformed
         * @param {number} inv The scale factor used to normalize the inverse transform
         */
        _realTransform4($, g, C) {
          const v = this._csize;
          let X = 1 << this._width, le = v / X << 1, ue, fe;
          const Ce = this._bitrev;
          if (le === 4)
            for (ue = 0, fe = 0; ue < v; ue += le, ++fe) {
              const qe = Ce[fe];
              this._singleRealTransform2(g, $, ue, qe >>> 1, X >>> 1);
            }
          else
            for (ue = 0, fe = 0; ue < v; ue += le, ++fe) {
              const qe = Ce[fe];
              this._singleRealTransform4(g, $, ue, qe >>> 1, X >>> 1, C);
            }
          const xe = this.table;
          for (X >>= 2; X >= 2; X >>= 2) {
            le = v / X << 1;
            const qe = le >>> 1, Ue = qe >>> 1, ut = Ue >>> 1;
            for (ue = 0; ue < v; ue += le)
              for (let de = 0, re = 0; de <= ut; de += 2, re += X) {
                const he = ue + de, Ee = he + Ue, Be = Ee + Ue, et = Be + Ue, Xe = $[he], ie = $[he + 1], Je = $[Ee], De = $[Ee + 1], ce = $[Be], ve = $[Be + 1], Re = $[et], je = $[et + 1], Ve = Xe, Ne = ie, Ze = xe[re], at = C * xe[re + 1], ft = Je * Ze - De * at, dt = Je * at + De * Ze, gt = xe[2 * re], F = C * xe[2 * re + 1], ne = ce * gt - ve * F, K = ce * F + ve * gt, pe = xe[3 * re], Oe = C * xe[3 * re + 1], Qe = Re * pe - je * Oe, st = Re * Oe + je * pe, pt = Ve + ne, It = Ne + K, St = Ve - ne, Ot = Ne - K, At = ft + Qe, nr = dt + st, gr = C * (ft - Qe), kr = C * (dt - st);
                if ($[he] = pt + At, $[he + 1] = It + nr, $[Ee] = St + kr, $[Ee + 1] = Ot - gr, de === 0) {
                  $[Be] = pt - At, $[Be + 1] = It - nr;
                  continue;
                }
                if (de === ut)
                  continue;
                const Ar = ue + Ue - de, Qr = ue + qe - de;
                $[Ar] = St - C * kr, $[Ar + 1] = -Ot - C * gr, $[Qr] = pt - C * At, $[Qr + 1] = -It + C * nr;
              }
          }
          const Le = v >>> 1;
          for (let qe = 2; qe < Le; qe += 2)
            $[v - qe] = $[qe], $[v - qe + 1] = -$[qe + 1];
        }
        /**
         * Performs a single real input radix-2 transformation on the provided data
         * 
         * @param {Float64Array} data The input data array
         * @param {Float64Array} out The output data array
         * @param {number} outOff The output offset
         * @param {number} off The input offset
         * @param {number} step The step
         * 
         * @returns {void}
         */
        _singleRealTransform2($, g, C, v, ee) {
          const X = $[v], le = $[v + ee];
          g[C] = X + le, g[C + 1] = 0, g[C + 2] = X - le, g[C + 3] = 0;
        }
        /**
         * Computes a single real-valued transform using radix-4 algorithm.
         * This method is only called for len=8.
         *
         * @param {Float64Array} data The input data array.
         * @param {Float64Array} out The output data array.
         * @param {number} outOff The offset into the output array.
         * @param {number} off The offset into the input array.
         * @param {number} step The step size for the input array.
         * @param {number} inv The value of inverse.
         */
        _singleRealTransform4($, g, C, v, ee, X) {
          const le = ee * 2, ue = ee * 3, fe = $[v], Ce = $[v + ee], xe = $[v + le], Le = $[v + ue], qe = fe + xe, Ue = fe - xe, ut = Ce + Le, de = X * (Ce - Le);
          g[C] = qe + ut, g[C + 1] = 0, g[C + 2] = Ue, g[C + 3] = -de, g[C + 4] = qe - ut, g[C + 5] = 0, g[C + 6] = Ue, g[C + 7] = de;
        }
      }
      class q {
        /**
         * Constructs a new NP2FFT object.
         * @param {number} fft_length The length of the FFT
         */
        constructor($) {
          const g = 2 * ($ - 1), C = 2 * (2 * $ - 1), v = 2 ** Math.ceil(Math.log2(C));
          this.bufferSize = v, this._a = g;
          const ee = new Float64Array(C), X = new Float64Array(v);
          this._chirpBuffer = new Float64Array(v), this._buffer1 = new Float64Array(v), this._buffer2 = new Float64Array(v), this._outBuffer1 = new Float64Array(v), this._outBuffer2 = new Float64Array(v);
          const le = -2 * Math.PI / $, ue = Math.cos(le), fe = Math.sin(le);
          for (let Ce = 0; Ce < C >> 1; ++Ce) {
            const xe = (Ce + 1 - $) ** 2 / 2, Le = Math.sqrt(ue ** 2 + fe ** 2) ** xe, qe = xe * Math.atan2(fe, ue), Ue = 2 * Ce;
            ee[Ue] = Le * Math.cos(qe), ee[Ue + 1] = Le * Math.sin(qe), X[Ue] = ee[Ue], X[Ue + 1] = -ee[Ue + 1];
          }
          this._slicedChirpBuffer = ee.subarray(g, C), this._f = new D(v >> 1), this._f.transform(this._chirpBuffer, X);
        }
        _transform($, g, C) {
          const v = this._buffer1, ee = this._buffer2, X = this._outBuffer1, le = this._outBuffer2, ue = this._chirpBuffer, fe = this._slicedChirpBuffer, Ce = this._a;
          if (C)
            for (let xe = 0; xe < fe.length; xe += 2) {
              const Le = xe + 1, qe = xe >> 1, Ue = g[qe];
              v[xe] = Ue * fe[xe], v[Le] = Ue * fe[Le];
            }
          else
            for (let xe = 0; xe < fe.length; xe += 2) {
              const Le = xe + 1;
              v[xe] = g[xe] * fe[xe] - g[Le] * fe[Le], v[Le] = g[xe] * fe[Le] + g[Le] * fe[xe];
            }
          this._f.transform(X, v);
          for (let xe = 0; xe < ue.length; xe += 2) {
            const Le = xe + 1;
            ee[xe] = X[xe] * ue[xe] - X[Le] * ue[Le], ee[Le] = X[xe] * ue[Le] + X[Le] * ue[xe];
          }
          this._f.inverseTransform(le, ee);
          for (let xe = 0; xe < le.length; xe += 2) {
            const Le = le[xe + Ce], qe = le[xe + Ce + 1], Ue = fe[xe], ut = fe[xe + 1];
            $[xe] = Le * Ue - qe * ut, $[xe + 1] = Le * ut + qe * Ue;
          }
        }
        transform($, g) {
          this._transform($, g, !1);
        }
        realTransform($, g) {
          this._transform($, g, !0);
        }
      }
      class se {
        constructor($) {
          this.fft_length = $, this.isPowerOfTwo = b($), this.isPowerOfTwo ? (this.fft = new D($), this.outputBufferSize = 2 * $) : (this.fft = new q($), this.outputBufferSize = this.fft.bufferSize);
        }
        realTransform($, g) {
          this.fft.realTransform($, g);
        }
        transform($, g) {
          this.fft.transform($, g);
        }
      }
      function oe(O, $) {
        if ($ % 2 === 0 || $ <= 0)
          throw new Error("Window size must be a positive odd number");
        const g = new O.constructor(O.length), C = new O.constructor($), v = Math.floor($ / 2);
        for (let ee = 0; ee < O.length; ++ee) {
          let X = 0;
          for (let le = -v; le <= v; ++le) {
            let ue = ee + le;
            ue < 0 ? ue = Math.abs(ue) : ue >= O.length && (ue = 2 * (O.length - 1) - ue), C[X++] = O[ue];
          }
          C.sort(), g[ee] = C[v];
        }
        return g;
      }
      function z(O, $) {
        const g = Math.pow(10, $);
        return Math.round(O * g) / g;
      }
      function V(O) {
        const $ = Math.round(O);
        return Math.abs(O) % 1 === 0.5 ? $ % 2 === 0 ? $ : $ - 1 : $;
      }
      function Y(O) {
        const $ = O.length, g = O[0].length, C = [$ + 1, g + 1], v = Array.from(
          { length: C[0] },
          () => Array(C[1]).fill(1 / 0)
        );
        v[0][0] = 0;
        const ee = Array.from(
          { length: C[0] },
          () => Array(C[1]).fill(-1)
        );
        for (let Ce = 1; Ce < C[1]; ++Ce)
          for (let xe = 1; xe < C[0]; ++xe) {
            const Le = v[xe - 1][Ce - 1], qe = v[xe - 1][Ce], Ue = v[xe][Ce - 1];
            let ut, de;
            Le < qe && Le < Ue ? (ut = Le, de = 0) : qe < Le && qe < Ue ? (ut = qe, de = 1) : (ut = Ue, de = 2), v[xe][Ce] = O[xe - 1][Ce - 1] + ut, ee[xe][Ce] = de;
          }
        for (let Ce = 0; Ce < C[1]; ++Ce)
          ee[0][Ce] = 2;
        for (let Ce = 0; Ce < C[0]; ++Ce)
          ee[Ce][0] = 1;
        let X = $, le = g, ue = [], fe = [];
        for (; X > 0 || le > 0; )
          switch (ue.push(X - 1), fe.push(le - 1), ee[X][le]) {
            case 0:
              --X, --le;
              break;
            case 1:
              --X;
              break;
            case 2:
              --le;
              break;
            default:
              throw new Error(
                `Internal error in dynamic time warping. Unexpected trace[${X}, ${le}]. Please file a bug report.`
              );
          }
        return ue.reverse(), fe.reverse(), [ue, fe];
      }
    }
  ),
  /***/
  "./src/utils/tensor.js": (
    /*!*****************************!*\
      !*** ./src/utils/tensor.js ***!
      \*****************************/
    /***/
    (ke, A, s) => {
      s.r(A), s.d(A, {
        /* harmony export */
        Tensor: () => (
          /* binding */
          W
        ),
        /* harmony export */
        cat: () => (
          /* binding */
          g
        ),
        /* harmony export */
        full: () => (
          /* binding */
          ue
        ),
        /* harmony export */
        full_like: () => (
          /* binding */
          fe
        ),
        /* harmony export */
        interpolate: () => (
          /* binding */
          y
        ),
        /* harmony export */
        interpolate_4d: () => (
          /* binding */
          M
        ),
        /* harmony export */
        layer_norm: () => (
          /* binding */
          V
        ),
        /* harmony export */
        matmul: () => (
          /* binding */
          b
        ),
        /* harmony export */
        mean: () => (
          /* binding */
          ee
        ),
        /* harmony export */
        mean_pooling: () => (
          /* binding */
          z
        ),
        /* harmony export */
        ones: () => (
          /* binding */
          Ce
        ),
        /* harmony export */
        ones_like: () => (
          /* binding */
          xe
        ),
        /* harmony export */
        permute: () => (
          /* binding */
          x
        ),
        /* harmony export */
        quantize_embeddings: () => (
          /* binding */
          ut
        ),
        /* harmony export */
        rand: () => (
          /* binding */
          Ue
        ),
        /* harmony export */
        rfft: () => (
          /* binding */
          D
        ),
        /* harmony export */
        slice: () => (
          /* binding */
          oe
        ),
        /* harmony export */
        stack: () => (
          /* binding */
          C
        ),
        /* harmony export */
        std_mean: () => (
          /* binding */
          v
        ),
        /* harmony export */
        topk: () => (
          /* binding */
          q
        ),
        /* harmony export */
        zeros: () => (
          /* binding */
          Le
        ),
        /* harmony export */
        zeros_like: () => (
          /* binding */
          qe
        )
        /* harmony export */
      });
      var f = s(
        /*! ./maths.js */
        "./src/utils/maths.js"
      ), L = s(
        /*! ../backends/onnx.js */
        "./src/backends/onnx.js"
      ), j = s(
        /*! ../ops/registry.js */
        "./src/ops/registry.js"
      );
      const J = Object.freeze({
        float32: Float32Array,
        float16: Uint16Array,
        float64: Float64Array,
        string: Array,
        // string[]
        int8: Int8Array,
        uint8: Uint8Array,
        int16: Int16Array,
        uint16: Uint16Array,
        int32: Int32Array,
        uint32: Uint32Array,
        int64: BigInt64Array,
        uint64: BigUint64Array,
        bool: Uint8Array,
        uint4: Uint8Array,
        int4: Int8Array
      });
      class W {
        /**
         * Create a new Tensor or copy an existing Tensor.
         * @param {[DataType, DataArray, number[]]|[ONNXTensor]} args
         */
        constructor(...re) {
          ge(this, "ort_tensor");
          return (0, L.isONNXTensor)(re[0]) ? this.ort_tensor = /** @type {ONNXTensor} */
          re[0] : this.ort_tensor = new L.Tensor(
            /** @type {DataType} */
            re[0],
            /** @type {Exclude<import('./maths.js').AnyTypedArray, Uint8ClampedArray>} */
            re[1],
            re[2]
          ), new Proxy(this, {
            get: (he, Ee) => {
              if (typeof Ee == "string") {
                let Be = Number(Ee);
                if (Number.isInteger(Be))
                  return he._getitem(Be);
              }
              return he[Ee];
            },
            set: (he, Ee, Be) => he[Ee] = Be
          });
        }
        /** @type {number[]} Dimensions of the tensor. */
        get dims() {
          return this.ort_tensor.dims;
        }
        set dims(re) {
          this.ort_tensor.dims = re;
        }
        /** @type {DataType} Type of the tensor. */
        get type() {
          return this.ort_tensor.type;
        }
        /** @type {DataArray} The data stored in the tensor. */
        get data() {
          return this.ort_tensor.data;
        }
        /** @type {number} The number of elements in the tensor. */
        get size() {
          return this.ort_tensor.size;
        }
        /** @type {string} The location of the tensor data. */
        get location() {
          return this.ort_tensor.location;
        }
        dispose() {
          this.ort_tensor.dispose();
        }
        /**
         * Returns an iterator object for iterating over the tensor data in row-major order.
         * If the tensor has more than one dimension, the iterator will yield subarrays.
         * @returns {Iterator} An iterator object for iterating over the tensor data in row-major order.
         */
        *[Symbol.iterator]() {
          const [re, ...he] = this.dims;
          if (he.length > 0) {
            const Ee = he.reduce((Be, et) => Be * et);
            for (let Be = 0; Be < re; ++Be)
              yield this._subarray(Be, Ee, he);
          } else
            yield* this.data;
        }
        /**
         * Index into a Tensor object.
         * @param {number} index The index to access.
         * @returns {Tensor} The data at the specified index.
         */
        _getitem(re) {
          const [he, ...Ee] = this.dims;
          if (re = $(re, he), Ee.length > 0) {
            const Be = Ee.reduce((et, Xe) => et * Xe);
            return this._subarray(re, Be, Ee);
          } else
            return new W(this.type, [this.data[re]], Ee);
        }
        /**
         * @param {number|bigint} item The item to search for in the tensor
         * @returns {number} The index of the first occurrence of item in the tensor data.
         */
        indexOf(re) {
          const he = this.data;
          for (let Ee = 0; Ee < he.length; ++Ee)
            if (he[Ee] == re)
              return Ee;
          return -1;
        }
        /**
         * @param {number} index
         * @param {number} iterSize
         * @param {any} iterDims
         * @returns {Tensor}
         */
        _subarray(re, he, Ee) {
          const Be = re * he, et = (re + 1) * he, Xe = "subarray" in this.data ? this.data.subarray(Be, et) : this.data.slice(Be, et);
          return new W(this.type, Xe, Ee);
        }
        /**
         * Returns the value of this tensor as a standard JavaScript Number. This only works
         * for tensors with one element. For other cases, see `Tensor.tolist()`.
         * @returns {number|bigint} The value of this tensor as a standard JavaScript Number.
         * @throws {Error} If the tensor has more than one element.
         */
        item() {
          const re = this.data;
          if (re.length !== 1)
            throw new Error(`a Tensor with ${re.length} elements cannot be converted to Scalar`);
          return re[0];
        }
        /**
         * Convert tensor data to a n-dimensional JS list
         * @returns {Array}
         */
        tolist() {
          return w(this.data, this.dims);
        }
        /**
         * Return a new Tensor with the sigmoid function applied to each element.
         * @returns {Tensor} The tensor with the sigmoid function applied.
         */
        sigmoid() {
          return this.clone().sigmoid_();
        }
        /**
         * Applies the sigmoid function to the tensor in place.
         * @returns {Tensor} Returns `this`.
         */
        sigmoid_() {
          const re = this.data;
          for (let he = 0; he < re.length; ++he)
            re[he] = 1 / (1 + Math.exp(-re[he]));
          return this;
        }
        /**
         * Return a new Tensor with a callback function applied to each element.
         * @param {Function} callback - The function to apply to each element. It should take three arguments:
         *                              the current element, its index, and the tensor's data array.
         * @returns {Tensor} A new Tensor with the callback function applied to each element.
         */
        map(re) {
          return this.clone().map_(re);
        }
        /**
         * Apply a callback function to each element of the tensor in place.
         * @param {Function} callback - The function to apply to each element. It should take three arguments:
         *                              the current element, its index, and the tensor's data array.
         * @returns {Tensor} Returns `this`.
         */
        map_(re) {
          const he = this.data;
          for (let Ee = 0; Ee < he.length; ++Ee)
            he[Ee] = re(he[Ee], Ee, he);
          return this;
        }
        /**
         * Return a new Tensor with every element multiplied by a constant.
         * @param {number} val The value to multiply by.
         * @returns {Tensor} The new tensor.
         */
        mul(re) {
          return this.clone().mul_(re);
        }
        /**
         * Multiply the tensor by a constant in place.
         * @param {number} val The value to multiply by.
         * @returns {Tensor} Returns `this`.
         */
        mul_(re) {
          const he = this.data;
          for (let Ee = 0; Ee < he.length; ++Ee)
            he[Ee] *= re;
          return this;
        }
        /**
         * Return a new Tensor with every element divided by a constant.
         * @param {number} val The value to divide by.
         * @returns {Tensor} The new tensor.
         */
        div(re) {
          return this.clone().div_(re);
        }
        /**
         * Divide the tensor by a constant in place.
         * @param {number} val The value to divide by.
         * @returns {Tensor} Returns `this`.
         */
        div_(re) {
          const he = this.data;
          for (let Ee = 0; Ee < he.length; ++Ee)
            he[Ee] /= re;
          return this;
        }
        /**
         * Return a new Tensor with every element added by a constant.
         * @param {number} val The value to add by.
         * @returns {Tensor} The new tensor.
         */
        add(re) {
          return this.clone().add_(re);
        }
        /**
         * Add the tensor by a constant in place.
         * @param {number} val The value to add by.
         * @returns {Tensor} Returns `this`.
         */
        add_(re) {
          const he = this.data;
          for (let Ee = 0; Ee < he.length; ++Ee)
            he[Ee] += re;
          return this;
        }
        /**
         * Return a new Tensor with every element subtracted by a constant.
         * @param {number} val The value to subtract by.
         * @returns {Tensor} The new tensor.
         */
        sub(re) {
          return this.clone().sub_(re);
        }
        /**
         * Subtract the tensor by a constant in place.
         * @param {number} val The value to subtract by.
         * @returns {Tensor} Returns `this`.
         */
        sub_(re) {
          const he = this.data;
          for (let Ee = 0; Ee < he.length; ++Ee)
            he[Ee] -= re;
          return this;
        }
        /**
         * Creates a deep copy of the current Tensor.
         * @returns {Tensor} A new Tensor with the same type, data, and dimensions as the original.
         */
        clone() {
          return new W(this.type, this.data.slice(), this.dims.slice());
        }
        /**
         * Performs a slice operation on the Tensor along specified dimensions.
         *
         * Consider a Tensor that has a dimension of [4, 7]:
         * ```
         * [ 1,  2,  3,  4,  5,  6,  7]
         * [ 8,  9, 10, 11, 12, 13, 14]
         * [15, 16, 17, 18, 19, 20, 21]
         * [22, 23, 24, 25, 26, 27, 28]
         * ```
         * We can slice against the two dims of row and column, for instance in this
         * case we can start at the second element, and return to the second last,
         * like this:
         * ```
         * tensor.slice([1, -1], [1, -1]);
         * ```
         * which would return:
         * ```
         * [  9, 10, 11, 12, 13 ]
         * [ 16, 17, 18, 19, 20 ]
         * ```
         *
         * @param {...(number|number[]|null)} slices The slice specifications for each dimension.
         * - If a number is given, then a single element is selected.
         * - If an array of two numbers is given, then a range of elements [start, end (exclusive)] is selected.
         * - If null is given, then the entire dimension is selected.
         * @returns {Tensor} A new Tensor containing the selected elements.
         * @throws {Error} If the slice input is invalid.
         */
        slice(...re) {
          const he = [], Ee = [];
          for (let De = 0; De < this.dims.length; ++De) {
            let ce = re[De];
            if (ce == null)
              Ee.push([0, this.dims[De]]), he.push(this.dims[De]);
            else if (typeof ce == "number")
              ce = $(ce, this.dims[De], De), Ee.push([ce, ce + 1]);
            else if (Array.isArray(ce) && ce.length === 2) {
              let [ve, Re] = ce;
              if (ve = ve === null ? 0 : $(ve, this.dims[De], De, !1), Re = Re === null ? this.dims[De] : $(Re, this.dims[De], De, !1), ve > Re)
                throw new Error(`Invalid slice: ${ce}`);
              const je = [
                Math.max(ve, 0),
                Math.min(Re, this.dims[De])
              ];
              Ee.push(je), he.push(je[1] - je[0]);
            } else
              throw new Error(`Invalid slice: ${ce}`);
          }
          const Be = Ee.map(([De, ce]) => ce - De), et = Be.reduce((De, ce) => De * ce), Xe = this.data, ie = new Xe.constructor(et), Je = this.stride();
          for (let De = 0; De < et; ++De) {
            let ce = 0;
            for (let ve = Be.length - 1, Re = De; ve >= 0; --ve) {
              const je = Be[ve];
              ce += (Re % je + Ee[ve][0]) * Je[ve], Re = Math.floor(Re / je);
            }
            ie[De] = Xe[ce];
          }
          return new W(this.type, ie, he);
        }
        /**
         * Return a permuted version of this Tensor, according to the provided dimensions.
         * @param  {...number} dims Dimensions to permute.
         * @returns {Tensor} The permuted tensor.
         */
        permute(...re) {
          return x(this, re);
        }
        // TODO: implement transpose. For now (backwards compatibility), it's just an alias for permute()
        transpose(...re) {
          return this.permute(...re);
        }
        // TODO add .max() and .min() methods
        /**
         * Returns the sum of each row of the input tensor in the given dimension dim.
         *
         * @param {number} [dim=null] The dimension or dimensions to reduce. If `null`, all dimensions are reduced.
         * @param {boolean} keepdim Whether the output tensor has `dim` retained or not.
         * @returns The summed tensor
         */
        sum(re = null, he = !1) {
          return this.norm(1, re, he);
        }
        /**
         * Returns the matrix norm or vector norm of a given tensor.
         * @param {number|string} [p='fro'] The order of norm
         * @param {number} [dim=null] Specifies which dimension of the tensor to calculate the norm across.
         * If dim is None, the norm will be calculated across all dimensions of input.
         * @param {boolean} [keepdim=false] Whether the output tensors have dim retained or not.
         * @returns {Tensor} The norm of the tensor.
         */
        norm(re = "fro", he = null, Ee = !1) {
          if (re === "fro")
            re = 2;
          else if (typeof re == "string")
            throw Error(`Unsupported norm: ${re}`);
          const Be = this.data;
          if (he === null) {
            let ie = Be.reduce((Je, De) => Je + De ** re, 0) ** (1 / re);
            return new W(this.type, [ie], []);
          }
          he = $(he, this.dims.length);
          const et = this.dims.slice();
          et[he] = 1;
          const Xe = new Be.constructor(Be.length / this.dims[he]);
          for (let ie = 0; ie < Be.length; ++ie) {
            let Je = 0;
            for (let De = this.dims.length - 1, ce = ie, ve = 1; De >= 0; --De) {
              const Re = this.dims[De];
              if (De !== he) {
                const je = ce % Re;
                Je += je * ve, ve *= et[De];
              }
              ce = Math.floor(ce / Re);
            }
            Xe[Je] += Be[ie] ** re;
          }
          if (re !== 1)
            for (let ie = 0; ie < Xe.length; ++ie)
              Xe[ie] = Xe[ie] ** (1 / re);
          return Ee || et.splice(he, 1), new W(this.type, Xe, et);
        }
        /**
         * Performs `L_p` normalization of inputs over specified dimension. Operates in place.
         * @param {number} [p=2] The exponent value in the norm formulation
         * @param {number} [dim=1] The dimension to reduce
         * @returns {Tensor} `this` for operation chaining.
         */
        normalize_(re = 2, he = 1) {
          he = $(he, this.dims.length);
          const Ee = this.norm(re, he, !0), Be = this.data, et = Ee.data;
          for (let Xe = 0; Xe < Be.length; ++Xe) {
            let ie = 0;
            for (let Je = this.dims.length - 1, De = Xe, ce = 1; Je >= 0; --Je) {
              const ve = this.dims[Je];
              if (Je !== he) {
                const Re = De % ve;
                ie += Re * ce, ce *= this.dims[Je];
              }
              De = Math.floor(De / ve);
            }
            Be[Xe] /= et[ie];
          }
          return this;
        }
        /**
         * Performs `L_p` normalization of inputs over specified dimension.
         * @param {number} [p=2] The exponent value in the norm formulation
         * @param {number} [dim=1] The dimension to reduce
         * @returns {Tensor} The normalized tensor.
         */
        normalize(re = 2, he = 1) {
          return this.clone().normalize_(re, he);
        }
        /**
         * Compute and return the stride of this tensor.
         * Stride is the jump necessary to go from one element to the next one in the specified dimension dim.
         * @returns {number[]} The stride of this tensor.
         */
        stride() {
          return X(this.dims);
        }
        /**
         * Returns a tensor with all specified dimensions of input of size 1 removed.
         *
         * NOTE: The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.
         * If you would like a copy, use `tensor.clone()` before squeezing.
         *
         * @param {number} [dim=null] If given, the input will be squeezed only in the specified dimensions.
         * @returns {Tensor} The squeezed tensor
         */
        squeeze(re = null) {
          return new W(
            this.type,
            this.data,
            Y(this.dims, re)
          );
        }
        /**
         * In-place version of @see {@link Tensor.squeeze}
         */
        squeeze_(re = null) {
          return this.dims = Y(this.dims, re), this;
        }
        /**
         * Returns a new tensor with a dimension of size one inserted at the specified position.
         *
         * NOTE: The returned tensor shares the same underlying data with this tensor.
         *
         * @param {number} dim The index at which to insert the singleton dimension
         * @returns {Tensor} The unsqueezed tensor
         */
        unsqueeze(re = null) {
          return new W(
            this.type,
            this.data,
            O(this.dims, re)
          );
        }
        /**
         * In-place version of @see {@link Tensor.unsqueeze}
         */
        unsqueeze_(re = null) {
          return this.dims = O(this.dims, re), this;
        }
        /**
         * In-place version of @see {@link Tensor.flatten}
         */
        flatten_(re = 0, he = -1) {
          he = (he + this.dims.length) % this.dims.length;
          let Ee = this.dims.slice(0, re), Be = this.dims.slice(re, he + 1), et = this.dims.slice(he + 1);
          return this.dims = [...Ee, Be.reduce((Xe, ie) => Xe * ie, 1), ...et], this;
        }
        /**
         * Flattens input by reshaping it into a one-dimensional tensor.
         * If `start_dim` or `end_dim` are passed, only dimensions starting with `start_dim`
         * and ending with `end_dim` are flattened. The order of elements in input is unchanged.
         * @param {number} start_dim the first dim to flatten
         * @param {number} end_dim the last dim to flatten
         * @returns {Tensor} The flattened tensor.
         */
        flatten(re = 0, he = -1) {
          return this.clone().flatten_(re, he);
        }
        /**
         * Returns a new tensor with the same data as the `self` tensor but of a different `shape`.
         * @param  {...number} dims the desired size
         * @returns {Tensor} The tensor with the same data but different shape
         */
        view(...re) {
          let he = -1;
          for (let Be = 0; Be < re.length; ++Be)
            if (re[Be] === -1) {
              if (he !== -1)
                throw new Error("Only one dimension can be inferred");
              he = Be;
            }
          const Ee = this.data;
          if (he !== -1) {
            const Be = re.reduce((et, Xe, ie) => ie !== he ? et * Xe : et, 1);
            re[he] = Ee.length / Be;
          }
          return new W(this.type, Ee, re);
        }
        neg_() {
          const re = this.data;
          for (let he = 0; he < re.length; ++he)
            re[he] = -re[he];
          return this;
        }
        neg() {
          return this.clone().neg_();
        }
        /**
         * In-place version of @see {@link Tensor.clamp}
         */
        clamp_(re, he) {
          const Ee = this.data;
          for (let Be = 0; Be < Ee.length; ++Be)
            Ee[Be] = Math.min(Math.max(Ee[Be], re), he);
          return this;
        }
        /**
         * Clamps all elements in input into the range [ min, max ]
         * @param {number} min lower-bound of the range to be clamped to
         * @param {number} max upper-bound of the range to be clamped to
         * @returns {Tensor} the output tensor.
         */
        clamp(re, he) {
          return this.clone().clamp_(re, he);
        }
        /**
         * In-place version of @see {@link Tensor.round}
         */
        round_() {
          const re = this.data;
          for (let he = 0; he < re.length; ++he)
            re[he] = Math.round(re[he]);
          return this;
        }
        /**
         * Rounds elements of input to the nearest integer.
         * @returns {Tensor} the output tensor.
         */
        round() {
          return this.clone().round_();
        }
        mean(re = null, he = !1) {
          return ee(this, re, he);
        }
        /**
         * Performs Tensor dtype conversion.
         * @param {DataType} type The desired data type.
         * @returns {Tensor} The converted tensor.
         */
        to(re) {
          if (this.type === re) return this;
          if (!J.hasOwnProperty(re))
            throw new Error(`Unsupported type: ${re}`);
          let he;
          const Ee = ["int64", "uint64"].includes(this.type), Be = ["int64", "uint64"].includes(re);
          return Ee && !Be ? he = Number : !Ee && Be && (he = BigInt), new W(re, J[re].from(this.data, he), this.dims);
        }
      }
      function w(de, re) {
        const he = de.length, Ee = re.reduce((et, Xe) => et * Xe);
        if (he !== Ee)
          throw Error(`cannot reshape array of size ${he} into shape (${re})`);
        let Be = de;
        for (let et = re.length - 1; et >= 0; et--)
          Be = Be.reduce((Xe, ie) => {
            let Je = Xe[Xe.length - 1];
            return Je.length < re[et] ? Je.push(ie) : Xe.push([ie]), Xe;
          }, [[]]);
        return Be[0];
      }
      function x(de, re) {
        const [he, Ee] = (0, f.permute_data)(de.data, de.dims, re);
        return new W(de.type, he, Ee);
      }
      function y(de, [re, he], Ee = "bilinear", Be = !1) {
        const et = de.dims.at(-3) ?? 1, Xe = de.dims.at(-2), ie = de.dims.at(-1);
        let Je = (0, f.interpolate_data)(
          /** @type {import('./maths.js').TypedArray}*/
          de.data,
          [et, Xe, ie],
          [re, he],
          Ee,
          Be
        );
        return new W(de.type, Je, [et, re, he]);
      }
      async function M(de, {
        size: re = null,
        mode: he = "bilinear"
      } = {}) {
        if (de.dims.length !== 4)
          throw new Error("`interpolate_4d` currently only supports 4D input.");
        if (!re)
          throw new Error("`interpolate_4d` requires a `size` argument.");
        let Ee;
        if (re.length === 2)
          Ee = [...de.dims.slice(0, 2), ...re];
        else if (re.length === 3)
          Ee = [de.dims[0], ...re];
        else if (re.length === 4)
          Ee = re;
        else
          throw new Error("`size` must be of length 2, 3, or 4.");
        let Be;
        if (he === "bilinear")
          Be = await j.TensorOpRegistry.bilinear_interpolate_4d;
        else if (he === "bicubic")
          Be = await j.TensorOpRegistry.bicubic_interpolate_4d;
        else
          throw new Error(`Unsupported mode: ${he}`);
        const et = new W("int64", new BigInt64Array(Ee.map(BigInt)), [Ee.length]);
        return await Be({ x: de, s: et });
      }
      async function b(de, re) {
        return await (await j.TensorOpRegistry.matmul)({ a: de, b: re });
      }
      async function D(de, re) {
        return await (await j.TensorOpRegistry.rfft)({ x: de, a: re });
      }
      async function q(de, re) {
        const he = await j.TensorOpRegistry.top_k;
        return re === null ? re = de.dims.at(-1) : re = Math.min(re, de.dims.at(-1)), await he({
          x: de,
          k: new W(
            "int64",
            [BigInt(re)],
            [1]
          )
        });
      }
      const se = (de) => new W("int64", de, [de.length]);
      async function oe(de, re, he, Ee, Be) {
        return await (await j.TensorOpRegistry.slice)({
          x: de,
          s: se(re),
          e: se(he),
          a: se(Ee),
          t: se(Be ?? new Array(Ee.length).fill(1))
        });
      }
      function z(de, re) {
        const he = de.data, Ee = re.data, Be = [de.dims[0], de.dims[2]], et = new he.constructor(Be[0] * Be[1]), [Xe, ie, Je] = de.dims;
        let De = 0;
        for (let ce = 0; ce < Xe; ++ce) {
          const ve = ce * Je * ie;
          for (let Re = 0; Re < Je; ++Re) {
            let je = 0, Ve = 0;
            const Ne = ce * ie, Ze = ve + Re;
            for (let ft = 0; ft < ie; ++ft) {
              const dt = Number(Ee[Ne + ft]);
              Ve += dt, je += he[Ze + ft * Je] * dt;
            }
            const at = je / Ve;
            et[De++] = at;
          }
        }
        return new W(
          de.type,
          et,
          Be
        );
      }
      function V(de, re, {
        eps: he = 1e-5
      } = {}) {
        if (de.dims.length !== 2)
          throw new Error("`layer_norm` currently only supports 2D input.");
        const [Ee, Be] = de.dims;
        if (re.length !== 1 && re[0] !== Be)
          throw new Error("`normalized_shape` must be a 1D array with shape `[input.dims[1]]`.");
        const [et, Xe] = v(de, 1, 0, !0), ie = (
          /** @type {Float32Array} */
          et.data
        ), Je = (
          /** @type {Float32Array} */
          Xe.data
        ), De = (
          /** @type {Float32Array} */
          de.data
        ), ce = new De.constructor(De.length);
        for (let ve = 0; ve < Ee; ++ve) {
          const Re = ve * Be;
          for (let je = 0; je < Be; ++je) {
            const Ve = Re + je;
            ce[Ve] = (De[Ve] - Je[ve]) / (ie[ve] + he);
          }
        }
        return new W(de.type, ce, de.dims);
      }
      function Y(de, re) {
        return de = de.slice(), re === null ? de = de.filter((he) => he !== 1) : typeof re == "number" ? de[re] === 1 && de.splice(re, 1) : Array.isArray(re) && (de = de.filter((he, Ee) => he !== 1 || !re.includes(Ee))), de;
      }
      function O(de, re) {
        return re = $(re, de.length + 1), de = de.slice(), de.splice(re, 0, 1), de;
      }
      function $(de, re, he = null, Ee = !0) {
        if (Ee && (de < -re || de >= re))
          throw new Error(`IndexError: index ${de} is out of bounds for dimension${he === null ? "" : " " + he} with size ${re}`);
        return de < 0 && (de = (de % re + re) % re), de;
      }
      function g(de, re = 0) {
        re = $(re, de[0].dims.length);
        const he = de[0].dims.slice();
        he[re] = de.reduce((Xe, ie) => Xe + ie.dims[re], 0);
        const Ee = he.reduce((Xe, ie) => Xe * ie, 1), Be = new de[0].data.constructor(Ee), et = de[0].type;
        if (re === 0) {
          let Xe = 0;
          for (const ie of de) {
            const Je = ie.data;
            Be.set(Je, Xe), Xe += Je.length;
          }
        } else {
          let Xe = 0;
          for (let ie = 0; ie < de.length; ++ie) {
            const { data: Je, dims: De } = de[ie];
            for (let ce = 0; ce < Je.length; ++ce) {
              let ve = 0;
              for (let Re = De.length - 1, je = ce, Ve = 1; Re >= 0; --Re) {
                const Ne = De[Re];
                let Ze = je % Ne;
                Re === re && (Ze += Xe), ve += Ze * Ve, Ve *= he[Re], je = Math.floor(je / Ne);
              }
              Be[ve] = Je[ce];
            }
            Xe += De[re];
          }
        }
        return new W(et, Be, he);
      }
      function C(de, re = 0) {
        return g(de.map((he) => he.unsqueeze(re)), re);
      }
      function v(de, re = null, he = 1, Ee = !1) {
        const Be = (
          /** @type {Float32Array} */
          de.data
        ), et = de.dims;
        if (re === null) {
          const Re = Be.reduce((Ze, at) => Ze + at, 0) / Be.length, je = Math.sqrt(Be.reduce((Ze, at) => Ze + (at - Re) ** 2, 0) / (Be.length - he)), Ve = new W(de.type, [Re], [
            /* scalar */
          ]);
          return [new W(de.type, [je], [
            /* scalar */
          ]), Ve];
        }
        re = $(re, et.length);
        const Xe = ee(de, re, Ee), ie = Xe.data, Je = et.slice();
        Je[re] = 1;
        const De = new Be.constructor(Be.length / et[re]);
        for (let ve = 0; ve < Be.length; ++ve) {
          let Re = 0;
          for (let je = et.length - 1, Ve = ve, Ne = 1; je >= 0; --je) {
            const Ze = et[je];
            if (je !== re) {
              const at = Ve % Ze;
              Re += at * Ne, Ne *= Je[je];
            }
            Ve = Math.floor(Ve / Ze);
          }
          De[Re] += (Be[ve] - ie[Re]) ** 2;
        }
        for (let ve = 0; ve < De.length; ++ve)
          De[ve] = Math.sqrt(De[ve] / (et[re] - he));
        return Ee || Je.splice(re, 1), [new W(de.type, De, Je), Xe];
      }
      function ee(de, re = null, he = !1) {
        const Ee = (
          /** @type {Float32Array} */
          de.data
        );
        if (re === null) {
          const ie = Ee.reduce((Je, De) => Je + De, 0);
          return new W(de.type, [ie / Ee.length], [
            /* scalar */
          ]);
        }
        const Be = de.dims;
        re = $(re, Be.length);
        const et = Be.slice();
        et[re] = 1;
        const Xe = new Ee.constructor(Ee.length / Be[re]);
        for (let ie = 0; ie < Ee.length; ++ie) {
          let Je = 0;
          for (let De = Be.length - 1, ce = ie, ve = 1; De >= 0; --De) {
            const Re = Be[De];
            if (De !== re) {
              const je = ce % Re;
              Je += je * ve, ve *= et[De];
            }
            ce = Math.floor(ce / Re);
          }
          Xe[Je] += Ee[ie];
        }
        if (Be[re] !== 1)
          for (let ie = 0; ie < Xe.length; ++ie)
            Xe[ie] = Xe[ie] / Be[re];
        return he || et.splice(re, 1), new W(de.type, Xe, et);
      }
      function X(de) {
        const re = new Array(de.length);
        for (let he = de.length - 1, Ee = 1; he >= 0; --he)
          re[he] = Ee, Ee *= de[he];
        return re;
      }
      function le(de, re, he, Ee) {
        const Be = de.reduce((et, Xe) => et * Xe, 1);
        return new W(
          he,
          new Ee(Be).fill(re),
          de
        );
      }
      function ue(de, re) {
        let he, Ee;
        if (typeof re == "number")
          he = "float32", Ee = Float32Array;
        else if (typeof re == "bigint")
          he = "int64", Ee = BigInt64Array;
        else if (typeof re == "boolean")
          he = "bool", Ee = Uint8Array;
        else
          throw new Error(`Unsupported data type: ${typeof re}`);
        return le(de, re, he, Ee);
      }
      function fe(de, re) {
        return ue(de.dims, re);
      }
      function Ce(de) {
        return le(de, 1n, "int64", BigInt64Array);
      }
      function xe(de) {
        return Ce(de.dims);
      }
      function Le(de) {
        return le(de, 0n, "int64", BigInt64Array);
      }
      function qe(de) {
        return Le(de.dims);
      }
      function Ue(de) {
        const re = de.reduce((he, Ee) => he * Ee, 1);
        return new W(
          "float32",
          Float32Array.from({ length: re }, () => Math.random()),
          de
        );
      }
      function ut(de, re) {
        if (de.dims.length !== 2)
          throw new Error("The tensor must have 2 dimensions");
        if (de.dims.at(-1) % 8 !== 0)
          throw new Error("The last dimension of the tensor must be a multiple of 8");
        if (!["binary", "ubinary"].includes(re))
          throw new Error("The precision must be either 'binary' or 'ubinary'");
        const he = re === "binary", Ee = he ? "int8" : "uint8", Be = he ? Int8Array : Uint8Array, et = de.data, Xe = new Be(et.length / 8);
        for (let ie = 0; ie < et.length; ++ie) {
          const Je = et[ie] > 0 ? 1 : 0, De = Math.floor(ie / 8), ce = ie % 8;
          Xe[De] |= Je << 7 - ce, he && ce === 0 && (Xe[De] -= 128);
        }
        return new W(Ee, Xe, [de.dims[0], de.dims[1] / 8]);
      }
    }
  )
  /******/
}, e_ = {};
function fr(ke) {
  var A = e_[ke];
  if (A !== void 0)
    return A.exports;
  var s = e_[ke] = {
    /******/
    // no module.id needed
    /******/
    // no module.loaded needed
    /******/
    exports: {}
    /******/
  };
  return t_[ke](s, s.exports, fr), s.exports;
}
fr.m = t_;
(() => {
  var ke = Object.getPrototypeOf ? (s) => Object.getPrototypeOf(s) : (s) => s.__proto__, A;
  fr.t = function(s, f) {
    if (f & 1 && (s = this(s)), f & 8 || typeof s == "object" && s && (f & 4 && s.__esModule || f & 16 && typeof s.then == "function"))
      return s;
    var L = /* @__PURE__ */ Object.create(null);
    fr.r(L);
    var j = {};
    A = A || [null, ke({}), ke([]), ke(ke)];
    for (var J = f & 2 && s; typeof J == "object" && !~A.indexOf(J); J = ke(J))
      Object.getOwnPropertyNames(J).forEach((W) => j[W] = () => s[W]);
    return j.default = () => s, fr.d(L, j), L;
  };
})();
fr.d = (ke, A) => {
  for (var s in A)
    fr.o(A, s) && !fr.o(ke, s) && Object.defineProperty(ke, s, { enumerable: !0, get: A[s] });
};
fr.o = (ke, A) => Object.prototype.hasOwnProperty.call(ke, A);
fr.r = (ke) => {
  typeof Symbol < "u" && Symbol.toStringTag && Object.defineProperty(ke, Symbol.toStringTag, { value: "Module" }), Object.defineProperty(ke, "__esModule", { value: !0 });
};
(() => {
  var ke;
  if (typeof import.meta.url == "string" && (ke = import.meta.url), !ke) throw new Error("Automatic publicPath is not supported in this browser");
  ke = ke.replace(/#.*$/, "").replace(/\?.*$/, "").replace(/\/[^\/]+$/, "/"), fr.p = ke;
})();
fr.b = new URL("./", import.meta.url);
var c = {};
(() => {
  /*!*****************************!*\
    !*** ./src/transformers.js ***!
    \*****************************/
  fr.r(c), fr.d(c, {
    /* harmony export */
    ASTFeatureExtractor: () => (
      /* reexport safe */
      y.ASTFeatureExtractor
    ),
    /* harmony export */
    ASTForAudioClassification: () => (
      /* reexport safe */
      s.ASTForAudioClassification
    ),
    /* harmony export */
    ASTModel: () => (
      /* reexport safe */
      s.ASTModel
    ),
    /* harmony export */
    ASTPreTrainedModel: () => (
      /* reexport safe */
      s.ASTPreTrainedModel
    ),
    /* harmony export */
    AlbertForMaskedLM: () => (
      /* reexport safe */
      s.AlbertForMaskedLM
    ),
    /* harmony export */
    AlbertForQuestionAnswering: () => (
      /* reexport safe */
      s.AlbertForQuestionAnswering
    ),
    /* harmony export */
    AlbertForSequenceClassification: () => (
      /* reexport safe */
      s.AlbertForSequenceClassification
    ),
    /* harmony export */
    AlbertModel: () => (
      /* reexport safe */
      s.AlbertModel
    ),
    /* harmony export */
    AlbertPreTrainedModel: () => (
      /* reexport safe */
      s.AlbertPreTrainedModel
    ),
    /* harmony export */
    AlbertTokenizer: () => (
      /* reexport safe */
      f.AlbertTokenizer
    ),
    /* harmony export */
    AudioClassificationPipeline: () => (
      /* reexport safe */
      A.AudioClassificationPipeline
    ),
    /* harmony export */
    AutoConfig: () => (
      /* reexport safe */
      L.AutoConfig
    ),
    /* harmony export */
    AutoFeatureExtractor: () => (
      /* reexport safe */
      M.AutoFeatureExtractor
    ),
    /* harmony export */
    AutoImageProcessor: () => (
      /* reexport safe */
      q.AutoImageProcessor
    ),
    /* harmony export */
    AutoModel: () => (
      /* reexport safe */
      s.AutoModel
    ),
    /* harmony export */
    AutoModelForAudioClassification: () => (
      /* reexport safe */
      s.AutoModelForAudioClassification
    ),
    /* harmony export */
    AutoModelForAudioFrameClassification: () => (
      /* reexport safe */
      s.AutoModelForAudioFrameClassification
    ),
    /* harmony export */
    AutoModelForCTC: () => (
      /* reexport safe */
      s.AutoModelForCTC
    ),
    /* harmony export */
    AutoModelForCausalLM: () => (
      /* reexport safe */
      s.AutoModelForCausalLM
    ),
    /* harmony export */
    AutoModelForDepthEstimation: () => (
      /* reexport safe */
      s.AutoModelForDepthEstimation
    ),
    /* harmony export */
    AutoModelForDocumentQuestionAnswering: () => (
      /* reexport safe */
      s.AutoModelForDocumentQuestionAnswering
    ),
    /* harmony export */
    AutoModelForImageClassification: () => (
      /* reexport safe */
      s.AutoModelForImageClassification
    ),
    /* harmony export */
    AutoModelForImageFeatureExtraction: () => (
      /* reexport safe */
      s.AutoModelForImageFeatureExtraction
    ),
    /* harmony export */
    AutoModelForImageMatting: () => (
      /* reexport safe */
      s.AutoModelForImageMatting
    ),
    /* harmony export */
    AutoModelForImageSegmentation: () => (
      /* reexport safe */
      s.AutoModelForImageSegmentation
    ),
    /* harmony export */
    AutoModelForImageToImage: () => (
      /* reexport safe */
      s.AutoModelForImageToImage
    ),
    /* harmony export */
    AutoModelForMaskGeneration: () => (
      /* reexport safe */
      s.AutoModelForMaskGeneration
    ),
    /* harmony export */
    AutoModelForMaskedLM: () => (
      /* reexport safe */
      s.AutoModelForMaskedLM
    ),
    /* harmony export */
    AutoModelForNormalEstimation: () => (
      /* reexport safe */
      s.AutoModelForNormalEstimation
    ),
    /* harmony export */
    AutoModelForObjectDetection: () => (
      /* reexport safe */
      s.AutoModelForObjectDetection
    ),
    /* harmony export */
    AutoModelForPoseEstimation: () => (
      /* reexport safe */
      s.AutoModelForPoseEstimation
    ),
    /* harmony export */
    AutoModelForQuestionAnswering: () => (
      /* reexport safe */
      s.AutoModelForQuestionAnswering
    ),
    /* harmony export */
    AutoModelForSemanticSegmentation: () => (
      /* reexport safe */
      s.AutoModelForSemanticSegmentation
    ),
    /* harmony export */
    AutoModelForSeq2SeqLM: () => (
      /* reexport safe */
      s.AutoModelForSeq2SeqLM
    ),
    /* harmony export */
    AutoModelForSequenceClassification: () => (
      /* reexport safe */
      s.AutoModelForSequenceClassification
    ),
    /* harmony export */
    AutoModelForSpeechSeq2Seq: () => (
      /* reexport safe */
      s.AutoModelForSpeechSeq2Seq
    ),
    /* harmony export */
    AutoModelForTextToSpectrogram: () => (
      /* reexport safe */
      s.AutoModelForTextToSpectrogram
    ),
    /* harmony export */
    AutoModelForTextToWaveform: () => (
      /* reexport safe */
      s.AutoModelForTextToWaveform
    ),
    /* harmony export */
    AutoModelForTokenClassification: () => (
      /* reexport safe */
      s.AutoModelForTokenClassification
    ),
    /* harmony export */
    AutoModelForUniversalSegmentation: () => (
      /* reexport safe */
      s.AutoModelForUniversalSegmentation
    ),
    /* harmony export */
    AutoModelForVision2Seq: () => (
      /* reexport safe */
      s.AutoModelForVision2Seq
    ),
    /* harmony export */
    AutoModelForXVector: () => (
      /* reexport safe */
      s.AutoModelForXVector
    ),
    /* harmony export */
    AutoModelForZeroShotObjectDetection: () => (
      /* reexport safe */
      s.AutoModelForZeroShotObjectDetection
    ),
    /* harmony export */
    AutoProcessor: () => (
      /* reexport safe */
      z.AutoProcessor
    ),
    /* harmony export */
    AutoTokenizer: () => (
      /* reexport safe */
      f.AutoTokenizer
    ),
    /* harmony export */
    AutomaticSpeechRecognitionPipeline: () => (
      /* reexport safe */
      A.AutomaticSpeechRecognitionPipeline
    ),
    /* harmony export */
    BartForConditionalGeneration: () => (
      /* reexport safe */
      s.BartForConditionalGeneration
    ),
    /* harmony export */
    BartForSequenceClassification: () => (
      /* reexport safe */
      s.BartForSequenceClassification
    ),
    /* harmony export */
    BartModel: () => (
      /* reexport safe */
      s.BartModel
    ),
    /* harmony export */
    BartPretrainedModel: () => (
      /* reexport safe */
      s.BartPretrainedModel
    ),
    /* harmony export */
    BartTokenizer: () => (
      /* reexport safe */
      f.BartTokenizer
    ),
    /* harmony export */
    BaseModelOutput: () => (
      /* reexport safe */
      s.BaseModelOutput
    ),
    /* harmony export */
    BaseStreamer: () => (
      /* reexport safe */
      V.BaseStreamer
    ),
    /* harmony export */
    BeitFeatureExtractor: () => (
      /* reexport safe */
      D.BeitFeatureExtractor
    ),
    /* harmony export */
    BeitForImageClassification: () => (
      /* reexport safe */
      s.BeitForImageClassification
    ),
    /* harmony export */
    BeitModel: () => (
      /* reexport safe */
      s.BeitModel
    ),
    /* harmony export */
    BeitPreTrainedModel: () => (
      /* reexport safe */
      s.BeitPreTrainedModel
    ),
    /* harmony export */
    BertForMaskedLM: () => (
      /* reexport safe */
      s.BertForMaskedLM
    ),
    /* harmony export */
    BertForQuestionAnswering: () => (
      /* reexport safe */
      s.BertForQuestionAnswering
    ),
    /* harmony export */
    BertForSequenceClassification: () => (
      /* reexport safe */
      s.BertForSequenceClassification
    ),
    /* harmony export */
    BertForTokenClassification: () => (
      /* reexport safe */
      s.BertForTokenClassification
    ),
    /* harmony export */
    BertModel: () => (
      /* reexport safe */
      s.BertModel
    ),
    /* harmony export */
    BertPreTrainedModel: () => (
      /* reexport safe */
      s.BertPreTrainedModel
    ),
    /* harmony export */
    BertTokenizer: () => (
      /* reexport safe */
      f.BertTokenizer
    ),
    /* harmony export */
    BitImageProcessor: () => (
      /* reexport safe */
      D.BitImageProcessor
    ),
    /* harmony export */
    BlenderbotForConditionalGeneration: () => (
      /* reexport safe */
      s.BlenderbotForConditionalGeneration
    ),
    /* harmony export */
    BlenderbotModel: () => (
      /* reexport safe */
      s.BlenderbotModel
    ),
    /* harmony export */
    BlenderbotPreTrainedModel: () => (
      /* reexport safe */
      s.BlenderbotPreTrainedModel
    ),
    /* harmony export */
    BlenderbotSmallForConditionalGeneration: () => (
      /* reexport safe */
      s.BlenderbotSmallForConditionalGeneration
    ),
    /* harmony export */
    BlenderbotSmallModel: () => (
      /* reexport safe */
      s.BlenderbotSmallModel
    ),
    /* harmony export */
    BlenderbotSmallPreTrainedModel: () => (
      /* reexport safe */
      s.BlenderbotSmallPreTrainedModel
    ),
    /* harmony export */
    BlenderbotSmallTokenizer: () => (
      /* reexport safe */
      f.BlenderbotSmallTokenizer
    ),
    /* harmony export */
    BlenderbotTokenizer: () => (
      /* reexport safe */
      f.BlenderbotTokenizer
    ),
    /* harmony export */
    BloomForCausalLM: () => (
      /* reexport safe */
      s.BloomForCausalLM
    ),
    /* harmony export */
    BloomModel: () => (
      /* reexport safe */
      s.BloomModel
    ),
    /* harmony export */
    BloomPreTrainedModel: () => (
      /* reexport safe */
      s.BloomPreTrainedModel
    ),
    /* harmony export */
    BloomTokenizer: () => (
      /* reexport safe */
      f.BloomTokenizer
    ),
    /* harmony export */
    CLIPFeatureExtractor: () => (
      /* reexport safe */
      D.CLIPFeatureExtractor
    ),
    /* harmony export */
    CLIPImageProcessor: () => (
      /* reexport safe */
      D.CLIPImageProcessor
    ),
    /* harmony export */
    CLIPModel: () => (
      /* reexport safe */
      s.CLIPModel
    ),
    /* harmony export */
    CLIPPreTrainedModel: () => (
      /* reexport safe */
      s.CLIPPreTrainedModel
    ),
    /* harmony export */
    CLIPSegForImageSegmentation: () => (
      /* reexport safe */
      s.CLIPSegForImageSegmentation
    ),
    /* harmony export */
    CLIPSegModel: () => (
      /* reexport safe */
      s.CLIPSegModel
    ),
    /* harmony export */
    CLIPSegPreTrainedModel: () => (
      /* reexport safe */
      s.CLIPSegPreTrainedModel
    ),
    /* harmony export */
    CLIPTextModel: () => (
      /* reexport safe */
      s.CLIPTextModel
    ),
    /* harmony export */
    CLIPTextModelWithProjection: () => (
      /* reexport safe */
      s.CLIPTextModelWithProjection
    ),
    /* harmony export */
    CLIPTokenizer: () => (
      /* reexport safe */
      f.CLIPTokenizer
    ),
    /* harmony export */
    CLIPVisionModel: () => (
      /* reexport safe */
      s.CLIPVisionModel
    ),
    /* harmony export */
    CLIPVisionModelWithProjection: () => (
      /* reexport safe */
      s.CLIPVisionModelWithProjection
    ),
    /* harmony export */
    CamembertForMaskedLM: () => (
      /* reexport safe */
      s.CamembertForMaskedLM
    ),
    /* harmony export */
    CamembertForQuestionAnswering: () => (
      /* reexport safe */
      s.CamembertForQuestionAnswering
    ),
    /* harmony export */
    CamembertForSequenceClassification: () => (
      /* reexport safe */
      s.CamembertForSequenceClassification
    ),
    /* harmony export */
    CamembertForTokenClassification: () => (
      /* reexport safe */
      s.CamembertForTokenClassification
    ),
    /* harmony export */
    CamembertModel: () => (
      /* reexport safe */
      s.CamembertModel
    ),
    /* harmony export */
    CamembertPreTrainedModel: () => (
      /* reexport safe */
      s.CamembertPreTrainedModel
    ),
    /* harmony export */
    CamembertTokenizer: () => (
      /* reexport safe */
      f.CamembertTokenizer
    ),
    /* harmony export */
    CausalLMOutput: () => (
      /* reexport safe */
      s.CausalLMOutput
    ),
    /* harmony export */
    CausalLMOutputWithPast: () => (
      /* reexport safe */
      s.CausalLMOutputWithPast
    ),
    /* harmony export */
    ChineseCLIPFeatureExtractor: () => (
      /* reexport safe */
      D.ChineseCLIPFeatureExtractor
    ),
    /* harmony export */
    ChineseCLIPModel: () => (
      /* reexport safe */
      s.ChineseCLIPModel
    ),
    /* harmony export */
    ChineseCLIPPreTrainedModel: () => (
      /* reexport safe */
      s.ChineseCLIPPreTrainedModel
    ),
    /* harmony export */
    ClapAudioModelWithProjection: () => (
      /* reexport safe */
      s.ClapAudioModelWithProjection
    ),
    /* harmony export */
    ClapFeatureExtractor: () => (
      /* reexport safe */
      y.ClapFeatureExtractor
    ),
    /* harmony export */
    ClapModel: () => (
      /* reexport safe */
      s.ClapModel
    ),
    /* harmony export */
    ClapPreTrainedModel: () => (
      /* reexport safe */
      s.ClapPreTrainedModel
    ),
    /* harmony export */
    ClapTextModelWithProjection: () => (
      /* reexport safe */
      s.ClapTextModelWithProjection
    ),
    /* harmony export */
    ClassifierFreeGuidanceLogitsProcessor: () => (
      /* reexport safe */
      O.ClassifierFreeGuidanceLogitsProcessor
    ),
    /* harmony export */
    CodeGenForCausalLM: () => (
      /* reexport safe */
      s.CodeGenForCausalLM
    ),
    /* harmony export */
    CodeGenModel: () => (
      /* reexport safe */
      s.CodeGenModel
    ),
    /* harmony export */
    CodeGenPreTrainedModel: () => (
      /* reexport safe */
      s.CodeGenPreTrainedModel
    ),
    /* harmony export */
    CodeGenTokenizer: () => (
      /* reexport safe */
      f.CodeGenTokenizer
    ),
    /* harmony export */
    CodeLlamaTokenizer: () => (
      /* reexport safe */
      f.CodeLlamaTokenizer
    ),
    /* harmony export */
    CohereForCausalLM: () => (
      /* reexport safe */
      s.CohereForCausalLM
    ),
    /* harmony export */
    CohereModel: () => (
      /* reexport safe */
      s.CohereModel
    ),
    /* harmony export */
    CoherePreTrainedModel: () => (
      /* reexport safe */
      s.CoherePreTrainedModel
    ),
    /* harmony export */
    CohereTokenizer: () => (
      /* reexport safe */
      f.CohereTokenizer
    ),
    /* harmony export */
    ConvBertForMaskedLM: () => (
      /* reexport safe */
      s.ConvBertForMaskedLM
    ),
    /* harmony export */
    ConvBertForQuestionAnswering: () => (
      /* reexport safe */
      s.ConvBertForQuestionAnswering
    ),
    /* harmony export */
    ConvBertForSequenceClassification: () => (
      /* reexport safe */
      s.ConvBertForSequenceClassification
    ),
    /* harmony export */
    ConvBertForTokenClassification: () => (
      /* reexport safe */
      s.ConvBertForTokenClassification
    ),
    /* harmony export */
    ConvBertModel: () => (
      /* reexport safe */
      s.ConvBertModel
    ),
    /* harmony export */
    ConvBertPreTrainedModel: () => (
      /* reexport safe */
      s.ConvBertPreTrainedModel
    ),
    /* harmony export */
    ConvBertTokenizer: () => (
      /* reexport safe */
      f.ConvBertTokenizer
    ),
    /* harmony export */
    ConvNextFeatureExtractor: () => (
      /* reexport safe */
      D.ConvNextFeatureExtractor
    ),
    /* harmony export */
    ConvNextForImageClassification: () => (
      /* reexport safe */
      s.ConvNextForImageClassification
    ),
    /* harmony export */
    ConvNextImageProcessor: () => (
      /* reexport safe */
      D.ConvNextImageProcessor
    ),
    /* harmony export */
    ConvNextModel: () => (
      /* reexport safe */
      s.ConvNextModel
    ),
    /* harmony export */
    ConvNextPreTrainedModel: () => (
      /* reexport safe */
      s.ConvNextPreTrainedModel
    ),
    /* harmony export */
    ConvNextV2ForImageClassification: () => (
      /* reexport safe */
      s.ConvNextV2ForImageClassification
    ),
    /* harmony export */
    ConvNextV2Model: () => (
      /* reexport safe */
      s.ConvNextV2Model
    ),
    /* harmony export */
    ConvNextV2PreTrainedModel: () => (
      /* reexport safe */
      s.ConvNextV2PreTrainedModel
    ),
    /* harmony export */
    DPTFeatureExtractor: () => (
      /* reexport safe */
      D.DPTFeatureExtractor
    ),
    /* harmony export */
    DPTForDepthEstimation: () => (
      /* reexport safe */
      s.DPTForDepthEstimation
    ),
    /* harmony export */
    DPTImageProcessor: () => (
      /* reexport safe */
      D.DPTImageProcessor
    ),
    /* harmony export */
    DPTModel: () => (
      /* reexport safe */
      s.DPTModel
    ),
    /* harmony export */
    DPTPreTrainedModel: () => (
      /* reexport safe */
      s.DPTPreTrainedModel
    ),
    /* harmony export */
    DebertaForMaskedLM: () => (
      /* reexport safe */
      s.DebertaForMaskedLM
    ),
    /* harmony export */
    DebertaForQuestionAnswering: () => (
      /* reexport safe */
      s.DebertaForQuestionAnswering
    ),
    /* harmony export */
    DebertaForSequenceClassification: () => (
      /* reexport safe */
      s.DebertaForSequenceClassification
    ),
    /* harmony export */
    DebertaForTokenClassification: () => (
      /* reexport safe */
      s.DebertaForTokenClassification
    ),
    /* harmony export */
    DebertaModel: () => (
      /* reexport safe */
      s.DebertaModel
    ),
    /* harmony export */
    DebertaPreTrainedModel: () => (
      /* reexport safe */
      s.DebertaPreTrainedModel
    ),
    /* harmony export */
    DebertaTokenizer: () => (
      /* reexport safe */
      f.DebertaTokenizer
    ),
    /* harmony export */
    DebertaV2ForMaskedLM: () => (
      /* reexport safe */
      s.DebertaV2ForMaskedLM
    ),
    /* harmony export */
    DebertaV2ForQuestionAnswering: () => (
      /* reexport safe */
      s.DebertaV2ForQuestionAnswering
    ),
    /* harmony export */
    DebertaV2ForSequenceClassification: () => (
      /* reexport safe */
      s.DebertaV2ForSequenceClassification
    ),
    /* harmony export */
    DebertaV2ForTokenClassification: () => (
      /* reexport safe */
      s.DebertaV2ForTokenClassification
    ),
    /* harmony export */
    DebertaV2Model: () => (
      /* reexport safe */
      s.DebertaV2Model
    ),
    /* harmony export */
    DebertaV2PreTrainedModel: () => (
      /* reexport safe */
      s.DebertaV2PreTrainedModel
    ),
    /* harmony export */
    DebertaV2Tokenizer: () => (
      /* reexport safe */
      f.DebertaV2Tokenizer
    ),
    /* harmony export */
    DecisionTransformerModel: () => (
      /* reexport safe */
      s.DecisionTransformerModel
    ),
    /* harmony export */
    DecisionTransformerPreTrainedModel: () => (
      /* reexport safe */
      s.DecisionTransformerPreTrainedModel
    ),
    /* harmony export */
    DeiTFeatureExtractor: () => (
      /* reexport safe */
      D.DeiTFeatureExtractor
    ),
    /* harmony export */
    DeiTForImageClassification: () => (
      /* reexport safe */
      s.DeiTForImageClassification
    ),
    /* harmony export */
    DeiTImageProcessor: () => (
      /* reexport safe */
      D.DeiTImageProcessor
    ),
    /* harmony export */
    DeiTModel: () => (
      /* reexport safe */
      s.DeiTModel
    ),
    /* harmony export */
    DeiTPreTrainedModel: () => (
      /* reexport safe */
      s.DeiTPreTrainedModel
    ),
    /* harmony export */
    DepthAnythingForDepthEstimation: () => (
      /* reexport safe */
      s.DepthAnythingForDepthEstimation
    ),
    /* harmony export */
    DepthAnythingPreTrainedModel: () => (
      /* reexport safe */
      s.DepthAnythingPreTrainedModel
    ),
    /* harmony export */
    DepthEstimationPipeline: () => (
      /* reexport safe */
      A.DepthEstimationPipeline
    ),
    /* harmony export */
    DepthProForDepthEstimation: () => (
      /* reexport safe */
      s.DepthProForDepthEstimation
    ),
    /* harmony export */
    DepthProPreTrainedModel: () => (
      /* reexport safe */
      s.DepthProPreTrainedModel
    ),
    /* harmony export */
    DetrFeatureExtractor: () => (
      /* reexport safe */
      D.DetrFeatureExtractor
    ),
    /* harmony export */
    DetrForObjectDetection: () => (
      /* reexport safe */
      s.DetrForObjectDetection
    ),
    /* harmony export */
    DetrForSegmentation: () => (
      /* reexport safe */
      s.DetrForSegmentation
    ),
    /* harmony export */
    DetrImageProcessor: () => (
      /* reexport safe */
      D.DetrImageProcessor
    ),
    /* harmony export */
    DetrModel: () => (
      /* reexport safe */
      s.DetrModel
    ),
    /* harmony export */
    DetrObjectDetectionOutput: () => (
      /* reexport safe */
      s.DetrObjectDetectionOutput
    ),
    /* harmony export */
    DetrPreTrainedModel: () => (
      /* reexport safe */
      s.DetrPreTrainedModel
    ),
    /* harmony export */
    DetrSegmentationOutput: () => (
      /* reexport safe */
      s.DetrSegmentationOutput
    ),
    /* harmony export */
    Dinov2ForImageClassification: () => (
      /* reexport safe */
      s.Dinov2ForImageClassification
    ),
    /* harmony export */
    Dinov2Model: () => (
      /* reexport safe */
      s.Dinov2Model
    ),
    /* harmony export */
    Dinov2PreTrainedModel: () => (
      /* reexport safe */
      s.Dinov2PreTrainedModel
    ),
    /* harmony export */
    DistilBertForMaskedLM: () => (
      /* reexport safe */
      s.DistilBertForMaskedLM
    ),
    /* harmony export */
    DistilBertForQuestionAnswering: () => (
      /* reexport safe */
      s.DistilBertForQuestionAnswering
    ),
    /* harmony export */
    DistilBertForSequenceClassification: () => (
      /* reexport safe */
      s.DistilBertForSequenceClassification
    ),
    /* harmony export */
    DistilBertForTokenClassification: () => (
      /* reexport safe */
      s.DistilBertForTokenClassification
    ),
    /* harmony export */
    DistilBertModel: () => (
      /* reexport safe */
      s.DistilBertModel
    ),
    /* harmony export */
    DistilBertPreTrainedModel: () => (
      /* reexport safe */
      s.DistilBertPreTrainedModel
    ),
    /* harmony export */
    DistilBertTokenizer: () => (
      /* reexport safe */
      f.DistilBertTokenizer
    ),
    /* harmony export */
    DocumentQuestionAnsweringPipeline: () => (
      /* reexport safe */
      A.DocumentQuestionAnsweringPipeline
    ),
    /* harmony export */
    DonutFeatureExtractor: () => (
      /* reexport safe */
      D.DonutFeatureExtractor
    ),
    /* harmony export */
    DonutImageProcessor: () => (
      /* reexport safe */
      D.DonutImageProcessor
    ),
    /* harmony export */
    DonutSwinModel: () => (
      /* reexport safe */
      s.DonutSwinModel
    ),
    /* harmony export */
    DonutSwinPreTrainedModel: () => (
      /* reexport safe */
      s.DonutSwinPreTrainedModel
    ),
    /* harmony export */
    EfficientNetForImageClassification: () => (
      /* reexport safe */
      s.EfficientNetForImageClassification
    ),
    /* harmony export */
    EfficientNetImageProcessor: () => (
      /* reexport safe */
      D.EfficientNetImageProcessor
    ),
    /* harmony export */
    EfficientNetModel: () => (
      /* reexport safe */
      s.EfficientNetModel
    ),
    /* harmony export */
    EfficientNetPreTrainedModel: () => (
      /* reexport safe */
      s.EfficientNetPreTrainedModel
    ),
    /* harmony export */
    ElectraForMaskedLM: () => (
      /* reexport safe */
      s.ElectraForMaskedLM
    ),
    /* harmony export */
    ElectraForQuestionAnswering: () => (
      /* reexport safe */
      s.ElectraForQuestionAnswering
    ),
    /* harmony export */
    ElectraForSequenceClassification: () => (
      /* reexport safe */
      s.ElectraForSequenceClassification
    ),
    /* harmony export */
    ElectraForTokenClassification: () => (
      /* reexport safe */
      s.ElectraForTokenClassification
    ),
    /* harmony export */
    ElectraModel: () => (
      /* reexport safe */
      s.ElectraModel
    ),
    /* harmony export */
    ElectraPreTrainedModel: () => (
      /* reexport safe */
      s.ElectraPreTrainedModel
    ),
    /* harmony export */
    ElectraTokenizer: () => (
      /* reexport safe */
      f.ElectraTokenizer
    ),
    /* harmony export */
    EosTokenCriteria: () => (
      /* reexport safe */
      Y.EosTokenCriteria
    ),
    /* harmony export */
    EsmForMaskedLM: () => (
      /* reexport safe */
      s.EsmForMaskedLM
    ),
    /* harmony export */
    EsmForSequenceClassification: () => (
      /* reexport safe */
      s.EsmForSequenceClassification
    ),
    /* harmony export */
    EsmForTokenClassification: () => (
      /* reexport safe */
      s.EsmForTokenClassification
    ),
    /* harmony export */
    EsmModel: () => (
      /* reexport safe */
      s.EsmModel
    ),
    /* harmony export */
    EsmPreTrainedModel: () => (
      /* reexport safe */
      s.EsmPreTrainedModel
    ),
    /* harmony export */
    EsmTokenizer: () => (
      /* reexport safe */
      f.EsmTokenizer
    ),
    /* harmony export */
    ExaoneForCausalLM: () => (
      /* reexport safe */
      s.ExaoneForCausalLM
    ),
    /* harmony export */
    ExaoneModel: () => (
      /* reexport safe */
      s.ExaoneModel
    ),
    /* harmony export */
    ExaonePreTrainedModel: () => (
      /* reexport safe */
      s.ExaonePreTrainedModel
    ),
    /* harmony export */
    FFT: () => (
      /* reexport safe */
      w.FFT
    ),
    /* harmony export */
    FalconForCausalLM: () => (
      /* reexport safe */
      s.FalconForCausalLM
    ),
    /* harmony export */
    FalconModel: () => (
      /* reexport safe */
      s.FalconModel
    ),
    /* harmony export */
    FalconPreTrainedModel: () => (
      /* reexport safe */
      s.FalconPreTrainedModel
    ),
    /* harmony export */
    FalconTokenizer: () => (
      /* reexport safe */
      f.FalconTokenizer
    ),
    /* harmony export */
    FastViTForImageClassification: () => (
      /* reexport safe */
      s.FastViTForImageClassification
    ),
    /* harmony export */
    FastViTModel: () => (
      /* reexport safe */
      s.FastViTModel
    ),
    /* harmony export */
    FastViTPreTrainedModel: () => (
      /* reexport safe */
      s.FastViTPreTrainedModel
    ),
    /* harmony export */
    FeatureExtractionPipeline: () => (
      /* reexport safe */
      A.FeatureExtractionPipeline
    ),
    /* harmony export */
    FeatureExtractor: () => (
      /* reexport safe */
      x.FeatureExtractor
    ),
    /* harmony export */
    FillMaskPipeline: () => (
      /* reexport safe */
      A.FillMaskPipeline
    ),
    /* harmony export */
    Florence2ForConditionalGeneration: () => (
      /* reexport safe */
      s.Florence2ForConditionalGeneration
    ),
    /* harmony export */
    Florence2PreTrainedModel: () => (
      /* reexport safe */
      s.Florence2PreTrainedModel
    ),
    /* harmony export */
    Florence2Processor: () => (
      /* reexport safe */
      oe.Florence2Processor
    ),
    /* harmony export */
    ForcedBOSTokenLogitsProcessor: () => (
      /* reexport safe */
      O.ForcedBOSTokenLogitsProcessor
    ),
    /* harmony export */
    ForcedEOSTokenLogitsProcessor: () => (
      /* reexport safe */
      O.ForcedEOSTokenLogitsProcessor
    ),
    /* harmony export */
    GLPNFeatureExtractor: () => (
      /* reexport safe */
      D.GLPNFeatureExtractor
    ),
    /* harmony export */
    GLPNForDepthEstimation: () => (
      /* reexport safe */
      s.GLPNForDepthEstimation
    ),
    /* harmony export */
    GLPNModel: () => (
      /* reexport safe */
      s.GLPNModel
    ),
    /* harmony export */
    GLPNPreTrainedModel: () => (
      /* reexport safe */
      s.GLPNPreTrainedModel
    ),
    /* harmony export */
    GPT2LMHeadModel: () => (
      /* reexport safe */
      s.GPT2LMHeadModel
    ),
    /* harmony export */
    GPT2Model: () => (
      /* reexport safe */
      s.GPT2Model
    ),
    /* harmony export */
    GPT2PreTrainedModel: () => (
      /* reexport safe */
      s.GPT2PreTrainedModel
    ),
    /* harmony export */
    GPT2Tokenizer: () => (
      /* reexport safe */
      f.GPT2Tokenizer
    ),
    /* harmony export */
    GPTBigCodeForCausalLM: () => (
      /* reexport safe */
      s.GPTBigCodeForCausalLM
    ),
    /* harmony export */
    GPTBigCodeModel: () => (
      /* reexport safe */
      s.GPTBigCodeModel
    ),
    /* harmony export */
    GPTBigCodePreTrainedModel: () => (
      /* reexport safe */
      s.GPTBigCodePreTrainedModel
    ),
    /* harmony export */
    GPTJForCausalLM: () => (
      /* reexport safe */
      s.GPTJForCausalLM
    ),
    /* harmony export */
    GPTJModel: () => (
      /* reexport safe */
      s.GPTJModel
    ),
    /* harmony export */
    GPTJPreTrainedModel: () => (
      /* reexport safe */
      s.GPTJPreTrainedModel
    ),
    /* harmony export */
    GPTNeoForCausalLM: () => (
      /* reexport safe */
      s.GPTNeoForCausalLM
    ),
    /* harmony export */
    GPTNeoModel: () => (
      /* reexport safe */
      s.GPTNeoModel
    ),
    /* harmony export */
    GPTNeoPreTrainedModel: () => (
      /* reexport safe */
      s.GPTNeoPreTrainedModel
    ),
    /* harmony export */
    GPTNeoXForCausalLM: () => (
      /* reexport safe */
      s.GPTNeoXForCausalLM
    ),
    /* harmony export */
    GPTNeoXModel: () => (
      /* reexport safe */
      s.GPTNeoXModel
    ),
    /* harmony export */
    GPTNeoXPreTrainedModel: () => (
      /* reexport safe */
      s.GPTNeoXPreTrainedModel
    ),
    /* harmony export */
    GPTNeoXTokenizer: () => (
      /* reexport safe */
      f.GPTNeoXTokenizer
    ),
    /* harmony export */
    Gemma2ForCausalLM: () => (
      /* reexport safe */
      s.Gemma2ForCausalLM
    ),
    /* harmony export */
    Gemma2Model: () => (
      /* reexport safe */
      s.Gemma2Model
    ),
    /* harmony export */
    Gemma2PreTrainedModel: () => (
      /* reexport safe */
      s.Gemma2PreTrainedModel
    ),
    /* harmony export */
    GemmaForCausalLM: () => (
      /* reexport safe */
      s.GemmaForCausalLM
    ),
    /* harmony export */
    GemmaModel: () => (
      /* reexport safe */
      s.GemmaModel
    ),
    /* harmony export */
    GemmaPreTrainedModel: () => (
      /* reexport safe */
      s.GemmaPreTrainedModel
    ),
    /* harmony export */
    GemmaTokenizer: () => (
      /* reexport safe */
      f.GemmaTokenizer
    ),
    /* harmony export */
    GraniteForCausalLM: () => (
      /* reexport safe */
      s.GraniteForCausalLM
    ),
    /* harmony export */
    GraniteModel: () => (
      /* reexport safe */
      s.GraniteModel
    ),
    /* harmony export */
    GranitePreTrainedModel: () => (
      /* reexport safe */
      s.GranitePreTrainedModel
    ),
    /* harmony export */
    Grok1Tokenizer: () => (
      /* reexport safe */
      f.Grok1Tokenizer
    ),
    /* harmony export */
    GroupViTModel: () => (
      /* reexport safe */
      s.GroupViTModel
    ),
    /* harmony export */
    GroupViTPreTrainedModel: () => (
      /* reexport safe */
      s.GroupViTPreTrainedModel
    ),
    /* harmony export */
    HerbertTokenizer: () => (
      /* reexport safe */
      f.HerbertTokenizer
    ),
    /* harmony export */
    HieraForImageClassification: () => (
      /* reexport safe */
      s.HieraForImageClassification
    ),
    /* harmony export */
    HieraModel: () => (
      /* reexport safe */
      s.HieraModel
    ),
    /* harmony export */
    HieraPreTrainedModel: () => (
      /* reexport safe */
      s.HieraPreTrainedModel
    ),
    /* harmony export */
    HubertForCTC: () => (
      /* reexport safe */
      s.HubertForCTC
    ),
    /* harmony export */
    HubertForSequenceClassification: () => (
      /* reexport safe */
      s.HubertForSequenceClassification
    ),
    /* harmony export */
    HubertModel: () => (
      /* reexport safe */
      s.HubertModel
    ),
    /* harmony export */
    HubertPreTrainedModel: () => (
      /* reexport safe */
      s.HubertPreTrainedModel
    ),
    /* harmony export */
    IJepaForImageClassification: () => (
      /* reexport safe */
      s.IJepaForImageClassification
    ),
    /* harmony export */
    IJepaModel: () => (
      /* reexport safe */
      s.IJepaModel
    ),
    /* harmony export */
    IJepaPreTrainedModel: () => (
      /* reexport safe */
      s.IJepaPreTrainedModel
    ),
    /* harmony export */
    Idefics3ForConditionalGeneration: () => (
      /* reexport safe */
      s.Idefics3ForConditionalGeneration
    ),
    /* harmony export */
    Idefics3ImageProcessor: () => (
      /* reexport safe */
      D.Idefics3ImageProcessor
    ),
    /* harmony export */
    Idefics3PreTrainedModel: () => (
      /* reexport safe */
      s.Idefics3PreTrainedModel
    ),
    /* harmony export */
    Idefics3Processor: () => (
      /* reexport safe */
      oe.Idefics3Processor
    ),
    /* harmony export */
    ImageClassificationPipeline: () => (
      /* reexport safe */
      A.ImageClassificationPipeline
    ),
    /* harmony export */
    ImageFeatureExtractionPipeline: () => (
      /* reexport safe */
      A.ImageFeatureExtractionPipeline
    ),
    /* harmony export */
    ImageFeatureExtractor: () => (
      /* reexport safe */
      y.ImageFeatureExtractor
    ),
    /* harmony export */
    ImageMattingOutput: () => (
      /* reexport safe */
      s.ImageMattingOutput
    ),
    /* harmony export */
    ImageProcessor: () => (
      /* reexport safe */
      b.ImageProcessor
    ),
    /* harmony export */
    ImageSegmentationPipeline: () => (
      /* reexport safe */
      A.ImageSegmentationPipeline
    ),
    /* harmony export */
    ImageToImagePipeline: () => (
      /* reexport safe */
      A.ImageToImagePipeline
    ),
    /* harmony export */
    ImageToTextPipeline: () => (
      /* reexport safe */
      A.ImageToTextPipeline
    ),
    /* harmony export */
    InterruptableStoppingCriteria: () => (
      /* reexport safe */
      Y.InterruptableStoppingCriteria
    ),
    /* harmony export */
    JAISLMHeadModel: () => (
      /* reexport safe */
      s.JAISLMHeadModel
    ),
    /* harmony export */
    JAISModel: () => (
      /* reexport safe */
      s.JAISModel
    ),
    /* harmony export */
    JAISPreTrainedModel: () => (
      /* reexport safe */
      s.JAISPreTrainedModel
    ),
    /* harmony export */
    JinaCLIPImageProcessor: () => (
      /* reexport safe */
      D.JinaCLIPImageProcessor
    ),
    /* harmony export */
    JinaCLIPModel: () => (
      /* reexport safe */
      s.JinaCLIPModel
    ),
    /* harmony export */
    JinaCLIPPreTrainedModel: () => (
      /* reexport safe */
      s.JinaCLIPPreTrainedModel
    ),
    /* harmony export */
    JinaCLIPProcessor: () => (
      /* reexport safe */
      oe.JinaCLIPProcessor
    ),
    /* harmony export */
    JinaCLIPTextModel: () => (
      /* reexport safe */
      s.JinaCLIPTextModel
    ),
    /* harmony export */
    JinaCLIPVisionModel: () => (
      /* reexport safe */
      s.JinaCLIPVisionModel
    ),
    /* harmony export */
    LlamaForCausalLM: () => (
      /* reexport safe */
      s.LlamaForCausalLM
    ),
    /* harmony export */
    LlamaModel: () => (
      /* reexport safe */
      s.LlamaModel
    ),
    /* harmony export */
    LlamaPreTrainedModel: () => (
      /* reexport safe */
      s.LlamaPreTrainedModel
    ),
    /* harmony export */
    LlamaTokenizer: () => (
      /* reexport safe */
      f.LlamaTokenizer
    ),
    /* harmony export */
    LlavaForConditionalGeneration: () => (
      /* reexport safe */
      s.LlavaForConditionalGeneration
    ),
    /* harmony export */
    LlavaOnevisionForConditionalGeneration: () => (
      /* reexport safe */
      s.LlavaOnevisionForConditionalGeneration
    ),
    /* harmony export */
    LlavaOnevisionImageProcessor: () => (
      /* reexport safe */
      D.LlavaOnevisionImageProcessor
    ),
    /* harmony export */
    LlavaPreTrainedModel: () => (
      /* reexport safe */
      s.LlavaPreTrainedModel
    ),
    /* harmony export */
    LogitsProcessor: () => (
      /* reexport safe */
      O.LogitsProcessor
    ),
    /* harmony export */
    LogitsProcessorList: () => (
      /* reexport safe */
      O.LogitsProcessorList
    ),
    /* harmony export */
    LogitsWarper: () => (
      /* reexport safe */
      O.LogitsWarper
    ),
    /* harmony export */
    LongT5ForConditionalGeneration: () => (
      /* reexport safe */
      s.LongT5ForConditionalGeneration
    ),
    /* harmony export */
    LongT5Model: () => (
      /* reexport safe */
      s.LongT5Model
    ),
    /* harmony export */
    LongT5PreTrainedModel: () => (
      /* reexport safe */
      s.LongT5PreTrainedModel
    ),
    /* harmony export */
    M2M100ForConditionalGeneration: () => (
      /* reexport safe */
      s.M2M100ForConditionalGeneration
    ),
    /* harmony export */
    M2M100Model: () => (
      /* reexport safe */
      s.M2M100Model
    ),
    /* harmony export */
    M2M100PreTrainedModel: () => (
      /* reexport safe */
      s.M2M100PreTrainedModel
    ),
    /* harmony export */
    M2M100Tokenizer: () => (
      /* reexport safe */
      f.M2M100Tokenizer
    ),
    /* harmony export */
    MBart50Tokenizer: () => (
      /* reexport safe */
      f.MBart50Tokenizer
    ),
    /* harmony export */
    MBartForCausalLM: () => (
      /* reexport safe */
      s.MBartForCausalLM
    ),
    /* harmony export */
    MBartForConditionalGeneration: () => (
      /* reexport safe */
      s.MBartForConditionalGeneration
    ),
    /* harmony export */
    MBartForSequenceClassification: () => (
      /* reexport safe */
      s.MBartForSequenceClassification
    ),
    /* harmony export */
    MBartModel: () => (
      /* reexport safe */
      s.MBartModel
    ),
    /* harmony export */
    MBartPreTrainedModel: () => (
      /* reexport safe */
      s.MBartPreTrainedModel
    ),
    /* harmony export */
    MBartTokenizer: () => (
      /* reexport safe */
      f.MBartTokenizer
    ),
    /* harmony export */
    MPNetForMaskedLM: () => (
      /* reexport safe */
      s.MPNetForMaskedLM
    ),
    /* harmony export */
    MPNetForQuestionAnswering: () => (
      /* reexport safe */
      s.MPNetForQuestionAnswering
    ),
    /* harmony export */
    MPNetForSequenceClassification: () => (
      /* reexport safe */
      s.MPNetForSequenceClassification
    ),
    /* harmony export */
    MPNetForTokenClassification: () => (
      /* reexport safe */
      s.MPNetForTokenClassification
    ),
    /* harmony export */
    MPNetModel: () => (
      /* reexport safe */
      s.MPNetModel
    ),
    /* harmony export */
    MPNetPreTrainedModel: () => (
      /* reexport safe */
      s.MPNetPreTrainedModel
    ),
    /* harmony export */
    MPNetTokenizer: () => (
      /* reexport safe */
      f.MPNetTokenizer
    ),
    /* harmony export */
    MT5ForConditionalGeneration: () => (
      /* reexport safe */
      s.MT5ForConditionalGeneration
    ),
    /* harmony export */
    MT5Model: () => (
      /* reexport safe */
      s.MT5Model
    ),
    /* harmony export */
    MT5PreTrainedModel: () => (
      /* reexport safe */
      s.MT5PreTrainedModel
    ),
    /* harmony export */
    MarianMTModel: () => (
      /* reexport safe */
      s.MarianMTModel
    ),
    /* harmony export */
    MarianModel: () => (
      /* reexport safe */
      s.MarianModel
    ),
    /* harmony export */
    MarianPreTrainedModel: () => (
      /* reexport safe */
      s.MarianPreTrainedModel
    ),
    /* harmony export */
    MarianTokenizer: () => (
      /* reexport safe */
      f.MarianTokenizer
    ),
    /* harmony export */
    Mask2FormerImageProcessor: () => (
      /* reexport safe */
      D.Mask2FormerImageProcessor
    ),
    /* harmony export */
    MaskFormerFeatureExtractor: () => (
      /* reexport safe */
      D.MaskFormerFeatureExtractor
    ),
    /* harmony export */
    MaskFormerForInstanceSegmentation: () => (
      /* reexport safe */
      s.MaskFormerForInstanceSegmentation
    ),
    /* harmony export */
    MaskFormerImageProcessor: () => (
      /* reexport safe */
      D.MaskFormerImageProcessor
    ),
    /* harmony export */
    MaskFormerModel: () => (
      /* reexport safe */
      s.MaskFormerModel
    ),
    /* harmony export */
    MaskFormerPreTrainedModel: () => (
      /* reexport safe */
      s.MaskFormerPreTrainedModel
    ),
    /* harmony export */
    MaskedLMOutput: () => (
      /* reexport safe */
      s.MaskedLMOutput
    ),
    /* harmony export */
    MaxLengthCriteria: () => (
      /* reexport safe */
      Y.MaxLengthCriteria
    ),
    /* harmony export */
    MgpstrForSceneTextRecognition: () => (
      /* reexport safe */
      s.MgpstrForSceneTextRecognition
    ),
    /* harmony export */
    MgpstrModelOutput: () => (
      /* reexport safe */
      s.MgpstrModelOutput
    ),
    /* harmony export */
    MgpstrPreTrainedModel: () => (
      /* reexport safe */
      s.MgpstrPreTrainedModel
    ),
    /* harmony export */
    MgpstrProcessor: () => (
      /* reexport safe */
      oe.MgpstrProcessor
    ),
    /* harmony export */
    MgpstrTokenizer: () => (
      /* reexport safe */
      f.MgpstrTokenizer
    ),
    /* harmony export */
    MinLengthLogitsProcessor: () => (
      /* reexport safe */
      O.MinLengthLogitsProcessor
    ),
    /* harmony export */
    MinNewTokensLengthLogitsProcessor: () => (
      /* reexport safe */
      O.MinNewTokensLengthLogitsProcessor
    ),
    /* harmony export */
    MistralForCausalLM: () => (
      /* reexport safe */
      s.MistralForCausalLM
    ),
    /* harmony export */
    MistralModel: () => (
      /* reexport safe */
      s.MistralModel
    ),
    /* harmony export */
    MistralPreTrainedModel: () => (
      /* reexport safe */
      s.MistralPreTrainedModel
    ),
    /* harmony export */
    MobileBertForMaskedLM: () => (
      /* reexport safe */
      s.MobileBertForMaskedLM
    ),
    /* harmony export */
    MobileBertForQuestionAnswering: () => (
      /* reexport safe */
      s.MobileBertForQuestionAnswering
    ),
    /* harmony export */
    MobileBertForSequenceClassification: () => (
      /* reexport safe */
      s.MobileBertForSequenceClassification
    ),
    /* harmony export */
    MobileBertModel: () => (
      /* reexport safe */
      s.MobileBertModel
    ),
    /* harmony export */
    MobileBertPreTrainedModel: () => (
      /* reexport safe */
      s.MobileBertPreTrainedModel
    ),
    /* harmony export */
    MobileBertTokenizer: () => (
      /* reexport safe */
      f.MobileBertTokenizer
    ),
    /* harmony export */
    MobileLLMForCausalLM: () => (
      /* reexport safe */
      s.MobileLLMForCausalLM
    ),
    /* harmony export */
    MobileLLMModel: () => (
      /* reexport safe */
      s.MobileLLMModel
    ),
    /* harmony export */
    MobileLLMPreTrainedModel: () => (
      /* reexport safe */
      s.MobileLLMPreTrainedModel
    ),
    /* harmony export */
    MobileNetV1FeatureExtractor: () => (
      /* reexport safe */
      D.MobileNetV1FeatureExtractor
    ),
    /* harmony export */
    MobileNetV1ForImageClassification: () => (
      /* reexport safe */
      s.MobileNetV1ForImageClassification
    ),
    /* harmony export */
    MobileNetV1ImageProcessor: () => (
      /* reexport safe */
      D.MobileNetV1ImageProcessor
    ),
    /* harmony export */
    MobileNetV1Model: () => (
      /* reexport safe */
      s.MobileNetV1Model
    ),
    /* harmony export */
    MobileNetV1PreTrainedModel: () => (
      /* reexport safe */
      s.MobileNetV1PreTrainedModel
    ),
    /* harmony export */
    MobileNetV2FeatureExtractor: () => (
      /* reexport safe */
      D.MobileNetV2FeatureExtractor
    ),
    /* harmony export */
    MobileNetV2ForImageClassification: () => (
      /* reexport safe */
      s.MobileNetV2ForImageClassification
    ),
    /* harmony export */
    MobileNetV2ImageProcessor: () => (
      /* reexport safe */
      D.MobileNetV2ImageProcessor
    ),
    /* harmony export */
    MobileNetV2Model: () => (
      /* reexport safe */
      s.MobileNetV2Model
    ),
    /* harmony export */
    MobileNetV2PreTrainedModel: () => (
      /* reexport safe */
      s.MobileNetV2PreTrainedModel
    ),
    /* harmony export */
    MobileNetV3FeatureExtractor: () => (
      /* reexport safe */
      D.MobileNetV3FeatureExtractor
    ),
    /* harmony export */
    MobileNetV3ForImageClassification: () => (
      /* reexport safe */
      s.MobileNetV3ForImageClassification
    ),
    /* harmony export */
    MobileNetV3ImageProcessor: () => (
      /* reexport safe */
      D.MobileNetV3ImageProcessor
    ),
    /* harmony export */
    MobileNetV3Model: () => (
      /* reexport safe */
      s.MobileNetV3Model
    ),
    /* harmony export */
    MobileNetV3PreTrainedModel: () => (
      /* reexport safe */
      s.MobileNetV3PreTrainedModel
    ),
    /* harmony export */
    MobileNetV4FeatureExtractor: () => (
      /* reexport safe */
      D.MobileNetV4FeatureExtractor
    ),
    /* harmony export */
    MobileNetV4ForImageClassification: () => (
      /* reexport safe */
      s.MobileNetV4ForImageClassification
    ),
    /* harmony export */
    MobileNetV4ImageProcessor: () => (
      /* reexport safe */
      D.MobileNetV4ImageProcessor
    ),
    /* harmony export */
    MobileNetV4Model: () => (
      /* reexport safe */
      s.MobileNetV4Model
    ),
    /* harmony export */
    MobileNetV4PreTrainedModel: () => (
      /* reexport safe */
      s.MobileNetV4PreTrainedModel
    ),
    /* harmony export */
    MobileViTFeatureExtractor: () => (
      /* reexport safe */
      D.MobileViTFeatureExtractor
    ),
    /* harmony export */
    MobileViTForImageClassification: () => (
      /* reexport safe */
      s.MobileViTForImageClassification
    ),
    /* harmony export */
    MobileViTImageProcessor: () => (
      /* reexport safe */
      D.MobileViTImageProcessor
    ),
    /* harmony export */
    MobileViTModel: () => (
      /* reexport safe */
      s.MobileViTModel
    ),
    /* harmony export */
    MobileViTPreTrainedModel: () => (
      /* reexport safe */
      s.MobileViTPreTrainedModel
    ),
    /* harmony export */
    MobileViTV2ForImageClassification: () => (
      /* reexport safe */
      s.MobileViTV2ForImageClassification
    ),
    /* harmony export */
    MobileViTV2Model: () => (
      /* reexport safe */
      s.MobileViTV2Model
    ),
    /* harmony export */
    MobileViTV2PreTrainedModel: () => (
      /* reexport safe */
      s.MobileViTV2PreTrainedModel
    ),
    /* harmony export */
    ModelOutput: () => (
      /* reexport safe */
      s.ModelOutput
    ),
    /* harmony export */
    ModernBertForMaskedLM: () => (
      /* reexport safe */
      s.ModernBertForMaskedLM
    ),
    /* harmony export */
    ModernBertForSequenceClassification: () => (
      /* reexport safe */
      s.ModernBertForSequenceClassification
    ),
    /* harmony export */
    ModernBertForTokenClassification: () => (
      /* reexport safe */
      s.ModernBertForTokenClassification
    ),
    /* harmony export */
    ModernBertModel: () => (
      /* reexport safe */
      s.ModernBertModel
    ),
    /* harmony export */
    ModernBertPreTrainedModel: () => (
      /* reexport safe */
      s.ModernBertPreTrainedModel
    ),
    /* harmony export */
    Moondream1ForConditionalGeneration: () => (
      /* reexport safe */
      s.Moondream1ForConditionalGeneration
    ),
    /* harmony export */
    MoonshineFeatureExtractor: () => (
      /* reexport safe */
      y.MoonshineFeatureExtractor
    ),
    /* harmony export */
    MoonshineForConditionalGeneration: () => (
      /* reexport safe */
      s.MoonshineForConditionalGeneration
    ),
    /* harmony export */
    MoonshineModel: () => (
      /* reexport safe */
      s.MoonshineModel
    ),
    /* harmony export */
    MoonshinePreTrainedModel: () => (
      /* reexport safe */
      s.MoonshinePreTrainedModel
    ),
    /* harmony export */
    MoonshineProcessor: () => (
      /* reexport safe */
      oe.MoonshineProcessor
    ),
    /* harmony export */
    MptForCausalLM: () => (
      /* reexport safe */
      s.MptForCausalLM
    ),
    /* harmony export */
    MptModel: () => (
      /* reexport safe */
      s.MptModel
    ),
    /* harmony export */
    MptPreTrainedModel: () => (
      /* reexport safe */
      s.MptPreTrainedModel
    ),
    /* harmony export */
    MultiModalityCausalLM: () => (
      /* reexport safe */
      s.MultiModalityCausalLM
    ),
    /* harmony export */
    MultiModalityPreTrainedModel: () => (
      /* reexport safe */
      s.MultiModalityPreTrainedModel
    ),
    /* harmony export */
    MusicgenForCausalLM: () => (
      /* reexport safe */
      s.MusicgenForCausalLM
    ),
    /* harmony export */
    MusicgenForConditionalGeneration: () => (
      /* reexport safe */
      s.MusicgenForConditionalGeneration
    ),
    /* harmony export */
    MusicgenModel: () => (
      /* reexport safe */
      s.MusicgenModel
    ),
    /* harmony export */
    MusicgenPreTrainedModel: () => (
      /* reexport safe */
      s.MusicgenPreTrainedModel
    ),
    /* harmony export */
    NllbTokenizer: () => (
      /* reexport safe */
      f.NllbTokenizer
    ),
    /* harmony export */
    NoBadWordsLogitsProcessor: () => (
      /* reexport safe */
      O.NoBadWordsLogitsProcessor
    ),
    /* harmony export */
    NoRepeatNGramLogitsProcessor: () => (
      /* reexport safe */
      O.NoRepeatNGramLogitsProcessor
    ),
    /* harmony export */
    NomicBertModel: () => (
      /* reexport safe */
      s.NomicBertModel
    ),
    /* harmony export */
    NomicBertPreTrainedModel: () => (
      /* reexport safe */
      s.NomicBertPreTrainedModel
    ),
    /* harmony export */
    NougatImageProcessor: () => (
      /* reexport safe */
      D.NougatImageProcessor
    ),
    /* harmony export */
    NougatTokenizer: () => (
      /* reexport safe */
      f.NougatTokenizer
    ),
    /* harmony export */
    OPTForCausalLM: () => (
      /* reexport safe */
      s.OPTForCausalLM
    ),
    /* harmony export */
    OPTModel: () => (
      /* reexport safe */
      s.OPTModel
    ),
    /* harmony export */
    OPTPreTrainedModel: () => (
      /* reexport safe */
      s.OPTPreTrainedModel
    ),
    /* harmony export */
    ObjectDetectionPipeline: () => (
      /* reexport safe */
      A.ObjectDetectionPipeline
    ),
    /* harmony export */
    Olmo2ForCausalLM: () => (
      /* reexport safe */
      s.Olmo2ForCausalLM
    ),
    /* harmony export */
    Olmo2Model: () => (
      /* reexport safe */
      s.Olmo2Model
    ),
    /* harmony export */
    Olmo2PreTrainedModel: () => (
      /* reexport safe */
      s.Olmo2PreTrainedModel
    ),
    /* harmony export */
    OlmoForCausalLM: () => (
      /* reexport safe */
      s.OlmoForCausalLM
    ),
    /* harmony export */
    OlmoModel: () => (
      /* reexport safe */
      s.OlmoModel
    ),
    /* harmony export */
    OlmoPreTrainedModel: () => (
      /* reexport safe */
      s.OlmoPreTrainedModel
    ),
    /* harmony export */
    OpenELMForCausalLM: () => (
      /* reexport safe */
      s.OpenELMForCausalLM
    ),
    /* harmony export */
    OpenELMModel: () => (
      /* reexport safe */
      s.OpenELMModel
    ),
    /* harmony export */
    OpenELMPreTrainedModel: () => (
      /* reexport safe */
      s.OpenELMPreTrainedModel
    ),
    /* harmony export */
    OwlViTFeatureExtractor: () => (
      /* reexport safe */
      D.OwlViTFeatureExtractor
    ),
    /* harmony export */
    OwlViTForObjectDetection: () => (
      /* reexport safe */
      s.OwlViTForObjectDetection
    ),
    /* harmony export */
    OwlViTImageProcessor: () => (
      /* reexport safe */
      D.OwlViTImageProcessor
    ),
    /* harmony export */
    OwlViTModel: () => (
      /* reexport safe */
      s.OwlViTModel
    ),
    /* harmony export */
    OwlViTPreTrainedModel: () => (
      /* reexport safe */
      s.OwlViTPreTrainedModel
    ),
    /* harmony export */
    OwlViTProcessor: () => (
      /* reexport safe */
      oe.OwlViTProcessor
    ),
    /* harmony export */
    Owlv2ForObjectDetection: () => (
      /* reexport safe */
      s.Owlv2ForObjectDetection
    ),
    /* harmony export */
    Owlv2ImageProcessor: () => (
      /* reexport safe */
      D.Owlv2ImageProcessor
    ),
    /* harmony export */
    Owlv2Model: () => (
      /* reexport safe */
      s.Owlv2Model
    ),
    /* harmony export */
    Owlv2PreTrainedModel: () => (
      /* reexport safe */
      s.Owlv2PreTrainedModel
    ),
    /* harmony export */
    PaliGemmaForConditionalGeneration: () => (
      /* reexport safe */
      s.PaliGemmaForConditionalGeneration
    ),
    /* harmony export */
    PaliGemmaPreTrainedModel: () => (
      /* reexport safe */
      s.PaliGemmaPreTrainedModel
    ),
    /* harmony export */
    PaliGemmaProcessor: () => (
      /* reexport safe */
      oe.PaliGemmaProcessor
    ),
    /* harmony export */
    PatchTSMixerForPrediction: () => (
      /* reexport safe */
      s.PatchTSMixerForPrediction
    ),
    /* harmony export */
    PatchTSMixerModel: () => (
      /* reexport safe */
      s.PatchTSMixerModel
    ),
    /* harmony export */
    PatchTSMixerPreTrainedModel: () => (
      /* reexport safe */
      s.PatchTSMixerPreTrainedModel
    ),
    /* harmony export */
    PatchTSTForPrediction: () => (
      /* reexport safe */
      s.PatchTSTForPrediction
    ),
    /* harmony export */
    PatchTSTModel: () => (
      /* reexport safe */
      s.PatchTSTModel
    ),
    /* harmony export */
    PatchTSTPreTrainedModel: () => (
      /* reexport safe */
      s.PatchTSTPreTrainedModel
    ),
    /* harmony export */
    Phi3ForCausalLM: () => (
      /* reexport safe */
      s.Phi3ForCausalLM
    ),
    /* harmony export */
    Phi3Model: () => (
      /* reexport safe */
      s.Phi3Model
    ),
    /* harmony export */
    Phi3PreTrainedModel: () => (
      /* reexport safe */
      s.Phi3PreTrainedModel
    ),
    /* harmony export */
    Phi3VForCausalLM: () => (
      /* reexport safe */
      s.Phi3VForCausalLM
    ),
    /* harmony export */
    Phi3VImageProcessor: () => (
      /* reexport safe */
      D.Phi3VImageProcessor
    ),
    /* harmony export */
    Phi3VPreTrainedModel: () => (
      /* reexport safe */
      s.Phi3VPreTrainedModel
    ),
    /* harmony export */
    Phi3VProcessor: () => (
      /* reexport safe */
      oe.Phi3VProcessor
    ),
    /* harmony export */
    PhiForCausalLM: () => (
      /* reexport safe */
      s.PhiForCausalLM
    ),
    /* harmony export */
    PhiModel: () => (
      /* reexport safe */
      s.PhiModel
    ),
    /* harmony export */
    PhiPreTrainedModel: () => (
      /* reexport safe */
      s.PhiPreTrainedModel
    ),
    /* harmony export */
    Pipeline: () => (
      /* reexport safe */
      A.Pipeline
    ),
    /* harmony export */
    PreTrainedModel: () => (
      /* reexport safe */
      s.PreTrainedModel
    ),
    /* harmony export */
    PreTrainedTokenizer: () => (
      /* reexport safe */
      f.PreTrainedTokenizer
    ),
    /* harmony export */
    PretrainedConfig: () => (
      /* reexport safe */
      L.PretrainedConfig
    ),
    /* harmony export */
    PretrainedMixin: () => (
      /* reexport safe */
      s.PretrainedMixin
    ),
    /* harmony export */
    Processor: () => (
      /* reexport safe */
      se.Processor
    ),
    /* harmony export */
    PvtForImageClassification: () => (
      /* reexport safe */
      s.PvtForImageClassification
    ),
    /* harmony export */
    PvtImageProcessor: () => (
      /* reexport safe */
      D.PvtImageProcessor
    ),
    /* harmony export */
    PvtModel: () => (
      /* reexport safe */
      s.PvtModel
    ),
    /* harmony export */
    PvtPreTrainedModel: () => (
      /* reexport safe */
      s.PvtPreTrainedModel
    ),
    /* harmony export */
    PyAnnoteFeatureExtractor: () => (
      /* reexport safe */
      y.PyAnnoteFeatureExtractor
    ),
    /* harmony export */
    PyAnnoteForAudioFrameClassification: () => (
      /* reexport safe */
      s.PyAnnoteForAudioFrameClassification
    ),
    /* harmony export */
    PyAnnoteModel: () => (
      /* reexport safe */
      s.PyAnnoteModel
    ),
    /* harmony export */
    PyAnnotePreTrainedModel: () => (
      /* reexport safe */
      s.PyAnnotePreTrainedModel
    ),
    /* harmony export */
    PyAnnoteProcessor: () => (
      /* reexport safe */
      oe.PyAnnoteProcessor
    ),
    /* harmony export */
    QuestionAnsweringModelOutput: () => (
      /* reexport safe */
      s.QuestionAnsweringModelOutput
    ),
    /* harmony export */
    QuestionAnsweringPipeline: () => (
      /* reexport safe */
      A.QuestionAnsweringPipeline
    ),
    /* harmony export */
    Qwen2ForCausalLM: () => (
      /* reexport safe */
      s.Qwen2ForCausalLM
    ),
    /* harmony export */
    Qwen2Model: () => (
      /* reexport safe */
      s.Qwen2Model
    ),
    /* harmony export */
    Qwen2PreTrainedModel: () => (
      /* reexport safe */
      s.Qwen2PreTrainedModel
    ),
    /* harmony export */
    Qwen2Tokenizer: () => (
      /* reexport safe */
      f.Qwen2Tokenizer
    ),
    /* harmony export */
    Qwen2VLForConditionalGeneration: () => (
      /* reexport safe */
      s.Qwen2VLForConditionalGeneration
    ),
    /* harmony export */
    Qwen2VLImageProcessor: () => (
      /* reexport safe */
      D.Qwen2VLImageProcessor
    ),
    /* harmony export */
    Qwen2VLPreTrainedModel: () => (
      /* reexport safe */
      s.Qwen2VLPreTrainedModel
    ),
    /* harmony export */
    Qwen2VLProcessor: () => (
      /* reexport safe */
      oe.Qwen2VLProcessor
    ),
    /* harmony export */
    RTDetrForObjectDetection: () => (
      /* reexport safe */
      s.RTDetrForObjectDetection
    ),
    /* harmony export */
    RTDetrImageProcessor: () => (
      /* reexport safe */
      D.RTDetrImageProcessor
    ),
    /* harmony export */
    RTDetrModel: () => (
      /* reexport safe */
      s.RTDetrModel
    ),
    /* harmony export */
    RTDetrObjectDetectionOutput: () => (
      /* reexport safe */
      s.RTDetrObjectDetectionOutput
    ),
    /* harmony export */
    RTDetrPreTrainedModel: () => (
      /* reexport safe */
      s.RTDetrPreTrainedModel
    ),
    /* harmony export */
    RawImage: () => (
      /* reexport safe */
      J.RawImage
    ),
    /* harmony export */
    RepetitionPenaltyLogitsProcessor: () => (
      /* reexport safe */
      O.RepetitionPenaltyLogitsProcessor
    ),
    /* harmony export */
    ResNetForImageClassification: () => (
      /* reexport safe */
      s.ResNetForImageClassification
    ),
    /* harmony export */
    ResNetModel: () => (
      /* reexport safe */
      s.ResNetModel
    ),
    /* harmony export */
    ResNetPreTrainedModel: () => (
      /* reexport safe */
      s.ResNetPreTrainedModel
    ),
    /* harmony export */
    RoFormerForMaskedLM: () => (
      /* reexport safe */
      s.RoFormerForMaskedLM
    ),
    /* harmony export */
    RoFormerForQuestionAnswering: () => (
      /* reexport safe */
      s.RoFormerForQuestionAnswering
    ),
    /* harmony export */
    RoFormerForSequenceClassification: () => (
      /* reexport safe */
      s.RoFormerForSequenceClassification
    ),
    /* harmony export */
    RoFormerForTokenClassification: () => (
      /* reexport safe */
      s.RoFormerForTokenClassification
    ),
    /* harmony export */
    RoFormerModel: () => (
      /* reexport safe */
      s.RoFormerModel
    ),
    /* harmony export */
    RoFormerPreTrainedModel: () => (
      /* reexport safe */
      s.RoFormerPreTrainedModel
    ),
    /* harmony export */
    RoFormerTokenizer: () => (
      /* reexport safe */
      f.RoFormerTokenizer
    ),
    /* harmony export */
    RobertaForMaskedLM: () => (
      /* reexport safe */
      s.RobertaForMaskedLM
    ),
    /* harmony export */
    RobertaForQuestionAnswering: () => (
      /* reexport safe */
      s.RobertaForQuestionAnswering
    ),
    /* harmony export */
    RobertaForSequenceClassification: () => (
      /* reexport safe */
      s.RobertaForSequenceClassification
    ),
    /* harmony export */
    RobertaForTokenClassification: () => (
      /* reexport safe */
      s.RobertaForTokenClassification
    ),
    /* harmony export */
    RobertaModel: () => (
      /* reexport safe */
      s.RobertaModel
    ),
    /* harmony export */
    RobertaPreTrainedModel: () => (
      /* reexport safe */
      s.RobertaPreTrainedModel
    ),
    /* harmony export */
    RobertaTokenizer: () => (
      /* reexport safe */
      f.RobertaTokenizer
    ),
    /* harmony export */
    SamImageProcessor: () => (
      /* reexport safe */
      D.SamImageProcessor
    ),
    /* harmony export */
    SamImageSegmentationOutput: () => (
      /* reexport safe */
      s.SamImageSegmentationOutput
    ),
    /* harmony export */
    SamModel: () => (
      /* reexport safe */
      s.SamModel
    ),
    /* harmony export */
    SamPreTrainedModel: () => (
      /* reexport safe */
      s.SamPreTrainedModel
    ),
    /* harmony export */
    SamProcessor: () => (
      /* reexport safe */
      oe.SamProcessor
    ),
    /* harmony export */
    SapiensForDepthEstimation: () => (
      /* reexport safe */
      s.SapiensForDepthEstimation
    ),
    /* harmony export */
    SapiensForNormalEstimation: () => (
      /* reexport safe */
      s.SapiensForNormalEstimation
    ),
    /* harmony export */
    SapiensForSemanticSegmentation: () => (
      /* reexport safe */
      s.SapiensForSemanticSegmentation
    ),
    /* harmony export */
    SapiensPreTrainedModel: () => (
      /* reexport safe */
      s.SapiensPreTrainedModel
    ),
    /* harmony export */
    SeamlessM4TFeatureExtractor: () => (
      /* reexport safe */
      y.SeamlessM4TFeatureExtractor
    ),
    /* harmony export */
    SegformerFeatureExtractor: () => (
      /* reexport safe */
      D.SegformerFeatureExtractor
    ),
    /* harmony export */
    SegformerForImageClassification: () => (
      /* reexport safe */
      s.SegformerForImageClassification
    ),
    /* harmony export */
    SegformerForSemanticSegmentation: () => (
      /* reexport safe */
      s.SegformerForSemanticSegmentation
    ),
    /* harmony export */
    SegformerImageProcessor: () => (
      /* reexport safe */
      D.SegformerImageProcessor
    ),
    /* harmony export */
    SegformerModel: () => (
      /* reexport safe */
      s.SegformerModel
    ),
    /* harmony export */
    SegformerPreTrainedModel: () => (
      /* reexport safe */
      s.SegformerPreTrainedModel
    ),
    /* harmony export */
    Seq2SeqLMOutput: () => (
      /* reexport safe */
      s.Seq2SeqLMOutput
    ),
    /* harmony export */
    SequenceClassifierOutput: () => (
      /* reexport safe */
      s.SequenceClassifierOutput
    ),
    /* harmony export */
    SiglipImageProcessor: () => (
      /* reexport safe */
      D.SiglipImageProcessor
    ),
    /* harmony export */
    SiglipModel: () => (
      /* reexport safe */
      s.SiglipModel
    ),
    /* harmony export */
    SiglipPreTrainedModel: () => (
      /* reexport safe */
      s.SiglipPreTrainedModel
    ),
    /* harmony export */
    SiglipTextModel: () => (
      /* reexport safe */
      s.SiglipTextModel
    ),
    /* harmony export */
    SiglipTokenizer: () => (
      /* reexport safe */
      f.SiglipTokenizer
    ),
    /* harmony export */
    SiglipVisionModel: () => (
      /* reexport safe */
      s.SiglipVisionModel
    ),
    /* harmony export */
    SpeechT5FeatureExtractor: () => (
      /* reexport safe */
      y.SpeechT5FeatureExtractor
    ),
    /* harmony export */
    SpeechT5ForSpeechToText: () => (
      /* reexport safe */
      s.SpeechT5ForSpeechToText
    ),
    /* harmony export */
    SpeechT5ForTextToSpeech: () => (
      /* reexport safe */
      s.SpeechT5ForTextToSpeech
    ),
    /* harmony export */
    SpeechT5HifiGan: () => (
      /* reexport safe */
      s.SpeechT5HifiGan
    ),
    /* harmony export */
    SpeechT5Model: () => (
      /* reexport safe */
      s.SpeechT5Model
    ),
    /* harmony export */
    SpeechT5PreTrainedModel: () => (
      /* reexport safe */
      s.SpeechT5PreTrainedModel
    ),
    /* harmony export */
    SpeechT5Processor: () => (
      /* reexport safe */
      oe.SpeechT5Processor
    ),
    /* harmony export */
    SpeechT5Tokenizer: () => (
      /* reexport safe */
      f.SpeechT5Tokenizer
    ),
    /* harmony export */
    SqueezeBertForMaskedLM: () => (
      /* reexport safe */
      s.SqueezeBertForMaskedLM
    ),
    /* harmony export */
    SqueezeBertForQuestionAnswering: () => (
      /* reexport safe */
      s.SqueezeBertForQuestionAnswering
    ),
    /* harmony export */
    SqueezeBertForSequenceClassification: () => (
      /* reexport safe */
      s.SqueezeBertForSequenceClassification
    ),
    /* harmony export */
    SqueezeBertModel: () => (
      /* reexport safe */
      s.SqueezeBertModel
    ),
    /* harmony export */
    SqueezeBertPreTrainedModel: () => (
      /* reexport safe */
      s.SqueezeBertPreTrainedModel
    ),
    /* harmony export */
    SqueezeBertTokenizer: () => (
      /* reexport safe */
      f.SqueezeBertTokenizer
    ),
    /* harmony export */
    StableLmForCausalLM: () => (
      /* reexport safe */
      s.StableLmForCausalLM
    ),
    /* harmony export */
    StableLmModel: () => (
      /* reexport safe */
      s.StableLmModel
    ),
    /* harmony export */
    StableLmPreTrainedModel: () => (
      /* reexport safe */
      s.StableLmPreTrainedModel
    ),
    /* harmony export */
    Starcoder2ForCausalLM: () => (
      /* reexport safe */
      s.Starcoder2ForCausalLM
    ),
    /* harmony export */
    Starcoder2Model: () => (
      /* reexport safe */
      s.Starcoder2Model
    ),
    /* harmony export */
    Starcoder2PreTrainedModel: () => (
      /* reexport safe */
      s.Starcoder2PreTrainedModel
    ),
    /* harmony export */
    StoppingCriteria: () => (
      /* reexport safe */
      Y.StoppingCriteria
    ),
    /* harmony export */
    StoppingCriteriaList: () => (
      /* reexport safe */
      Y.StoppingCriteriaList
    ),
    /* harmony export */
    SummarizationPipeline: () => (
      /* reexport safe */
      A.SummarizationPipeline
    ),
    /* harmony export */
    SuppressTokensAtBeginLogitsProcessor: () => (
      /* reexport safe */
      O.SuppressTokensAtBeginLogitsProcessor
    ),
    /* harmony export */
    Swin2SRForImageSuperResolution: () => (
      /* reexport safe */
      s.Swin2SRForImageSuperResolution
    ),
    /* harmony export */
    Swin2SRImageProcessor: () => (
      /* reexport safe */
      D.Swin2SRImageProcessor
    ),
    /* harmony export */
    Swin2SRModel: () => (
      /* reexport safe */
      s.Swin2SRModel
    ),
    /* harmony export */
    Swin2SRPreTrainedModel: () => (
      /* reexport safe */
      s.Swin2SRPreTrainedModel
    ),
    /* harmony export */
    SwinForImageClassification: () => (
      /* reexport safe */
      s.SwinForImageClassification
    ),
    /* harmony export */
    SwinModel: () => (
      /* reexport safe */
      s.SwinModel
    ),
    /* harmony export */
    SwinPreTrainedModel: () => (
      /* reexport safe */
      s.SwinPreTrainedModel
    ),
    /* harmony export */
    T5ForConditionalGeneration: () => (
      /* reexport safe */
      s.T5ForConditionalGeneration
    ),
    /* harmony export */
    T5Model: () => (
      /* reexport safe */
      s.T5Model
    ),
    /* harmony export */
    T5PreTrainedModel: () => (
      /* reexport safe */
      s.T5PreTrainedModel
    ),
    /* harmony export */
    T5Tokenizer: () => (
      /* reexport safe */
      f.T5Tokenizer
    ),
    /* harmony export */
    TableTransformerForObjectDetection: () => (
      /* reexport safe */
      s.TableTransformerForObjectDetection
    ),
    /* harmony export */
    TableTransformerModel: () => (
      /* reexport safe */
      s.TableTransformerModel
    ),
    /* harmony export */
    TableTransformerObjectDetectionOutput: () => (
      /* reexport safe */
      s.TableTransformerObjectDetectionOutput
    ),
    /* harmony export */
    TableTransformerPreTrainedModel: () => (
      /* reexport safe */
      s.TableTransformerPreTrainedModel
    ),
    /* harmony export */
    TemperatureLogitsWarper: () => (
      /* reexport safe */
      O.TemperatureLogitsWarper
    ),
    /* harmony export */
    Tensor: () => (
      /* reexport safe */
      W.Tensor
    ),
    /* harmony export */
    Text2TextGenerationPipeline: () => (
      /* reexport safe */
      A.Text2TextGenerationPipeline
    ),
    /* harmony export */
    TextClassificationPipeline: () => (
      /* reexport safe */
      A.TextClassificationPipeline
    ),
    /* harmony export */
    TextGenerationPipeline: () => (
      /* reexport safe */
      A.TextGenerationPipeline
    ),
    /* harmony export */
    TextStreamer: () => (
      /* reexport safe */
      V.TextStreamer
    ),
    /* harmony export */
    TextToAudioPipeline: () => (
      /* reexport safe */
      A.TextToAudioPipeline
    ),
    /* harmony export */
    TokenClassificationPipeline: () => (
      /* reexport safe */
      A.TokenClassificationPipeline
    ),
    /* harmony export */
    TokenClassifierOutput: () => (
      /* reexport safe */
      s.TokenClassifierOutput
    ),
    /* harmony export */
    TokenizerModel: () => (
      /* reexport safe */
      f.TokenizerModel
    ),
    /* harmony export */
    TopKLogitsWarper: () => (
      /* reexport safe */
      O.TopKLogitsWarper
    ),
    /* harmony export */
    TopPLogitsWarper: () => (
      /* reexport safe */
      O.TopPLogitsWarper
    ),
    /* harmony export */
    TrOCRForCausalLM: () => (
      /* reexport safe */
      s.TrOCRForCausalLM
    ),
    /* harmony export */
    TrOCRPreTrainedModel: () => (
      /* reexport safe */
      s.TrOCRPreTrainedModel
    ),
    /* harmony export */
    TranslationPipeline: () => (
      /* reexport safe */
      A.TranslationPipeline
    ),
    /* harmony export */
    UniSpeechForCTC: () => (
      /* reexport safe */
      s.UniSpeechForCTC
    ),
    /* harmony export */
    UniSpeechForSequenceClassification: () => (
      /* reexport safe */
      s.UniSpeechForSequenceClassification
    ),
    /* harmony export */
    UniSpeechModel: () => (
      /* reexport safe */
      s.UniSpeechModel
    ),
    /* harmony export */
    UniSpeechPreTrainedModel: () => (
      /* reexport safe */
      s.UniSpeechPreTrainedModel
    ),
    /* harmony export */
    UniSpeechSatForAudioFrameClassification: () => (
      /* reexport safe */
      s.UniSpeechSatForAudioFrameClassification
    ),
    /* harmony export */
    UniSpeechSatForCTC: () => (
      /* reexport safe */
      s.UniSpeechSatForCTC
    ),
    /* harmony export */
    UniSpeechSatForSequenceClassification: () => (
      /* reexport safe */
      s.UniSpeechSatForSequenceClassification
    ),
    /* harmony export */
    UniSpeechSatModel: () => (
      /* reexport safe */
      s.UniSpeechSatModel
    ),
    /* harmony export */
    UniSpeechSatPreTrainedModel: () => (
      /* reexport safe */
      s.UniSpeechSatPreTrainedModel
    ),
    /* harmony export */
    VLChatProcessor: () => (
      /* reexport safe */
      oe.VLChatProcessor
    ),
    /* harmony export */
    VLMImageProcessor: () => (
      /* reexport safe */
      D.VLMImageProcessor
    ),
    /* harmony export */
    ViTFeatureExtractor: () => (
      /* reexport safe */
      D.ViTFeatureExtractor
    ),
    /* harmony export */
    ViTForImageClassification: () => (
      /* reexport safe */
      s.ViTForImageClassification
    ),
    /* harmony export */
    ViTImageProcessor: () => (
      /* reexport safe */
      D.ViTImageProcessor
    ),
    /* harmony export */
    ViTMAEModel: () => (
      /* reexport safe */
      s.ViTMAEModel
    ),
    /* harmony export */
    ViTMAEPreTrainedModel: () => (
      /* reexport safe */
      s.ViTMAEPreTrainedModel
    ),
    /* harmony export */
    ViTMSNForImageClassification: () => (
      /* reexport safe */
      s.ViTMSNForImageClassification
    ),
    /* harmony export */
    ViTMSNModel: () => (
      /* reexport safe */
      s.ViTMSNModel
    ),
    /* harmony export */
    ViTMSNPreTrainedModel: () => (
      /* reexport safe */
      s.ViTMSNPreTrainedModel
    ),
    /* harmony export */
    ViTModel: () => (
      /* reexport safe */
      s.ViTModel
    ),
    /* harmony export */
    ViTPreTrainedModel: () => (
      /* reexport safe */
      s.ViTPreTrainedModel
    ),
    /* harmony export */
    VisionEncoderDecoderModel: () => (
      /* reexport safe */
      s.VisionEncoderDecoderModel
    ),
    /* harmony export */
    VitMatteForImageMatting: () => (
      /* reexport safe */
      s.VitMatteForImageMatting
    ),
    /* harmony export */
    VitMatteImageProcessor: () => (
      /* reexport safe */
      D.VitMatteImageProcessor
    ),
    /* harmony export */
    VitMattePreTrainedModel: () => (
      /* reexport safe */
      s.VitMattePreTrainedModel
    ),
    /* harmony export */
    VitPoseForPoseEstimation: () => (
      /* reexport safe */
      s.VitPoseForPoseEstimation
    ),
    /* harmony export */
    VitPoseImageProcessor: () => (
      /* reexport safe */
      D.VitPoseImageProcessor
    ),
    /* harmony export */
    VitPosePreTrainedModel: () => (
      /* reexport safe */
      s.VitPosePreTrainedModel
    ),
    /* harmony export */
    VitsModel: () => (
      /* reexport safe */
      s.VitsModel
    ),
    /* harmony export */
    VitsModelOutput: () => (
      /* reexport safe */
      s.VitsModelOutput
    ),
    /* harmony export */
    VitsPreTrainedModel: () => (
      /* reexport safe */
      s.VitsPreTrainedModel
    ),
    /* harmony export */
    VitsTokenizer: () => (
      /* reexport safe */
      f.VitsTokenizer
    ),
    /* harmony export */
    Wav2Vec2BertForCTC: () => (
      /* reexport safe */
      s.Wav2Vec2BertForCTC
    ),
    /* harmony export */
    Wav2Vec2BertForSequenceClassification: () => (
      /* reexport safe */
      s.Wav2Vec2BertForSequenceClassification
    ),
    /* harmony export */
    Wav2Vec2BertModel: () => (
      /* reexport safe */
      s.Wav2Vec2BertModel
    ),
    /* harmony export */
    Wav2Vec2BertPreTrainedModel: () => (
      /* reexport safe */
      s.Wav2Vec2BertPreTrainedModel
    ),
    /* harmony export */
    Wav2Vec2CTCTokenizer: () => (
      /* reexport safe */
      f.Wav2Vec2CTCTokenizer
    ),
    /* harmony export */
    Wav2Vec2FeatureExtractor: () => (
      /* reexport safe */
      y.Wav2Vec2FeatureExtractor
    ),
    /* harmony export */
    Wav2Vec2ForAudioFrameClassification: () => (
      /* reexport safe */
      s.Wav2Vec2ForAudioFrameClassification
    ),
    /* harmony export */
    Wav2Vec2ForCTC: () => (
      /* reexport safe */
      s.Wav2Vec2ForCTC
    ),
    /* harmony export */
    Wav2Vec2ForSequenceClassification: () => (
      /* reexport safe */
      s.Wav2Vec2ForSequenceClassification
    ),
    /* harmony export */
    Wav2Vec2Model: () => (
      /* reexport safe */
      s.Wav2Vec2Model
    ),
    /* harmony export */
    Wav2Vec2PreTrainedModel: () => (
      /* reexport safe */
      s.Wav2Vec2PreTrainedModel
    ),
    /* harmony export */
    Wav2Vec2ProcessorWithLM: () => (
      /* reexport safe */
      oe.Wav2Vec2ProcessorWithLM
    ),
    /* harmony export */
    WavLMForAudioFrameClassification: () => (
      /* reexport safe */
      s.WavLMForAudioFrameClassification
    ),
    /* harmony export */
    WavLMForCTC: () => (
      /* reexport safe */
      s.WavLMForCTC
    ),
    /* harmony export */
    WavLMForSequenceClassification: () => (
      /* reexport safe */
      s.WavLMForSequenceClassification
    ),
    /* harmony export */
    WavLMForXVector: () => (
      /* reexport safe */
      s.WavLMForXVector
    ),
    /* harmony export */
    WavLMModel: () => (
      /* reexport safe */
      s.WavLMModel
    ),
    /* harmony export */
    WavLMPreTrainedModel: () => (
      /* reexport safe */
      s.WavLMPreTrainedModel
    ),
    /* harmony export */
    WeSpeakerFeatureExtractor: () => (
      /* reexport safe */
      y.WeSpeakerFeatureExtractor
    ),
    /* harmony export */
    WeSpeakerResNetModel: () => (
      /* reexport safe */
      s.WeSpeakerResNetModel
    ),
    /* harmony export */
    WeSpeakerResNetPreTrainedModel: () => (
      /* reexport safe */
      s.WeSpeakerResNetPreTrainedModel
    ),
    /* harmony export */
    WhisperFeatureExtractor: () => (
      /* reexport safe */
      y.WhisperFeatureExtractor
    ),
    /* harmony export */
    WhisperForConditionalGeneration: () => (
      /* reexport safe */
      s.WhisperForConditionalGeneration
    ),
    /* harmony export */
    WhisperModel: () => (
      /* reexport safe */
      s.WhisperModel
    ),
    /* harmony export */
    WhisperPreTrainedModel: () => (
      /* reexport safe */
      s.WhisperPreTrainedModel
    ),
    /* harmony export */
    WhisperProcessor: () => (
      /* reexport safe */
      oe.WhisperProcessor
    ),
    /* harmony export */
    WhisperTextStreamer: () => (
      /* reexport safe */
      V.WhisperTextStreamer
    ),
    /* harmony export */
    WhisperTimeStampLogitsProcessor: () => (
      /* reexport safe */
      O.WhisperTimeStampLogitsProcessor
    ),
    /* harmony export */
    WhisperTokenizer: () => (
      /* reexport safe */
      f.WhisperTokenizer
    ),
    /* harmony export */
    XLMForQuestionAnswering: () => (
      /* reexport safe */
      s.XLMForQuestionAnswering
    ),
    /* harmony export */
    XLMForSequenceClassification: () => (
      /* reexport safe */
      s.XLMForSequenceClassification
    ),
    /* harmony export */
    XLMForTokenClassification: () => (
      /* reexport safe */
      s.XLMForTokenClassification
    ),
    /* harmony export */
    XLMModel: () => (
      /* reexport safe */
      s.XLMModel
    ),
    /* harmony export */
    XLMPreTrainedModel: () => (
      /* reexport safe */
      s.XLMPreTrainedModel
    ),
    /* harmony export */
    XLMRobertaForMaskedLM: () => (
      /* reexport safe */
      s.XLMRobertaForMaskedLM
    ),
    /* harmony export */
    XLMRobertaForQuestionAnswering: () => (
      /* reexport safe */
      s.XLMRobertaForQuestionAnswering
    ),
    /* harmony export */
    XLMRobertaForSequenceClassification: () => (
      /* reexport safe */
      s.XLMRobertaForSequenceClassification
    ),
    /* harmony export */
    XLMRobertaForTokenClassification: () => (
      /* reexport safe */
      s.XLMRobertaForTokenClassification
    ),
    /* harmony export */
    XLMRobertaModel: () => (
      /* reexport safe */
      s.XLMRobertaModel
    ),
    /* harmony export */
    XLMRobertaPreTrainedModel: () => (
      /* reexport safe */
      s.XLMRobertaPreTrainedModel
    ),
    /* harmony export */
    XLMRobertaTokenizer: () => (
      /* reexport safe */
      f.XLMRobertaTokenizer
    ),
    /* harmony export */
    XLMTokenizer: () => (
      /* reexport safe */
      f.XLMTokenizer
    ),
    /* harmony export */
    XLMWithLMHeadModel: () => (
      /* reexport safe */
      s.XLMWithLMHeadModel
    ),
    /* harmony export */
    XVectorOutput: () => (
      /* reexport safe */
      s.XVectorOutput
    ),
    /* harmony export */
    YolosFeatureExtractor: () => (
      /* reexport safe */
      D.YolosFeatureExtractor
    ),
    /* harmony export */
    YolosForObjectDetection: () => (
      /* reexport safe */
      s.YolosForObjectDetection
    ),
    /* harmony export */
    YolosImageProcessor: () => (
      /* reexport safe */
      D.YolosImageProcessor
    ),
    /* harmony export */
    YolosModel: () => (
      /* reexport safe */
      s.YolosModel
    ),
    /* harmony export */
    YolosObjectDetectionOutput: () => (
      /* reexport safe */
      s.YolosObjectDetectionOutput
    ),
    /* harmony export */
    YolosPreTrainedModel: () => (
      /* reexport safe */
      s.YolosPreTrainedModel
    ),
    /* harmony export */
    ZeroShotAudioClassificationPipeline: () => (
      /* reexport safe */
      A.ZeroShotAudioClassificationPipeline
    ),
    /* harmony export */
    ZeroShotClassificationPipeline: () => (
      /* reexport safe */
      A.ZeroShotClassificationPipeline
    ),
    /* harmony export */
    ZeroShotImageClassificationPipeline: () => (
      /* reexport safe */
      A.ZeroShotImageClassificationPipeline
    ),
    /* harmony export */
    ZeroShotObjectDetectionPipeline: () => (
      /* reexport safe */
      A.ZeroShotObjectDetectionPipeline
    ),
    /* harmony export */
    bankers_round: () => (
      /* reexport safe */
      w.bankers_round
    ),
    /* harmony export */
    cat: () => (
      /* reexport safe */
      W.cat
    ),
    /* harmony export */
    cos_sim: () => (
      /* reexport safe */
      w.cos_sim
    ),
    /* harmony export */
    dot: () => (
      /* reexport safe */
      w.dot
    ),
    /* harmony export */
    dynamic_time_warping: () => (
      /* reexport safe */
      w.dynamic_time_warping
    ),
    /* harmony export */
    env: () => (
      /* reexport safe */
      ke.env
    ),
    /* harmony export */
    full: () => (
      /* reexport safe */
      W.full
    ),
    /* harmony export */
    full_like: () => (
      /* reexport safe */
      W.full_like
    ),
    /* harmony export */
    getKeyValueShapes: () => (
      /* reexport safe */
      L.getKeyValueShapes
    ),
    /* harmony export */
    hamming: () => (
      /* reexport safe */
      j.hamming
    ),
    /* harmony export */
    hanning: () => (
      /* reexport safe */
      j.hanning
    ),
    /* harmony export */
    interpolate: () => (
      /* reexport safe */
      W.interpolate
    ),
    /* harmony export */
    interpolate_4d: () => (
      /* reexport safe */
      W.interpolate_4d
    ),
    /* harmony export */
    interpolate_data: () => (
      /* reexport safe */
      w.interpolate_data
    ),
    /* harmony export */
    is_chinese_char: () => (
      /* reexport safe */
      f.is_chinese_char
    ),
    /* harmony export */
    layer_norm: () => (
      /* reexport safe */
      W.layer_norm
    ),
    /* harmony export */
    load_image: () => (
      /* reexport safe */
      J.load_image
    ),
    /* harmony export */
    log_softmax: () => (
      /* reexport safe */
      w.log_softmax
    ),
    /* harmony export */
    magnitude: () => (
      /* reexport safe */
      w.magnitude
    ),
    /* harmony export */
    matmul: () => (
      /* reexport safe */
      W.matmul
    ),
    /* harmony export */
    max: () => (
      /* reexport safe */
      w.max
    ),
    /* harmony export */
    mean: () => (
      /* reexport safe */
      W.mean
    ),
    /* harmony export */
    mean_pooling: () => (
      /* reexport safe */
      W.mean_pooling
    ),
    /* harmony export */
    medianFilter: () => (
      /* reexport safe */
      w.medianFilter
    ),
    /* harmony export */
    mel_filter_bank: () => (
      /* reexport safe */
      j.mel_filter_bank
    ),
    /* harmony export */
    min: () => (
      /* reexport safe */
      w.min
    ),
    /* harmony export */
    ones: () => (
      /* reexport safe */
      W.ones
    ),
    /* harmony export */
    ones_like: () => (
      /* reexport safe */
      W.ones_like
    ),
    /* harmony export */
    permute: () => (
      /* reexport safe */
      W.permute
    ),
    /* harmony export */
    permute_data: () => (
      /* reexport safe */
      w.permute_data
    ),
    /* harmony export */
    pipeline: () => (
      /* reexport safe */
      A.pipeline
    ),
    /* harmony export */
    quantize_embeddings: () => (
      /* reexport safe */
      W.quantize_embeddings
    ),
    /* harmony export */
    rand: () => (
      /* reexport safe */
      W.rand
    ),
    /* harmony export */
    read_audio: () => (
      /* reexport safe */
      j.read_audio
    ),
    /* harmony export */
    rfft: () => (
      /* reexport safe */
      W.rfft
    ),
    /* harmony export */
    round: () => (
      /* reexport safe */
      w.round
    ),
    /* harmony export */
    slice: () => (
      /* reexport safe */
      W.slice
    ),
    /* harmony export */
    softmax: () => (
      /* reexport safe */
      w.softmax
    ),
    /* harmony export */
    spectrogram: () => (
      /* reexport safe */
      j.spectrogram
    ),
    /* harmony export */
    stack: () => (
      /* reexport safe */
      W.stack
    ),
    /* harmony export */
    std_mean: () => (
      /* reexport safe */
      W.std_mean
    ),
    /* harmony export */
    topk: () => (
      /* reexport safe */
      W.topk
    ),
    /* harmony export */
    window_function: () => (
      /* reexport safe */
      j.window_function
    ),
    /* harmony export */
    zeros: () => (
      /* reexport safe */
      W.zeros
    ),
    /* harmony export */
    zeros_like: () => (
      /* reexport safe */
      W.zeros_like
    )
    /* harmony export */
  });
  var ke = fr(
    /*! ./env.js */
    "./src/env.js"
  ), A = fr(
    /*! ./pipelines.js */
    "./src/pipelines.js"
  ), s = fr(
    /*! ./models.js */
    "./src/models.js"
  ), f = fr(
    /*! ./tokenizers.js */
    "./src/tokenizers.js"
  ), L = fr(
    /*! ./configs.js */
    "./src/configs.js"
  ), j = fr(
    /*! ./utils/audio.js */
    "./src/utils/audio.js"
  ), J = fr(
    /*! ./utils/image.js */
    "./src/utils/image.js"
  ), W = fr(
    /*! ./utils/tensor.js */
    "./src/utils/tensor.js"
  ), w = fr(
    /*! ./utils/maths.js */
    "./src/utils/maths.js"
  ), x = fr(
    /*! ./base/feature_extraction_utils.js */
    "./src/base/feature_extraction_utils.js"
  ), y = fr(
    /*! ./models/feature_extractors.js */
    "./src/models/feature_extractors.js"
  ), M = fr(
    /*! ./models/auto/feature_extraction_auto.js */
    "./src/models/auto/feature_extraction_auto.js"
  ), b = fr(
    /*! ./base/image_processors_utils.js */
    "./src/base/image_processors_utils.js"
  ), D = fr(
    /*! ./models/image_processors.js */
    "./src/models/image_processors.js"
  ), q = fr(
    /*! ./models/auto/image_processing_auto.js */
    "./src/models/auto/image_processing_auto.js"
  ), se = fr(
    /*! ./base/processing_utils.js */
    "./src/base/processing_utils.js"
  ), oe = fr(
    /*! ./models/processors.js */
    "./src/models/processors.js"
  ), z = fr(
    /*! ./models/auto/processing_auto.js */
    "./src/models/auto/processing_auto.js"
  ), V = fr(
    /*! ./generation/streamers.js */
    "./src/generation/streamers.js"
  ), Y = fr(
    /*! ./generation/stopping_criteria.js */
    "./src/generation/stopping_criteria.js"
  ), O = fr(
    /*! ./generation/logits_process.js */
    "./src/generation/logits_process.js"
  );
})();
c.ASTFeatureExtractor;
c.ASTForAudioClassification;
c.ASTModel;
c.ASTPreTrainedModel;
c.AlbertForMaskedLM;
c.AlbertForQuestionAnswering;
c.AlbertForSequenceClassification;
c.AlbertModel;
c.AlbertPreTrainedModel;
c.AlbertTokenizer;
c.AudioClassificationPipeline;
c.AutoConfig;
c.AutoFeatureExtractor;
c.AutoImageProcessor;
c.AutoModel;
c.AutoModelForAudioClassification;
c.AutoModelForAudioFrameClassification;
c.AutoModelForCTC;
c.AutoModelForCausalLM;
c.AutoModelForDepthEstimation;
c.AutoModelForDocumentQuestionAnswering;
c.AutoModelForImageClassification;
c.AutoModelForImageFeatureExtraction;
c.AutoModelForImageMatting;
c.AutoModelForImageSegmentation;
c.AutoModelForImageToImage;
c.AutoModelForMaskGeneration;
c.AutoModelForMaskedLM;
c.AutoModelForNormalEstimation;
c.AutoModelForObjectDetection;
c.AutoModelForPoseEstimation;
c.AutoModelForQuestionAnswering;
c.AutoModelForSemanticSegmentation;
c.AutoModelForSeq2SeqLM;
c.AutoModelForSequenceClassification;
c.AutoModelForSpeechSeq2Seq;
c.AutoModelForTextToSpectrogram;
c.AutoModelForTextToWaveform;
c.AutoModelForTokenClassification;
c.AutoModelForUniversalSegmentation;
c.AutoModelForVision2Seq;
c.AutoModelForXVector;
c.AutoModelForZeroShotObjectDetection;
c.AutoProcessor;
c.AutoTokenizer;
c.AutomaticSpeechRecognitionPipeline;
c.BartForConditionalGeneration;
c.BartForSequenceClassification;
c.BartModel;
c.BartPretrainedModel;
c.BartTokenizer;
c.BaseModelOutput;
c.BaseStreamer;
c.BeitFeatureExtractor;
c.BeitForImageClassification;
c.BeitModel;
c.BeitPreTrainedModel;
c.BertForMaskedLM;
c.BertForQuestionAnswering;
c.BertForSequenceClassification;
c.BertForTokenClassification;
c.BertModel;
c.BertPreTrainedModel;
c.BertTokenizer;
c.BitImageProcessor;
c.BlenderbotForConditionalGeneration;
c.BlenderbotModel;
c.BlenderbotPreTrainedModel;
c.BlenderbotSmallForConditionalGeneration;
c.BlenderbotSmallModel;
c.BlenderbotSmallPreTrainedModel;
c.BlenderbotSmallTokenizer;
c.BlenderbotTokenizer;
c.BloomForCausalLM;
c.BloomModel;
c.BloomPreTrainedModel;
c.BloomTokenizer;
c.CLIPFeatureExtractor;
c.CLIPImageProcessor;
c.CLIPModel;
c.CLIPPreTrainedModel;
c.CLIPSegForImageSegmentation;
c.CLIPSegModel;
c.CLIPSegPreTrainedModel;
c.CLIPTextModel;
c.CLIPTextModelWithProjection;
c.CLIPTokenizer;
c.CLIPVisionModel;
c.CLIPVisionModelWithProjection;
c.CamembertForMaskedLM;
c.CamembertForQuestionAnswering;
c.CamembertForSequenceClassification;
c.CamembertForTokenClassification;
c.CamembertModel;
c.CamembertPreTrainedModel;
c.CamembertTokenizer;
c.CausalLMOutput;
c.CausalLMOutputWithPast;
c.ChineseCLIPFeatureExtractor;
c.ChineseCLIPModel;
c.ChineseCLIPPreTrainedModel;
c.ClapAudioModelWithProjection;
c.ClapFeatureExtractor;
c.ClapModel;
c.ClapPreTrainedModel;
c.ClapTextModelWithProjection;
c.ClassifierFreeGuidanceLogitsProcessor;
c.CodeGenForCausalLM;
c.CodeGenModel;
c.CodeGenPreTrainedModel;
c.CodeGenTokenizer;
c.CodeLlamaTokenizer;
c.CohereForCausalLM;
c.CohereModel;
c.CoherePreTrainedModel;
c.CohereTokenizer;
c.ConvBertForMaskedLM;
c.ConvBertForQuestionAnswering;
c.ConvBertForSequenceClassification;
c.ConvBertForTokenClassification;
c.ConvBertModel;
c.ConvBertPreTrainedModel;
c.ConvBertTokenizer;
c.ConvNextFeatureExtractor;
c.ConvNextForImageClassification;
c.ConvNextImageProcessor;
c.ConvNextModel;
c.ConvNextPreTrainedModel;
c.ConvNextV2ForImageClassification;
c.ConvNextV2Model;
c.ConvNextV2PreTrainedModel;
c.DPTFeatureExtractor;
c.DPTForDepthEstimation;
c.DPTImageProcessor;
c.DPTModel;
c.DPTPreTrainedModel;
c.DebertaForMaskedLM;
c.DebertaForQuestionAnswering;
c.DebertaForSequenceClassification;
c.DebertaForTokenClassification;
c.DebertaModel;
c.DebertaPreTrainedModel;
c.DebertaTokenizer;
c.DebertaV2ForMaskedLM;
c.DebertaV2ForQuestionAnswering;
c.DebertaV2ForSequenceClassification;
c.DebertaV2ForTokenClassification;
c.DebertaV2Model;
c.DebertaV2PreTrainedModel;
c.DebertaV2Tokenizer;
c.DecisionTransformerModel;
c.DecisionTransformerPreTrainedModel;
c.DeiTFeatureExtractor;
c.DeiTForImageClassification;
c.DeiTImageProcessor;
c.DeiTModel;
c.DeiTPreTrainedModel;
c.DepthAnythingForDepthEstimation;
c.DepthAnythingPreTrainedModel;
c.DepthEstimationPipeline;
c.DepthProForDepthEstimation;
c.DepthProPreTrainedModel;
c.DetrFeatureExtractor;
c.DetrForObjectDetection;
c.DetrForSegmentation;
c.DetrImageProcessor;
c.DetrModel;
c.DetrObjectDetectionOutput;
c.DetrPreTrainedModel;
c.DetrSegmentationOutput;
c.Dinov2ForImageClassification;
c.Dinov2Model;
c.Dinov2PreTrainedModel;
c.DistilBertForMaskedLM;
c.DistilBertForQuestionAnswering;
c.DistilBertForSequenceClassification;
c.DistilBertForTokenClassification;
c.DistilBertModel;
c.DistilBertPreTrainedModel;
c.DistilBertTokenizer;
c.DocumentQuestionAnsweringPipeline;
c.DonutFeatureExtractor;
c.DonutImageProcessor;
c.DonutSwinModel;
c.DonutSwinPreTrainedModel;
c.EfficientNetForImageClassification;
c.EfficientNetImageProcessor;
c.EfficientNetModel;
c.EfficientNetPreTrainedModel;
c.ElectraForMaskedLM;
c.ElectraForQuestionAnswering;
c.ElectraForSequenceClassification;
c.ElectraForTokenClassification;
c.ElectraModel;
c.ElectraPreTrainedModel;
c.ElectraTokenizer;
c.EosTokenCriteria;
c.EsmForMaskedLM;
c.EsmForSequenceClassification;
c.EsmForTokenClassification;
c.EsmModel;
c.EsmPreTrainedModel;
c.EsmTokenizer;
c.ExaoneForCausalLM;
c.ExaoneModel;
c.ExaonePreTrainedModel;
c.FFT;
c.FalconForCausalLM;
c.FalconModel;
c.FalconPreTrainedModel;
c.FalconTokenizer;
c.FastViTForImageClassification;
c.FastViTModel;
c.FastViTPreTrainedModel;
c.FeatureExtractionPipeline;
c.FeatureExtractor;
c.FillMaskPipeline;
c.Florence2ForConditionalGeneration;
c.Florence2PreTrainedModel;
c.Florence2Processor;
c.ForcedBOSTokenLogitsProcessor;
c.ForcedEOSTokenLogitsProcessor;
c.GLPNFeatureExtractor;
c.GLPNForDepthEstimation;
c.GLPNModel;
c.GLPNPreTrainedModel;
c.GPT2LMHeadModel;
c.GPT2Model;
c.GPT2PreTrainedModel;
c.GPT2Tokenizer;
c.GPTBigCodeForCausalLM;
c.GPTBigCodeModel;
c.GPTBigCodePreTrainedModel;
c.GPTJForCausalLM;
c.GPTJModel;
c.GPTJPreTrainedModel;
c.GPTNeoForCausalLM;
c.GPTNeoModel;
c.GPTNeoPreTrainedModel;
c.GPTNeoXForCausalLM;
c.GPTNeoXModel;
c.GPTNeoXPreTrainedModel;
c.GPTNeoXTokenizer;
c.Gemma2ForCausalLM;
c.Gemma2Model;
c.Gemma2PreTrainedModel;
c.GemmaForCausalLM;
c.GemmaModel;
c.GemmaPreTrainedModel;
c.GemmaTokenizer;
c.GraniteForCausalLM;
c.GraniteModel;
c.GranitePreTrainedModel;
c.Grok1Tokenizer;
c.GroupViTModel;
c.GroupViTPreTrainedModel;
c.HerbertTokenizer;
c.HieraForImageClassification;
c.HieraModel;
c.HieraPreTrainedModel;
c.HubertForCTC;
c.HubertForSequenceClassification;
c.HubertModel;
c.HubertPreTrainedModel;
c.IJepaForImageClassification;
c.IJepaModel;
c.IJepaPreTrainedModel;
c.Idefics3ForConditionalGeneration;
c.Idefics3ImageProcessor;
c.Idefics3PreTrainedModel;
c.Idefics3Processor;
c.ImageClassificationPipeline;
c.ImageFeatureExtractionPipeline;
c.ImageFeatureExtractor;
c.ImageMattingOutput;
c.ImageProcessor;
c.ImageSegmentationPipeline;
c.ImageToImagePipeline;
c.ImageToTextPipeline;
c.InterruptableStoppingCriteria;
c.JAISLMHeadModel;
c.JAISModel;
c.JAISPreTrainedModel;
c.JinaCLIPImageProcessor;
c.JinaCLIPModel;
c.JinaCLIPPreTrainedModel;
c.JinaCLIPProcessor;
c.JinaCLIPTextModel;
c.JinaCLIPVisionModel;
c.LlamaForCausalLM;
c.LlamaModel;
c.LlamaPreTrainedModel;
c.LlamaTokenizer;
c.LlavaForConditionalGeneration;
c.LlavaOnevisionForConditionalGeneration;
c.LlavaOnevisionImageProcessor;
c.LlavaPreTrainedModel;
c.LogitsProcessor;
c.LogitsProcessorList;
c.LogitsWarper;
c.LongT5ForConditionalGeneration;
c.LongT5Model;
c.LongT5PreTrainedModel;
c.M2M100ForConditionalGeneration;
c.M2M100Model;
c.M2M100PreTrainedModel;
c.M2M100Tokenizer;
c.MBart50Tokenizer;
c.MBartForCausalLM;
c.MBartForConditionalGeneration;
c.MBartForSequenceClassification;
c.MBartModel;
c.MBartPreTrainedModel;
c.MBartTokenizer;
c.MPNetForMaskedLM;
c.MPNetForQuestionAnswering;
c.MPNetForSequenceClassification;
c.MPNetForTokenClassification;
c.MPNetModel;
c.MPNetPreTrainedModel;
c.MPNetTokenizer;
c.MT5ForConditionalGeneration;
c.MT5Model;
c.MT5PreTrainedModel;
c.MarianMTModel;
c.MarianModel;
c.MarianPreTrainedModel;
c.MarianTokenizer;
c.Mask2FormerImageProcessor;
c.MaskFormerFeatureExtractor;
c.MaskFormerForInstanceSegmentation;
c.MaskFormerImageProcessor;
c.MaskFormerModel;
c.MaskFormerPreTrainedModel;
c.MaskedLMOutput;
c.MaxLengthCriteria;
c.MgpstrForSceneTextRecognition;
c.MgpstrModelOutput;
c.MgpstrPreTrainedModel;
c.MgpstrProcessor;
c.MgpstrTokenizer;
c.MinLengthLogitsProcessor;
c.MinNewTokensLengthLogitsProcessor;
c.MistralForCausalLM;
c.MistralModel;
c.MistralPreTrainedModel;
c.MobileBertForMaskedLM;
c.MobileBertForQuestionAnswering;
c.MobileBertForSequenceClassification;
c.MobileBertModel;
c.MobileBertPreTrainedModel;
c.MobileBertTokenizer;
c.MobileLLMForCausalLM;
c.MobileLLMModel;
c.MobileLLMPreTrainedModel;
c.MobileNetV1FeatureExtractor;
c.MobileNetV1ForImageClassification;
c.MobileNetV1ImageProcessor;
c.MobileNetV1Model;
c.MobileNetV1PreTrainedModel;
c.MobileNetV2FeatureExtractor;
c.MobileNetV2ForImageClassification;
c.MobileNetV2ImageProcessor;
c.MobileNetV2Model;
c.MobileNetV2PreTrainedModel;
c.MobileNetV3FeatureExtractor;
c.MobileNetV3ForImageClassification;
c.MobileNetV3ImageProcessor;
c.MobileNetV3Model;
c.MobileNetV3PreTrainedModel;
c.MobileNetV4FeatureExtractor;
c.MobileNetV4ForImageClassification;
c.MobileNetV4ImageProcessor;
c.MobileNetV4Model;
c.MobileNetV4PreTrainedModel;
c.MobileViTFeatureExtractor;
c.MobileViTForImageClassification;
c.MobileViTImageProcessor;
c.MobileViTModel;
c.MobileViTPreTrainedModel;
c.MobileViTV2ForImageClassification;
c.MobileViTV2Model;
c.MobileViTV2PreTrainedModel;
c.ModelOutput;
c.ModernBertForMaskedLM;
c.ModernBertForSequenceClassification;
c.ModernBertForTokenClassification;
c.ModernBertModel;
c.ModernBertPreTrainedModel;
c.Moondream1ForConditionalGeneration;
c.MoonshineFeatureExtractor;
c.MoonshineForConditionalGeneration;
c.MoonshineModel;
c.MoonshinePreTrainedModel;
c.MoonshineProcessor;
c.MptForCausalLM;
c.MptModel;
c.MptPreTrainedModel;
c.MultiModalityCausalLM;
c.MultiModalityPreTrainedModel;
c.MusicgenForCausalLM;
c.MusicgenForConditionalGeneration;
c.MusicgenModel;
c.MusicgenPreTrainedModel;
c.NllbTokenizer;
c.NoBadWordsLogitsProcessor;
c.NoRepeatNGramLogitsProcessor;
c.NomicBertModel;
c.NomicBertPreTrainedModel;
c.NougatImageProcessor;
c.NougatTokenizer;
c.OPTForCausalLM;
c.OPTModel;
c.OPTPreTrainedModel;
c.ObjectDetectionPipeline;
c.Olmo2ForCausalLM;
c.Olmo2Model;
c.Olmo2PreTrainedModel;
c.OlmoForCausalLM;
c.OlmoModel;
c.OlmoPreTrainedModel;
c.OpenELMForCausalLM;
c.OpenELMModel;
c.OpenELMPreTrainedModel;
c.OwlViTFeatureExtractor;
c.OwlViTForObjectDetection;
c.OwlViTImageProcessor;
c.OwlViTModel;
c.OwlViTPreTrainedModel;
c.OwlViTProcessor;
c.Owlv2ForObjectDetection;
c.Owlv2ImageProcessor;
c.Owlv2Model;
c.Owlv2PreTrainedModel;
c.PaliGemmaForConditionalGeneration;
c.PaliGemmaPreTrainedModel;
c.PaliGemmaProcessor;
c.PatchTSMixerForPrediction;
c.PatchTSMixerModel;
c.PatchTSMixerPreTrainedModel;
c.PatchTSTForPrediction;
c.PatchTSTModel;
c.PatchTSTPreTrainedModel;
c.Phi3ForCausalLM;
c.Phi3Model;
c.Phi3PreTrainedModel;
c.Phi3VForCausalLM;
c.Phi3VImageProcessor;
c.Phi3VPreTrainedModel;
c.Phi3VProcessor;
c.PhiForCausalLM;
c.PhiModel;
c.PhiPreTrainedModel;
c.Pipeline;
c.PreTrainedModel;
c.PreTrainedTokenizer;
c.PretrainedConfig;
c.PretrainedMixin;
c.Processor;
c.PvtForImageClassification;
c.PvtImageProcessor;
c.PvtModel;
c.PvtPreTrainedModel;
c.PyAnnoteFeatureExtractor;
c.PyAnnoteForAudioFrameClassification;
c.PyAnnoteModel;
c.PyAnnotePreTrainedModel;
c.PyAnnoteProcessor;
c.QuestionAnsweringModelOutput;
c.QuestionAnsweringPipeline;
c.Qwen2ForCausalLM;
c.Qwen2Model;
c.Qwen2PreTrainedModel;
c.Qwen2Tokenizer;
c.Qwen2VLForConditionalGeneration;
c.Qwen2VLImageProcessor;
c.Qwen2VLPreTrainedModel;
c.Qwen2VLProcessor;
c.RTDetrForObjectDetection;
c.RTDetrImageProcessor;
c.RTDetrModel;
c.RTDetrObjectDetectionOutput;
c.RTDetrPreTrainedModel;
c.RawImage;
c.RepetitionPenaltyLogitsProcessor;
c.ResNetForImageClassification;
c.ResNetModel;
c.ResNetPreTrainedModel;
c.RoFormerForMaskedLM;
c.RoFormerForQuestionAnswering;
c.RoFormerForSequenceClassification;
c.RoFormerForTokenClassification;
c.RoFormerModel;
c.RoFormerPreTrainedModel;
c.RoFormerTokenizer;
c.RobertaForMaskedLM;
c.RobertaForQuestionAnswering;
c.RobertaForSequenceClassification;
c.RobertaForTokenClassification;
c.RobertaModel;
c.RobertaPreTrainedModel;
c.RobertaTokenizer;
c.SamImageProcessor;
c.SamImageSegmentationOutput;
c.SamModel;
c.SamPreTrainedModel;
c.SamProcessor;
c.SapiensForDepthEstimation;
c.SapiensForNormalEstimation;
c.SapiensForSemanticSegmentation;
c.SapiensPreTrainedModel;
c.SeamlessM4TFeatureExtractor;
c.SegformerFeatureExtractor;
c.SegformerForImageClassification;
c.SegformerForSemanticSegmentation;
c.SegformerImageProcessor;
c.SegformerModel;
c.SegformerPreTrainedModel;
c.Seq2SeqLMOutput;
c.SequenceClassifierOutput;
c.SiglipImageProcessor;
c.SiglipModel;
c.SiglipPreTrainedModel;
c.SiglipTextModel;
c.SiglipTokenizer;
c.SiglipVisionModel;
c.SpeechT5FeatureExtractor;
c.SpeechT5ForSpeechToText;
c.SpeechT5ForTextToSpeech;
c.SpeechT5HifiGan;
c.SpeechT5Model;
c.SpeechT5PreTrainedModel;
c.SpeechT5Processor;
c.SpeechT5Tokenizer;
c.SqueezeBertForMaskedLM;
c.SqueezeBertForQuestionAnswering;
c.SqueezeBertForSequenceClassification;
c.SqueezeBertModel;
c.SqueezeBertPreTrainedModel;
c.SqueezeBertTokenizer;
c.StableLmForCausalLM;
c.StableLmModel;
c.StableLmPreTrainedModel;
c.Starcoder2ForCausalLM;
c.Starcoder2Model;
c.Starcoder2PreTrainedModel;
c.StoppingCriteria;
c.StoppingCriteriaList;
c.SummarizationPipeline;
c.SuppressTokensAtBeginLogitsProcessor;
c.Swin2SRForImageSuperResolution;
c.Swin2SRImageProcessor;
c.Swin2SRModel;
c.Swin2SRPreTrainedModel;
c.SwinForImageClassification;
c.SwinModel;
c.SwinPreTrainedModel;
c.T5ForConditionalGeneration;
c.T5Model;
c.T5PreTrainedModel;
c.T5Tokenizer;
c.TableTransformerForObjectDetection;
c.TableTransformerModel;
c.TableTransformerObjectDetectionOutput;
c.TableTransformerPreTrainedModel;
c.TemperatureLogitsWarper;
c.Tensor;
c.Text2TextGenerationPipeline;
c.TextClassificationPipeline;
c.TextGenerationPipeline;
c.TextStreamer;
c.TextToAudioPipeline;
c.TokenClassificationPipeline;
c.TokenClassifierOutput;
c.TokenizerModel;
c.TopKLogitsWarper;
c.TopPLogitsWarper;
c.TrOCRForCausalLM;
c.TrOCRPreTrainedModel;
c.TranslationPipeline;
c.UniSpeechForCTC;
c.UniSpeechForSequenceClassification;
c.UniSpeechModel;
c.UniSpeechPreTrainedModel;
c.UniSpeechSatForAudioFrameClassification;
c.UniSpeechSatForCTC;
c.UniSpeechSatForSequenceClassification;
c.UniSpeechSatModel;
c.UniSpeechSatPreTrainedModel;
c.VLChatProcessor;
c.VLMImageProcessor;
c.ViTFeatureExtractor;
c.ViTForImageClassification;
c.ViTImageProcessor;
c.ViTMAEModel;
c.ViTMAEPreTrainedModel;
c.ViTMSNForImageClassification;
c.ViTMSNModel;
c.ViTMSNPreTrainedModel;
c.ViTModel;
c.ViTPreTrainedModel;
c.VisionEncoderDecoderModel;
c.VitMatteForImageMatting;
c.VitMatteImageProcessor;
c.VitMattePreTrainedModel;
c.VitPoseForPoseEstimation;
c.VitPoseImageProcessor;
c.VitPosePreTrainedModel;
c.VitsModel;
c.VitsModelOutput;
c.VitsPreTrainedModel;
c.VitsTokenizer;
c.Wav2Vec2BertForCTC;
c.Wav2Vec2BertForSequenceClassification;
c.Wav2Vec2BertModel;
c.Wav2Vec2BertPreTrainedModel;
c.Wav2Vec2CTCTokenizer;
c.Wav2Vec2FeatureExtractor;
c.Wav2Vec2ForAudioFrameClassification;
c.Wav2Vec2ForCTC;
c.Wav2Vec2ForSequenceClassification;
c.Wav2Vec2Model;
c.Wav2Vec2PreTrainedModel;
c.Wav2Vec2ProcessorWithLM;
c.WavLMForAudioFrameClassification;
c.WavLMForCTC;
c.WavLMForSequenceClassification;
c.WavLMForXVector;
c.WavLMModel;
c.WavLMPreTrainedModel;
c.WeSpeakerFeatureExtractor;
c.WeSpeakerResNetModel;
c.WeSpeakerResNetPreTrainedModel;
c.WhisperFeatureExtractor;
c.WhisperForConditionalGeneration;
c.WhisperModel;
c.WhisperPreTrainedModel;
c.WhisperProcessor;
c.WhisperTextStreamer;
c.WhisperTimeStampLogitsProcessor;
c.WhisperTokenizer;
c.XLMForQuestionAnswering;
c.XLMForSequenceClassification;
c.XLMForTokenClassification;
c.XLMModel;
c.XLMPreTrainedModel;
c.XLMRobertaForMaskedLM;
c.XLMRobertaForQuestionAnswering;
c.XLMRobertaForSequenceClassification;
c.XLMRobertaForTokenClassification;
c.XLMRobertaModel;
c.XLMRobertaPreTrainedModel;
c.XLMRobertaTokenizer;
c.XLMTokenizer;
c.XLMWithLMHeadModel;
c.XVectorOutput;
c.YolosFeatureExtractor;
c.YolosForObjectDetection;
c.YolosImageProcessor;
c.YolosModel;
c.YolosObjectDetectionOutput;
c.YolosPreTrainedModel;
c.ZeroShotAudioClassificationPipeline;
c.ZeroShotClassificationPipeline;
c.ZeroShotImageClassificationPipeline;
c.ZeroShotObjectDetectionPipeline;
c.bankers_round;
c.cat;
c.cos_sim;
c.dot;
c.dynamic_time_warping;
c.env;
c.full;
c.full_like;
c.getKeyValueShapes;
c.hamming;
c.hanning;
c.interpolate;
c.interpolate_4d;
c.interpolate_data;
c.is_chinese_char;
c.layer_norm;
c.load_image;
c.log_softmax;
c.magnitude;
c.matmul;
c.max;
c.mean;
c.mean_pooling;
c.medianFilter;
c.mel_filter_bank;
c.min;
c.ones;
c.ones_like;
c.permute;
c.permute_data;
var Sf = c.pipeline;
c.quantize_embeddings;
c.rand;
c.read_audio;
c.rfft;
c.round;
c.slice;
c.softmax;
c.spectrogram;
c.stack;
c.std_mean;
c.topk;
c.window_function;
c.zeros;
c.zeros_like;
var kc, Sc, Li;
class $f {
  constructor({ task: A = "text-classification", model: s = "Cohee/distilbert-base-uncased-go-emotions-onnx" } = {}) {
    Ip(this, kc, null);
    Ip(this, Sc, null);
    Ip(this, Li, null);
    Pc(this, kc, A), Pc(this, Sc, s);
  }
  destroy() {
    Pc(this, Li, null);
  }
  async loadPipeline() {
    return Ec(this, Li) || Pc(this, Li, await Sf(Ec(this, kc), Ec(this, Sc))), Promise.resolve(Ec(this, Li));
  }
  async generate(A) {
    return this.loadPipeline().then(async (s) => s(A)).then((s) => s[0]);
  }
}
kc = new WeakMap(), Sc = new WeakMap(), Li = new WeakMap();
let Fp = null;
self.onmessage = ({ data: { type: ke, data: A } }) => {
  switch (ke) {
    case "constructor":
      Fp = new $f(A || {});
      break;
    case "destroy":
      Fp.destroy();
      break;
    case "loadPipeline":
      Fp.loadPipeline(...A || []).then(() => self.postMessage(null));
      break;
    case "generate":
      Fp.generate(...A || []).then(self.postMessage);
      break;
    default:
      throw new Error("Unknown type " + ke);
  }
};
